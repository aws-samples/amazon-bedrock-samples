{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced chunking strategies provided by Knowledge Bases for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create 3 knowledge bases to provide sample code for the following chunking options supported by Knowledge Bases for Amazon Bedrock: \n",
    "1. Fixed chunking\n",
    "2. Semantic chunking\n",
    "3. Hierarchical chunking\n",
    "\n",
    "\n",
    "Chunking breaks down the text into smaller segments before embedding. The chunking strategy can't be modified after you create the data source.\n",
    "As of now, Knowledge bases for Amazon Bedrock only support a few built-in chunking options: no chunking, fixed sized chunking, and default chunking. \n",
    "\n",
    "* With Semantic, and Hierarchical chunking features (in addition to the existing options) customers can have more control over how their data is processed and chunked using Lambda function.\n",
    "\n",
    "\n",
    "We will use a synthetic 10K report as data for a fiticious company called `Octank Financial` to demo the solution.\n",
    "After creating knowledge bases we will evaluate the results on the same dataset. The focus will be on improving the quality of search results which in turn will improve the accuracy of responses generated by the foundation model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the needed libraries\n",
    "First step is to install the pre-requisites packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall -q -r utils/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "botocore.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "from utils.knowledge_base import BedrockKnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clients\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "region =  session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime') \n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\"\n",
    "knowledge_base_name_standard = 'standard-kb'\n",
    "knowledge_base_name_hierarchical = 'hierarchical-kb'\n",
    "knowledge_base_description = \"Knowledge Base containing complex PDF.\"\n",
    "bucket_name = f'{knowledge_base_name_standard}-{suffix}'\n",
    "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Create knowledge bases with fixed chunking strategy\n",
    "Let's start by creating a [Knowledge Base for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) to store the restaurant menus. Knowledge Bases allow you to integrate with different vector databases including [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/), [Amazon Aurora](https://aws.amazon.com/rds/aurora/), [Pinecone](http://app.pinecone.io/bedrock-integration), [Redis Enterprise]() and [MongoDB Atlas](). For this example, we will integrate the knowledge base with Amazon OpenSearch Serverless. To do so, we will use the helper class `BedrockKnowledgeBase` which will create the knowledge base and all of its pre-requisites:\n",
    "1. IAM roles and policies\n",
    "2. S3 bucket\n",
    "3. Amazon OpenSearch Serverless encryption, network and data access policies\n",
    "4. Amazon OpenSearch Serverless collection\n",
    "5. Amazon OpenSearch Serverless vector index\n",
    "6. Knowledge base\n",
    "7. Knowledge base data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a knowledge base using fixed chunking strategy followed by hierarchical chunking strategy. \n",
    "\n",
    "Parameter values: \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_standard = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_standard}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_bucket_name=bucket_name, \n",
    "    chunking_strategy = \"FIXED_SIZE\", \n",
    "    suffix = suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Upload the dataset to Amazon S3\n",
    "Now that we have created the knowledge base, let's populate it with the `Octank financial 10K` report dataset. The Knowledge Base data source expects the data to be available on the S3 bucket connected to it and changes on the data can be syncronized to the knowledge base using the `StartIngestionJob` API call. In this example we will use the [boto3 abstraction](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent/client/start_ingestion_job.html) of the API, via our helper classe. \n",
    "\n",
    "Let's first upload the menu's data available on the `dataset` folder to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(path, bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_to_upload = os.path.join(root,file)\n",
    "                print(f\"uploading file {file_to_upload} to {bucket_name}\")\n",
    "                s3_client.upload_file(file_to_upload,bucket_name,file)\n",
    "\n",
    "upload_directory(\"../synthetic_dataset\", bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the ingestion job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_standard.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the Knowledge Base Id to test the solution at a later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_standard = knowledge_base_standard.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_standard,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the retrieve and generate API we get the final response directly, now let's observe the citations for `RetreiveAndGenerate` API. Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_standard))\n",
    "def citations_rag_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret,1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n",
    "\n",
    "citations_rag_print(response_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chunks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_standard, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"# of retrieved results: \", len(response_standard_ret['retrievalResults']))\n",
    "def response_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n",
    "\n",
    "response_print(response_standard_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, that with `fixed chunking` we get 5 retrieved results as requested in the API using `semantic similarity` which is the default for `Retrieve API`. Let's now use `hierarchical chunking` strategy and inspect the retrieved results using `RetrieveAndGenerate` API as well as `Retrieve` API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create knowledge bases with hierarchical chunking strategy\n",
    "\n",
    "**Concept**\n",
    "\n",
    "Hierarchical chunking: Organizes your data into a hierarchical structure, allowing for more granular and efficient retrieval based on the inherent relationships within your data. Organizing your data into a hierarchical structure enables your RAG workflow to efficiently navigate and retrieve information from complex, nested datasets.\n",
    "After the documents are parsed, the first step is to chunk the documents based on the parent and child chunking size. The chunks are then organized into a hierarchical structure, where parent chunk (higher level) represents larger chunks (for example, documents or sections), and child chunks (lower level) represent smaller chunks (for example, paragraphs or sentences). The relationship between the parent and child chunks are maintained. This hierarchical structure allows for efficient retrieval and navigation of the corpus.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Efficient retrieval: The hierarchical structure allows faster and more targeted retrieval of relevant information; first by performing semantic search on the child chunk and then returning the parent chunk during retrieval. By replacing the children chunks with the parent chunk, we provide large and comprehensive context to the FM.\n",
    "- Context preservation: Organizing the corpus in a hierarchical manner helps preserve the contextual relationships between chunks, which can be beneficial for generating coherent and contextually relevant text.\n",
    "\n",
    "><br>          \n",
    ">Note:\n",
    ">In hierarchical chunking, parent chunks are returned and search is performed on children chunks, therefore, you might see less number of search results returned as one parent can have multiple children.\n",
    "> <br></br>       \n",
    "\n",
    "Hierarchical chunking is best suited for complex documents that have a nested or hierarchical structure, such as technical manuals, legal documents, or academic papers with complex formatting and nested tables.\n",
    "\n",
    "**Parameter values:** \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_hierarchical = BedrockKnowledgeBase(\n",
    "    kb_name=f'knowledge_base_name_hierarchical-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_bucket_name=bucket_name, \n",
    "    chunking_strategy = \"HIERARCHICAL\", \n",
    "    suffix = f'{suffix}-h'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the ingestion job. Since, we are using the same documents as used for fixed chunking, we are skipping the step to upload documents to s3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_hierarchical.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the knowledge base id for further testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_hierarchical = knowledge_base_hierarchical.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_hierarchical,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the `RetreiveAndGenerate` API we get the final response directly, now let's observe the citations for `RetreiveAndGenerate` API. Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_hierarchical = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_hierarchical))\n",
    "citations_rag_print(response_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now retrieve the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chuncks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_hierarchical_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_hierarchical, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"# of retrieved results: \", len(response_hierarchical_ret['retrievalResults']))\n",
    "response_print(response_hierarchical_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><br>\n",
    "> Note:\n",
    ">As you can see in the above response, that the `retrieve` API returned only 3 search results or chunks although 5 were passed in the request. The reason is that with `hiearchical` chunking, parent chunks are returned by the API whereas search is performed on `children chunks` and one `parent chunk` can have multiple `children chunks`. Therefore, response returned only 3 chunks while the search was performed on 5 `children chunks`.\n",
    "><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create knowledge bases with semantic chunking strategy\n",
    "\n",
    "**Concept**\n",
    "\n",
    "Semantic chunking analyzes the relationships within a text and divides it into meaningful and complete chunks, which are derived based on the semantic similarity calculated by the embedding model. This approach preserves the information’s integrity during retrieval, helping to ensure accurate and contextually appropriate results.\n",
    "Knowledge Bases for Amazon Bedrock first divides documents into chunks based on the specified token size. Embeddings are created for each chunk, and similar chunks in the embedding space are combined based on the similarity threshold and buffer size, forming new chunks. Consequently, the chunk size can vary across chunks.\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "- By focusing on the text’s meaning and context, semantic chunking significantly improves the quality of retrieval. It should be used in scenarios where maintaining the semantic integrity of the text is crucial.\n",
    "\n",
    "- Although this method is more computationally intensive than fixed-size chunking, it can be beneficial for chunking documents where contextual boundaries aren’t clear—for example, legal documents or technical manuals.[[1]](#https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/)\n",
    "\n",
    "**Parameter values:**\n",
    " \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_semantic = BedrockKnowledgeBase(\n",
    "    kb_name=f'knowledge_base_name_semantic-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_bucket_name=bucket_name, \n",
    "    chunking_strategy = \"SEMANTIC\", \n",
    "    suffix = f'{suffix}-s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the ingestion job. Since, we are using the same documents as used for fixed chunking, we are skipping the step to upload documents to s3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_semantic.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_semantic = knowledge_base_semantic.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_semantic,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the `RetreiveAndGenerate` API we get the final response directly, now let's observe the citations for `RetreiveAndGenerate` API. Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_semantic = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_semantic))\n",
    "citations_rag_print(response_semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now retrieve the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chuncks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_semantic_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_semantic, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_semantic_ret['retrievalResults']))\n",
    "response_print(response_semantic_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, while evaluating one query, we got the correct response. However, when you are building a RAG application, you need to evaluate with large number of Questions and Answers to figure out the accuracy improvements. In the next step, we will use RAG Assessment (RAGAS) open source framework to evaluate the responses on `your dataset` for the metrics related to evaluating the quality of the context or search results.\n",
    "We will focus only on 2 metrics: \n",
    "\n",
    "1. Context recall\n",
    "2. Context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating search results using RAG Assessment (RAGAS) framework on your dataset\n",
    "You can use RAGAS framework to evaluate your results for each chunking strategy. This approach can help you provide factual guidance on which chunking strategy to use for your dataset. \n",
    "\n",
    "Ideally, you should consider optimizing on other parameters as well for example in case of heirarchical chunking you should consider trying different sizes for parent chunk or child chunk. \n",
    "\n",
    "Below approach will provide you heuristics as to which strategy could be used based on the default parameters recommended by Knowledge Bases for Amazon Bedrock. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Semantic: \", kb_id_semantic)\n",
    "print(\"Standard: \", kb_id_standard)\n",
    "print(\"Hierarchical: \", kb_id_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "In this section we will utilize RAGAS for evaluating search results using following metrics:\n",
    "1. **Context Recall:** Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "2. **Context relevancy:** This metric gauges the relevancy of the retrieved context, calculated based on both the question and contexts. The values fall within the range of (0, 1), with higher values indicating better relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import KnowledgeBasesEvaluations\n",
    "\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    "    )\n",
    "\n",
    "metrics = [context_recall,\n",
    "           context_relevancy\n",
    "           ]\n",
    "\n",
    "MODEL_ID_EVAL = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "MODEL_ID_GEN = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "questions = [\n",
    "        \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\",\n",
    "]\n",
    "ground_truths = [\n",
    "    \"The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\\\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\\\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\\\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \\\n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\",\n",
    "]\n",
    "kb_evaluate_standard = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, \n",
    "                        KB_ID=kb_id_standard,\n",
    "                        )\n",
    "\n",
    "kb_evaluate_hierarchical = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, KB_ID=kb_id_hierarchical)\n",
    "\n",
    "kb_evaluate_semantic = KnowledgeBasesEvaluations(model_id_eval=MODEL_ID_EVAL, \n",
    "                        model_id_generation=MODEL_ID_GEN, \n",
    "                        metrics=metrics,\n",
    "                        questions=questions, \n",
    "                        ground_truth=ground_truths, KB_ID=kb_id_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_heirarchical = kb_evaluate_hierarchical.evaluate()\n",
    "results_standard = kb_evaluate_standard.evaluate()\n",
    "results_semantic = kb_evaluate_semantic.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 800\n",
    "print(\"Fixed Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_standard[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_standard[\"context_relevancy\"].mean())\n",
    "\n",
    "print(\"Hierarchical Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_heirarchical[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_heirarchical[\"context_relevancy\"].mean())\n",
    "\n",
    "print(\"Semantic Chunking Evaluation for synthetic 10K report\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"Average context_recall: \", results_semantic[\"context_recall\"].mean())\n",
    "print(\"Average context_relevancy: \", results_semantic[\"context_relevancy\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "# Empty and delete S3 Bucket\n",
    "\n",
    "objects = s3_client.list_objects(Bucket=bucket_name)  \n",
    "if 'Contents' in objects:\n",
    "    for obj in objects['Contents']:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=obj['Key']) \n",
    "s3_client.delete_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================Knowledge base with fixed chunking==============================\")\n",
    "knowledge_base_standard.delete_kb(delete_s3_bucket=True, delete_iam_roles_and_policies=True)\n",
    "print(\"===============================Knowledge base with hierarchical chunking==============================\")\n",
    "knowledge_base_hierarchical.delete_kb(delete_s3_bucket=False,delete_iam_roles_and_policies=True)\n",
    "print(\"===============================Knowledge base with semantic chunking==============================\")\n",
    "knowledge_base_semantic.delete_kb(delete_s3_bucket=False,delete_iam_roles_and_policies=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
