{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7e789d-5bbb-4c19-8e9a-29350d7a8af2",
   "metadata": {},
   "source": [
    "## Reranking Model with Hugging Face Transformers and Amazon SageMaker\n",
    "\n",
    "> *This notebook should work well with a **`Python 3`** kernel in SageMaker Studio Jupyterlab*\n",
    "\n",
    "The goal of using a reranking model is to improve search relevance by reordering the result set returned by a retriever using a different model.\n",
    "\n",
    "We will use the Hugging Face Inference DLCs and Amazon SageMaker Python SDK to create a real-time inference endpoint running a [BGE-Large](https://huggingface.co/BAAI/bge-reranker-large) as a reranking model. \n",
    "\n",
    "Currently, the SageMaker Hugging Face Inference Toolkit supports the pipeline feature from Transformers for zero-code deployment. This means you can run compatible Hugging Face Transformer models without providing pre- & post-processing code. \n",
    "\n",
    "Using SageMaker SDK to deploy a model from HuggingFace, you can override the following methods:\n",
    "\n",
    "* model_fn(model_dir) overrides the default method for loading a model. The return value model will be used in thepredict_fn for predictions.\n",
    "* model_dir is the the path to your unzipped model.tar.gz.\n",
    "* input_fn(input_data, content_type) overrides the default method for pre-processing. The return value data will be used in predict_fn for predictions. The inputs are:\n",
    "* input_data is the raw body of your request.\n",
    "* content_type is the content type from the request header.\n",
    "* predict_fn(processed_data, model) overrides the default method for predictions. The return value predictions will be used in output_fn.\n",
    "* model returned value from model_fn methond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9140cb-7066-4710-aa70-8648e0bd360e",
   "metadata": {},
   "source": [
    "First, let's make sure we are using the latest sagemaker library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9a14d-26e3-4f0b-a52e-3f88e81da14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2524b8-1fb4-4ea3-81d2-34715b94bc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982903b-e01a-4882-a0ac-2ee40523bd6c",
   "metadata": {},
   "source": [
    "Install git-lfs for downloading the huggingface model from HF model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fab74-2084-4db2-aebc-b7f60f9272f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update -y \n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs git -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ab9c1-ed3a-4494-b7a1-1627a4698276",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session\n",
    "Initialize a sagemaker session and define an IAM role for deploying the reranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce9db2-c8db-46b5-9a70-763fa8b2fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e314b76-6388-4ddc-a020-10c424af43d0",
   "metadata": {},
   "source": [
    "## Create custom an inference.py script\n",
    "To use the custom inference script, you need to create an inference.py script. \n",
    "In our example, we are going to overwrite the model_fn to load our reranking model correctly and the predict_fn to predict the scores for each input pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16ee4c-0ce2-490f-b7bb-c35dd53df9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc55afb-b704-4afc-9f32-d667fd70ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "  # Load model from HuggingFace Hub\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "  model.eval()\n",
    "  return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    query = data['query']\n",
    "    documents = data['documents']\n",
    "    topk = data['topk']\n",
    "    pair_list = [ [ query, x ] for x in documents ]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(pair_list, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        print(scores)\n",
    "        sorted_indexes = sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)[:topk]\n",
    "        response = [ { \"index\" : x, \"score\" : scores[x] } for x in sorted_indexes ]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957301fe-a39c-41fa-8d44-94a72c754c41",
   "metadata": {},
   "source": [
    "## Create model.tar.gz with inference script and model\n",
    "To use our inference.py we need to bundle it into a `model.tar.gz` archive with all our model-artifcats, e.g. `pytorch_model.bin`. The `inference.py` script will be placed into a code/ folder. We will use `git` and `git-lfs` to easily download our model from hf.co/models and upload it to Amazon S3 so we can use it when creating our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f3c3d-e800-46a6-bd76-7e7ff8358655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repository = \"BAAI/bge-reranker-large\" # Define the reranking HF model ID\n",
    "model_id=repository.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d26a4-f469-4dc9-a8c8-cfd3a652d9db",
   "metadata": {},
   "source": [
    "1. Download the model from hf.co/models with git clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c599eb-f70e-4c66-955e-17fcef05a90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad7a14-8bf4-42de-ac12-76ba40ba9b47",
   "metadata": {},
   "source": [
    "2. copy inference.py into the code/ directory of the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf158c2-ab75-498c-abad-e63f50be7bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf code/.ipynb_checkpoints/\n",
    "!cp -r ./code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb4d13-42b1-466e-9836-7c8a3d14ffe0",
   "metadata": {},
   "source": [
    "3. Create a `model.tar.gz` archive with all the model artifacts and the `inference.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577f14f-e6e7-4c83-9499-5f2b4da57581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd $model_id\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc606b-e6e9-4584-8bf7-8b68da454e58",
   "metadata": {},
   "source": [
    "4. Upload the model.tar.gz to Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5ba3e-839b-49ba-96b8-92faab37449c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_location=f\"s3://{sess.default_bucket()}/custom_inference/{model_id}/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e0cd6-e562-47e9-83f6-8ac4f21aa027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7de57d-0e13-465f-b1c6-b8c57cd0cbdc",
   "metadata": {},
   "source": [
    "## Create custom HuggingfaceModel\n",
    "After we have created and uploaded our `model.tar.gz` archive to Amazon S3. Can we create a custom `HuggingfaceModel` class. This class will be used to create and deploy our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17fbbc-45e9-403d-ba4c-f6b3f749ee0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=s3_location,       # path to your model and script\n",
    "\ttransformers_version='4.37.0',\n",
    "\tpytorch_version='2.1.0',\n",
    "\tpy_version='py310',\n",
    "\trole=role,\n",
    "    env = { \"SAGEMAKER_PROGRAM\" : \"inference.py\" },\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a30f52-fa58-473e-bd28-e4f1173a56aa",
   "metadata": {},
   "source": [
    "## Test \n",
    "In the following, we are going to test the deployed endpoint to ensure it will return the ranked documents using the reranker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448b932-4988-4a5d-8cab-3c6bddefcc83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"what is panda?\"\n",
    "documents = ['hi', \"panda is a restaurant\", 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']\n",
    "topk = 2\n",
    "response = predictor.predict({\n",
    "\t\"query\": query,\n",
    "    \"documents\" : documents,\n",
    "    \"topk\" : topk\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce2b6c-785f-4c48-8643-f05ed06b8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd16c7-8185-4d12-8652-e64a22a82bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79a3e6-e17d-4ade-a4e7-e463db84955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranking_model_endpoint = predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d1906-6411-4a54-a954-eba2bdce16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store reranking_model_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8553ee5-e8b5-4e00-b0e0-a8a41032eda5",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "Congratulations. You have completed the reranking model deployment step. You can now build a RAG application that integrates with a reranking model. \n",
    "Let's open the [kb-reranker.ipynb](kb-reranker.ipynb) file and follow the instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
