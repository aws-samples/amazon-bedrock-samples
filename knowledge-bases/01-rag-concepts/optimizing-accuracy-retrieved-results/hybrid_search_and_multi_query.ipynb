{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d645a8-e7a4-4345-b99c-a07c8d6d16cd",
   "metadata": {},
   "source": [
    "# Hybrid search and multi-query with RAG\n",
    "\n",
    "In this module, you'll learn about the hybrid search and multi query concepts, and how to apply them using Amazon Bedrock and Knowledge bases for Amazon Bedrock APIs.\n",
    "This module contains:\n",
    "1. [Overview](#1-Overview)\n",
    "2. [Pre-requisites](#2-Pre-requisites)\n",
    "3. [How hybrid search improves the FM generations?](#3-how-hybrid-search-improves-the-fm-generations)\n",
    "4. [How to apply multi-queries approach?](#4-how-to-apply-multi-queries-approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa2df8-376f-462a-bc3c-696010572d12",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "For RAG-based applications, the accuracy of the generated response from Foundation Models (FMs) is dependent on the context provided to the model. Context is retrieved from the vector database based on the user query. Semantic search is widely used because it is able to understand more human-like questions—a user’s query is not always directly related to the exact keywords in the content that answers it. \n",
    "\n",
    "Semantic search helps provide answers based on the meaning of the text. However, it has limitations in capturing all the relevant keywords. Its performance relies on the quality of the word embeddings used to represent meaning of the text. To overcome such limitations, you can either combine semantic search with keyword search (hybrid) will give better results.\n",
    "    \n",
    "[Slide placeholder](http)\n",
    "\n",
    "### 1.1 Hybrid search\n",
    "\n",
    "\n",
    "![hybrid search](images/hybrid-overview.png)\n",
    "\n",
    "\n",
    "Hybrid search takes advantage of the strengths of multiple search algorithms, integrating their unique capabilities to enhance the relevance of returned search results. For RAG-based applications, semantic search capabilities are commonly combined with traditional keyword-based search to improve the relevance of search results. It enables searching over both the content of documents and their underlying meaning.\n",
    "\n",
    "It works great for RAG-based applications where the retriever has to handle a wide variety of natural language queries. The keywords help cover specific entities in the query such as product name, color, and price, while semantics better understands the meaning and intent within the query. For example, if you have want to build a chatbot for an ecommerce website to handle customer queries such as the return policy or details of the product, using hybrid search will be most suitable.\n",
    "\n",
    "#### Benefits of hybrid search\n",
    "Both keyword and semantic search will return a separate set of results along with their relevancy scores, which are then combined to return the most relevant results.\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Multi-Query\n",
    "\n",
    "A different approach to improve the relevance of retrieved documents and subsequently FM generations; is based on multi-query concept. Vector databases find documents similar to your query based distance of its embedding to those docuemtns. In which, it creates problems if the wording used in the query is \"closer\" to less relevant documents. That are usually resolved by prompt engineering/query tuning.\n",
    "\n",
    "The approach is based on using an FM to generate multiple queries from different perspectives for a given query. For each query, it retrieves a set of relevant documents then uses a distinct set of document across all queries to get a larger set of potentially relevant documents. By generating multiple queries on the same question, this approach might be able to overcome some of the limitations of the distance-based retrieval and get a more comprehensive set of results.\n",
    "\n",
    "![Multi-Query](images/multi-query.png)\n",
    "<br>\n",
    "\n",
    "#### Notes:\n",
    "- You are going to use the ```Retrieve``` and ```RetrieveAndGenerate``` APIs from the Amazon Bedrock's agent runtime client, to illustrate the use cases. These APIs convert queries into embeddings, searches the knowledge base, and then augment (in case of ```RetrieveAndGenerate``` API) the foundation model prompt with the search results as context information and returns the FM-generated response to the question. The output of the ```RetrieveAndGenerate``` API includes the generated response, source attribution as well as the retrieved text chunks.\n",
    "\n",
    "- For this module, we will use the Anthropic Claude 3 Sonnet model on Amazon Bedrock as our FM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd352c78",
   "metadata": {},
   "source": [
    "## 2. Pre-requisites\n",
    "Before being able to answer the questions, the documents must be processed and stored in a knowledge base. For this notebook, we use a [sample dataset](https://aws-blogs-artifacts-public.s3.amazonaws.com/ML-16482/30_generated_video_game_records.zip) about fictional video games to illustrate how to ingest and retrieve metadata using Knowledge Bases for Amazon Bedrock. \n",
    "\n",
    "1. Upload your documents (data source) to Amazon S3 bucket. (Gaming data used in [this blog post](https://aws.amazon.com/blogs/machine-learning/knowledge-bases-for-amazon-bedrock-now-supports-metadata-filtering-to-improve-retrieval-accuracy/))\n",
    "2. Create an empty OpenSearch Serverless (OSS) index, Amazon Bedrock knowledge base and ingest documents (from step 1) into the index. ([00_prerequisites.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/a7e62b80669378de1bae414e0b646399c7934f8e/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb))\n",
    "3. Note the Knowledge Base ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83791e2-dc57-475e-b9f0-77afc5e4b161",
   "metadata": {},
   "source": [
    "### 2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415fd47-9256-47af-b332-f00e2778e652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU boto3 awscli botocore langchain langchain-community langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0821d-9a0d-49a0-823f-d58c2ff0afb1",
   "metadata": {},
   "source": [
    "### 2.2 Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27329ef4-6997-49c7-b0d5-d2b2672394d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b70d58-f2db-4e8c-b1cd-78a8ad3a6b59",
   "metadata": {},
   "source": [
    "### 2.3 Prepare clients and processing functions\n",
    "Through out the notebook, we are going to utilise RetrieveAndGenerate and Retrieve APIs to test knowledge base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae49e4b4-a689-4c15-bdff-35af66368689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pprint as pp\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "kb_id = \"$Knowledge_Base_ID\" # replace it with your Knowledge base id.\n",
    "\n",
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent client\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "# Create bedrock client\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "# Define FM to be used for generations \n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # we will be using Anthropic Claude 3 Haiku throughout the notebook\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'\n",
    "\n",
    "\n",
    "def retrieve_and_generate(query, kb_id, model_arn, max_results, search_type):\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': query\n",
    "            },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn, \n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results, # will fetch top N documents which closely match the query\n",
    "                    'overrideSearchType' : search_type # usese either SEMANTIC or HYBRID options\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def print_generation_results(response, print_context = True):\n",
    "    generated_text = response['output']['text']\n",
    "    print('Generated FM response:\\n')\n",
    "    pp.pprint(generated_text)\n",
    "    \n",
    "    if print_context is True:\n",
    "        ## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "        citations = response[\"citations\"]\n",
    "        contexts = []\n",
    "        for citation in citations:\n",
    "            retrievedReferences = citation[\"retrievedReferences\"]\n",
    "            for reference in retrievedReferences:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "    \n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pp.pprint(contexts)\n",
    "\n",
    "def retrieve(query, kb_id, model_arn, max_results, search_type):\n",
    "    response = bedrock_agent_client.retrieve(\n",
    "            retrievalQuery={\n",
    "                'text': query\n",
    "            },\n",
    "        knowledgeBaseId= kb_id,\n",
    "        retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results, # will fetch top N documents which closely match the query\n",
    "                    'overrideSearchType': search_type\n",
    "                    }\n",
    "            }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_results(response):\n",
    "\n",
    "    print('Retrieved documents:\\n')\n",
    "    \n",
    "    ## print out the source citations from the original documents\n",
    "    citations = response['retrievalResults']\n",
    "    for citation in citations:\n",
    "        pp.pprint(citation[\"content\"][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90af71-3a75-4f91-a70b-e892a1b15376",
   "metadata": {},
   "source": [
    "### 3. How hybrid search improves the documents retrieval?\n",
    "\n",
    "\n",
    "In the following examples, we are going to run the following query with 15 results each time:\n",
    "<br>\n",
    "```Name 3 recommended epic games```\n",
    "\n",
    "**Using semantic search**\n",
    "<br><br>\n",
    "![semantic search example](images/hybrid-search-1.png)\n",
    "<br><br>\n",
    "**Using hyprid search**\n",
    "<br>\n",
    "<br>\n",
    "![hybrid search example](images/hybrid-search-2.png)\n",
    "<br><br>\n",
    "As you could tell from the hybrid search based generation, the recommended games sounded more epic and interesting.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Let's apply this using the SDK:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bdb4c-9184-4b35-8c2b-6c6fd8df9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Name 3 recommended epic games\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 15, search_type='SEMANTIC')\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e136631-6439-40bb-9ce2-54484895b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with the Hybrid search \n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 15, search_type='HYBRID')\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce175a-0e6b-49b5-b5cb-d89180012425",
   "metadata": {},
   "source": [
    "### 4. How to apply multi queries approach? \n",
    "\n",
    "To demonstrate the the multi-queries benefits, we will use the same query we used in the previous section: \n",
    "<br>\n",
    "```Name 3 recommended epic games```\n",
    "<br>\n",
    "\n",
    "Although, this time we will limit the retrieved documents per query to `5`.\n",
    "\n",
    "***Note:*** to simplify the illustration of the multi-query concept, we will use the Langchain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40f50e-7d6c-4328-949e-fcdae12a7e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_aws import BedrockChat\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "import logging\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "multi_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is \n",
    "    to generate 3 different versions of the given user \n",
    "    question to retrieve relevant documents from a vector  database. \n",
    "    By generating multiple perspectives on the user question, \n",
    "    your goal is to help the user overcome some of the limitations \n",
    "    of distance-based similarity search. Provide these alternative \n",
    "    questions separated by a single '\\n'. Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# itiating Knowledge bases for Amazon Bedrock as Langchain retriever\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id= kb_id,\n",
    "    region_name = region_name,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": 5\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Amazon Bedrock runtime client - to invoke LLM\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "\n",
    "# prepare the FM inference configurations\n",
    "inference_modifier = {\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "# prepare model id and inference parameters\n",
    "llm = BedrockChat(\n",
    "    model_id = model_id,\n",
    "    client = bedrock_runtime,\n",
    "    model_kwargs = inference_modifier,\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiating the KB as a multi query retreiever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever = retriever, llm = llm, include_original=True, prompt = multi_query_prompt\n",
    ")\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query = query)\n",
    "\n",
    "print(len(unique_docs))\n",
    "\n",
    "#print(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac392d-07e3-475a-8aa4-913609c548dd",
   "metadata": {},
   "source": [
    "As you could tell from the count of unique documents, it is richer than the asking the original question only. It is also expected to get an improved answer based on variety of documents retrieve.\n",
    "\n",
    "Let's contiune with that example and check the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bae739-b49c-4e55-8d63-2ffb689513c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"documents\",\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to answer \n",
    "    a given user question based on provided context from a vector database. \n",
    "    Use only documents provided in the context to answer the user question. \n",
    "    <context>\n",
    "    {documents}\n",
    "    </context>\n",
    "    \n",
    "    User question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "answer= llm.invoke(final_answer_prompt.format(question= query, documents=unique_docs))\n",
    "\n",
    "pp.pprint(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc3fb3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Reminder:</b> Remember to CLEAN_UP the resources at the end of your session.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
