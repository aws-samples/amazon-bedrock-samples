{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d645a8-e7a4-4345-b99c-a07c8d6d16cd",
   "metadata": {},
   "source": [
    "# Hybrid search and multi-query with RAG\n",
    "\n",
    "In this module, you'll learn about the hybrid search and multi query concepts, and how to apply them using Amazon Bedrock and Knowledge bases for Amazon Bedrock APIs.\n",
    "This module contains:\n",
    "1. [Overview](#1-Overview)\n",
    "2. [Pre-requisites](#2-Pre-requisites)\n",
    "3. [How hybrid search improves the FM generations?](#3-how-hybrid-search-improves-the-fm-generations)\n",
    "4. [How to apply multi-queries approach?](#4-how-to-apply-multi-queries-approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa2df8-376f-462a-bc3c-696010572d12",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For RAG-based applications, the accuracy of the generated response from Foundation Models (FMs) is dependent on the context provided to the model. Context is retrieved from the vector database based on the user query. Semantic search is widely used because it is able to understand more human-like questions—a user’s query is not always directly related to the exact keywords in the content that answers it. \n",
    "\n",
    "Semantic search helps provide answers based on the meaning of the text. However, it has limitations in capturing all the relevant keywords. Its performance relies on the quality of the word embeddings used to represent meaning of the text. To overcome such limitations, you can combine semantic search with keyword search (hybrid search), which improve the relevance of search results.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f113eaf",
   "metadata": {},
   "source": [
    "### Hybrid search\n",
    "\n",
    "Hybrid search takes advantage of the strengths of multiple search algorithms, integrating their unique capabilities to enhance the relevance of returned search results. For RAG-based applications, semantic search capabilities are commonly combined with traditional keyword-based search to improve the relevance of search results. It enables searching over both the content of documents and their underlying meaning.\n",
    "\n",
    "<!-- ![hybrid search](images/hybrid-overview.png) -->\n",
    "\n",
    "<img src=\"images/hybrid-overview.png\" width=\"50%\" height=\"30%\">\n",
    "\n",
    "\n",
    "\n",
    "It works great for RAG-based applications where the retriever has to handle a wide variety of natural language queries. The keywords help cover specific entities in the query such as product name, color, and price, while semantics better understands the meaning and intent within the query. For example, if you have want to build a chatbot for an ecommerce website to handle customer queries such as the return policy or details of the product, using hybrid search will be most suitable.\n",
    "\n",
    "<b>Benefits: </b>\n",
    "Both keyword and semantic search will return a separate set of results along with their relevancy scores, which are then combined to return the most relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eacb97",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "Before being able to answer the questions, the documents must be processed and stored in a knowledge base. For this notebook, we use a `Synthetic data` (10K financial reports for fictitious company) to create the Knowledge Base. \n",
    "\n",
    "1. Upload your documents (data source) to Amazon S3 bucket.\n",
    "2. Create the Knowledge Base using [1a_create_ingest_documents_test_kb.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/knowledge-bases/01-rag-concepts/1a_create_ingest_documents_test_kb.ipynb)\n",
    "3. Note the Knowledge Base ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260249fa",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "1. For our notebook we will use the ```Retrieve``` and ```RetrieveAndGenerate``` APIs provided by Knowledge Bases for Amazon Bedrock. These APIs convert queries into embeddings, searches the knowledge base, and then augment (in case of ```RetrieveAndGenerate``` API) the foundation model prompt with the search results as context information and returns the FM-generated response to the question. \n",
    "\n",
    "    -   The output of the ```RetrieveAndGenerate``` API includes the `generated response`, `source attribution` as well as the `retrieved text chunks`.\n",
    "    -   The output of the ```Retrieve API``` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals. \n",
    "\n",
    "\n",
    "2. We will use the Anthropic Claude 3 Sonnet model on Amazon Bedrock for response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83791e2-dc57-475e-b9f0-77afc5e4b161",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3 --force-reinstall --quiet\n",
    "%pip install botocore --force-reinstall --quiet\n",
    "%pip install langchain--force-reinstall --quiet\n",
    "%pip install langchain-community --force-reinstall --quiet\n",
    "%pip install langchain-aws --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27329ef4-6997-49c7-b0d5-d2b2672394d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b70d58-f2db-4e8c-b1cd-78a8ad3a6b59",
   "metadata": {},
   "source": [
    "### Initialize boto3 client and variables\n",
    "Through out the notebook, we are going to utilise RetrieveAndGenerate and Retrieve APIs to test knowledge base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49e4b4-a689-4c15-bdff-35af66368689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pprint as pp\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "kb_id = \"<Knowledge_Base_ID>\" # replace it with your Knowledge base id.\n",
    "\n",
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent client\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "# Create bedrock client\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "# Define FM to be used for generations \n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # we will be using Anthropic Claude 3 Haiku throughout the notebook\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'\n",
    "\n",
    "\n",
    "def retrieve_and_generate(query, kb_id, model_arn, max_results, search_type):\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': query\n",
    "            },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn, \n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results, # will fetch top N documents which closely match the query\n",
    "                    'overrideSearchType' : search_type # usese either SEMANTIC or HYBRID options\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def print_generation_results(response, print_context = True):\n",
    "    generated_text = response['output']['text']\n",
    "    print('Generated FM response:\\n')\n",
    "    pp.pprint(generated_text)\n",
    "    \n",
    "    if print_context is True:\n",
    "        ## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "        citations = response[\"citations\"]\n",
    "        contexts = []\n",
    "        for citation in citations:\n",
    "            retrievedReferences = citation[\"retrievedReferences\"]\n",
    "            for reference in retrievedReferences:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "    \n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pp.pprint(contexts)\n",
    "\n",
    "def retrieve(query, kb_id, model_arn, max_results, search_type):\n",
    "    response = bedrock_agent_client.retrieve(\n",
    "            retrievalQuery={\n",
    "                'text': query\n",
    "            },\n",
    "        knowledgeBaseId= kb_id,\n",
    "        retrievalConfiguration={\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results, # will fetch top N documents which closely match the query\n",
    "                    'overrideSearchType': search_type\n",
    "                    }\n",
    "            }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def print_results(response):\n",
    "\n",
    "    print('Retrieved documents:\\n')\n",
    "    \n",
    "    ## print out the source citations from the original documents\n",
    "    citations = response['retrievalResults']\n",
    "    for citation in citations:\n",
    "        pp.pprint(citation[\"content\"][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90af71-3a75-4f91-a70b-e892a1b15376",
   "metadata": {},
   "source": [
    "### How hybrid search improves the documents retrieval?\n",
    "\n",
    "In the following examples, we are going to run the below query with semantic and hybrid search, and observe the documents retrieval and ultimately generated response.\n",
    "\n",
    "query = `What was the total operating lease cost recognized by the Company during the year ended December 31, 2022?`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "Expected response = `The Company recognized operating lease costs of $22.4 million during the year ended December 31, 2022.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f1ff7",
   "metadata": {},
   "source": [
    "**Using semantic search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bdb4c-9184-4b35-8c2b-6c6fd8df9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the total operating lease cost recognized by the Company during the year ended December 31, 2022?\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 5, search_type='SEMANTIC')\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbd46e",
   "metadata": {},
   "source": [
    "The generated respose using SEMANTIC search for above query is hallucinated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f185ea",
   "metadata": {},
   "source": [
    "**Using hyprid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e136631-6439-40bb-9ce2-54484895b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with the Hybrid search \n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 5, search_type='HYBRID')\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61730438",
   "metadata": {},
   "source": [
    "With HYBRID search we got the right response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41acbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2b7bf0b",
   "metadata": {},
   "source": [
    "### Multi-Query\n",
    "\n",
    "A different approach to improve the relevance of retrieved documents and subsequently FM generations; is based on multi-query concept. Vector databases find documents similar to your query based distance of its embedding to those docuemtns. In which, it creates problems if the wording used in the query is \"closer\" to less relevant documents. That are usually resolved by prompt engineering/query tuning.\n",
    "\n",
    "<img src=\"images/multi-query.png\" width=\"50%\" height=\"30%\">\n",
    "\n",
    "The approach is based on using an FM to generate multiple queries from different perspectives for a given query. For each query, it retrieves a set of relevant documents then uses a distinct set of document across all queries to get a larger set of potentially relevant documents. By generating multiple queries on the same question, this approach might be able to overcome some of the limitations of the distance-based retrieval and get a more comprehensive set of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce175a-0e6b-49b5-b5cb-d89180012425",
   "metadata": {},
   "source": [
    "### How to apply multi queries approach? \n",
    "\n",
    "To demonstrate the the multi-queries benefits, we will use the complex question-based approach: \n",
    "\n",
    "`What was the total operating lease liabilities and total sublease income of the Company as of December 31, 2022?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a213acd",
   "metadata": {},
   "source": [
    "**Using semantic search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was the total operating lease liabilities and total sublease income of the Company as of December 31, 2022?\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 5, search_type='SEMANTIC')\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a6924",
   "metadata": {},
   "source": [
    "**Using multi query search**\n",
    "\n",
    "***Note:*** to simplify the illustration of the multi-query concept, we will use the Langchain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40f50e-7d6c-4328-949e-fcdae12a7e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_aws import BedrockChat\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "import logging\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "multi_query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is \n",
    "    to generate 3 different versions of the given user \n",
    "    question to retrieve relevant documents from a vector  database. \n",
    "    By generating multiple perspectives on the user question, \n",
    "    your goal is to help the user overcome some of the limitations \n",
    "    of distance-based similarity search. Provide these alternative \n",
    "    questions separated by a single '\\n'. Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "# itiating Knowledge bases for Amazon Bedrock as Langchain retriever\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id= kb_id,\n",
    "    region_name = region_name,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": 5\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Amazon Bedrock runtime client - to invoke LLM\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "\n",
    "# prepare the FM inference configurations\n",
    "inference_modifier = {\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "# prepare model id and inference parameters\n",
    "llm = BedrockChat(\n",
    "    model_id = model_id,\n",
    "    client = bedrock_runtime,\n",
    "    model_kwargs = inference_modifier,\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiating the KB as a multi query retreiever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever = retriever, llm = llm, include_original=True, prompt = multi_query_prompt\n",
    ")\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query = query)\n",
    "\n",
    "print(len(unique_docs))\n",
    "\n",
    "#print(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac392d-07e3-475a-8aa4-913609c548dd",
   "metadata": {},
   "source": [
    "As you could tell from the count of unique documents, it is richer than the asking the original question only. It is also expected to get an improved answer based on variety of documents retrieve.\n",
    "\n",
    "Let's contiune with that example and check the final answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bae739-b49c-4e55-8d63-2ffb689513c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"documents\",\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to answer \n",
    "    a given user question based on provided context from a vector database. \n",
    "    Use only documents provided in the context to answer the user question. \n",
    "    <context>\n",
    "    {documents}\n",
    "    </context>\n",
    "    \n",
    "    User question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "query = \"What was the total operating lease liabilities and total sublease income of the Company as of December 31, 2022?\"\n",
    "answer= llm.invoke(final_answer_prompt.format(question= query, documents=unique_docs))\n",
    "\n",
    "pp.pprint(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690ee0d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Remember to delete KB, OSS index and related IAM roles and policies to avoid incurring any charges.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
