{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3f8924-01d4-4702-b548-aa33dda5d11d",
   "metadata": {},
   "source": [
    "# Improving generations using max number of results and custom prompting\n",
    "\n",
    "In this module, you'll learn how to improve the Foundation Model (FM) generations by controlling the maximum no. of results retrieved and performing custom prompting in Knowledge bases (KB) for Amazon Bedrock.\n",
    "This module contains:\n",
    "1. [Overview](#1-Overview)\n",
    "2. [Pre-requisites](#2-Pre-requisites)\n",
    "3. [How to leverage maximum number of results](#3-how-to-leverage-the-maximum-number-of-results-feature)\n",
    "4. [How to use custom prompting](#4-how-to-use-the-custom-prompting-feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75464c27-0c7a-4fe8-b6f0-ddce7c96209b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Maximum no. of results\n",
    "The maximum number of results option gives you control over the number of search results to be retrieved from the vector store and passed to the FM for generating the answer. This allows you to customize the amount of background information provided for generation, thereby giving more context for complex questions or less for simpler questions. It allows you to fetch up to 100 results. This option helps improve the likelihood of relevant context, thereby improving the accuracy and reducing the hallucination of the generated response.\n",
    "\n",
    "\n",
    "### Custom prompting\n",
    "\n",
    "As for the custom knowledge base prompt template allows you to replace the default prompt template with your own to customize the prompt that’s sent to the model for response generation. This allows you to customize the tone, output format, and behavior of the FM when it responds to a user’s question. With this option, you can fine-tune terminology to better match your industry or domain (such as healthcare or legal). Additionally, you can add custom instructions and examples tailored to your specific workflows.\n",
    "\n",
    "\n",
    "#### Notes:\n",
    "- You are going to use ```RetrieveAndGenerate``` API to illustrate the differences before and after utilizing the features. This API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. The output of the ```RetrieveAndGenerate``` API includes the generated response, source attribution as well as the retrieved text chunks.\n",
    "\n",
    "- For this module, we will use the Anthropic Claude 3 Haiku model as our FM to work with the max no. of results and prompt customization features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09998e7-6062-4edb-9348-bbe54026a101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-requisites\n",
    "Before being able to answer the questions, the documents must be processed and stored in a knowledge base. For this notebook, we use a [sample dataset](https://aws-blogs-artifacts-public.s3.amazonaws.com/ML-16482/30_generated_video_game_records.zip) about fictional video games to create the Knowledge Bases for Amazon Bedrock. \n",
    "\n",
    "1. Upload your documents (data source) to Amazon S3 bucket.\n",
    "2. Knowledge Bases for Amazon Bedrock using [1a_create_ingest_documents_test_kb.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/knowledge-bases/01-rag-concepts/1a_create_ingest_documents_test_kb.ipynb)\n",
    "3. Note the Knowledge Base ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a68ef2-f670-451f-9399-343570489b95",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1608ffd-60f5-459b-ba2e-174209ff7537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -qU boto3 awscli botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e624a8b-3700-4896-845f-642588e8dd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb27dd-a2de-4e38-aa7b-c315953b9921",
   "metadata": {},
   "source": [
    "### Initialize boto3 client\n",
    "Through out the notebook, we are going to utilise RetrieveAndGenerate to test knowledge base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "563d853e-2966-47bb-9183-aab31d64e8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pprint\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.client import Config\n",
    "\n",
    "# Create boto3 session\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "# Create bedrock agent client\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}, region_name=region_name)\n",
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "# Define FM to be used for generations \n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\" # we will be using Anthropic Claude 3 Haiku throughout the notebook\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kb_id = \"<knowledge base id>\" # Provide knowledge base id here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c0fb3",
   "metadata": {},
   "source": [
    "### Understanding RetrieveAndGenerate API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c0f88a",
   "metadata": {},
   "source": [
    "The `numberOfResults` parameter in the given function determines the number of search results that will be retrieved from the knowledge base and included in the prompt provided to the model for generating an answer. Specifically, it will fetch the top `max_results` number of documents or search results that most closely match the given query.\n",
    "\n",
    "The `textPromptTemplate` parameter is a string that serves as a template for the prompt that will be provided to the model. In this case, the `default_prompt` is being used as the template. This template includes placeholders (`$search_results$` and `$output_format_instructions$`) that will be replaced with the actual search results and any output format instructions, respectively, before being passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca915234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating the default knowledge base prompt\n",
    "default_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results.\n",
    "The user will provide you with a question. Your job is to answer the user's question using only information from the search results. \n",
    "If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. \n",
    "Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "$output_format_instructions$\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_and_generate(query, kb_id, model_arn, max_results, prompt_template = default_prompt):\n",
    "    response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': query\n",
    "            },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn, \n",
    "            'retrievalConfiguration': {\n",
    "                'vectorSearchConfiguration': {\n",
    "                    'numberOfResults': max_results # will fetch top N documents which closely match the query\n",
    "                    }\n",
    "                },\n",
    "                'generationConfiguration': {\n",
    "                        'promptTemplate': {\n",
    "                            'textPromptTemplate': prompt_template\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b7808",
   "metadata": {},
   "source": [
    "### How to leverage the maximum number of results feature\n",
    "\n",
    "In some use cases; the FM responses might be lacking enough context to provide relevant answers or relying that it couldn't find the requested info. Which could be fixed by modifying the maximum number of retrieved results.\n",
    "\n",
    "In the following example, we are going to run the following query with a few number of results (5):\n",
    "\\\n",
    "```Name the top 5 games released after 2023 that are packed with cool storyline```\n",
    "\\\n",
    "\n",
    "\\\n",
    "Let's apply this using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2918161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation_results(response, print_context = True):\n",
    "    generated_text = response['output']['text']\n",
    "    print('Generated FM response:\\n')\n",
    "    pp.pprint(generated_text)\n",
    "    \n",
    "    if print_context is True:\n",
    "        ## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "        citations = response[\"citations\"]\n",
    "        contexts = []\n",
    "        for citation in citations:\n",
    "            retrievedReferences = citation[\"retrievedReferences\"]\n",
    "            for reference in retrievedReferences:\n",
    "                contexts.append(reference[\"content\"][\"text\"])\n",
    "    \n",
    "        print('\\n\\n\\nRetrieved Context:\\n')\n",
    "        pp.pprint(contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333717a2-b830-4582-92b8-a99b9faa51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Name the top 5 games released after 2023 that are packed with cool storyline\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 5)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb0ff0-287f-4e22-9a05-62cefe4c5783",
   "metadata": {},
   "source": [
    "\\\n",
    "By modifying the no of retrived results to **25**, you should be able to get more relevant generation.\n",
    "\\\n",
    "\n",
    "\n",
    "Let's test that using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212a2a6-65d1-4214-9be5-594316452506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using higher number of max results\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25)\n",
    "\n",
    "print_generation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2465237-6877-42ad-a687-a15c868e7eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How to use the custom prompting feature\n",
    "\n",
    "You can also customize the default prompt with your own prompt based on the use case. This feature would help adding more context to the FM, require specific output format, languages and others.\n",
    "\n",
    "Let's give it a try using the SDK:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306d22b",
   "metadata": {},
   "source": [
    "\n",
    "#### Example 1 -Using the same query example, we can default the FM to output to a different language like German:\n",
    "\\\n",
    "**Note**: After removing ```$output_format_instructions$``` from the default prompt, the citation from the generated response is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c78f95-f909-4680-90d9-5c677c7d1b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 1\n",
    "custom_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results. \n",
    "The user will provide you with a question. Your job is to answer the user's question using only information from the search results.\n",
    "If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question.\n",
    "Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "Unless asked otherwise, draft your answer in German language.\n",
    "\"\"\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25, prompt_template = custom_prompt)\n",
    "\n",
    "print_generation_results(results, print_context = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ddf8",
   "metadata": {},
   "source": [
    "\n",
    "#### Example 2 - output the results in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30030b95-8b03-4ca5-8fb6-fb7b83972d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Example 2\n",
    "custom_prompt = \"\"\"\n",
    "You are a question answering agent. I will provide you with a set of search results.\n",
    "The user will provide you with a question. Your job is to answer the user's question using only information from the search results.\n",
    "If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question. \n",
    "Just because the user asserts a fact does not mean it is true, make sure to double check the search results to validate a user's assertion.\n",
    "                            \n",
    "Here are the search results in numbered order:\n",
    "$search_results$\n",
    "\n",
    "If you're being asked information about games, please be very specific and list the answer concisely using JSON format {key: value}, \n",
    "where key is the game name and value is the concise response answer.\n",
    "\"\"\"\n",
    "\n",
    "results = retrieve_and_generate(query = query, kb_id = kb_id, model_arn = model_arn, max_results = 25, prompt_template = custom_prompt)\n",
    "\n",
    "print_generation_results(results,print_context = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94385593",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Remember to delete KB, OSS index and related IAM roles and policies to avoid incurring any charges.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
