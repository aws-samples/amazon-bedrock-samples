{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock - End to end example using multiple data sources as data source(s)\n",
    "\n",
    "This notebook provides sample code for building an empty OpenSearch Serverless (OSS) index,Knowledge bases for Amazon Bedrock and ingest documents into the index from various data sources (S3, Confluence, Sharepoint, Salesforce, and Web). Please note that you can add upto 5 data sources.\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "A data pipeline that ingests documents (typically stored in multiple data sources) into a knowledge base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "- Load the documents into the knowledge base by connecting various data sources (S3, Confluence, Sharepoint, Salesforce, and Web). \n",
    "- Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "<!-- ![data_ingestion.png](./images/data_ingestion.png) -->\n",
    "<img src=\"../images/data_ingestion.png\" width=50% height=20% />\n",
    "\n",
    "\n",
    "#### Steps: \n",
    "- Create Knowledge Base execution role with necessary policies for accessing data from various data sources (S3, Confluence, Sharepoint, Salesforce, and Web) and writing embeddings into OSS.\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Pre-requisite: \n",
    "    - For S3 , create s3 bucket (if not exists) and upload the data\n",
    "    - for other data sources - Refer to the pre-requisites for corresponding [AWS documentation page](https://docs.aws.amazon.com/bedrock/latest/userguide/data-source-connectors.html)\n",
    "- Create knowledge base\n",
    "- Create data source(s) within knowledge base\n",
    "- For each data source, start ingestion jobs using KB APIs which will read data from the data source, chunk it, convert chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy and manage the data pipeline.\n",
    "\n",
    "Once the data is available in the Bedrock Knowledge Base then a question answering application can be built using the Knowledge Base APIs provided by Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "- create and delete Amazon IAM roles\n",
    "- create, update and delete Amazon S3 buckets\n",
    "- access Amazon Bedrock\n",
    "- access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "- IAMFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to enable `Anthropic Claude 3` and `Anthropic Claude Instant` model access in Amazon Bedrock Console, as the notebook will use Anthropic Claude 3 Sonnet and Claude instant models for testing the knowledge base once its created.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U opensearch-py==2.3.1 --quiet\n",
    "%pip install -U boto3 --quiet\n",
    "%pip install -U retrying==1.3.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_bedrock_execution_role_multi_ds, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent', region_name=region_name)\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=region_name)\n",
    "\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.144\n"
     ]
    }
   ],
   "source": [
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you can add multiple and different data sources (S3, Confluence, Sharepoint, Salesforce, Web Crawler) to a Knowledge Base. For this notebook, we'll test Knowledge Base creation with multiple and different data sources.\n",
    "\n",
    "Each data source may have different pre-requisites, please refer to the AWS documetation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we'll create Knowledge Base with multiple data sources ( 1 S3 bucket, 1 confluence page, 1 Sharepoint site, 1 Salesforce site, 1 Web Crawler)\n",
    "\n",
    "bucket_name = f'bedrock-kb-{s3_suffix}-1' # replace it with your first bucket name.\n",
    "\n",
    "## Below is a list of data sources including, 1 S3 buckets, 1 confluence, 1 Sharepoint, 1 Salesforce connectors\n",
    "## Please uncomment the data sources that you want to add and update the placeholder values accordingly.\n",
    "\n",
    "data_sources=[\n",
    "                {\"type\": \"S3\", \"bucket_name\": bucket_name}, \n",
    "                \n",
    "                # {\"type\": \"CONFLUENCE\", \"hostUrl\": \"https://example.atlassian.net\", \"authType\": \"BASIC\",\n",
    "                #  \"credentialsSecretArn\": f\"arn:aws::secretsmanager:{region_name}:secret:<<your_secret_name>>\"},\n",
    "\n",
    "                # {\"type\": \"SHAREPOINT\", \"tenantId\": \"888d0b57-69f1-4fb8-957f-e1f0bedf64de\", \"domain\": \"yourdomain\",\n",
    "                #   \"authType\": \"OAUTH2_CLIENT_CREDENTIALS\",\n",
    "                #  \"credentialsSecretArn\": f\"arn:aws::secretsmanager:{region_name}:secret:<<your_secret_name>>\",\n",
    "                #  \"siteUrls\": [\"https://yourdomain.sharepoint.com/sites/mysite\"]\n",
    "                # },\n",
    "\n",
    "                # {\"type\": \"SALESFORCE\", \"hostUrl\": \"https://company.salesforce.com/\", \"authType\": \"OAUTH2_CLIENT_CREDENTIALS\",\n",
    "                #  \"credentialsSecretArn\": f\"arn:aws::secretsmanager:{region_name}:secret:<<your_secret_name>>\"\n",
    "                # },\n",
    "\n",
    "                # {\"type\": \"WEB\", \"seedUrls\": [{ \"url\": \"https://www.examplesite.com\"}],\n",
    "                #  \"inclusionFilters\": [\"https://www\\.examplesite\\.com/.*\\.html\"],\n",
    "                #  \"exclusionFilters\": [\"https://www\\.examplesite\\.com/contact-us\\.html\"]\n",
    "                # }\n",
    "            ]\n",
    "                \n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-017444429555-1 Exists\n"
     ]
    }
   ],
   "source": [
    "# For S3 data source, check if S3 bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "\n",
    "for ds in [d for d in data_sources if d['type']== 'S3']:\n",
    "    bucket_name = ds['bucket_name']\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f'Bucket {bucket_name} Exists')\n",
    "    except ClientError as e:\n",
    "        print(f'Creating bucket {bucket_name}')\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            # CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "Firt of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your applicationâ€”without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role_multi_ds(bucket_names=[d[\"bucket_name\"] for d in data_sources if d['type']== 'S3'],\n",
    "                                secrets_arns = [d[\"credentialsSecretArn\"] for d in data_sources if d['type']== 'CONFLUENCE'or d['type']=='SHAREPOINT' or d['type']=='SALESFORCE'])\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::017444429555:role/AmazonBedrockExecutionRoleForKnowledgeBase_407'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_kb_execution_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Tue, 16 Jul 2024 00:22:50 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '9ee98f82-eed6-446e-89e8-45fadf0c6ac8'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '9ee98f82-eed6-446e-89e8-45fadf0c6ac8',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:017444429555:collection/kgtxpbat4xgcplm596ce',\n",
      "                              'createdDate': 1721089370302,\n",
      "                              'id': 'kgtxpbat4xgcplm596ce',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1721089370302,\n",
      "                              'name': 'bedrock-sample-rag-744',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kgtxpbat4xgcplm596ce.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "Done!.........................\n",
      "\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:017444429555:collection/kgtxpbat4xgcplm596ce',\n",
      "    'collectionEndpoint': 'https://kgtxpbat4xgcplm596ce.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1721089370302,\n",
      "    'dashboardEndpoint': 'https://kgtxpbat4xgcplm596ce.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': 'kgtxpbat4xgcplm596ce',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1721089394030,\n",
      "    'name': 'bedrock-sample-rag-744',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# wait for collection creation\n",
    "# This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::017444429555:policy/AmazonBedrockOSSPolicyForKnowledgeBase_407\n",
      "Done!.......................................................\n"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1024,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-744',\n",
      "  'shards_acknowledged': True}\n",
      "Done!.......................................................\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to ingest into our knowledge base.\n",
    "We'll use the following data:\n",
    " - sythetic data stored in a local directory as first data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 Bucket data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading file ../synthetic_dataset/octank_financial_10K.pdf to bedrock-kb-us-east-1-017444429555-1\n"
     ]
    }
   ],
   "source": [
    "def upload_directory(path, bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_to_upload = os.path.join(root,file)\n",
    "                print(f\"uploading file {file_to_upload} to {bucket_name}\")\n",
    "                s3_client.upload_file(file_to_upload,bucket_name,file)\n",
    "\n",
    "upload_directory(\"../synthetic_dataset\", bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "                    \n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 7, 16, 0, 25, 35, 666621, tzinfo=tzutc()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:017444429555:knowledge-base/BFW85AXYMB',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0'}},\n",
      "  'knowledgeBaseId': 'BFW85AXYMB',\n",
      "  'name': 'bedrock-sample-knowledge-base-744',\n",
      "  'roleArn': 'arn:aws:iam::017444429555:role/AmazonBedrockExecutionRoleForKnowledgeBase_407',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:017444429555:collection/kgtxpbat4xgcplm596ce',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-744'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 7, 16, 0, 25, 35, 666621, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create data source(s), which will be associated with the knowledge base created above. Once the data source(s) is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Source(s)\n",
    "Steps:\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "\n",
    "NOTE: In the current sample, we'll use FIXED_SIZE chunking Strategy but you can also use other chunking chunking strategies like HIERARCHICAL, SEMANTIC or NONE. For more details on the chunking startegies please refer to the [AWS documentation page](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_ChunkingConfiguration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create KB\n",
    "def create_ds(data_sources):\n",
    "    ds_list=[]\n",
    "    for idx, ds in enumerate(data_sources):\n",
    "        # Ingest strategy - How to ingest data from the data source\n",
    "        chunkingStrategyConfiguration = {\n",
    "            \"chunkingStrategy\": \"FIXED_SIZE\", \n",
    "            \"fixedSizeChunkingConfiguration\": {\n",
    "                \"maxTokens\": 512,\n",
    "                \"overlapPercentage\": 20\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "        \n",
    "        s3DataSourceConfiguration = {\n",
    "                \"type\": \"S3\",\n",
    "                \"s3Configuration\":{\n",
    "                    \"bucketArn\": \"\",\n",
    "                    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "                    }\n",
    "            }\n",
    "        \n",
    "        confluenceDataSourceConfiguration = {\n",
    "            \"confluenceConfiguration\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                    \"hostUrl\": \"\",\n",
    "                    \"hostType\": \"SAAS\",\n",
    "                    \"authType\": \"\", # BASIC | OAUTH2_CLIENT_CREDENTIALS\n",
    "                    \"credentialsSecretArn\": \"\"\n",
    "                    \n",
    "                },\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"filterConfiguration\": {\n",
    "                        \"type\": \"PATTERN\",\n",
    "                        \"patternObjectFilter\": {\n",
    "                            \"filters\": [\n",
    "                                {\n",
    "                                    \"objectType\": \"Attachment\",\n",
    "                                    \"inclusionFilters\": [\n",
    "                                        \".*\\\\.pdf\"\n",
    "                                    ],\n",
    "                                    \"exclusionFilters\": [\n",
    "                                        \".*private.*\\\\.pdf\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"CONFLUENCE\"\n",
    "        }\n",
    "\n",
    "        sharepointDataSourceConfiguration = {\n",
    "            \"sharePointConfiguration\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                    \"tenantId\": \"\",\n",
    "                    \"hostType\": \"ONLINE\",\n",
    "                    \"domain\": \"domain\",\n",
    "                    \"siteUrls\": [],\n",
    "                    \"authType\": \"\", # BASIC | OAUTH2_CLIENT_CREDENTIALS\n",
    "                    \"credentialsSecretArn\": \"\"\n",
    "                    \n",
    "                },\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"filterConfiguration\": {\n",
    "                        \"type\": \"PATTERN\",\n",
    "                        \"patternObjectFilter\": {\n",
    "                            \"filters\": [\n",
    "                                {\n",
    "                                    \"objectType\": \"Attachment\",\n",
    "                                    \"inclusionFilters\": [\n",
    "                                        \".*\\\\.pdf\"\n",
    "                                    ],\n",
    "                                    \"exclusionFilters\": [\n",
    "                                        \".*private.*\\\\.pdf\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"SHAREPOINT\"\n",
    "        }\n",
    "\n",
    "\n",
    "        salesforceDataSourceConfiguration = {\n",
    "            \"salesforceConfiguration\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                    \"hostUrl\": \"\",\n",
    "                    \"authType\": \"\", # BASIC | OAUTH2_CLIENT_CREDENTIALS\n",
    "                    \"credentialsSecretArn\": \"\"\n",
    "                },\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"filterConfiguration\": {\n",
    "                        \"type\": \"PATTERN\",\n",
    "                        \"patternObjectFilter\": {\n",
    "                            \"filters\": [\n",
    "                                {\n",
    "                                    \"objectType\": \"Attachment\",\n",
    "                                    \"inclusionFilters\": [\n",
    "                                        \".*\\\\.pdf\"\n",
    "                                    ],\n",
    "                                    \"exclusionFilters\": [\n",
    "                                        \".*private.*\\\\.pdf\"\n",
    "                                    ]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"SALESFORCE\"\n",
    "        }\n",
    "\n",
    "        webcrawlerDataSourceConfiguration = {\n",
    "            \"webConfiguration\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                    \"urlConfiguration\": {\n",
    "                        \"seedUrls\": []\n",
    "                    }\n",
    "                },\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"crawlerLimits\": {\n",
    "                        \"rateLimit\": 50\n",
    "                    },\n",
    "                    \"scope\": \"HOST_ONLY\",\n",
    "                    \"inclusionFilters\": [],\n",
    "                    \"exclusionFilters\": []\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"WEB\"\n",
    "        }\n",
    "\n",
    "        # Set the data source configuration based on the Data source type\n",
    "\n",
    "        if ds['type'] == \"S3\":\n",
    "            print(f'{idx +1 } data source: S3')\n",
    "            ds_name = f'{name}-{bucket_name}'\n",
    "            s3DataSourceConfiguration[\"s3Configuration\"][\"bucketArn\"] = f'arn:aws:s3:::{ds[\"bucket_name\"]}'\n",
    "            # print(s3DataSourceConfiguration)\n",
    "            data_source_configuration = s3DataSourceConfiguration\n",
    "        \n",
    "        if ds['type'] == \"CONFLUENCE\":\n",
    "            print(f'{idx +1 } data source: CONFLUENCE')\n",
    "            ds_name = f'{name}-confluence'\n",
    "            confluenceDataSourceConfiguration['confluenceConfiguration']['sourceConfiguration']['hostUrl'] = ds['hostUrl']\n",
    "            confluenceDataSourceConfiguration['confluenceConfiguration']['sourceConfiguration']['authType'] = ds['authType']\n",
    "            confluenceDataSourceConfiguration['confluenceConfiguration']['sourceConfiguration']['credentialsSecretArn'] = ds['credentialsSecretArn']\n",
    "            # print(confluenceDataSourceConfiguration)\n",
    "            data_source_configuration = confluenceDataSourceConfiguration\n",
    "\n",
    "        if ds['type'] == \"SHAREPOINT\":\n",
    "            print(f'{idx +1 } data source: SHAREPOINT')\n",
    "            ds_name = f'{name}-sharepoint'\n",
    "            sharepointDataSourceConfiguration['sharePointConfiguration']['sourceConfiguration']['tenantId'] = ds['tenantId']\n",
    "            sharepointDataSourceConfiguration['sharePointConfiguration']['sourceConfiguration']['domain'] = ds['domain']\n",
    "            sharepointDataSourceConfiguration['sharePointConfiguration']['sourceConfiguration']['authType'] = ds['authType']\n",
    "            sharepointDataSourceConfiguration['sharePointConfiguration']['sourceConfiguration']['siteUrls'] = ds[\"siteUrls\"]\n",
    "            sharepointDataSourceConfiguration['sharePointConfiguration']['sourceConfiguration']['credentialsSecretArn'] = ds['credentialsSecretArn']\n",
    "            # print(sharepointDataSourceConfiguration)\n",
    "            data_source_configuration = sharepointDataSourceConfiguration\n",
    "\n",
    "\n",
    "        if ds['type'] == \"SALESFORCE\":\n",
    "            print(f'{idx +1 } data source: SALESFORCE')\n",
    "            ds_name = f'{name}-salesforce'\n",
    "            salesforceDataSourceConfiguration['salesforceConfiguration']['sourceConfiguration']['hostUrl'] = ds['hostUrl']\n",
    "            salesforceDataSourceConfiguration['salesforceConfiguration']['sourceConfiguration']['authType'] = ds['authType']\n",
    "            salesforceDataSourceConfiguration['salesforceConfiguration']['sourceConfiguration']['credentialsSecretArn'] = ds['credentialsSecretArn']\n",
    "            # print(salesforceDataSourceConfiguration)\n",
    "            data_source_configuration = salesforceDataSourceConfiguration\n",
    "\n",
    "        if ds['type'] == \"WEB\":\n",
    "            print(f'{idx +1 } data source: WEB')\n",
    "            ds_name = f'{name}-web'\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['sourceConfiguration']['urlConfiguration']['seedUrls'] = ds['seedUrls']\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['crawlerConfiguration']['inclusionFilters'] = ds['inclusionFilters']\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['crawlerConfiguration']['exclusionFilters'] = ds['exclusionFilters']\n",
    "            # print(webcrawlerDataSourceConfiguration)\n",
    "            data_source_configuration = webcrawlerDataSourceConfiguration\n",
    "            \n",
    "\n",
    "        # Create a DataSource in KnowledgeBase \n",
    "        create_ds_response = bedrock_agent_client.create_data_source(\n",
    "            name = ds_name,\n",
    "            description = description,\n",
    "            knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "            dataSourceConfiguration = data_source_configuration,\n",
    "            vectorIngestionConfiguration = {\n",
    "                \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "            }\n",
    "        )\n",
    "        ds = create_ds_response[\"dataSource\"]\n",
    "        pp.pprint(ds)\n",
    "        ds_list.append(ds)\n",
    "    return ds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 data source: S3\n",
      "{ 'createdAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-1'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'EWVX76BDR0',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'BFW85AXYMB',\n",
      "  'name': 'bedrock-sample-knowledge-base-744-bedrock-kb-us-east-1-017444429555-1',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "data_sources_list = create_ds(data_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'createdAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-1'},\n",
       "   'type': 'S3'},\n",
       "  'dataSourceId': 'EWVX76BDR0',\n",
       "  'description': 'Amazon shareholder letter knowledge base.',\n",
       "  'knowledgeBaseId': 'BFW85AXYMB',\n",
       "  'name': 'bedrock-sample-knowledge-base-744-bedrock-kb-us-east-1-017444429555-1',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sources_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '09e69e1f-1188-46d2-b66f-99f186360e6f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 16 Jul 2024 00:25:37 GMT', 'content-type': 'application/json', 'content-length': '641', 'connection': 'keep-alive', 'x-amzn-requestid': '09e69e1f-1188-46d2-b66f-99f186360e6f', 'x-amz-apigw-id': 'a-qgRHZXIAMEqkg=', 'x-amzn-trace-id': 'Root=1-6695be01-7acded076de5a4132009678d'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-1'}, 'type': 'S3'}, 'dataSourceId': 'EWVX76BDR0', 'description': 'Amazon shareholder letter knowledge base.', 'knowledgeBaseId': 'BFW85AXYMB', 'name': 'bedrock-sample-knowledge-base-744-bedrock-kb-us-east-1-017444429555-1', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 7, 16, 0, 25, 37, 334067, tzinfo=tzutc()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE', 'fixedSizeChunkingConfiguration': {'maxTokens': 512, 'overlapPercentage': 20}}}}}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Get DataSource \n",
    "for idx, ds in enumerate(data_sources_list):\n",
    "    print(bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source(s) created, we can start the ingestion job for each data source.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS.\n",
    "\n",
    "NOTE: Currently, you can only kick-off one ingestion job at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 0 started successfully\n",
      "\n",
      "{ 'dataSourceId': 'EWVX76BDR0',\n",
      "  'failureReasons': [ '[\"Encountered error: Ignored 1 files as their file '\n",
      "                      'format was not supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 3 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/lambda_function.zip, '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/car-listings.zip, '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/jeep.jpg]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\",\"Encountered '\n",
      "                      'error: Ignored 1 files as their file format was not '\n",
      "                      'supported. [Files: '\n",
      "                      's3://bedrock-kb-us-east-1-017444429555-1/.DS_Store]. '\n",
      "                      'Call to Amazon S3 Source did not succeed.\"]'],\n",
      "  'ingestionJobId': 'AKAP29P2QK',\n",
      "  'knowledgeBaseId': 'BFW85AXYMB',\n",
      "  'startedAt': datetime.datetime(2024, 7, 16, 0, 25, 38, 167865, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 4,\n",
      "                  'numberOfDocumentsScanned': 11,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 7},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 7, 16, 0, 26, 7, 992740, tzinfo=tzutc())}\n",
      "Done!...................................\n"
     ]
    }
   ],
   "source": [
    "ingest_jobs=[]\n",
    "# Start an ingestion job\n",
    "for idx, ds in enumerate(data_sources_list):\n",
    "    try:\n",
    "        start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "        job = start_job_response[\"ingestionJob\"]\n",
    "        print(f\"job {idx} started successfully\\n\")\n",
    "    \n",
    "        while job['status'] not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "            get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "              knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "                dataSourceId = ds[\"dataSourceId\"],\n",
    "                ingestionJobId = job[\"ingestionJobId\"]\n",
    "          )\n",
    "            job = get_job_response[\"ingestionJob\"]\n",
    "        pp.pprint(job)\n",
    "        interactive_sleep(40)\n",
    "\n",
    "        ingest_jobs.append(job)\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't start {idx} job.\\n\")\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'BFW85AXYMB'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is:\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow.\n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the consolidated statements of cash flows for Octank Financial, in the fiscal year ended December 31, 2019:\n",
      "\n",
      "- Net cash provided by operating activities was $710 million.\n",
      "- Net cash used in investing activities was $240 million, primarily due to purchases of property, plant, and equipment as well as purchases of marketable securities.\n",
      "- Net cash provided by financing activities was $350 million, mainly from proceeds from issuance of common stock and long-term debt.\n",
      "- Overall, there was a net increase in cash and cash equivalents of $120 million.\n",
      "- Cash and cash equivalents at the end of 2019 were $210 million.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region_name, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In March 2020, Amazon opened 100,000 new positions across its fulfillment and delivery network to respond to increased customer demand during the COVID-19 pandemic. Later, in 2020, Amazon announced it was creating another 75,000 jobs across its fulfillment and delivery network to continue meeting customer demand.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": \"How many new positions were opened across Amazon's fulfillment and delivery network?\"\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region_name, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the retrieve and generate API we get the final response directly and we don't see the different sources used to generate this response. Let's now retrieve the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chuncks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:  In the early 2000s, it took us an average of 18 hours to get an item through our fulfillment centers and on the right truck for shipment. Now, it takes us two. To deliver as reliably and cost-effectively as we desire, and to serve Amazon Prime members expecting shipments in a couple of days, we spent years building out an expansive set of fulfillment centers, a substantial logistics and transportation capability, and reconfigured how we did virtually everything in our facilities. For perspective, in 2004, we had seven fulfillment centers in the U.S. and four in other parts of the world, and we hadnâ€™t yet added delivery stations, which connect our fulfillment and sortation centers to the last-mile delivery vans you see driving around your neighborhood. Fast forward to the end of 2021, we had 253 fulfillment centers, 110 sortation centers, and 467 delivery stations in North America, with an additional 157 fulfillment centers, 58 sortation centers, and 588 delivery stations across the globe. Our delivery network grew to more than 260,000 drivers worldwide, and our Amazon Air cargo fleet has more than 100 aircraft. This has represented a capital investment of over $100 billion and countless iterations and small process improvements by over a million Amazonians in the last decade and a half.   Ironically, just before COVID started, weâ€™d made the decision to invest billions of incremental dollars over several years to deliver an increasing number of Prime shipments in one day. This initiative was slowed by the challenges of the pandemic, but weâ€™ve since resumed our focus here. Delivering a substantial amount of shipments in one day is hard (especially across the millions of items that we offer) and initially expensive as we build out the infrastructure to scale this efficiently. But, we believe our over 200 million Prime customers, who will tell you very clearly that faster is almost always better, will love this. So, this capability to ship millions of items within a couple days (and increasingly one day) was not from one aha moment and not developed in a year or two. Itâ€™s been hard-earned by putting ourselves in the shoes of our customers, knowing what they wanted, organizing Amazonians to work together to invent better solutions, and investing a large amount of financial and people resources over 20 years (often well in advance of when it would pay out).\n",
      "\n",
      "Chunk 1 Location:  {'s3Location': {'uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2021-Shareholder-Letter.pdf'}, 'type': 'S3'}\n",
      "\n",
      "Chunk 1 Score:  0.5242253\n",
      "\n",
      "Chunk 1 Metadata:  {'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2021-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AbT7uuJABGPVBEv-Okf26', 'x-amz-bedrock-kb-data-source-id': 'EWVX76BDR0'}\n",
      "\n",
      "Chunk 2:  We also established the Amazon Relief Fundâ€”with an initial $25 million in fundingâ€”to support our independent delivery service partners and their drivers, Amazon Flex participants, and temporary employees under financial distress.   In March, we opened 100,000 new positions across our fulfillment and delivery network. Earlier this week, after successfully filling those roles, we announced we were creating another 75,000 jobs to respond to customer demand. These new hires are helping customers who depend on us to meet their critical needs. We know that many people around the world have suffered financially as jobs are lost or furloughed. We are happy to have them on our teams until things return to normal and either their former employer can bring them back or new jobs become available. Weâ€™ve welcomed Joe Duffy, who joined after losing his job as a mechanic at Newark airport and learned about an opening from a friend who is an Amazon operations analyst. Dallas preschool teacher Darby Griffin joined after her school closed on March 9th and now helps manage new inventory. Weâ€™re happy to have Darby with us until she can return to the classroom.   Amazon is acting aggressively to protect our customers from bad actors looking to exploit the crisis. Weâ€™ve removed over half a million offers from our stores due to COVID-based price gouging, and weâ€™ve suspended more than 6,000 selling accounts globally for violating our fair-pricing policies. Amazon turned over information about sellers we suspect engaged in price gouging of products related to COVID-19 to 42 state attorneys general offices. To accelerate our response to price-gouging incidents, we created a special communication channel for state attorneys general to quickly and easily escalate consumer complaints to us.   Amazon Web Services is also playing an important role in this crisis. The ability for organizations to access scalable, dependable, and highly secure computing powerâ€”whether for vital healthcare work, to help students continue learning, or to keep unprecedented numbers of employees online and productive from homeâ€”is critical in this situation. Hospital networks, pharmaceutical companies, and research labs are using AWS to care for patients, explore treatments, and mitigate the impacts of COVID-19 in many other ways. Academic institutions around the world are transitioning from in-person to virtual classrooms and are running on AWS to help ensure continuity of learning. And governments are leveraging AWS as a secure platform to build out new capabilities in their efforts to end this pandemic.\n",
      "\n",
      "Chunk 2 Location:  {'s3Location': {'uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2019-Shareholder-Letter.pdf'}, 'type': 'S3'}\n",
      "\n",
      "Chunk 2 Score:  0.50635225\n",
      "\n",
      "Chunk 2 Metadata:  {'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2019-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AxODuuJABIId83FjgouLY', 'x-amz-bedrock-kb-data-source-id': 'EWVX76BDR0'}\n",
      "\n",
      "Chunk 3:  However, not surprisingly, with that rate and scale of change, there was a lot of optimization needed to yield the intended productivity. Over the last several months, weâ€™ve scrutinized every process path in our fulfillment centers and transportation network and redesigned scores of processes and mechanisms, resulting in steady productivity gains and cost reductions over the last few quarters. Thereâ€™s more work to do, but weâ€™re pleased with our trajectory and the meaningful upside in front of us.   We also took this occasion to make larger structural changes that set us up better to deliver lower costs and faster speed for many years to come. A good example was reevaluating how our US fulfillment network was organized. Until recently, Amazon operated one national US fulfillment network that distributed inventory from fulfillment centers spread across the entire country. If a local fulfillment center didnâ€™t have the product a customer ordered, weâ€™d end up shipping it from other parts of the country, costing us more and increasing delivery times. This challenge became more pronounced as our fulfillment network expanded to hundreds of additional nodes over the last few years, distributing inventory across more locations and increasing the complexity of connecting the fulfillment center and delivery station nodes efficiently. Last year, we started rearchitecting our inventory placement strategy and leveraging our larger fulfillment center footprint to move from a national fulfillment network to a regionalized network model. We made significant internal changes (e.g. placement and logistics software, processes, physical operations) to create eight interconnected regions in smaller geographic areas. Each of these regions has broad, relevant selection to operate in a largely self- sufficient way, while still being able to ship nationally when necessary. Some of the most meaningful and hard        work came from optimizing the connections between this large amount of infrastructure. We also continue to improve our advanced machine learning algorithms to better predict what customers in various parts of the country will need so that we have the right inventory in the right regions at the right time. Weâ€™ve recently completed this regional roll out and like the early results. Shorter travel distances mean lower cost to serve, less impact on the environment, and customers getting their orders faster. On the latter, weâ€™re excited about seeing more next day and same-day deliveries, and weâ€™re on track to have our fastest Prime delivery speeds ever in 2023. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.\n",
      "\n",
      "Chunk 3 Location:  {'s3Location': {'uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2022-Shareholder-Letter.pdf'}, 'type': 'S3'}\n",
      "\n",
      "Chunk 3 Score:  0.47177294\n",
      "\n",
      "Chunk 3 Metadata:  {'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2022-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AmT7uuJABGPVBEv-OoP1g', 'x-amz-bedrock-kb-data-source-id': 'EWVX76BDR0'}\n",
      "\n",
      "Chunk 4:  Typical single-company data centers operate at roughly 18% server utilization. They need that excess capacity to handle large usage spikes. AWS benefits from multi- tenant usage patterns and operates at far higher server utilization rates. In addition, AWS has been successful in increasing the energy efficiency of its facilities and equipment, for instance by using more efficient evaporative cooling in certain data centers instead of traditional air conditioning. A study by 451 Research found that AWSâ€™s infrastructure is 3.6 times more energy efficient than the median U.S. enterprise data center surveyed. Along with our use of renewable energy, these factors enable AWS to do the same tasks as traditional data centers with an 88% lower carbon footprint. And donâ€™t think weâ€™re not going to get those last 12 pointsâ€”weâ€™ll make AWS 100% carbon free through more investments in renewable energy projects.   Leveraging scale for good   Over the last decade, no company has created more jobs than Amazon. Amazon directly employs 840,000 workers worldwide, including over 590,000 in the U.S., 115,000 in Europe, and 95,000 in Asia. In total, Amazon directly and indirectly supports 2 million jobs in the U.S., including 680,000-plus jobs created by Amazonâ€™s investments in areas like construction, logistics, and professional services, plus another 830,000 jobs created by small and medium-sized businesses selling on Amazon. Globally, we support nearly 4 million jobs. We are especially proud of the fact that many of these are entry-level jobs that give people their first opportunity to participate in the workforce.   And Amazonâ€™s jobs come with an industry-leading $15 minimum wage and comprehensive benefits. More than 40 million Americansâ€”many making the federal minimum wage of $7.25 an hourâ€”earn less than the lowest- paid Amazon associate. When we raised our starting minimum wage to $15 an hour in 2018, it had an immediate and meaningful impact on the hundreds of thousands of people working in our fulfillment centers. We want other big employers to join us by raising their own minimum pay rates, and we continue to lobby for a $15 federal minimum wage.        We want to improve workersâ€™ lives beyond pay. Amazon provides every full-time employee with health insurance, a 401(k) plan, 20 weeks paid maternity leave, and other benefits. These are the same benefits that Amazonâ€™s most senior executives receive.\n",
      "\n",
      "Chunk 4 Location:  {'s3Location': {'uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2019-Shareholder-Letter.pdf'}, 'type': 'S3'}\n",
      "\n",
      "Chunk 4 Score:  0.4686333\n",
      "\n",
      "Chunk 4 Metadata:  {'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2019-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AyeDuuJABIId83FjgouLY', 'x-amz-bedrock-kb-data-source-id': 'EWVX76BDR0'}\n",
      "\n",
      "Chunk 5:  We continued to increase compensation such that our average starting hourly salary is currently over $18. Along with this compensation, we offer very robust benefits, including full health insurance, a 401K plan, up to 20 weeks of parental leave, and full tuition coverage for associates who want to get a college education (whether they remain with us or not). Weâ€™re not close to being done in how we improve the lives of our employees. Weâ€™ve researched and created a list of what we believe are the top 100 employee experience pain points and are systematically solving them. Weâ€™re also passionate about further improving safety in our fulfillment network, with a focus on reducing strains, sprains, falls, and repetitive stress injuries. Our injury rates are sometimes misunderstood. We have operations jobs that fit both the â€œwarehousingâ€ and â€œcourier and deliveryâ€ categories. In the last U.S. public numbers, our recordable incident rates were a little higher than the average of our warehousing peers (6.4 vs. 5.5), and a little lower than the average of our courier and delivery peers (7.6 vs. 9.1). This makes us about average relative to peers, but we donâ€™t seek to be average. We want to be best in class. When I first started in my new role, I spent significant time in our fulfillment centers and with our safety team, and hoped there might be a silver bullet that could change the numbers quickly. I didnâ€™t find that. At our scale (we hired over 300,000 people in 2021 alone, many of whom were new to this sort of work and needed training), it takes rigorous analysis, thoughtful problem-solving, and a willingness to invent to get to where you want. Weâ€™ve been dissecting every process path to discern how we can further improve. We have a variety of programs in flight (e.g. rotational programs that help employees avoid spending too much time doing the same repetitive motions, wearables that prompt employees when        theyâ€™re moving in a dangerous way, improved shoes to provide better toe protection, training programs on body mechanics, wellness, and safety practices). But, we still have a ways to go, and weâ€™ll approach it like we do other customer experiencesâ€”weâ€™ll keep learning, inventing, and iterating until we have more transformational results. We wonâ€™t be satisfied until we do.   Similarly, at our scale, we have a significant carbon footprint.\n",
      "\n",
      "Chunk 5 Location:  {'s3Location': {'uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2021-Shareholder-Letter.pdf'}, 'type': 'S3'}\n",
      "\n",
      "Chunk 5 Score:  0.44738245\n",
      "\n",
      "Chunk 5 Metadata:  {'x-amz-bedrock-kb-source-uri': 's3://bedrock-kb-us-east-1-017444429555-1/AMZN-2021-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3Acz7uuJABGPVBEv-Okf27', 'x-amz-bedrock-kb-data-source-id': 'EWVX76BDR0'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        \"text\": \"How many new positions were opened across Amazon's fulfillment and delivery network?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def response_print(retrieve_resp):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n",
    "\n",
    "response_print(response_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Please make sure to comment the below section if you are planning to use the Knowledge Base that you created above for building your RAG application. If you only wanted to try out creating the KB using SDK, then please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '827901b6-cc54-4246-a78e-034d89ad18d3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '827901b6-cc54-4246-a78e-034d89ad18d3',\n",
       "   'date': 'Tue, 16 Jul 2024 00:27:03 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Delete KnowledgeBase\n",
    "for idx, ds in enumerate(data_sources_list):\n",
    "    bedrock_agent_client.delete_data_source(dataSourceId = ds[\"dataSourceId\"], knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "oss_client.indices.delete(index=index_name)\n",
    "aoss_client.delete_collection(id=collection_id)\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchEntityException",
     "evalue": "An error occurred (NoSuchEntity) when calling the DetachRolePolicy operation: Policy arn:aws:iam::017444429555:policy/AmazonBedrockSecretPolicyForKnowledgeBase_407 was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchEntityException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# delete role and policies\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delete_iam_role_and_policies\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdelete_iam_role_and_policies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Development/WWSO/Bedrock/forked/amazon-bedrock-samples/knowledge-bases/01-rag-concepts/utility.py:218\u001b[0m, in \u001b[0;36mdelete_iam_role_and_policies\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m iam_client\u001b[38;5;241m.\u001b[39mdetach_role_policy(\n\u001b[1;32m    211\u001b[0m     RoleName\u001b[38;5;241m=\u001b[39mbedrock_execution_role_name,\n\u001b[1;32m    212\u001b[0m     PolicyArn\u001b[38;5;241m=\u001b[39mfm_policy_arn\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    214\u001b[0m iam_client\u001b[38;5;241m.\u001b[39mdetach_role_policy(\n\u001b[1;32m    215\u001b[0m     RoleName\u001b[38;5;241m=\u001b[39mbedrock_execution_role_name,\n\u001b[1;32m    216\u001b[0m     PolicyArn\u001b[38;5;241m=\u001b[39moss_policy_arn\n\u001b[1;32m    217\u001b[0m )\n\u001b[0;32m--> 218\u001b[0m \u001b[43miam_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach_role_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRoleName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbedrock_execution_role_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPolicyArn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msm_policy_arn\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m iam_client\u001b[38;5;241m.\u001b[39mdelete_role(RoleName\u001b[38;5;241m=\u001b[39mbedrock_execution_role_name)\n\u001b[1;32m    223\u001b[0m iam_client\u001b[38;5;241m.\u001b[39mdelete_policy(PolicyArn\u001b[38;5;241m=\u001b[39ms3_policy_arn)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bedrock_env/lib/python3.10/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchEntityException\u001b[0m: An error occurred (NoSuchEntity) when calling the DetachRolePolicy operation: Policy arn:aws:iam::017444429555:policy/AmazonBedrockSecretPolicyForKnowledgeBase_407 was not found."
     ]
    }
   ],
   "source": [
    "# delete role and policies\n",
    "from utility import delete_iam_role_and_policies\n",
    "delete_iam_role_and_policies()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
