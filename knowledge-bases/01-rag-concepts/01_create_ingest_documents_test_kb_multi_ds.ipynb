{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock - End to end example using multiple s3 bucket(s) as data source(s)\n",
    "\n",
    "This notebook provides sample code for building an empty OpenSearch Serverless (OSS) index, Amazon Bedrock knowledge base and ingest documents into the index from multiple data sources. Now you can add multiple data sources with a Knowledge base.\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "A data pipeline that ingests documents (typically stored in multiple Amazon S3) into a knowledge base i.e. a vector database such as Amazon OpenSearch Service Serverless (AOSS) so that it is available for lookup when a question is received.\n",
    "\n",
    "- Load the documents into the knowledge base by connecting your multiple s3 bucket (multiple data source). \n",
    "- Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Steps: \n",
    "- Create Amazon Bedrock Knowledge Base execution role with necessary policies for accessing data from multiple S3 and writing embeddings into OSS.\n",
    "- Create an empty OpenSearch serverless index.\n",
    "- Download documents\n",
    "- Create Amazon Bedrock knowledge base\n",
    "- Create data source(s) within knowledge base which will connect to corresponding Amazon S3\n",
    "- For each data source, start ingestion jobs using KB APIs which will read data from s3, chunk it, convert chunks into embeddings using Amazon Titan Embeddings model and then store these embeddings in AOSS. All of this without having to build, deploy and manage the data pipeline.\n",
    "\n",
    "Once the data is available in the Bedrock Knowledge Base then a question answering application can be built using the Knowledge Base APIs provided by Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "- create and delete Amazon IAM roles\n",
    "- create, update and delete Amazon S3 buckets\n",
    "- access Amazon Bedrock\n",
    "- access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "- IAMFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "- Custom policy for Amazon OpenSearch Serverless such as:\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to enable `Anthropic Claude 3` and `Anthropic Claude Instant` model access in Amazon Bedrock Console, as the notebook will use Anthropic Claude 3 Sonnet and Claude instant models for testing the knowledge base once its created.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.34.85\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_bedrock_execution_role_multi_ds, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss, interactive_sleep\n",
    "import random\n",
    "from retrying import retry\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "\n",
    "# Now you can add multiple data sources to a Knowledge Base. For this notebook, we'll test Knowledge Base creation with TWO different data sources\n",
    "bucket_name_1 = f'bedrock-kb-{s3_suffix}-1' # replace it with your first bucket name.\n",
    "bucket_name_2 = f'bedrock-kb-{s3_suffix}-2' # replace it with your second bucket name.\n",
    "\n",
    "# NOTE: You can have only one bucket name in below list if you don't want to use multiple data sources for Knowledge Base creation.\n",
    "bucket_names=[bucket_name_1,bucket_name_2]\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket bedrock-kb-us-east-1-017444429555-1 Exists\n",
      "Bucket bedrock-kb-us-east-1-017444429555-2 Exists\n"
     ]
    }
   ],
   "source": [
    "# Check if bucket exists, and if not create S3 bucket for knowledge base data source\n",
    "for bucket_name in bucket_names:\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f'Bucket {bucket_name} Exists')\n",
    "    except ClientError as e:\n",
    "        print(f'Creating bucket {bucket_name}')\n",
    "        s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            # CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "Firt of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role_multi_ds(bucket_names=bucket_names)\n",
    "\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Mon, 10 Jun 2024 16:59:36 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': 'da95e5f4-9088-48cf-92dc-ee17551f9d99'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': 'da95e5f4-9088-48cf-92dc-ee17551f9d99',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-east-1:017444429555:collection/dlukh8g4zwzhq8e5mmq3',\n",
      "                              'createdDate': 1718038776199,\n",
      "                              'id': 'dlukh8g4zwzhq8e5mmq3',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1718038776199,\n",
      "                              'name': 'bedrock-sample-rag-867',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlukh8g4zwzhq8e5mmq3.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Get the OpenSearch serverless collection URL\n",
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "Done!.........................\n",
      "\n",
      "Collection successfully created:\n",
      "[ { 'arn': 'arn:aws:aoss:us-east-1:017444429555:collection/dlukh8g4zwzhq8e5mmq3',\n",
      "    'collectionEndpoint': 'https://dlukh8g4zwzhq8e5mmq3.us-east-1.aoss.amazonaws.com',\n",
      "    'createdDate': 1718038776199,\n",
      "    'dashboardEndpoint': 'https://dlukh8g4zwzhq8e5mmq3.us-east-1.aoss.amazonaws.com/_dashboards',\n",
      "    'id': 'dlukh8g4zwzhq8e5mmq3',\n",
      "    'kmsKeyArn': 'auto',\n",
      "    'lastModifiedDate': 1718038800956,\n",
      "    'name': 'bedrock-sample-rag-867',\n",
      "    'standbyReplicas': 'ENABLED',\n",
      "    'status': 'ACTIVE',\n",
      "    'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# wait for collection creation\n",
    "# This can take couple of minutes to finish\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    interactive_sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "pp.pprint(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::017444429555:policy/AmazonBedrockOSSPolicyForKnowledgeBase_564\n",
      "Done!.......................................................\n"
     ]
    }
   ],
   "source": [
    "# create opensearch serverless access policy and attach it to Bedrock execution role\n",
    "try:\n",
    "    create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                    bedrock_kb_execution_role=bedrock_kb_execution_role)\n",
    "    # It can take up to a minute for data access rules to be enforced\n",
    "    interactive_sleep(60)\n",
    "except Exception as e:\n",
    "    print(\"Policy already exists\")\n",
    "    pp.pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector index in Opensearch serverless, with the knn_vector field index mapping, specifying the dimension size, name and engine.\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\",\n",
    "                 \"space_type\": \"l2\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{ 'acknowledged': True,\n",
      "  'index': 'bedrock-sample-index-867',\n",
      "  'shards_acknowledged': True}\n",
      "Done!.......................................................\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "try:\n",
    "    response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "    print('\\nCreating index:')\n",
    "    pp.pprint(response)\n",
    "\n",
    "    # index creation can take up to a minute\n",
    "    interactive_sleep(60)\n",
    "except RequestError as e:\n",
    "    # you can delete the index if its already exists\n",
    "    # oss_client.indices.delete(index=index_name)\n",
    "    print(f'Error while trying to create the index, with error {e.error}\\nyou may unmark the delete above to delete, and recreate the index')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to ingest into our knowledge base.\n",
    "We'll use the following data:\n",
    " - sythetic data stored in a local directory as first data source\n",
    " - download data from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## First datasource - copied from local directory\n",
    "\n",
    "!mkdir -p ./data_1\n",
    "\n",
    "import shutil\n",
    "\n",
    "filenames = [\"./sythetic_data/octank_financial_10K.pdf\"] #source files\n",
    "data_root_1 = \"./data_1/\" # destination directory\n",
    "\n",
    "for item in filenames:\n",
    "    filename = os.path.basename(item)\n",
    "    shutil.copyfile(item, os.path.join(data_root_1, filename))\n",
    "\n",
    "\n",
    "# You can comment below lines if you don't want to use multiple data sources\n",
    "## Second datasource - copied from URLs\n",
    "!mkdir -p ./data_2\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "# source urls\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'AMZN-2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "data_root_2 = \"./data_2/\" # destination directory\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root_2 + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 Bucket data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Upload data to s3 to the bucket that was configured as a data source to the knowledge base\n",
    "s3_client = boto3.client(\"s3\")\n",
    "def uploadDirectory(path,bucket_name):\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for file in files:\n",
    "                s3_client.upload_file(os.path.join(root,file),bucket_name,file)\n",
    "\n",
    "uploadDirectory(data_root_1, bucket_name_1)\n",
    "\n",
    "# You can comment below line if you don't want to use multiple data sources\n",
    "uploadDirectory(data_root_2, bucket_name_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# The embedding model used by Bedrock to embed ingested documents, and realtime prompts\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Amazon shareholder letter knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 6, 10, 17, 2, 21, 648344, tzinfo=tzutc()),\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:017444429555:knowledge-base/OI4NXPPZYY',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'OI4NXPPZYY',\n",
      "  'name': 'bedrock-sample-knowledge-base-867',\n",
      "  'roleArn': 'arn:aws:iam::017444429555:role/AmazonBedrockExecutionRoleForKnowledgeBase_564',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-east-1:017444429555:collection/dlukh8g4zwzhq8e5mmq3',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-867'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 21, 648344, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create data source(s), which will be associated with the knowledge base created above. Once the data source(s) is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Source(s)\n",
    "Steps:\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create KB\n",
    "def create_ds(bucket_names):\n",
    "    ds_list=[]\n",
    "    for bucket_name in bucket_names:\n",
    "        # Ingest strategy - How to ingest data from the data source\n",
    "        chunkingStrategyConfiguration = {\n",
    "            \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "            \"fixedSizeChunkingConfiguration\": {\n",
    "                \"maxTokens\": 512,\n",
    "                \"overlapPercentage\": 20\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "        s3Configuration = {\n",
    "            \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "            # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "            }\n",
    "        # Create a DataSource in KnowledgeBase \n",
    "        create_ds_response = bedrock_agent_client.create_data_source(\n",
    "            name = f'{name}-{bucket_name}',\n",
    "            description = description,\n",
    "            knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "            dataSourceConfiguration = {\n",
    "                \"type\": \"S3\",\n",
    "                \"s3Configuration\":s3Configuration\n",
    "            },\n",
    "            vectorIngestionConfiguration = {\n",
    "                \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "            }\n",
    "        )\n",
    "        ds = create_ds_response[\"dataSource\"]\n",
    "        pp.pprint(ds)\n",
    "        ds_list.append(ds)\n",
    "    return ds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 378506, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-1'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'WV27YM9JYS',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'OI4NXPPZYY',\n",
      "  'name': 'bedrock-sample-knowledge-base-867-bedrock-kb-us-east-1-017444429555-1',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 378506, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n",
      "{ 'createdAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 577420, tzinfo=tzutc()),\n",
      "  'dataDeletionPolicy': 'DELETE',\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-2'},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'S9PWWGRGEX',\n",
      "  'description': 'Amazon shareholder letter knowledge base.',\n",
      "  'knowledgeBaseId': 'OI4NXPPZYY',\n",
      "  'name': 'bedrock-sample-knowledge-base-867-bedrock-kb-us-east-1-017444429555-2',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 577420, tzinfo=tzutc()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'FIXED_SIZE',\n",
      "                                                               'fixedSizeChunkingConfiguration': { 'maxTokens': 512,\n",
      "                                                                                                   'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "data_sources = create_ds(bucket_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '5867b5a8-4bfe-4685-9dde-8651464d1fc5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 10 Jun 2024 17:02:22 GMT', 'content-type': 'application/json', 'content-length': '641', 'connection': 'keep-alive', 'x-amzn-requestid': '5867b5a8-4bfe-4685-9dde-8651464d1fc5', 'x-amz-apigw-id': 'ZKSw1HzIoAMEO4w=', 'x-amzn-trace-id': 'Root=1-6667319e-70f8eb1f5b6d7ac733c0aefc'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 378506, tzinfo=tzutc()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-1'}, 'type': 'S3'}, 'dataSourceId': 'WV27YM9JYS', 'description': 'Amazon shareholder letter knowledge base.', 'knowledgeBaseId': 'OI4NXPPZYY', 'name': 'bedrock-sample-knowledge-base-867-bedrock-kb-us-east-1-017444429555-1', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 378506, tzinfo=tzutc()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE', 'fixedSizeChunkingConfiguration': {'maxTokens': 512, 'overlapPercentage': 20}}}}}\n",
      " \n",
      "{'ResponseMetadata': {'RequestId': 'ac7791dd-a071-4cf6-84cd-78371767cfe2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 10 Jun 2024 17:02:22 GMT', 'content-type': 'application/json', 'content-length': '641', 'connection': 'keep-alive', 'x-amzn-requestid': 'ac7791dd-a071-4cf6-84cd-78371767cfe2', 'x-amz-apigw-id': 'ZKSw3GDBIAMEvhA=', 'x-amzn-trace-id': 'Root=1-6667319e-74a744536ecac3d53877b8f3'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 577420, tzinfo=tzutc()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::bedrock-kb-us-east-1-017444429555-2'}, 'type': 'S3'}, 'dataSourceId': 'S9PWWGRGEX', 'description': 'Amazon shareholder letter knowledge base.', 'knowledgeBaseId': 'OI4NXPPZYY', 'name': 'bedrock-sample-knowledge-base-867-bedrock-kb-us-east-1-017444429555-2', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 22, 577420, tzinfo=tzutc()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE', 'fixedSizeChunkingConfiguration': {'maxTokens': 512, 'overlapPercentage': 20}}}}}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Get DataSource \n",
    "for idx, ds in enumerate(data_sources):\n",
    "    print(bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"]))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source(s) created, we can start the ingestion job for each data source.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 0 started successfully\n",
      "\n",
      "{ 'dataSourceId': 'WV27YM9JYS',\n",
      "  'failureReasons': [ '[\"Encountered error: Ignored 4 files as their file '\n",
      "                      'format was not supported. [Files: .DS_Store, '\n",
      "                      'lambda_function.zip, jeep.jpg, car-listings.zip]. Call '\n",
      "                      'to Amazon S3 Source did not succeed.\"]'],\n",
      "  'ingestionJobId': 'APTNMSAM9K',\n",
      "  'knowledgeBaseId': 'OI4NXPPZYY',\n",
      "  'startedAt': datetime.datetime(2024, 6, 10, 17, 2, 24, 658955, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 4,\n",
      "                  'numberOfDocumentsScanned': 9,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 5},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 6, 10, 17, 2, 58, 665644, tzinfo=tzutc())}\n",
      "Done!...................................\n",
      "job 1 started successfully\n",
      "\n",
      "{ 'dataSourceId': 'S9PWWGRGEX',\n",
      "  'ingestionJobId': 'PTLZ55C3OI',\n",
      "  'knowledgeBaseId': 'OI4NXPPZYY',\n",
      "  'startedAt': datetime.datetime(2024, 6, 10, 17, 3, 39, 346201, tzinfo=tzutc()),\n",
      "  'statistics': { 'numberOfDocumentsDeleted': 0,\n",
      "                  'numberOfDocumentsFailed': 0,\n",
      "                  'numberOfDocumentsScanned': 4,\n",
      "                  'numberOfMetadataDocumentsModified': 0,\n",
      "                  'numberOfMetadataDocumentsScanned': 0,\n",
      "                  'numberOfModifiedDocumentsIndexed': 0,\n",
      "                  'numberOfNewDocumentsIndexed': 4},\n",
      "  'status': 'COMPLETE',\n",
      "  'updatedAt': datetime.datetime(2024, 6, 10, 17, 4, 3, 118623, tzinfo=tzutc())}\n",
      "Done!...................................\n"
     ]
    }
   ],
   "source": [
    "ingest_jobs=[]\n",
    "# Start an ingestion job\n",
    "for idx, ds in enumerate(data_sources):\n",
    "    try:\n",
    "        start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "        job = start_job_response[\"ingestionJob\"]\n",
    "        print(f\"job {idx} started successfully\\n\")\n",
    "        \n",
    "        while(job['status']!='COMPLETE' ):\n",
    "            get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "              knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "                dataSourceId = ds[\"dataSourceId\"],\n",
    "                ingestionJobId = job[\"ingestionJobId\"]\n",
    "          )\n",
    "            job = get_job_response[\"ingestionJob\"]\n",
    "        pp.pprint(job)\n",
    "        interactive_sleep(40)\n",
    "\n",
    "        ingest_jobs.append(job)\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't start {idx} job.\\n\")\n",
    "        print(e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'OI4NXPPZYY'\n"
     ]
    }
   ],
   "source": [
    "# Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Note: If you plan to run any following notebooks, you can skip this section\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "# Lets see how different Anthropic models responds to the input text we provide\n",
    "claude_model_ids = [ [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Generated using Claude 3 Sonnet:\n",
      "('Amazon has been working on large language models (LLMs) for generative AI '\n",
      " 'for some time. The company believes generative AI will transform and improve '\n",
      " 'virtually every customer experience, and is investing substantially in these '\n",
      " 'models across its consumer, seller, brand, and creator experiences. Amazon '\n",
      " 'is also democratizing this technology through AWS so that companies of all '\n",
      " 'sizes can leverage generative AI. AWS is offering machine learning chips '\n",
      " 'like Trainium and Inferentia that provide high performance and low cost for '\n",
      " 'training and running LLMs in production. AWS also enables companies to '\n",
      " \"choose from various LLMs and build applications with AWS's security, privacy \"\n",
      " 'and other features. One example application is AWS CodeWhisperer, which '\n",
      " 'generates code suggestions in real-time to boost developer productivity.')\n",
      "---------- The citations for the response generated by Claude 3 Sonnet:\n",
      "[ 'This shift was driven by several factors, including access to higher '\n",
      "  'volumes of compute capacity at lower prices than was ever available. Amazon '\n",
      "  'has been using machine learning extensively for 25 years, employing it in '\n",
      "  'everything from personalized ecommerce recommendations, to fulfillment '\n",
      "  'center pick paths, to drones for Prime Air, to Alexa, to the many machine '\n",
      "  'learning services AWS offers (where AWS has the broadest machine learning '\n",
      "  'functionality and customer base of any cloud provider). More recently, a '\n",
      "  'newer form of machine learning, called Generative AI, has burst onto the '\n",
      "  'scene and promises to significantly accelerate machine learning adoption. '\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%).',\n",
      "  'This shift was driven by several factors, including access to higher '\n",
      "  'volumes of compute capacity at lower prices than was ever available. Amazon '\n",
      "  'has been using machine learning extensively for 25 years, employing it in '\n",
      "  'everything from personalized ecommerce recommendations, to fulfillment '\n",
      "  'center pick paths, to drones for Prime Air, to Alexa, to the many machine '\n",
      "  'learning services AWS offers (where AWS has the broadest machine learning '\n",
      "  'functionality and customer base of any cloud provider). More recently, a '\n",
      "  'newer form of machine learning, called Generative AI, has burst onto the '\n",
      "  'scene and promises to significantly accelerate machine learning adoption. '\n",
      "  'Generative AI is based on very Large Language Models (trained on up to '\n",
      "  'hundreds of billions of parameters, and growing), across expansive '\n",
      "  'datasets, and has radically general and broad recall and learning '\n",
      "  'capabilities. We have been working on our own LLMs for a while now, believe '\n",
      "  'it will transform and improve virtually every customer experience, and will '\n",
      "  'continue to invest substantially in these models across all of our '\n",
      "  'consumer, seller, brand, and creator experiences. Additionally, as we’ve '\n",
      "  'done for years in AWS, we’re democratizing this technology so companies of '\n",
      "  'all sizes can leverage Generative AI. AWS is offering the most '\n",
      "  'price-performant machine learning chips in Trainium and Inferentia so small '\n",
      "  'and large companies can afford to train and run their LLMs in production. '\n",
      "  'We enable companies to choose from various LLMs and build applications with '\n",
      "  'all of the AWS security, privacy and other features that customers are '\n",
      "  'accustomed to using. And, we’re delivering applications like AWS’s '\n",
      "  'CodeWhisperer, which revolutionizes        developer productivity by '\n",
      "  'generating code suggestions in real time. I could write an entire letter on '\n",
      "  'LLMs and Generative AI as I think they will be that transformative, but '\n",
      "  'I’ll leave that for a future letter. Let’s just say that LLMs and '\n",
      "  'Generative AI are going to be a big deal for customers, our shareholders, '\n",
      "  'and Amazon.   So, in closing, I’m optimistic that we’ll emerge from this '\n",
      "  'challenging macroeconomic time in a stronger position than when we entered '\n",
      "  'it. There are several reasons for it and I’ve mentioned many of them above. '\n",
      "  'But, there are two relatively simple statistics that underline our immense '\n",
      "  'future opportunity. While we have a consumer business that’s $434B in 2022, '\n",
      "  'the vast majority of total market segment share in global retail still '\n",
      "  'resides in physical stores (roughly 80%).']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id[1]}'\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# retrieve api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pp.pprint(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Please make sure to comment the below section if you are planning to use the Knowledge Base that you created above for building your RAG application. If you only wanted to try out creating the KB using SDK, then please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Delete KnowledgeBase\n",
    "for idx, ds in enumerate(data_sources):\n",
    "    bedrock_agent_client.delete_data_source(dataSourceId = ds[\"dataSourceId\"], knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "oss_client.indices.delete(index=index_name)\n",
    "aoss_client.delete_collection(id=collection_id)\n",
    "aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete role and policies\n",
    "from utility import delete_iam_role_and_policies\n",
    "delete_iam_role_and_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
