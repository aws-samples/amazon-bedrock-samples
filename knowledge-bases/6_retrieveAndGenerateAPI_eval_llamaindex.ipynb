{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrieveAndGenerate API with different model offerings from Bedrock - evaluating on LLaMa Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting boto3\n",
      "  Using cached boto3-1.33.5-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.34.0,>=1.33.5 (from boto3)\n",
      "  Using cached botocore-1.33.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3)\n",
      "  Using cached s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.34.0,>=1.33.5->boto3)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore<1.34.0,>=1.33.5->boto3)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.5->boto3)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached boto3-1.33.5-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.33.5-py3-none-any.whl (11.8 MB)\n",
      "Using cached s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.33.5\n",
      "    Uninstalling botocore-1.33.5:\n",
      "      Successfully uninstalled botocore-1.33.5\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.8.2\n",
      "    Uninstalling s3transfer-0.8.2:\n",
      "      Successfully uninstalled s3transfer-0.8.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.33.5\n",
      "    Uninstalling boto3-1.33.5:\n",
      "      Successfully uninstalled boto3-1.33.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.5 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.33.5 botocore-1.33.5 jmespath-1.0.1 python-dateutil-2.8.2 s3transfer-0.8.2 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting botocore\n",
      "  Using cached botocore-1.33.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached botocore-1.33.5-py3-none-any.whl (11.8 MB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, six, jmespath, python-dateutil, botocore\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.33.5\n",
      "    Uninstalling botocore-1.33.5:\n",
      "      Successfully uninstalled botocore-1.33.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.5 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.33.5 jmespath-1.0.1 python-dateutil-2.8.2 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting botocore\n",
      "  Using cached botocore-1.33.5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<2.1,>=1.25.4 (from botocore)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached botocore-1.33.5-py3-none-any.whl (11.8 MB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, six, jmespath, python-dateutil, botocore\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.33.5\n",
      "    Uninstalling botocore-1.33.5:\n",
      "      Successfully uninstalled botocore-1.33.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.5 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.0.7 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.33.5 jmespath-1.0.1 python-dateutil-2.8.2 six-1.16.0 urllib3-2.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "awscli 1.29.63 requires botocore==1.31.63, but you have botocore 1.33.5 which is incompatible.\n",
      "awscli 1.29.63 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "botocore 1.33.5 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.3.3 which is incompatible.\n",
      "hdijupyterutils 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.3 which is incompatible.\n",
      "jupyterlab 3.4.4 requires jupyter-server~=1.16, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "jupyterlab-server 2.10.3 requires jupyter-server~=1.4, but you have jupyter-server 2.7.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires dataclasses-json<0.6.0,>=0.5.7, but you have dataclasses-json 0.6.3 which is incompatible.\n",
      "llama-index 0.9.3.post1 requires urllib3<2, but you have urllib3 2.1.0 which is incompatible.\n",
      "notebook 6.5.6 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.4.0 which is incompatible.\n",
      "notebook 6.5.6 requires pyzmq<25,>=17, but you have pyzmq 25.1.1 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.0 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.8 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.16.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires ipython<8,>=7.31.1; python_version >= \"3\", but you have ipython 8.16.1 which is incompatible.\n",
      "spyder-kernels 2.3.3 requires jupyter-client<8,>=7.3.4; python_version >= \"3\", but you have jupyter-client 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install knowledge base sdk\n",
    "%pip install --upgrade pip\n",
    "%pip install boto3 --force-reinstall\n",
    "%pip install botocore --force-reinstall\n",
    "%pip install botocore --force-reinstall\n",
    "%pip install langchain --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow the steps below to initiate hte bedrock client:\n",
    "\n",
    "1. Import the necessary libraries, along with langchain for bedrock model selection, llama index to store the service context containing the llm and embedding model instances.\n",
    "\n",
    "2. Use langchain to import bedrock embeddings and llama index for langchain embeddings\n",
    "\n",
    "3. Configure the bedrock-runtime and the bedrock-agent-runtime to be able to initiate execution with the knowledge base associated to your account toe perform RAG and model evaluation using llama index.\n",
    "\n",
    "4. Use the amazon.titan-embed-text-v1 as our embeddings model for chunk enbeddings during the RAG performance on user queries.\n",
    "\n",
    "5. Initialize 'anthropic.claude-v2' as our large language model to perform query completions on using the RAG with the given knowledge base, once we get all vector searches through the retrieve API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    set_global_service_context\n",
    ")\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              # endpoint_url=endpoint_url,\n",
    "                              region_name='us-east-1',\n",
    "                              config=bedrock_config)\n",
    "                              # aws_access_key_id=ACCESS_KEY,\n",
    "                              # aws_secret_access_key=SECRET_KEY)\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 3000\n",
    "}\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")\n",
    ")\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\",\n",
    "              model_kwargs=model_kwargs_claude,\n",
    "              client = bedrock_client,)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm,\n",
    "                                               embed_model=embed_model)\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrieveAndGenerate API: Process flow  - first evaluating retrieve API chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask in-context question. \n",
    "kb_id =  ''# replace it with the Knowledge base id which you created in the first half of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon SageMaker is a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.\n"
     ]
    }
   ],
   "source": [
    "def retrieveAndGenerate(input, kbId):\n",
    "    return bedrock_agent_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': input\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "response = retrieveAndGenerate(\"What is Amazon Sagemaker?\", kb_id)[\"output\"][\"text\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize your Knowledge base id before querying responses from the initialized LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '62abb249-9211-4d12-9b2d-16e267adf2dd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 01 Dec 2023 20:05:47 GMT', 'content-type': 'application/json', 'content-length': '2205', 'connection': 'keep-alive', 'x-amzn-requestid': '62abb249-9211-4d12-9b2d-16e267adf2dd'}, 'RetryAttempts': 0}, 'sessionId': '22c5ca4c-672f-49fc-b4e1-69cc6b57346a', 'output': {'text': 'Amazon SageMaker is a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.'}, 'citations': [{'generatedResponsePart': {'textResponsePart': {'text': 'Amazon SageMaker is a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.', 'span': {'start': 0, 'end': 171}}}, 'retrievedReferences': [{'content': {'text': \"Then, you need to tune the model so it delivers the best possible predictions, which is often a tedious and manual effort. After youâ€™ve developed a fully trained model, you need to integrate the model with your application and deploy this application on infrastructure that will scale. All of this takes a lot of specialized expertise, access to large amounts of compute and storage, and a lot of time to experiment and optimize every part of the process. In the end, it's not a surprise that the whole thing feels out of reach for most developers.   SageMaker removes the complexity that holds back developer success with each of these steps. SageMaker includes modules that can be used together or independently to build, train, and deploy your machine learning models.   Amazon SageMaker Ground Truth Amazon SageMaker Ground Truth helps you build highly accurate training datasets for machine learning quickly. SageMaker Ground Truth offers easy access to public and private human labelers and provides them with built-in workflows and interfaces for common labeling tasks. Additionally, SageMaker Ground Truth can lower your labeling costs by up to 70% using automatic labeling, which works by training Ground Truth from data labeled by humans so that the service learns to label data independently.   Successful machine learning models are built on the shoulders of large volumes of high-quality training data. But, the process to create the training data necessary to build these models is often expensive, complicated, and time-consuming.\"}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://awsdocumentation2039/aws-overview.pdf'}}}]}]}\n"
     ]
    }
   ],
   "source": [
    "input = \"What is Amazon sagemaker?\"\n",
    "response = retrieveAndGenerate(input, kb_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can view the scores above to double down on which chunk retrieve is the most relevant to the information that you are trying to retreive on your given query!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering Phase: Engineer LLaMa-2-70b to personalize responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an advanced AI system specialized in Amazon Web Services (AWS), capable of providing detailed and accurate information about various AWS services. \n",
    "Use the available resources and knowledge to answer the question enclosed in <question> tags. \n",
    "If the answer to a question is not within your current scope of knowledge, please indicate that you don't know, and do not attempt to speculate or fabricate a response.\n",
    "<context>\n",
    "{context_str}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{query_str}\n",
    "</question>\n",
    "\n",
    "Your response should be precise, detailed, and include any relevant AWS-specific terminology, features, or concepts. Utilize your extensive knowledge base about AWS services to provide the most accurate and current information available.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "claude_prompt = PromptTemplate(template=PROMPT_TEMPLATE, \n",
    "                               input_variables=[input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer the Model to pick up the most relevant and accurate information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <response>\n",
      "Amazon SageMaker is a fully managed machine learning service on AWS. It enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. The key capabilities of Amazon SageMaker include:\n",
      "\n",
      "- Managed training - SageMaker provides machine learning algorithms and frameworks like TensorFlow, PyTorch, XGBoost, etc to train models using compute instances managed by SageMaker. \n",
      "\n",
      "- Managed model hosting - Trained models can be easily deployed to SageMaker hosting to get scalable, low latency inference endpoints. SageMaker manages the infrastructure and availability for model endpoints.\n",
      "\n",
      "- Notebook instances - SageMaker provides Jupyter notebook instances with pre-installed machine learning and data science libraries to explore and process data and train models. \n",
      "\n",
      "- Automated machine learning - SageMaker Autopilot can automate steps like data preprocessing, feature engineering, model tuning, selection and deployment to arrive at the best model.\n",
      "\n",
      "- Model monitoring - SageMaker includes tools to monitor model quality and drift to detect when a re-training is needed.\n",
      "\n",
      "- ML pipelines - SageMaker Pipelines simplifies creating end-to-end workflows for a machine learning process.\n",
      "\n",
      "In summary, Amazon SageMaker is a fully managed service to build, train, and deploy machine learning models quickly, at scale, and in a simplified manner.\n",
      "\n",
      "</response>\n"
     ]
    }
   ],
   "source": [
    "prompt = str(claude_prompt.format(context_str = response, query_str=input))\n",
    "responsefromllm = llm(prompt)\n",
    "print(responsefromllm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline: Utilizing LLaMaIndex for end-end evaluations on Faithfulness, Correctness, Guidelines given, and Relevancy of answers generated by the model - for RetrieveAndGenerate API\n",
    "\n",
    "- Faithfulness - to measure if the response from the model matches any source nodes. This is useful for measuring if the response was hallucinated.\n",
    "- Relevancy - to measure if the response + source nodes match the query.This is useful for measuring if the query was actually answered by the response.\n",
    "- Correctness - to evaluate the relevance and correctness of a generated answer against a reference answer.\n",
    "- Guidelines - to evaluate a question answer system given user specified guidelines for example, if the response generated is complete, not toxic, or biased or uses facts in the context.\n",
    "\n",
    "### 1. Faithfulness Evaluation of Prompt Completions: Using LLaMa Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' <response>\\nAmazon SageMaker is a fully managed machine learning service on AWS. It enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. The key capabilities of Amazon SageMaker include:\\n\\n- Managed training - SageMaker provides machine learning algorithms and frameworks like TensorFlow, PyTorch, XGBoost, etc to train models using compute instances managed by SageMaker. \\n\\n- Managed model hosting - Trained models can be easily deployed to SageMaker hosting to get scalable, low latency inference endpoints. SageMaker manages the infrastructure and availability for model endpoints.\\n\\n- Notebook instances - SageMaker provides Jupyter notebook instances with pre-installed machine learning and data science libraries to explore and process data and train models.\\n\\n- Automated machine learning - SageMaker Autopilot can automate steps like data preprocessing, feature engineering, model tuning, selection and deployment to arrive at the best model.\\n\\n- Model monitoring - SageMaker includes tools to monitor model quality and drift to detect when a re-training is needed. \\n\\n- ML pipelines - SageMaker Pipelines simplifies creating end-to-end workflows for a machine learning process.\\n\\nIn summary, Amazon SageMaker is a fully managed service to build, train, and deploy machine learning models quickly, at scale, and in a simplified manner.\\n\\n</response>']\n",
      "Faithful response?: True\n",
      "('Reason:  YES\\n'\n",
      " '\\n'\n",
      " 'The context supports the information provided. The context describes Amazon '\n",
      " 'SageMaker and its key capabilities as a fully managed machine learning '\n",
      " 'service on AWS for building, training, and deploying ML models. This aligns '\n",
      " 'with and supports the information statement. ')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "faith_eval = faithfulness_evaluator.evaluate(query=str(input),\n",
    "                                              response=str(responsefromllm), \n",
    "                                              contexts=[str(response)])\n",
    "\n",
    "print([str(response)])\n",
    "\n",
    "print(f\"Faithful response?: {str(faith_eval.passing)}\"  )\n",
    "pp.pprint(f\"Reason: {faith_eval.feedback} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### above, we can see how retrieveAndGenerate API works in a manner that is faithful where the response directly relates back to the query by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relevancy Evaluation of Prompt Completions: Using LLaMa Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant response?: True\n",
      "('Reason:  YES\\n'\n",
      " '\\n'\n",
      " 'The response provides an accurate and relevant overview of Amazon '\n",
      " \"SageMaker's key capabilities as a fully managed machine learning service on \"\n",
      " 'AWS. The details on managed training, hosting, notebook instances, automated '\n",
      " 'ML, model monitoring, and ML pipelines align with the context information. '\n",
      " 'Therefore, I would evaluate that the response is in line with the context '\n",
      " 'provided. ')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "relevant_eval = relevancy_evaluator.evaluate(query=str(input),\n",
    "                                              response=str(responsefromllm), \n",
    "                                              contexts=[str(response)])\n",
    "print(f'Relevant response?: {str(relevant_eval.passing)}')\n",
    "pp.pprint(f\"Reason: {relevant_eval.feedback} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we can see whatever we get from the KB is relevant and matches the query ingested by the Kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's test the same for a bunch of questions below for correction checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_question_answer_pair = [\n",
    "    (\"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "     \"AWS had a 29% year-over-year ('YoY') revenue growth in 2022 on a $62B revenue base.\"),\n",
    "\n",
    "    (\"Approximately how many new features and services did AWS launch in 2022?\",\n",
    "     \"AWS launched over 3,300 new features and services in 2022.\"),\n",
    "\n",
    "    (\"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver?\",\n",
    "     \"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"),\n",
    "\n",
    "    (\"Which was the first inference chip launched by AWS?\",\n",
    "     \"AWS launched their first inference chips ('Inferentia') in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"),\n",
    "\n",
    "    (\"What kind of throughput and latency improvements does the new Inferentia2 chip offer compared to the original Inferentia chip?\",\n",
    "     \"Inferentia2 chip, launched by AWS, offers up to four times higher throughput and ten times lower latency than the first Inferentia processor.\"),\n",
    "\n",
    "    (\"What are some of the key benefits of AWS's Inferentia and Inferentia2 chips?\",\n",
    "     \"AWS's Inferentia and Inferentia2 chips are known for their high throughput and low latency, significantly reducing capital expenses for companies using them.\"),\n",
    "\n",
    "    (\"How has the introduction of Graviton3 chips impacted AWS's computing capabilities?\",\n",
    "     \"The introduction of Graviton3 chips has significantly enhanced AWS's computing capabilities, offering a 25% performance improvement over the previous generation Graviton2 processors.\"),\n",
    "\n",
    "    (\"Can you describe the growth of AWS in terms of new service launches in 2022?\",\n",
    "     \"AWS saw considerable growth in 2022, marked by the launch of over 3,300 new features and services.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, LLaMa Index Evaluations with Claude-Instant Prompt Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setting the embeddings model \n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")\n",
    ")\n",
    "\n",
    "## Setting the claude instant model as our llm\n",
    "\n",
    "instant_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\",\n",
    "              model_kwargs=model_kwargs_claude,\n",
    "              client = bedrock_client,)\n",
    "\n",
    "service_context_new = ServiceContext.from_defaults(llm=instant_llm,\n",
    "                                               embed_model=embed_model)\n",
    "set_global_service_context(service_context_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' <response>\\n'\n",
      " 'Amazon SageMaker is a fully managed machine learning service provided by '\n",
      " 'Amazon Web Services (AWS) that enables developers and data scientists to '\n",
      " 'quickly and easily build, train, and deploy machine learning models at any '\n",
      " 'scale. It provides a comprehensive and integrated platform for the entire '\n",
      " 'machine learning workflow including data exploration, model training, model '\n",
      " 'tuning, model hosting and monitoring. Some key capabilities and features of '\n",
      " 'Amazon SageMaker include:\\n'\n",
      " '\\n'\n",
      " '- Managed machine learning infrastructure - SageMaker handles all the '\n",
      " 'infrastructure setup and management required for machine learning workloads '\n",
      " 'including compute clusters, storage, networking etc. This allows users to '\n",
      " 'focus on their models instead of infrastructure.\\n'\n",
      " '\\n'\n",
      " '- Built-in algorithms and frameworks - Popular machine learning frameworks '\n",
      " 'like TensorFlow, PyTorch, XGBoost etc are pre-installed and can be used to '\n",
      " 'build and train models on SageMaker managed compute instances. \\n'\n",
      " '\\n'\n",
      " '- Notebook instances - Jupyter notebook instances with common ML libraries '\n",
      " 'allow for interactive data exploration, preprocessing, and model training.\\n'\n",
      " '\\n'\n",
      " '- Model hosting - Trained models can be deployed to SageMaker hosting '\n",
      " 'endpoints which provide scalable real-time inference in a fully managed '\n",
      " 'way.\\n'\n",
      " '\\n'\n",
      " '- Automated machine learning - SageMaker Autopilot automates machine '\n",
      " 'learning tasks like data preprocessing, feature engineering, model tuning '\n",
      " 'and selection to build high quality models with less effort.\\n'\n",
      " '\\n'\n",
      " '- Monitoring and drift detection - Built-in tools help monitor model '\n",
      " 'performance over time and detect when models need to be retrained due to '\n",
      " 'data or target drift.\\n'\n",
      " '\\n'\n",
      " '- ML Pipelines - SageMaker Pipelines allow users to build reproducible '\n",
      " 'end-to-end ML workflows combining all the stages of model development and '\n",
      " 'deployment.\\n'\n",
      " '\\n'\n",
      " '</response>')\n"
     ]
    }
   ],
   "source": [
    "response_instant = instant_llm(prompt)\n",
    "pp.pprint(response_instant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Evaluation - Claude Instant responses on KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithful response?: True\n",
      "('Reason:  YES\\n'\n",
      " '\\n'\n",
      " 'The context supports the information provided. The context describes Amazon '\n",
      " 'SageMaker and its key capabilities as a fully managed machine learning '\n",
      " 'service on AWS for building, training, and deploying ML models. This aligns '\n",
      " 'with and supports the information statement. ')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_new)\n",
    "faith_eval = faithfulness_evaluator.evaluate(query=str(input),\n",
    "                                              response=str(responsefromllm), \n",
    "                                              contexts=[str(response)])\n",
    "print(f\"Faithful response?: {str(faith_eval.passing)}\"  )\n",
    "pp.pprint(f\"Reason: {faith_eval.feedback} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy Evaluation - Claude Instant responses on KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant response?: True\n",
      "('Reason:  YES\\n'\n",
      " '\\n'\n",
      " 'The response provides an accurate and relevant overview of Amazon '\n",
      " \"SageMaker's key capabilities as a fully managed machine learning service on \"\n",
      " 'AWS. The details on managed training, hosting, notebook instances, automated '\n",
      " 'ML, model monitoring, and ML pipelines align with the context information. '\n",
      " 'Therefore, I would evaluate that the response is in line with the context '\n",
      " 'provided. ')\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_new)\n",
    "relevant_eval = relevancy_evaluator.evaluate(query=str(input),\n",
    "                                              response=str(responsefromllm), \n",
    "                                              contexts=[str(response)])\n",
    "print(f'Relevant response?: {str(relevant_eval.passing)}')\n",
    "pp.pprint(f\"Reason: {relevant_eval.feedback} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the content above, we can see how retrieveAndGenerate API works faithfully, accurately and is relevant to the query the user enters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
