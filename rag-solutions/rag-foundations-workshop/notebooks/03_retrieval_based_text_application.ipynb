{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Solving Contextual Limitations with RAG\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Previously we saw that Amazon Bedrock could provide an answer to a technical question, however we had to manually provide it with the relevant data and provide the contex ourselves. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model.\n",
    "\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "## Solution\n",
    "To the above challenges, this notebook uses the following strategy\n",
    "\n",
    "### Prepare documents for search\n",
    "![](./images/embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "### Respond to user question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup the `boto3` client connection to Amazon Bedrock\n",
    "\n",
    "Just like previous notebooks, we will create a client side connection to Amazon Bedrock with the `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO,format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "claude3 = 'claude3'\n",
    "llama2 = 'llama2'\n",
    "llama3='llama3'\n",
    "mistral='mistral'\n",
    "titan='titan'\n",
    "models_dict = {\n",
    "    claude3 : 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    llama2: 'meta.llama2-13b-chat-v1',\n",
    "    llama3: 'meta.llama3-8b-instruct-v1:0',\n",
    "    mistral: 'mistral.mistral-7b-instruct-v0:2',\n",
    "    titan : 'amazon.titan-text-premier-v1:0'\n",
    "}\n",
    "max_tokens_val = 200\n",
    "temperature_val = 0.1\n",
    "dict_add_params = {\n",
    "    llama3: {}, #\"max_gen_len\":max_tokens_val, \"temperature\":temperature_val} , \n",
    "    claude3: {\"top_k\": 200, },# \"temperature\": temperature_val, \"max_tokens\": max_tokens_val},\n",
    "    mistral: {}, #{\"max_tokens\":max_tokens_val, \"temperature\": temperature_val} , \n",
    "    titan:  {\"topK\": 200, },# \"maxTokenCount\": max_tokens_val}\n",
    "}\n",
    "inference_config={\n",
    "    \"temperature\": temperature_val,\n",
    "    \"maxTokens\": max_tokens_val,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "\n",
    "def generate_conversation(bedrock_client,model_id,system_text,input_text):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_text (JSON) : The system prompt.\n",
    "        input text : The input message.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating message with model %s\", model_id)\n",
    "\n",
    "    # Message to send.\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    system_prompts = [{\"text\" : system_text}]\n",
    "\n",
    "    if model_id in [models_dict.get(mistral), models_dict.get(titan)]:\n",
    "        system_prompts = [] # not supported\n",
    "\n",
    "    # Inference parameters to use.\n",
    "\n",
    "\n",
    "    #Base inference parameters to use.\n",
    "    #inference_config \n",
    "\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=get_additional_model_fields(model_id)\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_additional_model_fields(modelId):\n",
    "\n",
    "    return dict_add_params.get(modelId)\n",
    "    #{\"top_k\": top_k, \"max_tokens\": max_tokens}}\n",
    "    \n",
    "def get_converse_output(response_obj):\n",
    "    ret_messages=[]\n",
    "    output_message = response['output']['message']\n",
    "    role_out = output_message['role']\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        ret_messages.append(content['text'])\n",
    "        \n",
    "    return ret_messages, role_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6832f0-92fa-44d0-a736-505f83890c7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Similarity with Amazon Titan Embeddings\n",
    "\n",
    "Semantic search refers to searching for information based on the meaning and concepts of words and phrases, rather than just matching keywords. Embedding models like Amazon Titan Embeddings allow semantic search by representing words and sentences as dense vectors that encode their semantic meaning.\n",
    "\n",
    "Semantic matching is extremely helpful for RAG because it returns results that are conceptually related to the user's query, even if they don't contain the exact keywords. This leads to more relevant and useful search results which can be injected into our LLM's prompts.\n",
    "\n",
    "First, let's take a look below to illustrate the capabilities of semantic search with Amazon Titan.\n",
    "\n",
    "The `embed_text_input` function below is an example function which will return an embedding output based on text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec61c7e-5d7b-4404-98ac-28d9246e8489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def embed_text_input(bedrock_client, prompt_data, modelId=\"amazon.titan-embed-text-v1\"):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    body = json.dumps({\"inputText\": prompt_data})\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b26ef-b83c-4c41-908f-61f7007038b4",
   "metadata": {},
   "source": [
    "To give an example of how this works, lets take a look at matching a user input to two \"documents\". We use a dot product calculation to rank the similarity between the input and each document, but there are many ways to do this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd435b1-4cc4-4fc2-bc14-ea3912424d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Things to do on vacation\" matches \"swimming, site seeing, sky diving\" with a score of 219.6\n",
      "\"Things to do on vacation\" matches \"cleaning, note taking, studying\" with a score of 150.2\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Things to do on vacation'\n",
    "document_1 = 'swimming, site seeing, sky diving'\n",
    "document_2 = 'cleaning, note taking, studying'\n",
    "\n",
    "user_input_vector = embed_text_input(boto3_bedrock, user_input)\n",
    "document_1_vector = embed_text_input(boto3_bedrock, document_1)\n",
    "document_2_vector = embed_text_input(boto3_bedrock, document_2)\n",
    "\n",
    "doc_1_match_score = np.dot(user_input_vector, document_1_vector)\n",
    "doc_2_match_score = np.dot(user_input_vector, document_2_vector)\n",
    "\n",
    "print(f'\"{user_input}\" matches \"{document_1}\" with a score of {doc_1_match_score:.1f}')\n",
    "print(f'\"{user_input}\" matches \"{document_2}\" with a score of {doc_2_match_score:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad6f495-c290-4f9d-a5a5-5920d534e027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Things to do that are productive\" matches \"swimming, site seeing, sky diving\" with a score of 99.9\n",
      "\"Things to do that are productive\" matches \"cleaning, note taking, studying\" with a score of 210.1\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Things to do that are productive'\n",
    "document_1 = 'swimming, site seeing, sky diving'\n",
    "document_2 = 'cleaning, note taking, studying'\n",
    "\n",
    "user_input_vector = embed_text_input(boto3_bedrock, user_input)\n",
    "document_1_vector = embed_text_input(boto3_bedrock, document_1)\n",
    "document_2_vector = embed_text_input(boto3_bedrock, document_2)\n",
    "\n",
    "doc_1_match_score = np.dot(user_input_vector, document_1_vector)\n",
    "doc_2_match_score = np.dot(user_input_vector, document_2_vector)\n",
    "\n",
    "print(f'\"{user_input}\" matches \"{document_1}\" with a score of {doc_1_match_score:.1f}')\n",
    "print(f'\"{user_input}\" matches \"{document_2}\" with a score of {doc_2_match_score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2e609-e784-4c8f-802b-841c14aade56",
   "metadata": {},
   "source": [
    "The example above shows how the semantic meaning behind the user input and provided documents can be effectively ranked by Amazon Titan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0574847-34b2-464e-b2de-b0c7bfdce58a",
   "metadata": {},
   "source": [
    "---\n",
    "## Simplifying Search with LangChain and FAISS\n",
    "\n",
    "Two helpful tools that help set up these semantic similarity vector search engines are LangChain and FAISS. We will use LangChain to help prepare text documents, create an easy to use abstration to the Amazon Bedrock embedding model. We will use FAISS to create a searchable data structure for documents in vector formats.\n",
    "\n",
    "First, let's import the required LangChain libraries for the system. Notice that LangChain has a FAISS wrapper class which we will be using as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3ad922-108b-4530-b40c-af79f2562ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2acbb-ced8-4839-97de-46b8becf889f",
   "metadata": {},
   "source": [
    "### Prepare Text with LangChain\n",
    "\n",
    "In order to load our document into FAISS, we first need to split the document into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limited length of input tokens, so for the sake of this use-case we are creating chunks of roughly 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0eaf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents=11\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "loader = PyPDFLoader('../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf') # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "documents_aws = loader.load() #\n",
    "print(f\"Number of documents={len(documents_aws)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7501e4-ec5e-4ee6-bd6e-1c1a25ecd294",
   "metadata": {},
   "source": [
    "### Create an Embedding Store with FAISS\n",
    "\n",
    "Once the documents are prepared, LangChain's `BedrockEmbeddings` and `FAISS` classes make it very easy to create an in memory vector store as shown below.\n",
    "\n",
    "```python\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.from_documents(split_docs, embedding_model)\n",
    "```\n",
    "\n",
    "For times sake in this lab, we have already run the code above and provided the FAISS index as a persistent file in the `faiss-index/langchain` directory. We load the vector store (along with a connection to the Titan embedding model) into memory with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34cd3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents after split and chunking=31\n",
      "vectorstore_faiss_aws: number of elements in the index=31::\n"
     ]
    }
   ],
   "source": [
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\"\\n\").split_documents(documents_aws) #-  separator=\",\"\n",
    "\n",
    "print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "\n",
    "\n",
    "vs = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "     embedding = br_embeddings\n",
    ")\n",
    "\n",
    "print(f\"vectorstore_faiss_aws: number of elements in the index={vs.index.ntotal}::\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b3691-a569-4eb1-939e-9858cc6ef5c4",
   "metadata": {},
   "source": [
    "Below is an example of one of the document chunks. Notice how the semantic text could easily be searched to answer a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e8c2f7-5d2f-41a6-ba98-b9cc8e4115fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Dear Shareholders:\\nLast year at this time, I shared my enthusiasm and optimism for Amazon’s future. Today, I have even more.\\nThe reasons are many, but start with the progress we’ve made in our financial results and customerexperiences, and extend to our continued innovation and the remarkable opportunities in front of us.\\nIn 2023, Amazon’s total revenue grew 12% year-over-year (“Y oY”) from $514B to $575B. By segment, North\\nAmerica revenue increased 12% Y oY from $316B to $353B, International revenue grew 11% Y oY from$118B to $131B, and AWS revenue increased 13% Y oY from $80B to $91B.\\nFurther, Amazon’s operating income and Free Cash Flow (“FCF”) dramatically improved. Operating\\nincome in 2023 improved 201% Y oY from $12.2B (an operating margin of 2.4%) to $36.9B (an operatingmargin of 6.4%). Trailing Twelve Month FCF adjusted for equipment finance leases improved from -$12.8Bin 2022 to $35.5B (up $48.3B).\\nWhile we’ve made meaningful progress on our financial measures, what we’re most pleased about is the\\ncontinued customer experience improvements across our businesses.\\nIn our Stores business, customers have enthusiastically responded to our relentless focus on selection, price,\\nand convenience. We continue to have the broadest retail selection, with hundreds of millions of products\\navailable, tens of millions added last year alone, and several premium brands starting to list on Amazon(e.g. Coach, Victoria’s Secret, Pit Viper, Martha Stewart, Clinique, Lancôme, and Urban Decay).\\nBeing sharp on price is always important, but particularly in an uncertain economy, where customers are\\ncareful about how much they’re spending. As a result, in Q4 2023, we kicked off the holiday season with Prime', metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcef82-dcc8-4b7b-a523-5d5e11450de4",
   "metadata": {},
   "source": [
    "### Search the FAISS Vector Store\n",
    "\n",
    "We can now use the `similarity_search` function to match a question to the best 3 chunks of text from our document which was loaded into FAISS. Notice how the search result is correctly matched to the input question :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c40af3-4956-4d66-8eb5-6a1efce56834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='expand selection and features, and move toward profitability (in Q4 2023, Mexico became our latest\\ninternational Stores locale to turn profitable). We have high conviction that these new geographies willcontinue to grow and be profitable in the long run.\\nAlongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% Y oY from\\n$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\\noffering, a self-service solution for brands to create campaigns that can appear on up to 30+ streamingTV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expandedour streaming TV advertising by introducing ads into Prime Video shows and movies, where brands canreach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies andshows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football . Streaming\\nTV advertising is growing quickly and off to a strong start.\\nShifting to AWS, we started 2023 seeing substantial cost optimization, with most companies trying to save\\nmoney in an uncertain economy. Much of this optimization was catalyzed by AWS helping customers use the\\ncloud more efficiently and leverage more powerful, price-performant AWS capabilities like Graviton chips(our generalized CPU chips that provide ~40% better price-performance than other leading x86 processors),S3 Intelligent Tiering (a storage class that uses AI to detect objects accessed less frequently and store themin less expensive storage layers), and Savings Plans (which give customers lower prices in exchange for longercommitments). This work diminished short-term revenue, but was best for customers, much appreciated,and should bode well for customers and AWS longer-term. By the end of 2023, we saw cost optimizationattenuating, new deals accelerating, customers renewing at larger commitments over longer time periods, andmigrations growing again.' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "search_results = vs.similarity_search(\n",
    "    \"How did Amazon's Advertising business do in 2023?\", k=3\n",
    ")\n",
    "print(search_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adbf60-3efb-4589-9772-d8b8fd6068e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Combine Search Results with Text Generation\n",
    "\n",
    "In the final section of this notebook, we can now combine our vector search capability with our LLM in order to dynamically provide context to answer questions effectively with RAG. \n",
    "\n",
    "First, we will start by using a utility from LangChain called prompt templates. The `PromptTemplate` class allows us to easily inject context and a human input into the Claude prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f869b78e-055f-4adc-bfec-66039b29d5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "System: Here is some important context which can help inform the questions the Human asks.\n",
    "Make sure to not make anything up to answer the question if it is not provided in the context.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "HUMAN_MESSAGE = \"{text}\"\n",
    "\n",
    "messages = [\n",
    "    (\"system\", SYSTEM_MESSAGE),\n",
    "    (\"human\", HUMAN_MESSAGE)\n",
    "]\n",
    "\n",
    "prompt_data = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a88ccb-2cd7-4d42-92cb-986a1b32622d",
   "metadata": {},
   "source": [
    "Just like before, we will again use the `similarity_search` function to provide relevant context from our documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e0370-3813-42bf-8c54-530ac8620ce7",
   "metadata": {},
   "source": [
    "Now we will augment the LangChain prompt template with the human input and the context from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "745986d4-37f0-4dcb-8173-776f65238db0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5691"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_input =  \"How did Amazon's Advertising business do in 2023?\"\n",
    "search_results = vs.similarity_search(human_input, k=3)\n",
    "context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content for ind, i in enumerate(search_results)])\n",
    "len(context_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7cfbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_query =  \"How did Amazon's Advertising business do in 2023?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19678b9-8141-4112-af1d-4876954a09fc",
   "metadata": {},
   "source": [
    "Finally, we will use the LangChain `Bedrock` class to call the Claude model with our augmented prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5218aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context provided, Amazon's Advertising business had strong progress in 2023, growing 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. This growth was primarily driven by Amazon's sponsored ads offerings. Some key points about Amazon's Advertising business in 2023:\n",
      "\n",
      "- They added a new \"Sponsored TV\" self-service solution for brands to create ad campaigns across over 30 "
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# turn verbose to true to see the full logs and documents\n",
    "modelId = models_dict.get(claude3)\n",
    "cl_llm = BedrockChat(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\"temperature\": 0.1, 'max_tokens': 100},\n",
    ")\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "chain = prompt_data | cl_llm | StrOutputParser()\n",
    "chain_input = {\n",
    "        \"context\": context_string, #\"This is a sample context doc\", #context_doc,\n",
    "        \"text\": human_query,\n",
    "    }\n",
    "\n",
    "\n",
    "for chunk in chain.stream(chain_input):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056971d2-7a44-4511-a6af-38f5de365848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the context provided, Amazon's Advertising business had strong progress in 2023, growing 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. This growth was primarily driven by Amazon's sponsored ads offerings. Some key points about Amazon's Advertising business in 2023:\n",
       "\n",
       "- They added a new \"Sponsored TV\" self-service solution for brands to create ad campaigns across over 30 "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chain.invoke(chain_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25428296",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scaling Vector Databases\n",
    "\n",
    "In this lab, we have only used a local, in-memory vector database with FAISS. This is due to the fact that is this is a workshop and not a production setting. If you are looking for a way to easily scale this FAISS solution on AWS, check out [this example](https://github.com/aws-samples/sagemaker-vector-store-microservice) which utilize Amazon SageMaker to deploy a vector search microservice with FAISS.\n",
    "\n",
    "However, once you get to production and have billions (or more) vectors which need to be used in a RAG architecture, you will need to employ a larger scale solution which is purpose built and tuned for distributed vector search. AWS offers multiple ways to accomplish this this. Here are a few of the notable options available today.\n",
    "\n",
    "### Amazon Open Search\n",
    "\n",
    "The vector engine for Amazon OpenSearch Serverless introduces a simple, scalable, and high-performing vector storage and search capability that helps developers build machine learning (ML)–augmented search experiences and generative artificial intelligence (AI) applications without having to manage the vector database infrastructure. Get contextually relevant responses across billions of vectors in milliseconds by querying vector embeddings, which can be combined with text-based keywords in a single hybrid request.\n",
    "\n",
    "Check out these links for more information...\n",
    "* [Vector Engine for Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/serverless-vector-engine/)\n",
    "* [Amazon OpenSearch Service’s vector database capabilities explained](https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/)\n",
    "\n",
    "### Amazon Aurora with `pgvector`\n",
    "\n",
    "Amazon Aurora PostgreSQL-Compatible Edition now supports the pgvector extension to store embeddings from machine learning (ML) models in your database and to perform efficient similarity searches. pgvector can store and search embeddings from Amazon Bedrock which helps power vector search for RAG. pgvector on Aurora PostgreSQL is a great option for a vector database for teams who are looking for the power of semantic search in combination with tried and trusted Amazon Relational Database Services (RDS).\n",
    "\n",
    "Check out these links for more information...\n",
    "* [Feature announcement](https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-aurora-postgresql-pgvector-vector-storage-similarity-search/)\n",
    "* [Leverage pgvector and Amazon Aurora PostgreSQL for Natural Language Processing, Chatbots and Sentiment Analysis](https://aws.amazon.com/blogs/database/leverage-pgvector-and-amazon-aurora-postgresql-for-natural-language-processing-chatbots-and-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now you have been able to enhance your Amazon Bedrock LLM with RAG in order to better answer user questions with up-to-date context. In the next section, we will learn how to combine this solution with a chat based paradigm in order to create a more interactive application which utilizes RAG."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
