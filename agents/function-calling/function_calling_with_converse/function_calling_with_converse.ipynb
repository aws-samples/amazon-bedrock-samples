{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do function calling with the Converse API \n",
    "\n",
    "This notebook demonstrates how we can improve model capability by using the Converse or ConverseStream API with external functions.\n",
    "\n",
    "Converse and ConverseStream provide a unified structured text action for simplifying the invocations to Bedrock LLMs. It includes the possibility to define tools for implementing external functions that can be called or triggered from the LLMs.\n",
    "\n",
    "As part of these APIs, the `toolConfig` parameter takes a list of `tools` in JSON schema. Each tool is defined as a `ToolSpec`, which includes its name, description and input schema. The model can be forced to call a given tool by supplying `toolChoice` within `toolConfig` with `{\"tool\": {\"name\": <some_tool_name>}}`. Each tool receives a unique identifier.\n",
    "\n",
    "When the model chooses to call a tool, the response contains a `toolUse` object. It includes the tool's identifier, name and the input parameters supplied by the model. The model will also set the `stopReason` response field to `tool_use`.\n",
    "\n",
    "## Overview\n",
    "- **Basic tool definition and function calling** We define a single tool for simulating a weather forecast lookup tool (`get_weather`) and allow the model to call this tool.\n",
    "- **Supplying the model with multiple tools to choose from** Starting from the previous example, we add a tool to search the web (`web_search`) and give the the model the liberty to decide the tool that is best fit for a given request.\n",
    "- **Letting the model call multiple tools in a single turn** We modify the conversation flow to support parallel function calling. Then, we present an example where the model needs to call multiple tools in succession.\n",
    "\n",
    "If you want to learn more details about the parameters supported in the Bedrock Converse API, read the [api reference](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) and [user guide](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "## Basic tool definition and function-calling\n",
    "\n",
    "We set our tools through Python functions and start by defining a tool for simulating a weather forecast lookup tool (`get_weather`). Note in our example we're just returning a constant weather forecast to illustrate the concept, but you could make it fully functional by connecting any weather service API. Later in the example, we call the Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3 --quiet\n",
    "!pip3 install googlesearch-python --quiet\n",
    "!pip3 install lxml --quiet\n",
    "!pip3 install pydantic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this example leverages Claude 3 Sonnet, Bedrock supports many other models. The full list of models and supported features can be found [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). The models are invoked via `bedrock-runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List\n",
    "import inspect\n",
    "import boto3\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "region = 'us-east-1'\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool definition \n",
    "We define `ToolsList` as a class where individual tools are defined as functions. Note that there is nothing specific to the model used or Bedrock in this definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within `toolConfig`, setting `toolChoice` to `{auto: {}}` gives the model the choice to decide if a tool should be called or if it can rely on its internal body of knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {\n",
    "    'tools': [],\n",
    "    'toolChoice': {\n",
    "        'auto': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's structure our tool configuration and append it to our `tools`. We have to clearly define the schema that our tools are expecting in the corresponding functions. The descriptions we provide allow the model to get a sense of the external function's purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply calls the Converse API given some `toolConfig` and returns the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow\n",
    "\n",
    "Next, we define a function to manage the conversation flow. For this simple case, the function starts by invoking the model. Should the model choose to execute the tool we have defined, it returns it in `toolUse`. With this, the function runs the selected tool. Lastly, the tool's output is returned in `toolResults` to the model who can be given instructions to format it in a more conversational tone for the user. \n",
    "\n",
    "#### Prompt flow\n",
    "![basic tool call](./assets/basic_tool_call.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything setup and are ready for testing our function calling bot.\n",
    "\n",
    "If we prompt our model about the weather, it will respond with the formatted string defined in our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather like in Queens, NY?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_3s96E3unSFe1RET9RcTPVA\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Queens\",\n",
      "            \"state\": \"NY\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (get_weather) tool...\n",
      "Tool result: Weather in Queens, NY is 70F and clear skies.\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in Queens, NY is currently 70°F (21°C) with clear skies and sunshine.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the weather like in Queens, NY?\"\n",
    "\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "        only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "    prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask a question about another topic, the model will answer the question directly without making a tool call. Notice the absence of `toolUse` in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the capital of France?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The capital of France is Paris.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "        only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "    prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether or not to call the `get_weather` tool depending on the question. You can further improve this example by playing with the system prompts.\n",
    "\n",
    "## Supplying the model with multiple tools to choose from\n",
    "\n",
    "In many cases, it makes sense to supply the model with multiple external functions to choose from. We explore a slightly more evolved case from the one described above to not only have a `get_weather` tool, but also have a fully functional `web_search` tool that can look up information in the Internet for enriching our responses. For this, we'll leverage a simple public library with a simple web scrapping implementation.\n",
    "\n",
    "### Tool definition\n",
    "\n",
    "We again define `ToolsList` as a class where individual tools are defined as functions.\n",
    "\n",
    "![basic tool call](./assets/function_calling_multiple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        return result\n",
    "\n",
    "    # Define our web_search tool function...\n",
    "    def web_search(self, query):\n",
    "        results = []\n",
    "        response_list = []\n",
    "        results.extend([r for r in search(query, 3, 'en')])\n",
    "        for j in results:\n",
    "            response = requests.get(j)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                response_list.append(soup.get_text().strip())\n",
    "        response_text = \",\".join(str(i) for i in response_list)\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset `toolConfig` to load it with the new set of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig = {'tools': [], 'toolChoice': {'auto': {}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, we append the new tools to the `tools` section of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the get_weather tool...\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Add the web_search tool...\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'web_search',\n",
    "            'description': 'Search a term in the public Internet. \\\n",
    "                Useful for getting up to date information.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'search_term': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'Term to search in the Internet'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['search_term']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow\n",
    "\n",
    "We reuse the `converse` method defined above to control the flow of conversation. It can be reused to support any number of tools. \n",
    "\n",
    "If we prompt our model about the weather, it will respond with the same formatted string defined in our `get_weather` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather like in Queens, NY?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Here is the weather information for Queens, NY:\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_lCr9CokVQ_6gm1IQnWGyEg\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Queens\",\n",
      "            \"state\": \"NY\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (get_weather) tool...\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in Queens, New York today is pleasant with a temperature of around 70F and clear, sunny skies. It's a nice day to spend some time outdoors in Queens.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the weather like in Queens, NY?\"\n",
    "    \n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather', and another tool to perform a web search for up to date information 'web_search'; \\\n",
    "        use those tools if required. Don't mention the tools in your final answer.\"}],\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we prompt our model with another topic, it will respond a result from the web by using the `web_search` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"In which team is Caitlin Clark playing in the WNBA in 2024?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_uf1keB-YSQyiCGPrlImPgQ\",\n",
      "          \"name\": \"web_search\",\n",
      "          \"input\": {\n",
      "            \"query\": \"Caitlin Clark WNBA draft 2024\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "Running (web_search) tool...\n",
      "Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Based on the article, Caitlin Clark was selected as the No. 1 overall pick in the 2024 WNBA Draft by the Indiana Fever. So she will be playing for the Indiana Fever in the 2024 WNBA season.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In which team is Caitlin Clark playing in the WNBA in 2024?\"\n",
    "\n",
    "converse(\n",
    "    system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather', and another tool to perform a web search for up to date information 'web_search'; \\\n",
    "        use those tools if required. Don't mention the tools in your final answer.\"}],\n",
    "    prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether to call the `get_weather` tool, provide an answer without any tool, or searching in the public Internet with the `web_search` tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letting the model call multiple tools in a single turn\n",
    "\n",
    "### Parallel functions\n",
    "Parallel function refers to the model's ability to make multiple calls to one or more tools in a single turn. By allowing the model to decompose the requester's question into multiple subproblems each solvable with a tool, it is able to increase its level flexibility.\n",
    "\n",
    "#### Tool as Pydantic definition\n",
    "To do this, we use Pydantic, a data-validation library. We rely on a Pydantic-based helper function for doing the tool config translation for us in a way that ensures we avoid potential mistakes when defining our tool config schema in a JSON dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool(name, description):\n",
    "    def decorator(func):\n",
    "        # defining our model inheriting from pydantic.BaseModel and define fields as annotated attributes\n",
    "        input_model = create_model(\n",
    "            func.__name__ + \"_input\",\n",
    "            **{\n",
    "                name: (param.annotation, param.default)\n",
    "                for name, param in inspect.signature(func).parameters.items()\n",
    "                if param.default is not inspect.Parameter.empty\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # bedrock tool schema\n",
    "        func.bedrock_schema = {\n",
    "            'toolSpec': {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'inputSchema': {\n",
    "                    'json': input_model.schema()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again define our tool list, and all we need to specify is the name, description, any relevant attributes that are required in our function. Of course we should also add the content of our tool. We define the `get_lat_long` tool to call Open Streat Map to retrieve these coordinates.\n",
    "\n",
    "If you have more tools, you can adapt this cell for testing with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tools\n",
    "class ToolsList:\n",
    "    # define get_lat_long tool\n",
    "    @tool(\n",
    "        name=\"get_lat_long\",\n",
    "        description=\"Get the coordinates of a city based on a location.\"\n",
    "    )\n",
    "    def get_lat_long(self, place: str = Field(..., description=\"City of the location\")) -> dict:\n",
    "        \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "        header_dict = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "            \"referer\": 'https://www.guichevirtual.com.br'\n",
    "        }\n",
    "        url = \"http://nominatim.openstreetmap.org/search\"\n",
    "        params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "        response = requests.get(url, params=params, headers=header_dict).json()\n",
    "        if response:\n",
    "            lat = response[0][\"lat\"]\n",
    "            lon = response[0][\"lon\"]\n",
    "            return {\"latitude\": lat, \"longitude\": lon}\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our configuration, setup the function for invoking Bedrock with Converse, and define our workflow orchestration function as per the previous examples. Using the Pydantic definition allows us to generalize `toolConfig` to any tool supplied to `ToolsList`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the conversation flow\n",
    "\n",
    "The following `converse` method has been adapted from the previous one to support parallel function calling in a single turn of the conversation. As it did before, the conversation begins with an invocation of the model dedicing whether to use a tool or not. During this step, the model can choose to call any number of tool as many times as it is necessary to complete the request. The model is then supplied the result of the executions in `toolResult` during the subsequent invokation.\n",
    "\n",
    "![basic tool call](./assets/parallel_function_call.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(tool_class, prompt, system='', toolConfig=None, modelId=modelId):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    print(\"Invoking model...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    messages.append(output['output']['message'])\n",
    "    print(\"Got output from model...\")\n",
    "\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "        for function in function_calling:\n",
    "            print(\"Function calling - Calling tool...\")\n",
    "            tool_name = function['name']\n",
    "            tool_args = function['input'] or {}\n",
    "            tool_response = json.dumps(getattr(tool_class, tool_name)(**tool_args))\n",
    "            print(\"Function calling - Got tool response...\")\n",
    "            tool_result_message['content'].append({\n",
    "                'toolResult': {\n",
    "                    'toolUseId': function['toolUseId'],\n",
    "                    'content': [{\"text\": tool_response}]\n",
    "                }\n",
    "            })\n",
    "        messages.append(tool_result_message)\n",
    "        print(\"Function calling - Calling model with result...\")\n",
    "        \n",
    "        output = converse_with_tools(messages, system, toolConfig)\n",
    "    return messages, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a system prompt describing the model's role and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt building an optional chain of thought response\n",
    "system_prompt = \"\"\"You're provided with a tool that can get the coordinates for a specific city 'get_lat_long';\n",
    "    only use the tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "    Don't make reference to the tools in your final answer.\"\"\"\n",
    "system_prompt = [{\"text\": system_prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the coordinates of two cities, the model calls `get_lat_long` twice and aggregates the model answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "{'ResponseMetadata': {'RequestId': '672bb1ec-b156-44c4-960e-f995d540c919', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Aug 2024 11:59:47 GMT', 'content-type': 'application/json', 'content-length': '280', 'connection': 'keep-alive', 'x-amzn-requestid': '672bb1ec-b156-44c4-960e-f995d540c919'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'toolUse': {'toolUseId': 'tooluse_C3ie0cSZR82bOMkKbxXCiw', 'name': 'get_lat_long', 'input': {'place': 'Berlin'}}}]}}, 'stopReason': 'tool_use', 'usage': {'inputTokens': 431, 'outputTokens': 56, 'totalTokens': 487}, 'metrics': {'latencyMs': 1283}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'toolUse': {'toolUseId': 'tooluse_C3ie0cSZR82bOMkKbxXCiw', 'name': 'get_lat_long', 'input': {'place': 'Berlin'}}}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What are the coordinates for both Paris and in Berlin??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Here are the coordinates for Paris and Berlin obtained using the `get_lat_long` tool:\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_77EBAFMeS92g3zHisyE2UA\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Paris\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_77EBAFMeS92g3zHisyE2UA\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"48.8534951\\\", \\\"longitude\\\": \\\"2.3483915\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt to get up-to-date coordinates of Montreal and Toronto\n",
    "prompt = \"What are the coordinates for both Paris and in Berlin??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool chaining refers to the ability of the model to reach its goal by calling more than one tool where the output of one tool serves as the input to the next. This more complex workflow requires the model to breakdown the query into subproblems that can individually be met with a call to an external function or the model's own body of knowledge.\n",
    "\n",
    "We define `ToolList` again to include `get_lat_long` and a modified version of the previous `get_weather` function. This new version lerverages the Open-Meteo service translating a given set of coordinates to the currrent weather at those coordinates. Logically, this means that for a given question about the weather in a location, the model must first retrieve the coordinates of that place using `get_lat_long` and use those coordinates when calling `get_weather`. These calls occur in two seperate steps where the model is able to observe outputs, which differs from parallel functions call where the model selects multiple tools in a single turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tools\n",
    "class ToolsList:\n",
    "    # define get_lat_long tool\n",
    "    @tool(\n",
    "        name=\"get_lat_long\",\n",
    "        description=\"Get the coordinates of a city based on a location.\"\n",
    "    )\n",
    "    def get_lat_long(self, place: str = Field(..., description=\"City of the location\")) -> dict:\n",
    "        \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "        header_dict = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\",\n",
    "            \"referer\": 'https://www.guichevirtual.com.br'\n",
    "        }\n",
    "        url = \"http://nominatim.openstreetmap.org/search\"\n",
    "        params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "        response = requests.get(url, params=params, headers=header_dict).json()\n",
    "        if response:\n",
    "            lat = response[0][\"lat\"]\n",
    "            lon = response[0][\"lon\"]\n",
    "            return {\"latitude\": lat, \"longitude\": lon}\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    # define get_weather tool...\n",
    "    @tool(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Get weather of a location.\"\n",
    "    )\n",
    "    def get_weather(self,\n",
    "        latitude: str = Field(..., description=\"latitude of the location\"), \n",
    "        longitude: str = Field(..., description=\"longitude of the location\")) -> dict:\n",
    "        \"\"\"Returns weather data for a given latitude and longitude.\"\"\"\n",
    "        url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "        response = requests.get(url)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset `toolConfig` with the new set of functions in `ToolList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the conversation flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the `converse` function, we create a helper function to wrap the call of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_function_and_add_message(tool_class, messages, previous_output):\n",
    "    function_calling = [c['toolUse'] for c in previous_output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        function = function_calling[0]\n",
    "        tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "        print(\"Function calling - Calling tool...\")\n",
    "        tool_name = function['name']\n",
    "        tool_args = function['input'] or {}\n",
    "        tool_response = json.dumps(getattr(tool_class, tool_name)(**tool_args))\n",
    "        print(\"Function calling - Got tool response...\")\n",
    "        tool_result_message['content'].append({\n",
    "            'toolResult': {\n",
    "                'toolUseId': function['toolUseId'],\n",
    "                'content': [{\"text\": tool_response}]\n",
    "            }\n",
    "        })\n",
    "        messages.append(tool_result_message)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously described, our workflow involves at most two function calls. We adjust the `converse` function again to match this flow.\n",
    "\n",
    "![basic tool call](./assets/function_call_succession.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse(tool_class, prompt, system='', toolConfig=None, modelId=modelId):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    \n",
    "    print(\"Invoking model...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        messages.append(output['output']['message'])\n",
    "    \n",
    "    print(\"Got output from model...\")\n",
    "    messages = call_function_and_add_message(tool_class, messages, output)\n",
    "    \n",
    "    print(\"Function calling - Calling model with result...\")\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "    if function_calling:\n",
    "        messages.append(output['output']['message'])\n",
    "    \n",
    "    print(\"Got output from model...\")\n",
    "    messages = call_function_and_add_message(tool_class, messages, output)\n",
    "    output = converse_with_tools(messages, system, toolConfig)\n",
    "    return messages, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adjust the system prompt to account for the availability of multiple tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt building an optional chain of thought response\n",
    "system_prompt = \"\"\"You're provided with a tool that can get the coordinates for a specific city 'get_lat_long'\n",
    "    and a tool that can get the weather for that city, but requires the coordinates 'get_weather';\n",
    "    only use the tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "    Don't make reference to the tools in your final answer.\"\"\"\n",
    "system_prompt = [{\"text\": system_prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the weather of a given city, the model calls `get_lat_long` then `get_weather`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "{'ResponseMetadata': {'RequestId': 'e5cc2e1a-ca74-4485-b8f9-fad5c9675374', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Aug 2024 13:42:52 GMT', 'content-type': 'application/json', 'content-length': '525', 'connection': 'keep-alive', 'x-amzn-requestid': 'e5cc2e1a-ca74-4485-b8f9-fad5c9675374'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': 'Based on the weather information retrieved, the current weather in Montreal is:\\n\\nTemperature: 19°C\\nWind Speed: 8.3 km/h \\nWind Direction: 358 degrees (from the north)\\nWeather Condition Code 0 indicates clear sky\\n\\nSo in Montreal right now, it is a clear sunny day with a comfortable temperature of 19°C and light winds from the north.'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 845, 'outputTokens': 91, 'totalTokens': 936}, 'metrics': {'latencyMs': 2897}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': 'Based on the weather information retrieved, the current weather in Montreal is:\\n\\nTemperature: 19°C\\nWind Speed: 8.3 km/h \\nWind Direction: 358 degrees (from the north)\\nWeather Condition Code 0 indicates clear sky\\n\\nSo in Montreal right now, it is a clear sunny day with a comfortable temperature of 19°C and light winds from the north.'}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather in Montreal??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Here are the steps to get the weather for Montreal:\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_8ioEjLQZSnuijNJ0Xx_e_w\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Montreal\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_8ioEjLQZSnuijNJ0Xx_e_w\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"45.5031824\\\", \\\"longitude\\\": \\\"-73.5698065\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_FmtT9M4URLmUjuKwOjA3Vg\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"latitude\": \"45.5031824\",\n",
      "            \"longitude\": \"-73.5698065\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_FmtT9M4URLmUjuKwOjA3Vg\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": 45.49215, \\\"longitude\\\": -73.56103, \\\"generationtime_ms\\\": 0.07903575897216797, \\\"utc_offset_seconds\\\": 0, \\\"timezone\\\": \\\"GMT\\\", \\\"timezone_abbreviation\\\": \\\"GMT\\\", \\\"elevation\\\": 51.0, \\\"current_weather_units\\\": {\\\"time\\\": \\\"iso8601\\\", \\\"interval\\\": \\\"seconds\\\", \\\"temperature\\\": \\\"\\\\u00b0C\\\", \\\"windspeed\\\": \\\"km/h\\\", \\\"winddirection\\\": \\\"\\\\u00b0\\\", \\\"is_day\\\": \\\"\\\", \\\"weathercode\\\": \\\"wmo code\\\"}, \\\"current_weather\\\": {\\\"time\\\": \\\"2024-08-07T13:30\\\", \\\"interval\\\": 900, \\\"temperature\\\": 19.0, \\\"windspeed\\\": 8.3, \\\"winddirection\\\": 358, \\\"is_day\\\": 1, \\\"weathercode\\\": 0}}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What is the weather in Montreal??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask for the coordinates of a given city, the model calls `get_lat_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling tool...\n",
      "Function calling - Got tool response...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "{'ResponseMetadata': {'RequestId': '43fabc20-fc41-4f1c-8308-c3002b1a4a47', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Aug 2024 13:42:56 GMT', 'content-type': 'application/json', 'content-length': '262', 'connection': 'keep-alive', 'x-amzn-requestid': '43fabc20-fc41-4f1c-8308-c3002b1a4a47'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': 'The coordinates of Montreal are latitude 45.5031824 and longitude -73.5698065.'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 539, 'outputTokens': 26, 'totalTokens': 565}, 'metrics': {'latencyMs': 1611}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': 'The coordinates of Montreal are latitude 45.5031824 and longitude -73.5698065.'}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What are the coordinates in Montreal??\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_6R4XxuhbRGOL1cVxCnhxXg\",\n",
      "          \"name\": \"get_lat_long\",\n",
      "          \"input\": {\n",
      "            \"place\": \"Montreal\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolResult\": {\n",
      "          \"toolUseId\": \"tooluse_6R4XxuhbRGOL1cVxCnhxXg\",\n",
      "          \"content\": [\n",
      "            {\n",
      "              \"text\": \"{\\\"latitude\\\": \\\"45.5031824\\\", \\\"longitude\\\": \\\"-73.5698065\\\"}\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What are the coordinates in Montreal??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we ask an unrelated question to our tools, the model will not call the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking model...\n",
      "Got output from model...\n",
      "Function calling - Calling model with result...\n",
      "Got output from model...\n",
      "{'ResponseMetadata': {'RequestId': '11ed997f-4b5e-45a0-b4c8-8100a9622832', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Aug 2024 13:43:24 GMT', 'content-type': 'application/json', 'content-length': '1455', 'connection': 'keep-alive', 'x-amzn-requestid': '11ed997f-4b5e-45a0-b4c8-8100a9622832'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': 'SageMaker is a fully-managed machine learning service provided by Amazon Web Services (AWS). It allows developers and data scientists to build, train, and deploy machine learning models quickly and easily.\\n\\nSome key features and capabilities of SageMaker include:\\n\\n1. Jupyter Notebook Instances: Provides managed Jupyter notebook instances for data exploration, model building, and deployment.\\n\\n2. Automatic Model Tuning: Automatically tuning machine learning models to get the best version of the model based on data.\\n\\n3. Built-in Algorithms: Provides built-in machine learning algorithms like XGBoost, Linear Learner, etc. to quickly build models.\\n\\n4. Training and Hosting: Allows training machine learning models on GPU or CPU and then deploying/hosting the model for inference.\\n\\n5. Model Monitoring: Monitors the performance of deployed models to detect drift and manage issues.\\n\\n6. Batch Transform: Allows running inferences on an entire dataset in batch mode.\\n\\nSageMaker integrates with many AWS services like S3, Lambda, IoT and simplifies the process of building, training, and deploying machine learning models at scale in the cloud or on-premises environments. It abstracts away a lot of the heavy lifting involved in machine learning projects.'}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 452, 'outputTokens': 279, 'totalTokens': 731}, 'metrics': {'latencyMs': 8817}}\n",
      "Output:\n",
      "{'message': {'role': 'assistant', 'content': [{'text': 'SageMaker is a fully-managed machine learning service provided by Amazon Web Services (AWS). It allows developers and data scientists to build, train, and deploy machine learning models quickly and easily.\\n\\nSome key features and capabilities of SageMaker include:\\n\\n1. Jupyter Notebook Instances: Provides managed Jupyter notebook instances for data exploration, model building, and deployment.\\n\\n2. Automatic Model Tuning: Automatically tuning machine learning models to get the best version of the model based on data.\\n\\n3. Built-in Algorithms: Provides built-in machine learning algorithms like XGBoost, Linear Learner, etc. to quickly build models.\\n\\n4. Training and Hosting: Allows training machine learning models on GPU or CPU and then deploying/hosting the model for inference.\\n\\n5. Model Monitoring: Monitors the performance of deployed models to detect drift and manage issues.\\n\\n6. Batch Transform: Allows running inferences on an entire dataset in batch mode.\\n\\nSageMaker integrates with many AWS services like S3, Lambda, IoT and simplifies the process of building, training, and deploying machine learning models at scale in the cloud or on-premises environments. It abstracts away a lot of the heavy lifting involved in machine learning projects.'}]}}\n",
      "\n",
      "Messages:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is SageMaker??\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt to get up-to-date weather on Montreal\n",
    "prompt = \"What is SageMaker??\"\n",
    "\n",
    "messages, output = converse(ToolsList(), prompt, system_prompt, toolConfig)\n",
    "print(output)\n",
    "print(f\"Output:\\n{output['output']}\\n\")\n",
    "print(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "This notebook demonstrates function calling with the Converse API with one or multiple tools.These tools can be called both in parallel and in succession. As a next step, explore similar implementation using the **Langchain** integration with Amazon Bedrock's Converse API to reduce the amount of code necessary for the solution."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
