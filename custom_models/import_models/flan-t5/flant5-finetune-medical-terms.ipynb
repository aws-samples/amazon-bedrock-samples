{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c81179-f87d-4632-a94c-b16d53230cab",
   "metadata": {},
   "source": [
    "### Fine tuning & deploying Flan-T5-Large to Amazon Bedrock using Custom Model Import (Using PEFT + FSDP)\n",
    "\n",
    "this notebook was tested on a [\"g5.12xlarge\"](https://aws.amazon.com/ec2/instance-types/g5/) instance. \n",
    "\n",
    "This notebook will use the HuggingFace Transformers library to fine tune FLAN-T5-Large. due to the fine tuning process being \"local\" you can run this notebook anywhere as long as the proper compute is available. \n",
    "\n",
    "1. Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: [Link](https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html)\n",
    "2. Configuring an Amazon Sagemaker environment: [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html)\n",
    "3. Configure your own environment, with equivalent compute\n",
    "\n",
    "This notebook covers the step by step process of fine tuning a [FLAN-T5 large](https://huggingface.co/google/flan-t5-large) mode and deploying it using Bedrock's [Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html) feature. \n",
    "The fine tuning data we will be using is based on medical terminology this data can be found on HuggingFace [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). In many discplines, industry specific jargon is used and an LLM may not have the correct understanding of words, context or abbreviations. By fine tuning a model on medical terminology in this case, the LLM is given the ability to understand specific jargon and answer questions the user might have. \n",
    "\n",
    "The resulting files are imported into Amazon Bedrock via custom model import\n",
    "\n",
    "WARNING: This method of Custom Model Import will only work with \"FLAN-t5-large\". \"FLAN-t5-small\" is incompatible with Bedrock Custom Model Import in its current state. This is due to the number of heads in the model needing to be a multiple of 4, due to the model needing to be sharded accordingly in the GPU. the number of heads for FLAN-t5-small is 6. this can be checked in the model's config.json file under the parameter \"num_heads\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ace43-f355-4421-aa21-f9c67093da9c",
   "metadata": {},
   "source": [
    "### Installs & Imports \n",
    "\n",
    "we will be utilizing HuggingFace Transformers library to pull a pretrained model from the Hub and fine tune it. The dataset we will be finetuning on will also be pulled from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a76af-9965-46cf-84f3-cc1e9a317eda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers --quiet\n",
    "%pip install torch\n",
    "%pip install -U bitsandbytes accelerate transformers peft trl\n",
    "%pip install datasets --quiet\n",
    "%pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bcfeb-0298-49c8-bc75-b50c0c89fbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, T5ForConditionalGeneration, T5Tokenizer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a958ef-c7ef-4d8a-9bef-2146a8c2345c",
   "metadata": {},
   "source": [
    "### Pull a pre-trained model from HuggingFace \n",
    "\n",
    "as mentioned at the beginning of the notebook, we will be fine tuning google's FLAN-t5-large from HuggingFace. This model is free to pull - no HuggingFace account needed.\n",
    "\n",
    "The model is loaded with the \"bitsandbytes\" library. This allows the model to be quantized in 4-bit, to set it up for fine tuning with FSDP-QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a1ca-453a-4316-8348-65d26597d0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "#Set Model Attributes\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8c807-c2b0-478d-a612-68ca168b3ac0",
   "metadata": {},
   "source": [
    "### Pull a dataset from HuggingFace \n",
    "\n",
    "The dataset we are pulling can be looked at [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). This is a dataset containing over 6000+ medical terms, and their wiki definitions. \n",
    "\n",
    "Since the model is being trained to recognize & understand medical terminology, with the function below the dataset will be converted to a Q & A format. \"What is\" is added as a prefix to the medical terms column, and the other column stays as the definition.\n",
    "\n",
    "One important aspect here is ensuring the \"padding=True\" argument is passed. This is needed when training a FLAN-T5 model with FSDP + QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5542-2754-4248-a2b0-7d608fe7c25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load data from huggingface\n",
    "ds = load_dataset(\"gamino/wiki_medical_terms\")\n",
    "\n",
    "#Using 70% of the dataset to train\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.3)\n",
    "\n",
    "#Process data to fit a Q & A format \n",
    "prefix = \"What is \"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # Transform page title into an answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"page_title\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "  \n",
    "   # keep explanations as is:\n",
    "   labels = tokenizer(text_target=examples[\"page_text\"], \n",
    "                      max_length=512, truncation=True, padding=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs\n",
    "\n",
    "# Map the preprocessing function across our dataset\n",
    "ds = ds.map(preprocess_function, batched=True)\n",
    "#Take the training portion of the dataset\n",
    "ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717088e-10bb-43ee-8b4d-9eb1381ac16b",
   "metadata": {},
   "source": [
    "### Set up training parameters\n",
    "\n",
    "In the first cell below, the PEFT configurations are being set up. The term PEFT has been mentioned a couple of times in this notebook already, but what is it? PEFT stands for Parameter Efficient Fine Tuning. It allows the majority of the model parameters to be frozen, and only fine tune a small number of them. This technique decreases computation & storage costs, without performance suffering. Read more about HuggingFace's PEFT library [here](https://huggingface.co/docs/peft/en/index) \n",
    "\n",
    "In the second all training parameters are being passed into the \"SFTTrainer\" class. This is powered by the accelerate library. It allows for FSDP - Fully Sharded Data Parallel. At a high level this training technique, shards a model's parameters across the number of available GPU's. Read more about HuggingFace's FSDP library [here](https://huggingface.co/docs/transformers/main/en/fsdp)  \n",
    "\n",
    "Combining these techniques can lead to cost effective, high performance fine tuning for models. Read more about combining PEFT & FSDP on HuggingFace [here](https://huggingface.co/docs/bitsandbytes/main/en/fsdp_qlora)\n",
    "\n",
    "Note: Training is set to 1 epoch. This is for a faster training time to showcase Bedrock Custom Model Import. If you require fine tuning with higher performance, consider increasing the epochs & optimizing hyperparameters passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fa5c4-ccfd-45bf-9396-b40cd18e3392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up PEFT Configurations \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccca90-a4f3-42e9-8e22-e7ae4c1f365c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pass all parameters to SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args= TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        num_train_epochs=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_accumulation_steps=2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8049a0a-dcc5-4ad7-ad81-31eaa01e4d70",
   "metadata": {},
   "source": [
    "### Start model training\n",
    "\n",
    "In the first cell we empty the pytorch cache to free as much memory on the device possible. In the next cell, \"trainer.train()\" actually stars the training job.\n",
    "\n",
    "In the training parameters passed, the line output_dir=\"./results\" saves the model checkpoints into the \"results\" folder in the device directory (creates it if not already created). The final model is in this directory as \"checkpoint-XXX\" - XXX being the largest number. \n",
    "\n",
    "This training job will take approx. 30 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11331833-0fdc-4374-9dda-9031b5a5f072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Empty pytorch cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8ef5a-180c-42b3-b5dd-4c688927a39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97aef8-eeac-4383-b446-37e8154e2561",
   "metadata": {},
   "source": [
    "### Model Inference \n",
    "\n",
    "We will now take our latest checkpoint and generate text with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9177198-1a0c-4c04-84f3-34bd34957768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model Inference \n",
    "last_checkpoint = \"./results/checkpoint-600\" #Load checkpoint that you want to test \n",
    "\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "\n",
    "med_term = \"what is Dexamethasone suppression test\" \n",
    "\n",
    "query = tokenizer(med_term, return_tensors=\"pt\")\n",
    "output = finetuned_model.generate(**query, max_length=512, no_repeat_ngram_size=True)\n",
    "answer = tokenizer.decode(output[0])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221185f9-646b-44d6-9369-78312fb8d95b",
   "metadata": {},
   "source": [
    "### Model Upload \n",
    "\n",
    "with out model now generating text related to medical terminology we will now upload it to S3 to ensure readiness for Bedrock Custom Model Import. Depending on the environment you have chosen to run this notebook in, your AWS credentials will have to be initialized to upload the model files to your S3 bucket of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d7c0c-9d35-4c16-b1fe-30d1463edade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload model to S3 Bucket\n",
    "import boto3\n",
    "import os\n",
    "# Set up S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name and the prefix (folder) where you want to upload the files\n",
    "bucket_name = 'your-bucket-here'#YOU BUCKET HERE\n",
    "model_name = \"results/checkpoint-#\" #YOUR LATEST CHECKPOINT HERE (this will be in the \"results\" folder in your notebook directory replace the \"#\" with the latest checkpoint number)\n",
    "prefix = 'flan-t5-large-medical/' + model_name\n",
    "\n",
    "# Upload files to S3\n",
    "def upload_directory_to_s3(local_directory, bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "            \n",
    "            print(f'Uploading {local_path} to s3://{bucket}/{s3_path}')\n",
    "            s3.upload_file(local_path, bucket, s3_path)\n",
    "\n",
    "# Call the function to upload the downloaded model files to S3\n",
    "upload_directory_to_s3(model_name, bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c1dce-bd2c-4d86-b8b6-f67365d71fdf",
   "metadata": {},
   "source": [
    "### Importing Model to Amazon Bedrock\n",
    "\n",
    "Now that our model artifacts are uploaded into an S3 bucket, we can import it into Amazon Bedrock \n",
    "\n",
    "in the AWS console, we can go to the Amazon Bedrock page. On the left side under \"Foundation models\" we will click on \"Imported models\"\n",
    "\n",
    "![Step 1](./images/step1.png \"Step 1\")\n",
    "\n",
    "\n",
    "You can now click on \"Import model\"\n",
    "\n",
    "![Step 2](./images/step2.png \"Step 2\")\n",
    "\n",
    "In this next step you will have to configure:\n",
    "\n",
    "1. Model Name \n",
    "2. Import Job Name \n",
    "3. Model Import Settings \n",
    "    a. Select Amazon S3 bucket \n",
    "    b. Select your bucket location (uploaded in the previous section)\n",
    "4. Create a IAM role, or use an existing one (not shown in image)\n",
    "5. Click Import (not shown in image)\n",
    "\n",
    "![Step 3](./images/step3.png \"Step 3\")\n",
    "\n",
    "\n",
    "You will now be taken to the page below. Your model may take up to an hour to import. \n",
    "\n",
    "![Step 4](./images/step4.png \"Step 4\")\n",
    "\n",
    "After your model imports you will then be able to test it via the playground or API! \n",
    "\n",
    "![Playground](./images/playground.gif \"Playground\")\n",
    "\n",
    "END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
