{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building a Travel Planner with a Simple LangGraph\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab guides you through the process of creating a simple Travel Planner using LangGraph, a library for building stateful, multi-step applications with language models. The Travel Planner demonstrates how to structure a conversational AI application that collects user input and generates personalized travel itineraries.\n",
    "\n",
    "## Intro to Agents\n",
    "- Agents are leverage LLM to `think step by step` and then plan the execution\n",
    "- Agents have access to tools\n",
    "- Agents have access to memory. Below diagram illustrates this concept\n",
    "\n",
    "#### Memory Management\n",
    "Memory is key for any agentic conversation which is `Multi-Turn` or `Multi-Agent` colloboration conversation and more so if it spans multiple days. The 3 main aspects of Agents are:\n",
    "1. Tools\n",
    "2. Memory\n",
    "3. Planners\n",
    "\n",
    "- ![Agent memory](./images/agents_memory_light.png)\n",
    "\n",
    "## Use Case Details\n",
    "\n",
    "Our Travel Planner follows a straightforward, three-step process:\n",
    "\n",
    "1. **Initial User Input**: \n",
    "   - The application prompts the user to enter their desired travel plan to get assistance from AI Agent.\n",
    "   - This information is stored in the state.\n",
    "\n",
    "2. **Interests Input**:\n",
    "   - The user is asked to provide their interests for the trip.\n",
    "   - These interests are stored as a list in the state.\n",
    "\n",
    "3. **Itinerary Creation**:\n",
    "   - Using the collected city and interests, the application leverages a language model to generate a personalized day trip itinerary.\n",
    "   - The generated itinerary is presented to the user.\n",
    "\n",
    "The flow between these steps is managed by LangGraph, which handles the state transitions and ensures that each step is executed in the correct order.\n",
    "\n",
    "The below diagram illustrates this:\n",
    "\n",
    "![Travel Planner Agent](./images/agents_itinerary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "# langchain>=0.3.7 \\ \n",
    "# langchain-anthropic>=0.1.15 \\\n",
    "# langchain-aws>=0.2.6 \\\n",
    "# langchain-community>=0.3.5 \\\n",
    "# langchain-core>=0.3.15 \\\n",
    "# langchain-text-splitters>=0.3.2 \\\n",
    "# langchainhub>=0.1.20 \\\n",
    "# langgraph>=0.2.45 \\\n",
    "# langgraph-checkpoint>=2.0.2 \\\n",
    "# langgraph-sdk>=0.1.35 \\\n",
    "# langsmith>=0.1.140 \\\n",
    "# sqlalchemy -U \\\n",
    "# \"faiss-cpu>=1.7,<2\" \\\n",
    "# \"pypdf>=3.8,<4\" \\\n",
    "# \"ipywidgets>=7,<8\" \\\n",
    "# matplotlib>=3.9.0 \\\n",
    "\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install -U --no-cache-dir boto3\n",
    "#%pip install grandalf==3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph -- State Graph, Nodes and Edges\n",
    "\n",
    "First, we are initializing the ```StateGraph```. This object will encapsulate the graph being traversed during excecution.\n",
    "\n",
    "Then we define the **nodes** in our graph. In LangGraph, nodes are typically python functions. There are two main nodes we will use for our graph:\n",
    "- The agent node: responsible for deciding what (if any) actions to take.\n",
    "- The tool node: This node will orchestrate calling the respective tool and returning the output. This means if the agent decides to take an action, this node will then execute that action.\n",
    "\n",
    "**Edges** define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n",
    "\n",
    "- Normal Edges: Go directly from one node to the next.\n",
    "- Conditional Edges: Call a function to determine which node(s) to go to next.\n",
    "- Entry Point: Which node to call first when user input arrives.\n",
    "- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\n",
    "\n",
    "In our case we need to define a conditional edge that routes to the ```ToolNode``` when a tool get called in the agent node, i.e. when the LLM determines the requirement of tool use. With ```tools_condition```, LangGraph provides a preimplemented function handling this. Further, an edge from the ```START```node to the ```assistant```and from the ```ToolNode``` back to the ```assistant``` are required.\n",
    "\n",
    "We are adding the nodes, edges as well as a persistant memory to the ```StateGraph``` before we compile it. \n",
    "\n",
    "### Define Agent State\n",
    "\n",
    "We'll define the state that our agent will maintain throughout its operation. First, define the [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of the graph.  The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **StateGraph**: The core of our application, defining the flow of our Travel Planner.\n",
    "2. **PlannerState**: A custom type representing the state of our planning process.\n",
    "3. **Node Functions**: Individual steps in our planning process (input_city, input_interests, create_itinerary).\n",
    "4. **LLM Integration**: Utilizing a language model to generate the final itinerary.\n",
    "5. **Memory Integration**: Utilizing long term and short term memory for conversations\n",
    "\n",
    "#### Advanced concepts\n",
    "\n",
    "Now we'll define the main functions nodes that our agent will use: get interests, create itinerary. \n",
    "\n",
    "- [Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) are python functions.\n",
    "- The first positional argument is the state, as defined above. \n",
    "- State is a `TypedDict` with schema as defined above, each node can access the key, `graph_state`, with `state['graph_state']`. \n",
    "- Each node returns a new value of the state key `graph_state`.\n",
    "  \n",
    "By default, the new value returned by each node [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior state value.\n",
    "\n",
    "### Memory\n",
    "\n",
    "1. We will disscuss memory is detail in subsequent sections below how some some key points\n",
    "2. We have `Conversational Memory` which is needed for all agents to have context since this is a `ChatBot` which needs history of conversations\n",
    "3. We usually summarize these into a **Summary Conversation** alternating with Human | AI for multi session conversations\n",
    "4. We have the concept of Graph State which is for Async workflows where we need to resume the workflow from a **certain point** in history\n",
    "\n",
    "For `A-Sync` workflows where we need to persist the state of the graph and bring it back once we get the required data. below diagram explains this concept. \n",
    "\n",
    "\n",
    "<img src=\"./images/graph_state_light.png\" width=\"45%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent Nodes\n",
    "\n",
    "we will create a simple graph with \n",
    "- user travel plans\n",
    "- invoke with Bedrock\n",
    "- generate the travel plan for the day \n",
    "- ability to add or modify the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], \"The messages in the conversation\"]\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Language Model and Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import ChatBedrock\n",
    "import boto3\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "#model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"#\n",
    "#model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "provider_id = \"anthropic\"\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=model_id,\n",
    "    provider=provider_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "\n",
    "itinerary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful travel assistant. Create a day trip itinerary for {city} based on the user's interests. Use the below chat conversation and the latest input from Human to get the user interests. Provide a brief, bulleted itinerary.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{user_message}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def input_interests(state: PlannerState) -> PlannerState:\n",
    "    user_message = state['user_message'] #input(\"Your input: \")\n",
    "    #print(f\"We are going to :: {user_message}:: for trip to {state['city']} based on your interests mentioned in the prompt....\")\n",
    "\n",
    "    if not state.get('messages', None) : state['messages'] = []\n",
    "    return {\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState) -> PlannerState:\n",
    "    response = llm.invoke(itinerary_prompt.format_messages(city=state['city'], user_message=state['user_message'], chat_history=state['messages']))\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state['messages'] + [HumanMessage(content=state['user_message']),AIMessage(content=response.content)],\n",
    "        \"itinerary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Graph\n",
    "\n",
    "Now we'll create our LangGraph workflow and compile it. We build the graph from our [components](\n",
    "https://langchain-ai.github.io/langgraph/concepts/low_level/) defined above. The [StateGraph class](https://langchain-ai.github.io/langgraph/concepts/low_level/#stategraph) is the graph class that we can use.\n",
    " \n",
    "First, we initialize a StateGraph with the `State` class we defined above. Then, we add our nodes and edges. We use the [`START` Node, a special node](https://langchain-ai.github.io/langgraph/concepts/low_level/#start-node) that sends user input to the graph, to indicate where to start our graph. The [`END` Node](https://langchain-ai.github.io/langgraph/concepts/low_level/#end-node) is a special node that represents a terminal node. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "#workflow.add_node(\"input_city\", input_city)\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "\n",
    "#workflow.add_edge(\"input_city\", \"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the graph structure\n",
    "\n",
    "Finally, we [compile our graph](https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) to perform a few basic checks on the graph structure. We can visualize the graph as a [Mermaid diagram](https://github.com/mermaid-js/mermaid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that runs the graph\n",
    "\n",
    "When we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs. In the following example, we run `stream()` to invoke the graph with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : \"Seattle\"}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Planner Example\n",
    "\n",
    "- To run this the system prompts and asks for user input for activities \n",
    "- We have initialized the graph state with city Seattle which usually will be dynamic and we will see in subsequrnt labs\n",
    "- You can enter like boating, swiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you create a itinerary for boating, swim. Need a complete plan\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leverage the memory saver to manipulate the Graph State\n",
    "- Since the `Conversation Messages` are part of the graph state we can leverage that\n",
    "- However the graph state is tied to `session_id` which will be passed in as a `thread_id` which ties to a session\n",
    "- If we add a request with different thread id it will create a new session which will not have the previous `Interests`\n",
    "- However this this has the other check points variables as well and so this pattern is good for `A-Sync` workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you add white water rafting to this itinerary\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run with another session\n",
    "\n",
    "Now this session will not have the previous conversations and we see it will create a new travel plan with the `white water rafting`  interests, not boating or swim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"11\"}}\n",
    "\n",
    "user_request = \"Can you add white water rafting to itinerary\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `External Store` for memory\n",
    "\n",
    "\n",
    "For Memory we further need short term and long term memory which can be explained below. Further reading can be at this [link](https://langchain-ai.github.io/langgraph/concepts/memory/#what-is-memory)\n",
    "\n",
    "Conversation memory can be explained by this diagram below which explains the `turn by turn` conversations which needs to be accessed by agents and then saved as a summary for long term memory\n",
    "\n",
    "![long term memory](./images/short-vs-long.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an external `Memory persistence`\n",
    "\n",
    "In this section we will leverage multi-thread, multi-session persistence to Chat Messages. Ideally you will leverage persistence like Redis Store etc to save messages per session\n",
    "\n",
    "##### Memory Management\n",
    "- We can have several Patterns - we can have each Agents with it's own Session memory\n",
    "- Or we can have the whole Graph have a combined memory in which case each agent will get it's own memory\n",
    "\n",
    "The MemorySaver or the Store have the concept of separating sections of memory by Namespaces or by Thread ID's and those can be leveraged to either 1/ Use the graph level message or memory 2/ Ecah agent can have it's own memory via space in saver or else having it's own saver like we do in the `ReACT agent`\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/multi_memory_light.png\" width=\"45%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore, Item, Op, Result\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from typing import Any, Iterable, Literal, NamedTuple, Optional, Union, cast\n",
    "\n",
    "class CustomMemoryStore(BaseStore):\n",
    "\n",
    "    def __init__(self, ext_store):\n",
    "        self.store = ext_store\n",
    "\n",
    "    def get(self, namespace: tuple[str, ...], key: str) -> Optional[Item]:\n",
    "        return self.store.get(namespace,key)\n",
    "\n",
    "    def put(self, namespace: tuple[str, ...], key: str, value: dict[str, Any]) -> None:\n",
    "        return self.store.put(namespace, key, value)\n",
    "    def batch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        return self.store.batch(ops)\n",
    "    async def abatch(self, ops: Iterable[Op]) -> list[Result]:\n",
    "        return self.store.abatch(ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at how to use this store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = CustomMemoryStore(InMemoryStore())\n",
    "namespace_u = (\"chat_messages\", \"user_id_1\")\n",
    "key_u=\"user_id_1\"\n",
    "in_memory_store.put(namespace_u, key_u, {\"data\":[\"list a\"]})\n",
    "item_u = in_memory_store.get(namespace_u, key_u)\n",
    "print(item_u.value, item_u.value['data'])\n",
    "\n",
    "in_memory_store.list_namespaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the similiar graph as earlier -- note we will not have any mesages in the Graph state as that has been externalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_interests(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    user_message = state['user_message'] #input(\"Your input: \")\n",
    "    return {\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    #- get the history from the store\n",
    "    user_u = f\"user_id_{config['configurable']['thread_id']}\"\n",
    "    namespace_u = (\"chat_messages\", user_u)\n",
    "    store_item = store.get(namespace=namespace_u, key=user_u)\n",
    "    chat_history_messages = store_item.value['data'] if store_item else []\n",
    "    print(user_u,chat_history_messages)\n",
    "\n",
    "    response = llm.invoke(itinerary_prompt.format_messages(city=state['city'], user_message=state['user_message'], chat_history=chat_history_messages))\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "\n",
    "    #- add back to the store\n",
    "    store.put(namespace=namespace_u, key=user_u, value={\"data\":chat_history_messages+[HumanMessage(content=state['user_message']),AIMessage(content=response.content)]})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"itinerary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store_n = CustomMemoryStore(InMemoryStore())\n",
    "\n",
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "#workflow.add_node(\"input_city\", input_city)\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "\n",
    "#workflow.add_edge(\"input_city\", \"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "\n",
    "app = workflow.compile(store=in_memory_store_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : \"Seattle\"}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you create a itinerary for boating, swim. Need a complete plan\"\n",
    "run_travel_planner(user_request, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you add itinerary for white water rafting to this\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at the store\n",
    "\n",
    "it will show the History of the Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_memory_store_n.list_namespaces())\n",
    "print(in_memory_store_n.get(('chat_messages', 'user_id_1'),'user_id_1').value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we review the concept of having Each `Agent` be backed by it's own memory\n",
    "\n",
    "For this we will leverage the RunnableWithMessageHistory when creating the agent\n",
    "- Here we create to simulate a InMemoryChatMessageHistory, but this will be externalized in produftion use cases\n",
    "- use this this as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "#model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"#\n",
    "#model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "provider_id = \"anthropic\"\n",
    "\n",
    "chatbedrock_llm = ChatBedrockConverse(\n",
    "    model=model_id,\n",
    "    provider=provider_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "itinerary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful travel assistant. Create a day trip itinerary for {city} based on the user's interests. Use the below chat conversation and the latest input from Human to get the user interests. Provide a brief, bulleted itinerary.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{user_message}\"),\n",
    "])\n",
    "chain = itinerary_prompt | chatbedrock_llm \n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str\n",
    "\n",
    "def input_interests(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    user_message = state['user_message'] #input(\"Your input: \")\n",
    "    return {\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    #- each agent manages it's memory\n",
    "    response = wrapped_chain.invoke({\"city\": state['city'], \"user_message\": state['user_message'], \"input\": state['user_message']} )\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"itinerary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "#workflow.add_node(\"input_city\", input_city)\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "\n",
    "#workflow.add_edge(\"input_city\", \"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : \"Seattle\"}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you create a itinerary for boating, swim. Need a complete plan\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_request = \"Can you add white water rafting to this itinerary\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us examine a `ReACT` agent with external memory\n",
    "\n",
    "langGraph executor is \n",
    "\n",
    "    - invoke with \"messages\" as  the query or you could pass in the chat history with the latest user message\n",
    "    - returns a message list variable and [-1] of that is the answer from the agents\n",
    "    - messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
    "\n",
    "LangGraph's prebuilt create_react_agent does not take a prompt template directly as a parameter, but instead takes a state_modifier parameter. This modifies the graph state before the llm is called, and can be one of four values:\n",
    "\n",
    "    - A SystemMessage, which is added to the beginning of the list of messages.\n",
    "    - A string, which is converted to a SystemMessage and added to the beginning of the list of messages.\n",
    "    - A Callable, which should take in full graph state. The output is then passed to the language model.\n",
    "    - Or a Runnable, which should take in full graph state. The output is then passed to the language model.\n",
    "\n",
    "Memory is just checkpointing -- you could use the external store\n",
    "\n",
    "    - Add a checkpointer to the agent and you get chat memory for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "#model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"#\n",
    "#model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "provider_id = \"anthropic\"\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=model_id,\n",
    "    provider=provider_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    ")\n",
    "@tool (\"book_appointment\")\n",
    "def book_appointment(date: str, time:str) -> dict:\n",
    "    \"\"\"Use this function to book an appointment. This function needs date and time as a string to books the appointment with the travel agent. This function returns the booking id back which you must send to the user\"\"\"\n",
    "\n",
    "    print(date, time)\n",
    "    return {\"status\" : True, \"date\": date, \"booking_id\": \"id_123\"}\n",
    "\n",
    "@tool (\"create_travel_plan\")\n",
    "def create_travel_plan( city: str, interest: str) -> dict:\n",
    "    \"\"\"Call to create a complete itinerary for the user. Input is the city and comman separated interests \"\"\"\n",
    "    print(f\"Travel tool {city} :: called with {interest}\")\n",
    "    interest_list = interest.split(\",\")\n",
    "    return_message = f\"Here is a complete itinerary and travel plan. You can do many things in {city} like {interest}.\"\n",
    "    for one in interest_list:\n",
    "        return_message += f\" For {one}: you should go to Pier_{one}:: and do the activity {one}.\"\n",
    "    \n",
    "    return {\"travel_plan\": return_message+\".That completes the itinerary.\"}\n",
    "\n",
    "\n",
    "tool_list_str = [ \"book_appointment\", \"create_travel_plan\" ]\n",
    "tools_list = [book_appointment,create_travel_plan ]\n",
    "\n",
    "itinerary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant to create travel itinerary for city {city} based on the user interests. You can use tools as needed. Only call the tool once\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def callable_modify_state_messages(state: AgentState):\n",
    "    #print(f\"\\nstate={state}::\")\n",
    "    input_to_model = itinerary_prompt.invoke({ \"city\": \"Seattle\", }).to_messages() + state['messages']\n",
    "    #print(input_to_model)\n",
    "    return input_to_model\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "memory = MemorySaver()\n",
    "langgraph_agent_executor = create_react_agent(llm, tools_list,  checkpointer=memory, state_modifier=callable_modify_state_messages)\n",
    "\n",
    "#display(Image(langgraph_agent_executor.get_graph().draw_mermaid_png()))\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"test-thread-1\",}, \"recursion_limit\": 6, 'stream_mode':\"updates\"}\n",
    "response_agent = langgraph_agent_executor.invoke(\n",
    "    input={\"messages\": [HumanMessage(content=\"Can you create a itinerary for interests boating, swiming only for the city of Seattle. Need a complete plan, do not summarize\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"input\": response_agent[\"messages\"][0].content,#query,\n",
    "        \"output\": response_agent[\"messages\"][-1].content,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To view the `Chat_history` just print the full response\n",
    "\n",
    "- we will now add to the conversation as say add white water rafting and we will see it will add that to the itinerary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm.bind_tools(tools_list).invoke(\"can you create itenary for me for boat in Seattle?\")\n",
    "response_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_agent = langgraph_agent_executor.invoke(\n",
    "    input={\"messages\": [HumanMessage(content=\"can you white water rafting to this itinerary\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\n",
    "    {\n",
    "        \"input\": response_agent[\"messages\"][0].content,#query,\n",
    "        \"output\": response_agent[\"messages\"][-1].content,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    itinerary: str\n",
    "    city: str\n",
    "    user_message: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_interests(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    user_message = state['user_message'] #input(\"Your input: \")\n",
    "    return {\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState, config: RunnableConfig, *, store: BaseStore) -> PlannerState:\n",
    "    #- get the history from the store\n",
    "    user_u = f\"user_id_{config['configurable']['thread_id']}\"\n",
    "    namespace_u = (\"chat_messages\", user_u)\n",
    "    store_item = store.get(namespace=namespace_u, key=user_u)\n",
    "    chat_history_messages = store_item.value['data'] if store_item else []\n",
    "    print(user_u,chat_history_messages)\n",
    "\n",
    "    response = llm.invoke(itinerary_prompt.format_messages(city=state['city'], user_message=state['user_message'], chat_history=chat_history_messages))\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "\n",
    "    #- add back to the store\n",
    "    store.put(namespace=namespace_u, key=user_u, value={\"data\":chat_history_messages+[HumanMessage(content=state['user_message']),AIMessage(content=response.content)]})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"itinerary\": response.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store_n = CustomMemoryStore(InMemoryStore())\n",
    "\n",
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "#workflow.add_node(\"input_city\", input_city)\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "\n",
    "#workflow.add_edge(\"input_city\", \"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "\n",
    "app = workflow.compile(store=in_memory_store_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str, config_dict: dict):\n",
    "    print(f\"Current User Request: {user_request}\\n\")\n",
    "    init_input = {\"user_message\": user_request,\"city\" : \"Seattle\"}\n",
    "\n",
    "    for output in app.stream(init_input, config=config_dict, stream_mode=\"values\"):\n",
    "        pass  # The nodes themselves now handle all printing\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "user_request = \"Can you create a itinerary for boating, swim. Need a complete plan\"\n",
    "run_travel_planner(user_request, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully executed a simple LangGraph implementation, this lab demonstrates how LangGraph can be used to create a simple yet effective Travel Planner. By structuring our application as a graph of interconnected nodes, we achieve a clear separation of concerns and a easily modifiable workflow. This approach can be extended to more complex applications, showcasing the power and flexibility of graph-based designs in AI-driven conversational interfaces.\n",
    "\n",
    "Please proceed to the next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToolNode\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
