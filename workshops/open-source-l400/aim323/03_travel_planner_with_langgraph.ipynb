{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building a Travel Planner with a Simple LangGraph\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab guides you through the process of creating a simple Travel Planner using LangGraph, a library for building stateful, multi-step applications with language models. The Travel Planner demonstrates how to structure a conversational AI application that collects user input and generates personalized travel itineraries.\n",
    "\n",
    "\n",
    "## Use Case Details\n",
    "\n",
    "Our Travel Planner follows a straightforward, three-step process:\n",
    "\n",
    "1. **Initial User Input**: \n",
    "   - The application prompts the user to enter their desired travel plan to get assistance from AI Agent.\n",
    "   - This information is stored in the state.\n",
    "\n",
    "2. **Interests Input**:\n",
    "   - The user is asked to provide their interests for the trip.\n",
    "   - These interests are stored as a list in the state.\n",
    "\n",
    "3. **Itinerary Creation**:\n",
    "   - Using the collected city and interests, the application leverages a language model to generate a personalized day trip itinerary.\n",
    "   - The generated itinerary is presented to the user.\n",
    "\n",
    "The flow between these steps is managed by LangGraph, which handles the state transitions and ensures that each step is executed in the correct order.\n",
    "\n",
    "The below diagram illustrates this:\n",
    "\n",
    "![Travel Planner Agent](./images/agents_itinerary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "# langchain>=0.3.7 \\ \n",
    "# langchain-anthropic>=0.1.15 \\\n",
    "# langchain-aws>=0.2.6 \\\n",
    "# langchain-community>=0.3.5 \\\n",
    "# langchain-core>=0.3.15 \\\n",
    "# langchain-text-splitters>=0.3.2 \\\n",
    "# langchainhub>=0.1.20 \\\n",
    "# langgraph>=0.2.45 \\\n",
    "# langgraph-checkpoint>=2.0.2 \\\n",
    "# langgraph-sdk>=0.1.35 \\\n",
    "# langsmith>=0.1.140 \\\n",
    "# sqlalchemy -U \\\n",
    "# \"faiss-cpu>=1.7,<2\" \\\n",
    "# \"pypdf>=3.8,<4\" \\\n",
    "# \"ipywidgets>=7,<8\" \\\n",
    "# matplotlib>=3.9.0 \\\n",
    "\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install -U --no-cache-dir boto3\n",
    "#%pip install grandalf==3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent State\n",
    "\n",
    "We'll define the state that our agent will maintain throughout its operation. First, define the [State](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of the graph.  The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "Let's use the `TypedDict` class from python's `typing` module as our schema, which provides type hints for the keys.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **StateGraph**: The core of our application, defining the flow of our Travel Planner.\n",
    "2. **PlannerState**: A custom type representing the state of our planning process.\n",
    "3. **Node Functions**: Individual steps in our planning process (input_city, input_interests, create_itinerary).\n",
    "4. **LLM Integration**: Utilizing a language model to generate the final itinerary.\n",
    "5. **Memory Integration**: Utilizing long term and short term memory for conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PlannerState(TypedDict):\n",
    "    messages: Annotated[List[HumanMessage | AIMessage], \"The messages in the conversation\"]\n",
    "    interests: List[str]\n",
    "    itinerary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Language Model and Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import ChatBedrock\n",
    "import boto3\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    # model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "\n",
    "itinerary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful travel assistant. Create a day trip itinerary for {city} based on the user's interests: {interests}. Provide a brief, bulleted itinerary.\"),\n",
    "    (\"human\", \"Create an itinerary for my day trip.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def input_interests(state: PlannerState) -> PlannerState:\n",
    "    print(f\"Please enter your interests for the trip to {state['city']} (comma-separated):\")\n",
    "    user_message = input(\"Your input: \")\n",
    "    return {\n",
    "        **state,\n",
    "        \"interests\": [interest.strip() for interest in user_message.split(',')],\n",
    "        \"messages\": state['messages'] + [HumanMessage(content=user_message)],\n",
    "    }\n",
    "\n",
    "def create_itinerary(state: PlannerState) -> PlannerState:\n",
    "    print(f\"Creating an itinerary for {state['city']} based on interests: {', '.join(state['interests'])}...\")\n",
    "    response = llm.invoke(itinerary_prompt.format_messages(city=state['city'], interests=\", \".join(state['interests'])))\n",
    "    print(\"\\nFinal Itinerary:\")\n",
    "    print(response.content)\n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state['messages'] + [AIMessage(content=response.content)],\n",
    "        \"itinerary\": response.content,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Compile the Graph\n",
    "\n",
    "Now we'll create our LangGraph workflow and compile it. We build the graph from our [components](\n",
    "https://langchain-ai.github.io/langgraph/concepts/low_level/) defined above. The [StateGraph class](https://langchain-ai.github.io/langgraph/concepts/low_level/#stategraph) is the graph class that we can use.\n",
    " \n",
    "First, we initialize a StateGraph with the `State` class we defined above. Then, we add our nodes and edges. We use the [`START` Node, a special node](https://langchain-ai.github.io/langgraph/concepts/low_level/#start-node) that sends user input to the graph, to indicate where to start our graph. The [`END` Node](https://langchain-ai.github.io/langgraph/concepts/low_level/#end-node) is a special node that represents a terminal node. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(PlannerState)\n",
    "\n",
    "#workflow.add_node(\"input_city\", input_city)\n",
    "workflow.add_node(\"input_interests\", input_interests)\n",
    "workflow.add_node(\"create_itinerary\", create_itinerary)\n",
    "\n",
    "workflow.set_entry_point(\"input_interests\")\n",
    "\n",
    "#workflow.add_edge(\"input_city\", \"input_interests\")\n",
    "workflow.add_edge(\"input_interests\", \"create_itinerary\")\n",
    "workflow.add_edge(\"create_itinerary\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the graph structure\n",
    "\n",
    "Finally, we [compile our graph](https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) to perform a few basic checks on the graph structure. We can visualize the graph as a [Mermaid diagram](https://github.com/mermaid-js/mermaid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that runs the graph\n",
    "\n",
    "When we compile the graph, we turn it into a LangChain Runnable, which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs. In the following example, we run `stream()` to invoke the graph with inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_travel_planner(user_request: str):\n",
    "    print(f\"Initial Request: {user_request}\\n\")\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=user_request)],\n",
    "        \"interests\": [],\n",
    "        \"itinerary\": \"\",\n",
    "    }\n",
    "    \n",
    "    for output in app.stream(state):\n",
    "        pass  # The nodes themselves now handle all printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Planner Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_request = \"I want to plan a day trip to lake tahoe with my family. Need a complete plan\"\n",
    "run_travel_planner(user_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Agent Nodes\n",
    "\n",
    "Now we'll define the main functions nodes that our agent will use: get interests, create itinerary. [Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) are just python functions.\n",
    "\n",
    "The first positional argument is the state, as defined above. Because the state is a `TypedDict` with schema as defined above, each node can access the key, `graph_state`, with `state['graph_state']`. Each node returns a new value of the state key `graph_state`.\n",
    "  \n",
    "By default, the new value returned by each node [will override](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) the prior state value.\n",
    "\n",
    "#### Memory Management\n",
    "memory is key for any agentic conversation which is `Multi-Turn` or `Multi-Agent` colloboration conversation and more so if it spans multiple days. The 3 main aspects of Agents are:\n",
    "1. Tools\n",
    "2. Memory\n",
    "3. Planners\n",
    "\n",
    "![Agent memory](./images/agents_memory.png)\n",
    "\n",
    "For Memory we further need short term and long term memory which can be explained below. Further reading can be at this ![link ](https://langchain-ai.github.io/langgraph/concepts/memory/#what-is-memory)\n",
    "\n",
    "![long term memory](./images/short-vs-long.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This below LEVARAGES the In-memory with multiple sessions and session id\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = prompt | chatbedrock_llm | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"what is the weather like in Seattle WA\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now ask another question and we will see the History conversation was maintained\")\n",
    "print_ww(wrapped_chain.invoke(\n",
    "    {\"input\": \"Ok what are benefits of this weather in 100 words?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "))\n",
    "\n",
    "print(\"\\n\\n now check the history\")\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use with external memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_aws.chat_models.bedrock_converse import ChatBedrockConverse\n",
    "\n",
    "book_cancel_agent, agent_executor_book_cancel = None, None\n",
    "\n",
    "def create_book_cancel_agent():\n",
    "\n",
    "    @tool (\"book_appointment\")\n",
    "    def book_appointment(date: str, time:str) -> dict:\n",
    "        \"\"\"Use this function to book an appointment. This function needs date and time as a string to books the appointment with the doctor. This function returns the booking id back which you must send to the user\"\"\"\n",
    "\n",
    "        print(date, time)\n",
    "        return {\"status\" : True, \"date\": date, \"booking_id\": \"id_123\"}\n",
    "        \n",
    "    @tool (\"cancel_appointment\")\n",
    "    def cancel_appointment(booking_id: str) -> dict:\n",
    "        \"\"\"Use this function to cancel the appointment. This function needs a booking id to cancel the appointment with the doctor. This function returns the status of the booking and the booking id which you must return back to the user \"\"\"\n",
    "\n",
    "        print(booking_id)\n",
    "        return {\"status\" : True, \"booking_id\": booking_id}\n",
    "\n",
    "    @tool (\"need_more_info\")\n",
    "    def need_more_info() -> dict:\n",
    "        \"\"\"Use this function to get more information from the user.  This function returns the date and time needed for the booking of appointment \"\"\"\n",
    "\n",
    "        return {\"date\": \"August 11, 2024\", \"time\": \"11:00 am\"}\n",
    "\n",
    "    # BOTH prompt templates work -- \n",
    "\n",
    "    prompt_template_sys = \"\"\"\n",
    "\n",
    "    Use the following format:\n",
    "    Question: the input question you must answer\n",
    "    Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "    Action: the action to take, should be one of [ \"book_appointment\", \"cancel_appointment\"]\n",
    "    Action Input: the input to the action\\nObservation: the result of the action\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    Question: {input}\n",
    "\n",
    "    Assistant:\n",
    "    {agent_scratchpad}'\n",
    "\n",
    "    \"\"\"\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "        HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "    ]\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate(\n",
    "        input_variables=['agent_scratchpad', 'input'], \n",
    "        messages=messages\n",
    "    )\n",
    "    #print_ww(f\"\\nCrafted::prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "\n",
    "    prompt_template_sys = \"\"\"\n",
    "\n",
    "    Use the following format:\n",
    "    Question: the input question you must answer. \n",
    "    Thought: you should always think about what to do, Also try to follow steps mentioned above. If you need information do not make it up but return with \"need_more_info\"\n",
    "    Action: the action to take, should be one of [ \"book_appointment\", \"cancel_appointment\", \"need_more_info\"]\n",
    "    Action Input: the input to the action\\nObservation: the result of the action\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "            messages = [\n",
    "                (\"system\", prompt_template_sys),\n",
    "                (\"placeholder\", \"{chat_history}\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    #print_ww(f\"\\nCrafted::prompt:template:{chat_prompt_template}\")\n",
    "\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "\n",
    "    model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "    chat_bedrock_appointment = ChatBedrock(\n",
    "        model_id=modelId,\n",
    "        client=boto3_bedrock,\n",
    "        model_kwargs=model_parameter, \n",
    "        beta_use_converse_api=True\n",
    "    )\n",
    "\n",
    "\n",
    "    tools_list_book = [ book_appointment, cancel_appointment, need_more_info]\n",
    "\n",
    "    # Construct the Tools agent\n",
    "    book_cancel_agent_t = create_tool_calling_agent(chat_bedrock_appointment, tools_list_book,chat_prompt_template)\n",
    "    \n",
    "    #return book_cancel_agent_t\n",
    "    agent_executor_t = AgentExecutor(agent=book_cancel_agent_t, tools=tools_list_book, verbose=True, max_iterations=5, return_intermediate_steps=True)\n",
    "    return book_cancel_agent_t, agent_executor_t\n",
    "\n",
    "book_cancel_history = InMemoryChatMessageHistory()\n",
    "book_cancel_history.add_user_message(\"can you book an appointment?\")\n",
    "book_cancel_history.add_ai_message(\"What is the date and time you wish for the appointment\")\n",
    "book_cancel_history.add_user_message(\"I need for August 10, 2024 at 10:00 am?\")\n",
    "\n",
    "user_query = \"can you book an appointment for me?\" # \"can you book an appointment for me for August 10, 2024 at 10:00 am?\"\n",
    "\n",
    "if book_cancel_agent == None:\n",
    "    book_cancel_agent, agent_executor_book_cancel = create_book_cancel_agent()\n",
    "    \n",
    "agent_executor_book_cancel.invoke(\n",
    "    {\"input\": user_query, \"chat_history\": book_cancel_history.messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ") # ['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully executed a simple LangGraph implementation, this lab demonstrates how LangGraph can be used to create a simple yet effective Travel Planner. By structuring our application as a graph of interconnected nodes, we achieve a clear separation of concerns and a easily modifiable workflow. This approach can be extended to more complex applications, showcasing the power and flexibility of graph-based designs in AI-driven conversational interfaces.\n",
    "\n",
    "Please proceed to the next lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
