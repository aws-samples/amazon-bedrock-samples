{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ccccde-1a5f-4576-8a4a-c7ce49ecf49f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lab2: Data Prep for Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this notebook, we will ingest all the travel document PDF into a Vector Store for RAG workflow. Retrieval Augmented Generation (RAG) requires the indexation of relevant unstructured documents into a vector database. Then given a end uset query, the relevant chunks are retrieved and passed as context to the model, which generates an answer. This can best be described by the following flow.\n",
    "\n",
    "<img src=\"./images/rag-workflow.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73dee7e-8c25-4180-a7d5-e453d2020dfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Ingestion Workflow\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) system, data ingestion is a critical workflow where raw data is prepared and loaded into a vector database (VectorDB) so that it can be efficiently retrieved and used by a language model during query time. The ingestion process involves several key steps, including sourcing the data, chunking it into manageable pieces, embedding it into vector representations, and loading these embeddings into the vector database. Here's an expanded explanation of each step:\n",
    "\n",
    "1. Data Source\n",
    "Description: The data source is the origin from which the information is collected. This could be a variety of formats such as documents (PDFs, Word files), databases, APIs, websites (using web scraping), or even multimedia content like images or videos.\n",
    "\n",
    "2. Chunking\n",
    "Description: Chunking involves breaking down the data into smaller, manageable pieces or “chunks.” This is essential because embedding entire documents may not be efficient or effective, and smaller chunks allow the system to provide more precise responses based on specific segments of information.\n",
    "\n",
    "3. Embedding\n",
    "Description: Embedding is the process of converting text chunks into numerical vector representations using a pre-trained embedding model. These vectors capture the semantic meaning of the chunks, allowing the system to compare and retrieve chunks based on their content rather than exact keyword matches.\n",
    "\n",
    "4. Loading to VectorDB\n",
    "Description: The final step in the ingestion workflow is loading the embedded vectors and their metadata into a vector database (VectorDB). This allows for efficient storage and retrieval of the chunks based on their semantic similarity to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2484a06-2548-49a3-bce4-faf104bc24b8",
   "metadata": {},
   "source": [
    "#### Imports and Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e608058-df69-49bd-9e70-da8e45f8206e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_aws.chat_models import ChatBedrock\n",
    "from pathlib import Path\n",
    "from rich import print as rprint\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS, DistanceStrategy\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "\n",
    "import faiss\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695360fe-707c-4916-ba35-eda64738cafd",
   "metadata": {},
   "source": [
    "#### Amazon Bedrock Models\n",
    "\n",
    "The Amazon Titan Embedding v2 model is part of the Amazon Titan family of foundation models available through Amazon Bedrock, optimized for creating high-quality vector embeddings from textual data. Embedding models convert text into numerical representations (vectors) that capture the semantic meaning of the content. These vectors can then be used for various tasks, such as information retrieval, clustering, similarity search, and more, which are critical in applications like search engines, recommendation systems, and Retrieval-Augmented Generation (RAG) workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f5981-3267-48fe-9b83-9df00cbfd368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    model_kwargs={\"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41074367-4093-4b01-9760-d314d472e93a",
   "metadata": {},
   "source": [
    "#### Load the Travel Documents Dataset \n",
    "\n",
    "- `PyPDFLoader:` This is a utility in LangChain that efficiently reads and extracts structured text data from PDF documents. It can handle complex PDF structures, making it ideal for preprocessing documents for use in information retrieval, RAG systems, or other document-based AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76deb6-f5bc-467d-8cdd-797c8ddf73c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_path = Path(\"./data/us/\")\n",
    "doc_files = list(docs_path.glob(\"*.pdf\"))\n",
    "\n",
    "section_chunks = []\n",
    "\n",
    "for doc_path in doc_files:\n",
    "    loader = PyPDFLoader(file_path=doc_path.as_posix())\n",
    "    #loader.parser = PyPDFOutlineParser()\n",
    "    sections = loader.load()\n",
    "    for sec in sections:\n",
    "        sec.metadata.update({\"file\": doc_path.name})\n",
    "    \n",
    "    section_chunks += sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b574d5f-f315-4967-a7c3-def78ee26f8e",
   "metadata": {},
   "source": [
    "A PyPDFLoader instance is created, which is a utility in LangChain designed to load and extract text from PDF files.\n",
    "The loader.load() method is called to extract sections (chunks of text) from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c242ace-1b0c-4ae6-a5a4-23aa8034ab19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rprint(section_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b5dda-efc0-4740-9dcf-52e899a74728",
   "metadata": {},
   "source": [
    "#### Explore the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f04a1c-135e-4793-ab68-f993b9ebef98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_embedding = bedrock_embeddings.embed_query(section_chunks[0].page_content)\n",
    "    modelId = bedrock_embeddings.model_id\n",
    "    rprint(\"Embedding model Id :\", modelId)\n",
    "    rprint(\"Size of the embedding: \", len(sample_embedding))\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding[:30])\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "              \\x1b[0m\")\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702eb0a-591d-40c9-a47d-0cd59cd11ed2",
   "metadata": {},
   "source": [
    "#### Ingest Data into FAISS Vector DB\n",
    "\n",
    "We will utilize a few different techniques when loading the documents that will help improve the retrieval quality.\n",
    "\n",
    "#### 1. Outline based splitting\n",
    "By default LangChain's `PyPDFLoader` will break each document up into pages. We could then potentially use a chunking strategy such as `RecursiveCharacterTextSplitter` to further break down the pages into smaller chunks. \n",
    "However, this could lead to suboptimal results if the most relevant information we are looking for is split across multiple pages. \n",
    "\n",
    "Now we are ready to ingest the documents into the vector store. This can be done easily using the [LangChain FAISS integration](https://python.langchain.com/docs/integrations/vectorstores/faiss/) which takes in the embeddings model and the documents to create the entire vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc537ae2-cf31-4bee-9a93-cde17aa626f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_store_time_stamp = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "docstore = InMemoryDocstore()\n",
    "index = faiss.IndexFlatL2(len(sample_embedding))\n",
    "vector_db = FAISS(embedding_function=bedrock_embeddings, \n",
    "                  index=index, \n",
    "                  index_to_docstore_id={},\n",
    "                  docstore=docstore, \n",
    "                  distance_strategy=DistanceStrategy.COSINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08acc1de-e9d5-477b-9980-77da228e2aae",
   "metadata": {},
   "source": [
    "#### 2. Parent Document Retriever\n",
    "After we've loaded the document as individual sections, we will further split these sections by paragraphs using the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/). These are the chunks that will be used for embeddings, however during retrieval we'll utilize the [ParentDocumentRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever/) to retrieve the entire section that the chunk belongs to. This is done to ensure that the context provided to the model is as complete as possible.\n",
    "\n",
    "Next we build the `ParentDocumentRetriever` combining an FAISSbased vector store and key-value based `InMemoryStore`. The vector store will be used to find section segments that were generated using through splitting with the `RecursiveCharacterSplitter`. Each section segment will contain a key reference to the full section document. The key reference will be used to retrieve the entire section text. Note that the `InMemoryStore` is essentially a python dictionary, in production you would want to use a persistent store such as [DynamoDB](https://aws.amazon.com/dynamodb/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019233e-8a89-45cd-b02b-fa9aeb0444fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\", \"\\n\\n\"], chunk_size=2000, chunk_overlap=250\n",
    ")\n",
    "\n",
    "in_memory_store_file = f\"section_doc_store_{vec_store_time_stamp}.pkl\"\n",
    "vector_store_file = f\"section_vector_store_{vec_store_time_stamp}.pkl\"\n",
    "local_vector_config = \"local_config.json\"\n",
    "\n",
    "# if we previously ingested the docs we can reuse the existing index\n",
    "if Path(local_vector_config).exists():\n",
    "    in_memory_store_file = json.load(open(local_vector_config))[\"in_memory_store_file\"]\n",
    "    vector_store_file = json.load(open(local_vector_config))[\"vector_store_file\"]\n",
    "    \n",
    "    store = pickle.load(open(in_memory_store_file, \"rb\"))\n",
    "    vector_db_buff = BytesIO(pickle.load(open(vector_store_file, \"rb\")))\n",
    "    vector_db = FAISS.deserialize_from_bytes(serialized=vector_db_buff.read(), embeddings=bedrock_embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vector_db,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "    )\n",
    "\n",
    "# ingest the document into the index\n",
    "else:\n",
    "    store = InMemoryStore()\n",
    "    \n",
    "    retriever = ParentDocumentRetriever(\n",
    "        vectorstore=vector_db,\n",
    "        docstore=store,\n",
    "        child_splitter=child_splitter,\n",
    "    )\n",
    "    \n",
    "    retriever.add_documents(section_chunks, ids=None)\n",
    "    pickle.dump(store, open(in_memory_store_file, \"wb\"))\n",
    "    pickle.dump(vector_db.serialize_to_bytes(), open(vector_store_file, \"wb\"))\n",
    "    \n",
    "    with open(local_vector_config, \"w\") as f:\n",
    "        json.dump({\"in_memory_store_file\": in_memory_store_file, \"vector_store_file\": vector_store_file}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca047fe0-00ca-4b60-8b44-df89cb2a32a8",
   "metadata": {},
   "source": [
    "### Content Generation Workflow\n",
    "Let's first explore the various ways that we can query the vector store exclusively. [Semantic search](https://www.elastic.co/what-is/semantic-search) considers the context and intent of a query. Unlike traditional keyword based searches, semantic search utilize embedding that capture the meaning of the text. This allows for more relevant results to be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8358e6-71c1-4b66-96bf-dc50bbf73ec6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search query\n",
    "query = \"Top attractions in New York City?\"\n",
    "\n",
    "# Search for the 3 most relevant documents\n",
    "results = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28b869-8117-480a-9f85-f298c1b53b49",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance search (MMR)\n",
    "If you’d like to look up for some similar documents, but you’d also like to receive diverse results, MMR is a method you should consider. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbec7e2-d9cf-4791-9f8b-60efaed4d130",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = vector_db.max_marginal_relevance_search(query, k=3, fetch_k=10)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9251c6-5ca6-4bd0-9b78-66d101c011cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search query\n",
    "query = \"Summarize top attractions in athens and barcelona\"\n",
    "\n",
    "# Search for the 3 most relevant documents\n",
    "results = vector_db.similarity_search(query, k=3)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbcc7f-3f56-4a7e-afb8-17243db78f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. \n",
    "If the context does not provide sufficient information to answer the question, politely indicate that you are unable to assist. \n",
    "Only answer questions related to model risk and model governance.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# in the first step we retrieve the context and pass through the input question\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    \n",
    ")\n",
    "\n",
    "# In the subsequent steps pass the context and question to the prompt, send the prompt to the llm and parse the output as a string\n",
    "chain = setup_and_retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7143bf-968e-471c-aff1-7961b8360ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Summarize top attractions in athens and barcelona\"\n",
    "\n",
    "response = chain.invoke(query)\n",
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8dba6-c27b-4d3a-8f2d-3c936b3bf3ff",
   "metadata": {},
   "source": [
    "### Improve RAG Performance with Advanced methods of retrieval\n",
    "\n",
    "The main driver of performance for RAG pipelines is the retrieval mechanism. This step involves identifying a subset of documents that are most relevant to the original query. The common baseline is generally to embed the query in its original form and pull the top-K nearest documents. However, for some datasets this begins to fall short in cases where queries address multiple topics or, more generally, are phrased in a way that is incompatible or is dissimilar to the documents that should be retrieved. We look at how it is possible to improve on these types of queries. \n",
    "\n",
    "Given the increase complexity of the tasks in this section, we choose to leverage Claude 3 Sonnet in this part of the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5032b4-8692-490a-8d95-1e00ddbd3bfd",
   "metadata": {},
   "source": [
    "#### 1.Decomposition\n",
    "\n",
    "For more complex queries, it may be helpful to breakdown the original question into sub-problems each having their own retrieval step. We perform query decomposition to return the original question or an equivalent set of questions each with a single target.\n",
    "\n",
    "This process is driven by the underlying model. We define the system prompt describing the intended task and supply static few-shot examples to enable the model to better generalize. Removing these examples yields results that are less robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19046684-98ff-48a4-86f9-e4185b9a518b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decomp_system_prompt = \"\"\"You are an expert travel assistant that prepares queries to be sent to a search component.\n",
    "These queries may be very complex, involving multiple destinations, activities, or conditions. Your job is to simplify complex travel-related queries into multiple queries that can be answered in isolation from each other.\n",
    "\n",
    "If the query is simple, keep it as it is.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Here are some examples of how to respond in a standard interaction:\n",
    "\n",
    "<example>\n",
    "- Query: Compare the best beach destinations in Hawaii and the Bahamas for a family vacation.\n",
    "Decomposed Questions: [SubQuery(sub_query='What are the best beach destinations in Hawaii for a family vacation?'), SubQuery(sub_query='What are the best beach destinations in the Bahamas for a family vacation?')]\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "- Query: Find the best time to visit Paris and what activities are recommended during that time.\n",
    "Decomposed Questions: [SubQuery(sub_query='What is the best time to visit Paris?'), SubQuery(sub_query='What activities are recommended in Paris during the best time to visit?')]\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "- Query: Suggest hiking spots in Colorado and nearby accommodations.\n",
    "Decomposed Questions: [SubQuery(sub_query='What are the best hiking spots in Colorado?'), SubQuery(sub_query='What are the nearby accommodations available for hiking spots in Colorado?')]\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "- Query: What is the capital of Australia?\n",
    "Decomposed Questions: [SubQuery(sub_query='What is the capital of Australia?')]\n",
    "</example>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5af346-85be-4761-8d7d-dafc73e58e7a",
   "metadata": {},
   "source": [
    "To ensure a consistent format is returned for subsequent steps, we use Pydantic, a data-validation library. We rely on a Pydantic-based helper function for doing the tool config translation for us in a way that ensures we avoid potential mistakes when defining our tool config schema in a JSON dictionary.\n",
    "\n",
    "We define `SubQuery` to be a query corresponding to a subset of the points of a larger parent query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081f7fe-6ee8-4f20-ba3a-32b7e2139f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubQuery(BaseModel):\n",
    "    \"\"\"You have performed query decomposition to generate a subquery of a question\"\"\"\n",
    "\n",
    "    sub_query: str = Field(description=\"A unique subquery of the original question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b54603-bb61-44b9-8984-578326c95b71",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `SubQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf87d4-3599-42d9-bf38-e66df98d8365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_decomposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", decomp_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([SubQuery])\n",
    "decomp_query_analyzer = query_decomposition_prompt | llm_with_tools | PydanticToolsParser(tools=[SubQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606049b-ca7d-478c-97ff-ae3378897f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = decomp_query_analyzer.invoke({\"question\": \"Summarize top attractions in athens and barcelona\"})\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a72b1a-b849-4c72-bd50-cabdb0dd758c",
   "metadata": {},
   "source": [
    "#### Expansion\n",
    "\n",
    "Query expansion is similar to decomposition in that it produces multiple queries as a strategy to improve the odds of hitting a relevant result. However, expansion returns multiple different wordings of the original query.  \n",
    "\n",
    "We define the system prompt to consistently return 3 versions of the original query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1f036-f228-4346-b106-0298861cd9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paraphrase_system_prompt = \"\"\"You are an expert at converting user questions into database queries. \n",
    "You have access to a database of travel destinations and a list of recent destinations for travelers. \n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \n",
    "or common synonyms for key words in the question, make sure to return multiple versions \n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Always return at least 3 versions of the question.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eba1da-65d4-44fd-aacb-c93cc92c5004",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `ParaphrasedQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf42914-c61b-4bca-9d64-857dc32c31d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "\n",
    "    paraphrased_query: str = Field(description=\"A unique paraphrasing of the original question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4beb7-515b-49d7-8d7e-b1244eafd264",
   "metadata": {},
   "source": [
    "We define the prompt template leveraging the previously defined system prompt. We then expose `ParaphrasedQuery` as a tool the model can leverage. This enables to model to format one or more requests to this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c139b-0d84-40bd-acc1-119fe0bb9f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", paraphrase_system_prompt),\n",
    "        (\"human\", \"Here is the customer's question: <question>{question}</question> How do you answer to the instructions?\"),\n",
    "    ]\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_expansion = query_expansion_prompt | llm_with_tools | PydanticToolsParser(tools=[ParaphrasedQuery])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46feb2-c86b-4c25-af43-2b5643a0f3fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now no matter the nature of the query, the model generates alternatives that can be sent for retrieval in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dabd5a-9cd8-4dc2-bbec-efff5aa5a37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion.invoke({\"question\": \"Summarize top attractions in athens and barcelona\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567eaa1c-4388-40f4-b14c-dcfb76497b8a",
   "metadata": {},
   "source": [
    "### Key Take Aways\n",
    "\n",
    "In Retrieval-Augmented Generation (RAG), query analysis through decomposition and expansion enhances performance by refining search accuracy and retrieval quality:\n",
    "\n",
    "- Query Decomposition: Breaks complex queries into manageable sub-queries, helping retrieve more precise, relevant information for each part of the question.\n",
    "- Query Expansion: Enriches the query with synonyms or related terms, increasing the likelihood of retrieving contextually relevant documents.\n",
    "\n",
    "Using these query analysis techniques improve RAG's relevance, context accuracy, and overall response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a189230-da44-4ffc-a03c-183c6c13d629",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5986d52-c250-47a1-8377-6a43013b420a",
   "metadata": {},
   "source": [
    "We will leverage the FAISS vector DB we established in the lab to integrate with Agents in the upcoming labs. You have successfully completed RAG lab, please proceed to the next labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3b460-6e52-42b7-afe3-9a0beb322944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
