{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51194982-9bd9-4b79-85a3-16cbdf36d63b",
   "metadata": {},
   "source": [
    "# Agent Evaluation\n",
    "\n",
    "In this section, we will explore the evaluation of agentic systems. Agentic systems are complex constructs consisting of multiple sub-components. In Lab 3, we examined a simple singleton agent orchestrating between two tools. Lab 4 demonstrated a more sophisticated multi-layer agentic system with a top-level router agent coordinating multiple agentic sub-systems.\n",
    "\n",
    "One direct implication of the interdependent and potentially nested nature of agentic systems is that evaluation can occur at both macro and micro levels. This means that either the entire system as a whole is being evaluated (macro view) or each individual sub-component is assessed (micro view). For nested systems, this applies to every level of abstraction.\n",
    "\n",
    "Typically, evaluation begins at the macro level. In many cases, positive evaluation results at the macro level indicate sufficient agent performance. If macro-level performance evaluation proves insufficient or yields poor results, micro-level evaluation can help decompose the performance metrics and attribute results to specific sub-components.\n",
    "\n",
    "In this lab, we will first explore macro-level agent performance evaluation. Then, we will evaluate tool usage as a lower-level task. To maintain focus on the evaluation process, we will keep the agentic system simple by utilizing the singleton agent composed in Lab 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64169599-95e0-4b76-a81e-c68d479eb79a",
   "metadata": {},
   "source": [
    "## Agent Setup\n",
    "\n",
    "As covered in the previous section, we will reuse the Agent we built in Lab 3. This agent has access to tools designed to help find vacation destinations. You'll be able to interact with the agent by asking questions, observe it utilizing various tools, and engage in meaningful conversations.\n",
    "\n",
    "Let's begin by installing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861b440-3598-4140-af69-6bd993f861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community langgraph langchain-chroma langchain_aws pandas ragas==0.2.3 pypdf rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910fdd3-006b-42b1-88f3-9fee1bc80707",
   "metadata": {},
   "source": [
    "### Util functions part 1 - importing singleton agent\n",
    "\n",
    "To maintain a clean and focused approach in this notebook, we have moved the agent creation logic to a module in `utils.py`. The `create_agent` function replicates the agent creation process we developed in Lab 3, with an additional boolean parameter `enable_memory` that controls whether memory checkpointing is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783694df-e31d-448a-af7b-0c3fc6747c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_agent\n",
    "agent_executor = create_agent(enable_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3776a1d-7566-4652-acd2-7eed21f83d59",
   "metadata": {},
   "source": [
    "The ```create_agent``` function returns a ```CompiledStateGraph``` object that represents the Agent from Lab 3's scenario. \n",
    "Now, let's proceed to visualize this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d0ae3-13ca-4bdc-bcc8-087eece6b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2275d7-cdc2-439d-a78e-5b0c08b49a12",
   "metadata": {},
   "source": [
    "Now, we are ready to proceed with evaluating our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa84e32-ac33-411d-a075-a32258ed3926",
   "metadata": {},
   "source": [
    "\n",
    "## Agent Evaluation with RAGAS\n",
    "\n",
    "In this section, we will explore sophisticated methods for evaluating agentic systems using the RAGAS framework. Building upon our previous work with the vacation destination agent from Lab 3, we'll implement both high-level (macro) and low-level (micro) evaluation approaches.\n",
    "\n",
    "RAGAS provides specialized tools for evaluating Large Language Model (LLM) applications, with particular emphasis on agentic systems. We'll focus on two key evaluation dimensions:\n",
    "\n",
    "1. [High-Level Agent Accuracy](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/agents/#agent-goal-accuracy):\n",
    "   - Agent Goal Accuracy (with reference): Measures how well the agent achieves specified goals by comparing outcomes against annotated reference responses\n",
    "   - Agent Goal Accuracy (without reference): Evaluates goal achievement by inferring desired outcomes from user interactions\n",
    "\n",
    "2. [Low-Level Tool Usage](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/agents/#tool-call-accuracy):\n",
    "   - Tool Call Accuracy: Assesses the agent's ability to identify and utilize appropriate tools by comparing actual tool calls against reference tool calls\n",
    "   - The metric ranges from 0 to 1, with higher values indicating better performance\n",
    "\n",
    "This structured evaluation approach allows us to comprehensively assess our vacation destination agent's performance, both at the system level and the component level. By maintaining our focus on the singleton agent from Lab 3, we can clearly demonstrate these evaluation techniques without the added complexity of nested agent systems.\n",
    "\n",
    "Let's proceed with implementing these evaluation methods to analyze our agent's effectiveness in handling vacation-related queries and tool interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf00b75-1051-47ec-80d1-f3d65ee8fdc6",
   "metadata": {},
   "source": [
    "### Util functions part 2 - Message Format Conversion\n",
    "\n",
    "Our singleton agent is built using the LangChain/LangGraph framework. LangChain defines several [message objects](https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/) to handle different types of communication within an agentic system. According to the LangChain documentation, these include:\n",
    "\n",
    "- HumanMessage: This represents a message from the user. Generally consists only of content.\n",
    "- AIMessage: This represents a message from the model. This may have additional_kwargs in it - for example tool_calls if using Amazon Bedrock tool calling.\n",
    "- ToolMessage: This represents the result of a tool call. In addition to role and content, this message has a `tool_call_id` parameter which conveys the id of the call to the tool that was called to produce this result.\n",
    "\n",
    "Similarly, our evaluation framework RAGAS implements its own message wrapper objects:\n",
    "\n",
    "- [HumanMessage](https://docs.ragas.io/en/latest/references/evaluation_schema/?h=aimessage#ragas.messages.HumanMessage): Represents a message from a human user.\n",
    "- [AIMessage](https://docs.ragas.io/en/latest/references/evaluation_schema/?h=aimessage#ragas.messages.AIMessage): Represents a message from an AI.\n",
    "- [ToolMessage](https://docs.ragas.io/en/latest/references/evaluation_schema/?h=aimessage#ragas.messages.ToolMessage): Represents a message from a tool.\n",
    "- [ToolCall](https://docs.ragas.io/en/latest/references/evaluation_schema/?h=aimessage#ragas.messages.ToolCall):  Represents a tool invocation with name and arguments (typically contained within an `AIMessage` when tool calling is used)\n",
    "\n",
    "To evaluate the conversation flow generated by the LangGraph agent, we need to convert between these two message type systems. For convenience, we've implemented the `convert_message_langchian_to_ragas` function in the `utils.py` module. This function handles the conversion process seamlessly. You can import it along with the message wrapper objects, which are assigned appropriate aliases to ensure cross-compatibility between the frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a9191-9677-4174-8d15-d20efaaeea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_agent, convert_message_langchain_to_ragas\n",
    "\n",
    "from langchain_core.messages import HumanMessage as LCHumanMessage\n",
    "from langchain_core.messages import AIMessage as LCAIMessage\n",
    "from langchain_core.messages import ToolMessage as LCToolMessage\n",
    "\n",
    "\n",
    "from ragas.messages import Message as RGMessage\n",
    "from ragas.messages import HumanMessage as RGHumanMessage\n",
    "from ragas.messages import AIMessage as RGAIMessage\n",
    "from ragas.messages import ToolMessage as RGToolMessage\n",
    "from ragas.messages import ToolCall as RGToolCall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c467d0-c358-4baa-ab70-a00cf6e01e8f",
   "metadata": {},
   "source": [
    "Since we typically evaluate multi-turn conversations, we will implement a helper function capable of handling arrays of messages. This function will enable us to process and analyze multiple conversational exchanges seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cd23e-1298-466b-8f8a-71776c40b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages(response):\n",
    "    return list(map((lambda m: convert_message_langchain_to_ragas(m)), response['messages']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e97c81-3dbb-40fc-900d-d1fd1cd5fa7f",
   "metadata": {},
   "source": [
    "For the evaluation, we will examine three distinct scenarios that represent different user profiles:\n",
    "\n",
    "1. **Stephen Walker - The User with Travel History**\n",
    "   * A 20-year-old resident of Honolulu\n",
    "   * Exists in our travel history database\n",
    "   * Previous travel records enable precise, personalized recommendations\n",
    "   * Expected to have a smooth, seamless conversation flow\n",
    "\n",
    "2. **Jane Doe - The First-Time User**\n",
    "   * No prior interaction with our travel recommendation system\n",
    "   * Requires the agent to rely on creative recommendation strategies\n",
    "   * Will test the system's ability to gather information and provide relevant suggestions\n",
    "   * May experience a slightly more exploratory conversation flow\n",
    "\n",
    "3. **Homer Simpson - The Distracted User**\n",
    "   * No previous travel history in our system\n",
    "   * Exhibits difficulty maintaining focused conversation\n",
    "   * Will challenge the agent's ability to handle divergent discussions\n",
    "   * Tests the system's capability to guide and redirect conversations effectively\n",
    "\n",
    "These scenarios will help us evaluate our travel agent's performance across different user types and interaction patterns. Let's proceed with executing these conversation flows to assess our system's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e1309-3fa4-4a1c-8f9f-3e05c05cd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"My name is Stephen Walker, please suggest me a good vacation destination.\",\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_stephen = agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"I'm interested in more information about the location you suggested. Please provide me with a summary of what a travel guide would suggest!\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_stephen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dad47-102f-45d0-9145-fbe16de2bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"My name is Jane Doe, please suggest me a good vacation destination.\",\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_jane = agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"I'm interested in more information about the location you suggested. Please provide me with a summary of what a travel guide would suggest!\")\n",
    "\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_jane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebdc21-6033-457d-addb-0471d5d008a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"My name is Homer Simpson, please suggest me a good vacation destination.\",\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"How can I brew beer over there? Which sort of hops is best suited for this?\"\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\n",
    "                    \"user\",\n",
    "                    \"What is the best local beer brand in this specific place?\"\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_confused_homer = agent_executor.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                (\"user\", \"I'm confused. Why does my beer taste bitter?\")\n",
    "            ]\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "response_confused_homer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b086dc3-5203-4531-beb5-5566a2b13815",
   "metadata": {},
   "source": [
    "Now that we have collected the agent conversations from Stephen, Jane, and Homer, we can proceed with converting them from LangChain's message format into RAGAS message format. For this conversion, we will utilize the previously defined `convert_messages` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68447b7a-85e9-461f-a0db-742050547734",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_messages_stephen = convert_messages(response_stephen)\n",
    "rg_messages_stephen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea6f13-5d45-4331-98fe-5d030d6516c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_messages_confused_homer = convert_messages(response_confused_homer)\n",
    "rg_messages_confused_homer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb642fba-8d53-4ff2-b1b6-e7f421733adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_messages_jane = convert_messages(response_jane)\n",
    "rg_messages_jane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae3c79-521f-4962-9e4e-9a87e44a51f3",
   "metadata": {},
   "source": [
    "With our conversation flows now properly formatted, we can proceed with the actual evaluation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf995770-f6f7-4f86-909f-b60328ddfeaf",
   "metadata": {},
   "source": [
    "### Agent Goal Accuracy\n",
    "\n",
    "Agent Goal Accuracy is a metric designed to evaluate how well an LLM identifies and achieves user goals. It's a binary metric where 1 indicates successful goal achievement and 0 indicates failure. The evaluation is performed using an evaluator LLM, which needs to be defined and configured before metric calculation.\n",
    "\n",
    "The Agent Goal Accuracy metric comes in two distinct variants:\n",
    "\n",
    "- Agent Goal Accuracy without reference\n",
    "- Agent Goal Accuracy with reference\n",
    "\n",
    "Before exploring these variants in detail, we need to establish our evaluator LLM. For this purpose, we will utilize Anthropic Claude 3 Sonnet as our judge. While this is our choice in this lab, the selection of the evaluator LLM always needs to be adjusted based on specific use cases and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6786f5-4c5f-4ca4-a5da-30a275fbbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "# ---- ⚠️ Update region for your AWS setup ⚠️ ----\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "judge_llm = LangchainLLMWrapper(ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    "    # other params...\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff6e40-26c3-4ccb-b32b-65d2def73410",
   "metadata": {},
   "source": [
    "#### Agent Goal Accuracy Without Reference\n",
    "AgentGoalAccuracyWithoutReference operates without a predefined reference point. Instead, it evaluates the LLM's performance by inferring the desired outcome from the human interactions within the workflow. This approach is particularly useful when explicit reference outcomes are not available or when the success criteria can be determined from the conversation context.\n",
    "\n",
    "To evaluate this metric, we first encapsulate our agent conversation in a `MultiTurnSample` object, which is designed to handle multi-turn agentic conversations within the RAGAS ecosystem. Next, we initialize an `AgentGoalAccuracyWithoutReference` object to implement our evaluation metric. Finally, we configure the judge LLM and execute the evaluation across our three agent conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a10ce3-e3bf-486d-bb6d-17e2a0638562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import AgentGoalAccuracyWithoutReference\n",
    "\n",
    "\n",
    "sample_stephen = MultiTurnSample(user_input=rg_messages_stephen)\n",
    "\n",
    "sample_jane = MultiTurnSample(user_input=rg_messages_jane)\n",
    "\n",
    "sample_confused_homer = MultiTurnSample(user_input=rg_messages_confused_homer)\n",
    "\n",
    "\n",
    "scorer = AgentGoalAccuracyWithoutReference()\n",
    "scorer.llm = judge_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db55ba-e93c-4c2d-8a0d-c365b6222983",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_stephen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e411bc-0934-4223-bbbf-7089163b0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_jane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14481573-8554-42a0-8672-b525919b538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_confused_homer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a484c-f0ef-49a6-a43c-f0c387322867",
   "metadata": {},
   "source": [
    "#### Agent Goal Accuracy With Reference\n",
    "AgentGoalAccuracyWithReference requires two key inputs: the user_input and a reference outcome. This variant evaluates the LLM's performance by comparing its achieved outcome against an annotated reference that serves as the ideal outcome. The metric is calculated at the end of the workflow by assessing how closely the LLM's result matches the predefined reference outcome.\n",
    "\n",
    "To evaluate this metric, we will follow a similar approach. First, we encapsulate our agent conversation within a `MultiTurnSample` object, which is specifically designed to manage multi-turn agent conversations in the RAGAS framework. For this evaluation, we need to provide an annotated reference that will serve as a benchmark for the judge's assessment. We then initialize an `AgentGoalAccuracyWithReference` object to implement our evaluation metric. Finally, we set up the judge LLM and conduct the evaluation across all three agent conversations to measure their performance against our defined criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bf878-43d0-4aea-9e27-4ec16af602e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics import AgentGoalAccuracyWithReference\n",
    "\n",
    "\n",
    "sample_stephen = MultiTurnSample(user_input=rg_messages_stephen,\n",
    "    reference=\"Provide detailed information about suggested holiday destination.\")\n",
    "\n",
    "sample_jane = MultiTurnSample(user_input=rg_messages_jane,\n",
    "    reference=\"Provide detailed information about suggested holiday destination.\")\n",
    "\n",
    "sample_confused_homer = MultiTurnSample(user_input=rg_messages_confused_homer,\n",
    "    reference=\"Provide detailed information about suggested holiday destination.\")\n",
    "\n",
    "scorer = AgentGoalAccuracyWithReference()\n",
    "scorer.llm = judge_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebac6f9-3443-473a-af4c-3e33588258a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_stephen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde20da-8227-48d9-8dec-8de757a786bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_jane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5168-0c27-4a35-9ba5-a267a6b79926",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_confused_homer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e40f9-cdeb-4a62-86c9-38367c284a1a",
   "metadata": {},
   "source": [
    "Let's analyze the results and their relationship to each persona's interaction patterns with the agent. We encourage you to discuss these findings with your workshop group to gain deeper insights. Keep in mind that agent conversations are dynamic and non-deterministic, meaning evaluation results may vary across different runs. \n",
    "\n",
    "However, certain patterns emerge:\n",
    "- Stephen's and Jane's conversations typically achieve a 1.0 rating due to their focused and goal-oriented approach\n",
    "- Homer's interactions, characterized by chaotic and divergent patterns without a consistent objective, are more likely to receive a 0.0 rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e37b94-2166-4f4c-9bca-bc68a16d1fa0",
   "metadata": {},
   "source": [
    "### Tool Call accuracy\n",
    "\n",
    "ToolCallAccuracy is a metric that can be used to evaluate the performance of the LLM in identifying and calling the required tools to complete a given task. This metric needs user_input and reference_tool_calls to evaluate the performance of the LLM in identifying and calling the required tools to complete a given task. The metric is computed by comparing the reference_tool_calls with the Tool calls made by the AI. Therefore, in this particular scenario, there is no need for an evaluator LLM. The values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "To evaluate the tool call accuracy metric, we follow a different process. First, we encapsulate our agent conversation within a `MultiTurnSample` object, which is specifically designed to handle multi-turn agent conversations in the RAGAS framework. This evaluation requires a set of annotated reference tool calls that serve as a benchmark for assessment. Next, we initialize a `ToolCallAccuracy` object to implement our evaluation metric. While the default behavior compares tool names and arguments using exact string matching, this may not always be optimal, particularly when dealing with natural language arguments. Therefore, we set the `arg_comparison_metric` to `NonLLMStringSimilarity`, which employs a semantic distance metric to determine the relevance of retrieved contexts more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158250b-ca76-46e9-a60c-c621e1215f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import ToolCallAccuracy\n",
    "from ragas.dataset_schema import  MultiTurnSample\n",
    "from ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\n",
    "from ragas.metrics._string import NonLLMStringSimilarity\n",
    "\n",
    "\n",
    "\n",
    "sample_stephen = MultiTurnSample(\n",
    "    user_input=rg_messages_stephen,\n",
    "    reference_tool_calls=[\n",
    "        ToolCall(name=\"compare_and_recommend_destination\", args={\"name\": \"Stephen Walker\"}),\n",
    "        ToolCall(name=\"travel_guide\", args={\"query\": \"Key West\"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_jane = MultiTurnSample(\n",
    "    user_input=rg_messages_jane,\n",
    "    reference_tool_calls=[\n",
    "        ToolCall(name=\"compare_and_recommend_destination\", args={\"name\": \"Jane Doe\"}),\n",
    "        ToolCall(name=\"travel_guide\", args={\"query\": \"Key West\"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_homer = MultiTurnSample(\n",
    "    user_input=rg_messages_confused_homer,\n",
    "    reference_tool_calls=[\n",
    "        ToolCall(name=\"compare_and_recommend_destination\", args={\"name\": \"Jane Doe\"}),\n",
    "        ToolCall(name=\"travel_guide\", args={\"query\": \"Key West\"}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer = ToolCallAccuracy()\n",
    "scorer.arg_comparison_metric = NonLLMStringSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a688a-4d99-4b24-b487-5918c62414a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_stephen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ba036-9c7e-44ce-b9c0-9e11c793a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_jane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612dc5d7-902e-41f7-92cf-6fa912f21b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scorer.multi_turn_ascore(sample_homer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be7ce6-c419-4135-80c9-f3e974ca97c1",
   "metadata": {},
   "source": [
    "Let's analyze the results and their relationship to each persona's interaction patterns with the agent. We encourage you to discuss these findings with your workshop group to gain deeper insights. Keep in mind that agent conversations are dynamic and non-deterministic, meaning evaluation results may vary across different runs.\n",
    "\n",
    "However, certain patterns emerge:\n",
    "- Stephen's conversations typically achieve a very high rating due to his focused and goal-oriented approach and the fact that he can be found in the travel database - this helps matching all tool call arguments\n",
    "- Jane's conversations typically achieve a high but slightly lower rating. While her conversation is focused and goal-oriented, she is not in the travel database. This causes the first tool call suggestions to be less deterministic, reducing the likelihood of Florida-based recommendations\n",
    "- Homer's interactions achieve very low scores since they are characterized by chaotic and divergent patterns. This makes it highly unlikely to match the correct tool call sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a38535-bd42-40be-9ab1-5a72e3536e6f",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed this lab and the entire workshop. Thank you for your active participation in today's session. If you have any questions or need clarification on any topics covered, please don't hesitate to reach out to the instructors.\n",
    "\n",
    "**Your feedback is valuable to us! Before you leave, we kindly ask you to take a moment to scan the QR code and share your thoughts about the workshop. Your insights will help us enhance future sessions and provide an even better learning experience.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
