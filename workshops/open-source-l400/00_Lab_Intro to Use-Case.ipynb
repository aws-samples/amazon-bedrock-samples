{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e02189b-6919-4637-841f-7210ea54c189",
   "metadata": {},
   "source": [
    "# Build agentic workflows with Amazon Bedrock and open source frameworks - Introduction\n",
    "\n",
    "The goal of this workshop is to provide in-depth examples on key concepts and frameworks for Retrieval Augmented Generation (RAG) and Agentic application. We introduce an example use case to serve as a backdrop for curated and prescriptive guidance for RAG and Agentic workflows including libraries and blueprints for some of the top trends in the market today.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Through web-scale training, foundation models (FMs) are built to support a wide variety of tasks across a large body of general knowledge. Without being exposed to additional information or further fine-tuning, they suffer from a knowledge cutoff preventing them from reliably completing tasks requiring specific data not available at training time. Furthermore, their inability to call external functions limits their capacity to resolve complex tasks beyond ones that can be solved with their own internal body of knowledge.\n",
    "\n",
    "In this notebook, we introduce the requirements that lead us to build our **Virtual Travel Agent**. We end by running some **course-grained model evaluation** across a subset of the models available in Amazon Bedrock.\n",
    "\n",
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19c31d6c-ce36-4ef2-b2f5-8599f5f894e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1df98-36f0-4471-97c2-2aa3f6114caa",
   "metadata": {},
   "source": [
    "## Functional requirements\n",
    "\n",
    "The purpose of the solution is to improve the experience for customers searching for their dream travel destination. To do this, a customer needs the ability to do the following:\n",
    "- Rapidly get a sense a given destination with a representative description.\n",
    "- Discover new destinations based on location, weather or other aspects that may be of interest.\n",
    "- Book travel dates for a given destination ensuring it does not collide with their other travel.\n",
    "\n",
    "Before diving deeper into the solution, we begin with some lite testing of the various models available in the `us-west-2` region."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa1134d1-3c10-4106-b504-eb52f64ba27e",
   "metadata": {},
   "source": [
    "## Course-grained model evaluation\n",
    "\n",
    "In this section, we experiment with multiple models available on Amazon Bedrock and run course-grained evaluation on one of our task of interest. With the thousands of available models on the market, it is intractable to evaluation every single one. Hence, it is generally necessary to pre-filter for the ones that are not only from trusted providers, but have shown strong performance on a variety of benchmarks. \n",
    "\n",
    "Amazon Bedrock allows you to make a quick short-list by supporting a growing list providers such as Anthropic, Meta, Mistral, Cohere, AI21Labs, Stability AI and Amazon. This lets you start with a strong base to continue the model selection process.\n",
    "\n",
    "![model selection](./assets/model-selection.png)\n",
    "\n",
    "Since, academic benchmarks are known to model providers and often used as marketing materials, it is important to not to rely too heavily on them, but rather use them as a soft measure. \n",
    "\n",
    "Next we perform course-grained model evalution on the following models to inform our initial choice of model for our task of interest:\n",
    "- Anthropic: Claude Sonnet 3.5, Claude 3 Sonnet, Claude 3 Haiku\n",
    "- Meta: Llama 3.1 70B, Llama 3.1 8B\n",
    "- Mistral: Mistral Large\n",
    "- Cohere: Command R+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706866fa-fcee-4210-963d-bd65cf384c62",
   "metadata": {},
   "source": [
    "We start by importing the boto3 client for the Bedrock Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59eb68d8-f717-4277-b937-597b601c285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "region = 'us-west-2'\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    ")\n",
    "\n",
    "\n",
    "bedrock_service = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name=region,\n",
    ")\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81664451",
   "metadata": {},
   "source": [
    "#### Validate the connection\n",
    "\n",
    "We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11461f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_service.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a6c91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common inference parameter definitions\n",
    "\n",
    "### Randomness and Diversity\n",
    "\n",
    "Foundation models support the following parameters to control randomness and diversity in the \n",
    "response.\n",
    "\n",
    "**Temperature** – Large language models use probability to construct the words in a sequence. For any \n",
    "given next word, there is a probability distribution of options for the next word in the sequence. When \n",
    "you set the temperature closer to zero, the model tends to select the higher-probability words. When \n",
    "you set the temperature further away from zero, the model may select a lower-probability word.\n",
    "\n",
    "In technical terms, the temperature modulates the probability density function for the next tokens, \n",
    "implementing the temperature sampling technique. This parameter can deepen or flatten the density \n",
    "function curve. A lower value results in a steeper curve with more deterministic responses, and a higher \n",
    "value results in a flatter curve with more random responses.\n",
    "\n",
    "**Top K** – Temperature defines the probability distribution of potential words, and Top K defines the cut \n",
    "off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the \n",
    "most probable words that could be next in a given sequence. This reduces the probability that an unusual \n",
    "word gets selected next in a sequence.\n",
    "In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-\n",
    "K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-\n",
    "probability tokens.\n",
    "\n",
    "**Top P** – Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top \n",
    "P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is \n",
    "similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their \n",
    "probabilities.\n",
    "For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\" \n",
    "\"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping \n",
    "Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the \n",
    "temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or \n",
    "Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability \n",
    "of \"unicorns.\"\n",
    "\n",
    "### Length\n",
    "\n",
    "The following parameters control the length of the generated response.\n",
    "\n",
    "**Response length** – Configures the minimum and maximum number of tokens to use in the generated \n",
    "response.\n",
    "\n",
    "**Length penalty** – Length penalty optimizes the model to be more concise in its output by penalizing \n",
    "longer responses. Length penalty differs from response length as the response length is a hard cut off for \n",
    "the minimum or maximum response length.\n",
    "\n",
    "In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 \n",
    "means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value \n",
    "greater than 0.0 for the model to produce shorter sequences.\n",
    "\n",
    "### Repetitions\n",
    "\n",
    "The following parameters help control repetition in the generated response.\n",
    "\n",
    "**Repetition penalty (presence penalty)** – Prevents repetitions of the same words (tokens) in responses. \n",
    "1.0 means no penalty. Greater than 1.0 decreases repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f53c0a-297d-43ce-b306-918a037d86b8",
   "metadata": {},
   "source": [
    "We use the `ChatBedrock` object part of `langchain-aws` to interact with the Bedrock service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e14cbc-e66c-4a2a-a550-019fdd70c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=bedrock,\n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "llm.invoke(\"Help me with my travel needs.\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9341fc0",
   "metadata": {},
   "source": [
    "### Converse API\n",
    "\n",
    "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
    "\n",
    "To use the Converse API, you call the `Converse` or `ConverseStream` operations to send messages to a model. To call Converse, you require permission for the `bedrock:InvokeModel` operation. To call ConverseStream, you require permission for the `bedrock:InvokeModelWithResponseStream` operation.\n",
    "\n",
    "<h3> Prerequisites </h3>\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access). \n",
    "    \n",
    "    We have used below Foundation Models in our examples in this Notebook in `us-west-2` (Oregon) region.\n",
    "    \n",
    "| Provider Name | Foundation Model Name | Model Id |\n",
    "| ------- | ------------- | ------------- |\n",
    "| Amazon | Titan Text G1 - Express  | amazon.titan-text-express-v1 |\n",
    "| Amazon | Titan Text G1 - Lite | amazon.titan-text-lite-v1 |\n",
    "| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n",
    "| Anthropic | Claude 3 Haiku  | anthropic.claude-3-haiku-20240307-v1:0 |\n",
    "| Cohere | Command R+ | cohere.command-r-plus-v1:0 |\n",
    "| Cohere | Command R | cohere.command-r-v1:0 |\n",
    "| Meta | Llama 3.1 70B Instruct | meta.llama3-1-70b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 8B Instruct | meta.llama3-1-8b-instruct-v1:0 |\n",
    "| Mistral AI | Mistral Large 2 (24.07) | mistral.mistral-large-2407-v1:0 |\n",
    "| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a86e68",
   "metadata": {},
   "source": [
    "<h3> ConverseStream for streaming invocations </h3>\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9693b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8531ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"Help me with my travel needs.\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "MODEL_IDS = ['anthropic.claude-3-haiku-20240307-v1:0',]\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    print(f'\\n\\nModel: {i}')\n",
    "    invoke_bedrock_model_stream(bedrock, i, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a419725-2d0e-4255-8511-3c7571c387a4",
   "metadata": {},
   "source": [
    "To perform an initial evaluation, we create a small curated dataset of 10 examples. The optimal initial number of examples should be sufficiently big to roughly cover the types of queries our customers will send our model. Since this stage of the model evaluation process is meant to get a rough idea, the number of examples can be small. To come up with our examples, we use [HELM's](https://crfm.stanford.edu/helm/lite/latest/) definition of a scenario, which is broken down by the following diagram:\n",
    "\n",
    "![helm scenario](./assets/helm-scenario.png)\n",
    "\n",
    "To start, our scenario can be described by summarization (**task**) of vacation destinations (**what**) asked by travelers (**who**) at the time of development (**when**) in English (**language**). The set of initial questions can be found in [examples.txt](./data/examples.txt). We could expand our test by changing one or more of the variables composing the scenario of interesting. For instance, we could generate equivalent examples, but asked by people who aren't travelers or by others speaking in any other languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "231c3f9f-06e7-47a0-bc21-4acbf3a8c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/examples.txt\", \"r\") as file:\n",
    "    examples = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb02c93-6fdf-49a2-b908-ca337ac4c3e2",
   "metadata": {},
   "source": [
    "Once we retrieved our limited set of examples, we defined `generate_answers`, which outputs a dataframe where each column is populated by a given model's answers. This allows us to quickly capture model answers across a set of `examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecfbe2cd-a234-4c92-a729-986584affb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "def generate_answers(\n",
    "    examples: list = [],\n",
    "    system_prompt: SystemMessage = None\n",
    "):\n",
    "    modelIds = [\n",
    "        \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"cohere.command-r-plus-v1:0\",\n",
    "        \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "        \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "        \"mistral.mistral-large-2407-v1:0\"\n",
    "    ]\n",
    "    output = pd.DataFrame({\n",
    "        'example': [],\n",
    "        'Claude35Sonnet': [],\n",
    "        'Claude3Sonnet': [],\n",
    "        'Claude3Haiku': [],\n",
    "        'CommandRplus': [],\n",
    "        'Llama8b': [],\n",
    "        'Llama70b': [],\n",
    "        'MistralLarge': [],\n",
    "    })\n",
    "    for example in examples:\n",
    "        results = [example]\n",
    "        for modelId in modelIds:\n",
    "            messages = [\n",
    "                system_prompt if system_prompt else SystemMessage(content=\"\"),\n",
    "                HumanMessage(content=example)\n",
    "            ]\n",
    "            llm = ChatBedrock(\n",
    "                model_id=modelId,\n",
    "                client=bedrock,\n",
    "                beta_use_converse_api=True\n",
    "            )\n",
    "            resp = llm.invoke(messages).content\n",
    "            results.append(resp)\n",
    "        output.loc[len(output)] = results\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577e10d-4ae4-49c6-b006-66e96b91317c",
   "metadata": {},
   "source": [
    "We generate model outputs without a system prompt for a single example. This example is pulled from the top of the examples list and contains just the words *New York*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7308b88-5e31-48ea-915c-307f88781397",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = examples[:1]\n",
    "output = generate_answers(one_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f7010-c9cd-4fcd-832c-4caf7178daed",
   "metadata": {},
   "source": [
    "We should the answers generated by the various models for this example. Quickly, we notice Llama 3.1 70B has produce the longest input. As expected, we also see some consistency in the outputs within a given model family.\n",
    "\n",
    "When diving deeper into the examples, it is clear the model has been trained has broad knowledge of the subject and is able to give us some facts about it. However, we do not provide additional information into the model's current role. This results in fairly long and generic answers. Hence, in the next step we will continue to tailor model output by supplying it with a consistent system prompt reused across all examples.\n",
    "\n",
    "To get a better sense of model functionality without additional context, it may be helpful to rerun the previous cells on other examples or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769c9d6-ed20-4137-9060-44bc2befddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c726d7-8080-411a-b7c3-ade3ee7dc50e",
   "metadata": {},
   "source": [
    "We define a `SystemMessage` passed as a system prompt that is passed to all models for every example. The purpose is to provide more context to the model as to what is expected from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e8e2e04-2d7a-43d4-9d5c-d35d25b0f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = examples[:1]\n",
    "output = generate_answers(\n",
    "    one_example,\n",
    "    SystemMessage(content=\"You are a text summarizer for travelers who are on the go. Generate your summary in a single sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0594b8f-615f-4420-b8c0-233cc355c707",
   "metadata": {},
   "source": [
    "When looking through the model responses, the difference in size of response is immediately obvious and is a direct result of the content of the system prompt.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e1aa8-c898-4b03-a25d-7281254a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bb26d-988d-40b3-bf61-ea26f9077a2d",
   "metadata": {},
   "source": [
    "Next, we modify the original `generate_answers` function to accomodate for few-shots. The purpose of few-shot learning is to enable machine learning models to learn from a small number of examples or training data points, rather than requiring a large labeled dataset. This is particularly useful in scenarios where obtaining a large amount of labeled data is difficult, expensive, or time-consuming. There are several advantages of few-shot learning:\n",
    "\n",
    "- **Data efficiency**: Few-shot learning allows models to learn from limited data, which is beneficial when obtaining large labeled datasets is challenging or costly.\n",
    "- **Adaptability**: Few-shot learning enables models to quickly adapt to new tasks or domains without the need for extensive retraining from scratch, making the models more flexible and versatile.\n",
    "- **Transfer learning**: Few-shot learning relies on transfer learning principles, where knowledge gained from one task or domain is transferred and applied to a different but related task or domain.\n",
    "- **Human-like learning**: Few-shot learning aims to mimic the way humans can learn new concepts from just a few examples, leveraging prior knowledge and experience.\n",
    "\n",
    "As we start adding more repeated elements to our prompt, we also introduce the `ChatPromptTemplate` a core component of Langchain allowing us to define a template receiving runtime inputs. We pipe the resulting prompt to the model for inference. `FewShotChatMessagePromptTemplate` extends this object to provide prompt template that supports few-shot examples. \n",
    "\n",
    "Although we supply a static set of examples, the library does support dynamic few-shots where examples are chosen based on semantic similarity to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "686fd071-b4ea-4d5d-9874-640489704175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answers(\n",
    "    examples: list = [],\n",
    "    system_prompt: str = None,\n",
    "    few_shots: list = []\n",
    "):\n",
    "    modelIds = [\n",
    "        \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "        \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        \"cohere.command-r-plus-v1:0\",\n",
    "        \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "        \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "        \"mistral.mistral-large-2407-v1:0\"\n",
    "    ]\n",
    "    output = pd.DataFrame({\n",
    "        'example': [],\n",
    "        'Claude35Sonnet': [],\n",
    "        'Claude3Sonnet': [],\n",
    "        'Claude3Haiku': [],\n",
    "        'CommandRplus': [],\n",
    "        'Llama8b': [],\n",
    "        'Llama70b': [],\n",
    "        'MistralLarge': [],\n",
    "    })\n",
    "    for example in examples:\n",
    "        results = [example]\n",
    "        for modelId in modelIds:\n",
    "            messages = [\n",
    "                system_prompt if system_prompt else SystemMessage(content=\"\"),\n",
    "                HumanMessage(content=example)\n",
    "            ]\n",
    "            llm = ChatBedrock(\n",
    "                model_id=modelId,\n",
    "                client=bedrock,\n",
    "                beta_use_converse_api=True\n",
    "            )\n",
    "\n",
    "            example_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"human\", \"{input}\"),\n",
    "                    (\"ai\", \"{output}\"),\n",
    "                ]\n",
    "            )\n",
    "            few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "                example_prompt=example_prompt,\n",
    "                examples=few_shots,\n",
    "            )\n",
    "            final_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system_prompt),\n",
    "                    few_shot_prompt,\n",
    "                    (\"human\", \"{input}\"),\n",
    "                ]\n",
    "            )\n",
    "            chain = final_prompt | llm\n",
    "\n",
    "            resp = chain.invoke(messages).content\n",
    "            results.append(resp)\n",
    "        output.loc[len(output)] = results\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78be15-b955-4977-af2b-70640abc6f61",
   "metadata": {},
   "source": [
    "We create few examples requesting for description, comparisons and lists. In all cases, the examples include a description followed by some type of recommendation. For the requests for summaries, we prefix the response with *Nice!*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d3d26385-bc02-4c58-8b9c-a23d4a776c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shots = [\n",
    "    {\"input\": \"Describe the culinary scene in Tokyo.\", \"output\": \"Nice! Tokyo's culinary scene is diverse and vibrant, offering everything from traditional Japanese cuisine to international flavors, street food, Michelin-starred restaurants, and unique dining experiences abound, so I highly recommend trying some of the city's famous ramen shops for a quintessential Tokyo dining experience with rich, flavorful broths and perfectly cooked noodles.\"},\n",
    "    {\"input\": \"List the top attractions in Barcelona.\", \"output\": \"Barcelona's top attractions include Sagrada Familia, Park Güell, Casa Batlló, La Rambla, Gothic Quarter, and Camp Nou, offering a mix of stunning architecture, history, and vibrant culture, so I highly recommend exploring the winding streets of the Gothic Quarter for an immersive experience in the city's rich heritage.\"},\n",
    "    {\"input\": \"Compare the climates of Seattle and Miami\", \"output\": \"Seattle has a temperate climate with mild, wet winters and cool, dry summers, while Miami has a tropical climate with hot, humid summers and warm, dry winters, with Seattle receiving more rainfall and Miami experiencing more sunshine and occasional hurricanes, so I highly recommend visiting Seattle during the summer months to enjoy the city's beautiful parks and outdoor activities without the constant rain.\"},\n",
    "    {\"input\": \"Outline the best ways to experience Paris on a budget.\", \"output\": \"budget-friendly ways to experience Paris include walking tours, visiting free museums on certain days, picnicking in parks, using public transportation, staying in budget accommodations, and enjoying street food, with many iconic sights like Notre-Dame Cathedral and the Eiffel Tower able to be appreciated from the outside for free, so I highly recommend taking advantage of the city's numerous free or low-cost attractions and activities to fully immerse yourself in the Parisian atmosphere without breaking the bank.\"},\n",
    "    {\"input\": \"Summarize the cultural etiquette for visitors to Japan.\", \"output\": \"Nice! cultural etiquette in Japan includes bowing as a greeting, removing shoes before entering homes or certain establishments, using chopsticks correctly, being punctual, avoiding public displays of affection, respecting quiet in public spaces, gift-giving is common, and it's important to be mindful of table manners and bathing customs in public baths, so I highly recommend familiarizing yourself with these customs and traditions to ensure a respectful and enriching cultural experience during your visit to Japan.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d1444-ebee-45f9-adaa-32b4f2c758ea",
   "metadata": {},
   "source": [
    "We prompt the model with an example asking for a summary about Bali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4d4ec592-496c-4e5a-af37-c3ce29e18ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = examples[1]\n",
    "output = generate_answers(\n",
    "    [one_example],\n",
    "    \"You are a text summarizer for travelers who are on the go.\",\n",
    "    few_shots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206c228-9020-467e-859b-c75a43c297e7",
   "metadata": {},
   "source": [
    "For our specific few-shots and prompt, we notice **Claude 3.5 Sonnet**, **Claude 3 Sonnet**, **Llama 3.1 8B** and **Llama 3.1 70B** all responded with the correct prefix and a recommendation. \n",
    "\n",
    "We suggest tailoring the few-shots and system prompt to further understand model behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90f2a2-7c2e-49a8-a5d4-fd5cfbdaa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9eade-1726-4579-a99e-2c2548e9b86f",
   "metadata": {},
   "source": [
    "Next, we generate answers for our set of examples reusing the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d84e43e8-6063-4393-b564-d4a1ad88327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_answers(\n",
    "    examples,\n",
    "    \"You are a text summarizer for travelers who are on the go.\",\n",
    "    few_shots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e4b0e-36e1-42ee-b69a-49056c65e1cd",
   "metadata": {},
   "source": [
    "Although the models are able to adequatly answer the most general questions, queries about current events or requiring data not available at training time remain unanswered.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab4e5f-abba-4b0c-8bc5-f8765dcb0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f929e3-e5a9-467e-9fef-742c9edf5e51",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated simple interactions between Langchain and Bedrock. We tailored model outputs by suppliying it with a system prompt and few-shots, which both help guide behavior. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
