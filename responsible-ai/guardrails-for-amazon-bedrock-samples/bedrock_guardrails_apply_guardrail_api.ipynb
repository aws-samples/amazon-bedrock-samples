{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfca55-c771-4e26-a13b-3451f6bef06a",
   "metadata": {},
   "source": [
    "# Apply Guardrail API - Boto3 Python Code Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41c9b6-e40e-4408-8209-faa9db60da8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Guardrails can be used to implement safeguards for your generative AI applications that are customized to your use cases and aligned with your responsible AI policies. Guardrails allows you to:\n",
    "\n",
    "- Configure denied topics\n",
    "- Filter harmful content\n",
    "- Remove sensitive information\n",
    "\n",
    "For more information on publicly available capabilities:\n",
    "\n",
    "- [Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html)\n",
    "- [Guardrail Policies](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html)\n",
    "- [Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [WebPage](https://aws.amazon.com/bedrock/guardrails/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fa5fb-589a-4d83-8083-ba926f327b4e",
   "metadata": {},
   "source": [
    "## The new `ApplyGuardrail` API allows customers to assess any text using their pre-configured Bedrock Guardrails, without invoking the foundation models.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Content Validation**: Send any text input or output to the ApplyGuardrail API to have it evaluated against your defined topic avoidance rules, content filters, PII detectors, and word blocklists. You can evaluate user inputs and FM generated outputs independently.\n",
    "\n",
    "2. **Flexible Deployment**: Integrate the Guardrails API anywhere in your application flow to validate data before processing or serving results to users. E.g. For a RAG application, you can now evaluate the user input prior to performing the retrieval instead of waiting until the final response generation.\n",
    "\n",
    "3. **Decoupled from Foundation Models**: ApplyGuardrail is decoupled from foundational models. You can now use Guardrails without invoking Foundation Models.\n",
    "\n",
    "You can use the assessment results to design the experience on your generative AI application. Let's now walk through a code-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd569723-3c66-4569-82a0-a2538eb73921",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Start by installing the dependencies to ensure we have a recent version\n",
    "!pip install --upgrade --force-reinstall boto3\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a3480e72-04ac-4caf-86bd-0fae2cb8fcc1",
   "metadata": {},
   "source": [
    "### Important: Create a Guardrail First\n",
    "\n",
    "Before running the code to apply a guardrail, you need to create a guardrail in Amazon Bedrock. If you haven't created a guardrail yet, please follow these steps:\n",
    "\n",
    "1. Visit the following GitHub notebook for detailed instructions on creating and using guardrails:\n",
    "   [Guardrails for Amazon Bedrock Samples](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/responsible-ai/guardrails-for-amazon-bedrock-samples/guardrails-api.ipynb)\n",
    "\n",
    "2. Follow the instructions in the notebook to create your guardrail.\n",
    "\n",
    "3. Make note of the `guardrail_id` and `guardrail_version` that you create, as you'll need these values for the code in this notebook.\n",
    "\n",
    "4. Once you have created your guardrail and have the necessary information, you can return to this notebook and run the code to apply the guardrail.\n",
    "\n",
    "Remember: The `guardrail_id` and `guardrail_version` variables in the code must be set to the values of the guardrail you created before running the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a845db-851d-405d-b6f5-819a214ffa52",
   "metadata": {
    "tags": []
   },
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Specific guardrail ID and version\n",
    "guardrail_id = \"\" # Adjust with your Guardrail Info\n",
    "guardrail_version = \"\"# Adjust with your Guardrail Info"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054717b-c1d8-4c33-b0ac-80c446011a93",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98f4b1-129f-4c1a-8ce9-15482a52c52b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example of Input Prompt being Analyzed\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Is the AB503 Product a better investment than the S&P 500?\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Here's an example of something that should pass\n",
    "\n",
    "#content = [\n",
    "    #{\n",
    "    #    \"text\": {\n",
    "   #         \"text\": \"What is the rate you offer for the AB503 Product?\"\n",
    "  #      }\n",
    " #   }\n",
    "#]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189e5ab-4a38-417e-b766-e84efbea17b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An Example of Analyzing an Output Response, This time using Contexual Grounding\n",
    "\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\",\n",
    "            \"qualifiers\": [\"grounding_source\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Whats the Guaranteed return rate of your AB503 Product\",\n",
    "            \"qualifiers\": [\"query\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Our Guaranteed Rate is 7%\",\n",
    "            \"qualifiers\": [\"guard_content\"],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='OUTPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5225dd51-c1a3-4621-a902-f9700d8ac6b4",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API with a Third-Party or Self-Hosted Model\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host. This combination allows you to apply guardrails to the input or output of any request.\n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Model for Inference\n",
    "4. Receive the output from your Model\n",
    "5. Apply the Guardrail to your output\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/applyguardrail.png\" alt=\"ApplyGuardrail API Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00340f-253e-451b-840e-ebd77165d740",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model, but this could be any third-party model as well\n",
    "\n",
    "We will use the `Meta-Llama-3-8B` model hosted on a SageMaker Endpoint. To deploy your own version of this model on Amazon SageMaker please checkout the guide here: [Meta Llama 3 models are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-models-are-now-available-in-amazon-sagemaker-jumpstart/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba3ec3-f553-4e46-9475-a0f96809b238",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configure our Endpoint to take Requests\n",
    "from sagemaker.predictor import retrieve_default\n",
    "endpoint_name = \"\" # Adjust this line with the name of your Endpoint\n",
    "predictor = retrieve_default(endpoint_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744019c-f3f1-435e-95c6-42857996c3f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "payload = {\n",
    "    \"inputs\": \"How do I save for retirement?\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "540f2797-24ec-4361-bd09-20d31efb5509",
   "metadata": {},
   "source": [
    "### Incorporating the ApplyGuardrail API into our Self-Hosted Model\n",
    "\n",
    "---\n",
    "We've created a `TextGenerationWithGuardrails` class that integrates the ApplyGuardrail API with our SageMaker endpoint to ensure protected text generation. This class includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It int|erprets the API response to determine if the guardrail passed or intervened.\n",
    "\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "\n",
    "The class looks to implement the diagram above. It works as follows:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it generates text using `generate_text`.\n",
    "3. The generated text is then checked using `analyze_output`.\n",
    "4. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, with clear handling of cases where guardrails intervene. It's designed to easily integrate with larger applications while providing flexibility for error handling and customization based on guardrail results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c9654-dab2-41bb-9359-817adedf27ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, guardrail_id: str, guardrail_version: str):\n",
    "        self.predictor = retrieve_default(endpoint_name)\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[0].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def generate_and_analyze(self, grounding_source: str, query: str, max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # First, analyze the prompt\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(grounding_source, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # If prompt passes, generate text\n",
    "        generated_text = self.generate_text(query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text\n",
    "        output_passed, output_message, _ = self.analyze_output(grounding_source, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6f367f87-0f3f-40d2-bb2f-b155d7eb5c5f",
   "metadata": {},
   "source": [
    "### Now let's see a Sample Usage in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c1f96-c913-4f54-8bf3-b01697df587c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def main():\n",
    "    query = \"What are is the Guarenteed Rate of Return for AB503 Product\"\n",
    "    grounding_source = \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%\"\n",
    "    max_new_tokens = 512  # You can change this value as needed\n",
    "    temperature = 0.0  # Default value, can be edited\n",
    "\n",
    "    text_gen = TextGenerationWithGuardrails(\n",
    "        endpoint_name=endpoint_name,\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version\n",
    "    )\n",
    "\n",
    "    # Bold text function\n",
    "    def bold(text):\n",
    "        return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(grounding_source, query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: {input_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "\n",
    "    # Generate text\n",
    "    print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "    generated_text = text_gen.generate_text(query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f\"Here is what the Model Responded with: {generated_text}\\n\")\n",
    "\n",
    "    # Analyze output\n",
    "    print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "    print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "    output_passed, output_message, output_details = text_gen.analyze_output(grounding_source, query, generated_text)\n",
    "    if not output_passed:\n",
    "        print(f\"Output Guardrail Intervened. The response to the User is: {output_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(output_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Model Response Passed. The information presented to the user is: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13df7c66-193d-433a-8ccb-65eb98e3675e",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API within a Self-Managed RAG Pattern\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host, and applied within a Retrival Augmented Generation Pattern. \n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Embeddings Model for Query Embedding, and Query your Vector Embeddings\n",
    "4. Receive the output from your Embeddings Model\n",
    "5. Provide it as Context for your Language Model\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/managed_rag.png\" alt=\"ApplyGuardrail API RAG Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598ed89-de88-4965-9c35-3211384974dc",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model for our Large Language Model, but this could be any third-party model as well, and a third-party embeddings model hosted on VoyageAI\n",
    "\n",
    "We will use the `Meta-Llama-3-8B` model hosted on a SageMaker Endpoint. To deploy your own version of this model on Amazon SageMaker please checkout the guide here: [Meta Llama 3 models are now available in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-models-are-now-available-in-amazon-sagemaker-jumpstart/). For embeddings, we'll use the `voyage-large-2-instruct` model. To learn more about VoyageAI Embeddings models, check them out here: [Voyage AI](https://www.voyageai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220078b-e2de-4b31-82ea-5eb9c0183f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's Start by Embedding our Source Documents by creating a session with the VoyageAI SDK\n",
    "import voyageai\n",
    "vo = voyageai.Client()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f87f2-58a6-4c32-b069-f90e9bc4576a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# We have some sample documents here with descriptions of our products \n",
    "documents = [\n",
    "    \"The AG701 Global Growth Fund is currently projecting an annual return of 8.5%, focusing on emerging markets and technology sectors.\",\n",
    "    \"The AB205 Balanced Income Trust offers a steady 4% dividend yield, combining blue-chip stocks and investment-grade bonds.\",\n",
    "    \"The AE309 Green Energy ETF has outperformed the market with a 12% return over the past year, investing in renewable energy companies.\",\n",
    "    \"The AH504 High-Yield Corporate Bond Fund is offering a current yield of 6.75%, targeting BB and B rated corporate debt.\",\n",
    "    \"The AR108 Real Estate Investment Trust focuses on commercial properties and is projecting a 7% annual return including quarterly distributions.\",\n",
    "    \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%, providing a balance of growth potential and flexible investment options.\"\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a18345-28ec-4aa6-8795-306fd7c8c620",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embed the documents\n",
    "documents_embeddings = vo.embed(documents, model=\"voyage-2\", input_type=\"document\").embeddings"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb88bc-a5c7-43b3-8807-8c8408150aae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Let's ask a question\n",
    "query = \"What is the return rate on AB503?\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57066f43-434f-4423-a7e7-93719ae62351",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embedd the Query\n",
    "query_embedding = vo.embed([query], model=\"voyage-2\", input_type=\"query\").embeddings[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100b3b7-7d59-42a5-ab9f-d79c0cb4d02c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Sample KNN Implementation to find most relevant documents to query, this is typically done at the Vector Database Level\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def k_nearest_neighbors(query_embedding, documents_embeddings, k=5):\n",
    "  query_embedding = np.array(query_embedding) # convert to numpy array\n",
    "  documents_embeddings = np.array(documents_embeddings) # convert to numpy array\n",
    "\n",
    "  # Reshape the query vector embedding to a matrix of shape (1, n) to make it compatible with cosine_similarity\n",
    "  query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "  # Calculate the similarity for each item in data\n",
    "  cosine_sim = cosine_similarity(query_embedding, documents_embeddings)\n",
    "\n",
    "  # Sort the data by similarity in descending order and take the top k items\n",
    "  sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
    "\n",
    "  # Take the top k related embeddings\n",
    "  top_k_related_indices = sorted_indices[:k]\n",
    "  top_k_related_embeddings = documents_embeddings[sorted_indices[:k]]\n",
    "  top_k_related_embeddings = [list(row[:]) for row in top_k_related_embeddings] # convert to list\n",
    "\n",
    "  return top_k_related_embeddings, top_k_related_indices"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe470d6-529d-47f7-b37a-f4ed2c45a9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get the most relevant documents\n",
    "\n",
    "retrieved_embd, retrieved_embd_index = k_nearest_neighbors(query_embedding, documents_embeddings, k=1)\n",
    "retrieved_doc = [documents[index] for index in retrieved_embd_index]\n",
    "\n",
    "print(retrieved_doc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bfa3a6e-5bef-403d-bebf-a7f7c1e05b18",
   "metadata": {},
   "source": [
    "### Incorporating Embeddings, Document Retrieval, and the ApplyGuardrail API into our Self-Hosted Model\n",
    "---\n",
    "We've enhanced our `TextGenerationWithGuardrails` class to integrate embeddings, document retrieval, and the ApplyGuardrail API with our SageMaker endpoint. This ensures protected text generation with contextually relevant information. The class now includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It interprets the API response to determine if the guardrail passed or intervened.\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "4. `embed_text`: Embeds the given text using a specified embedding model.\n",
    "5. `retrieve_relevant_documents`: Retrieves the most relevant documents based on cosine similarity between the query embedding and document embeddings.\n",
    "6. `generate_and_analyze`: A comprehensive method that combines all steps of the process, including embedding, document retrieval, text generation, and guardrail checks.\n",
    "\n",
    "The enhanced class implements the following workflow:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it embeds the query and retrieves relevant documents.\n",
    "3. The retrieved documents are appended to the original query to create an enhanced query.\n",
    "4. Text is generated using `generate_text` with the enhanced query.\n",
    "5. The generated text is then checked using `analyze_output`, with the retrieved documents serving as the grounding source.\n",
    "6. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, while also incorporating relevant context from a document collection. It's designed to:\n",
    "\n",
    "- Ensure safety through multiple guardrail checks\n",
    "- Enhance relevance by incorporating retrieved documents into the generation process\n",
    "- Provide flexibility for error handling and customization based on guardrail results\n",
    "- Easily integrate with larger applications\n",
    "\n",
    "The class can be further customized to adjust the number of retrieved documents, modify the embedding process, or alter how retrieved documents are incorporated into the query. This makes it a versatile tool for safe and context-aware text generation in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5528f06-f564-4fe0-838f-d8e0cb8d4cb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, guardrail_id: str, guardrail_version: str, embedding_model: str = \"voyage-2\"):\n",
    "        self.predictor = retrieve_default(endpoint_name)\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[0].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def embed_text(self, text: str, input_type: str = \"query\") -> List[float]:\n",
    "        \"\"\"Embed the given text using the specified embedding model.\"\"\"\n",
    "        return vo.embed([text], model=self.embedding_model, input_type=input_type).embeddings[0]\n",
    "\n",
    "    def retrieve_relevant_documents(self, query_embedding: List[float], documents_embeddings: List[List[float]], k: int = 1) -> Tuple[List[List[float]], List[int]]:\n",
    "        \"\"\"Retrieve the k most relevant documents based on cosine similarity.\"\"\"\n",
    "        query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "        documents_embeddings = np.array(documents_embeddings)\n",
    "        \n",
    "        cosine_sim = cosine_similarity(query_embedding, documents_embeddings)\n",
    "        sorted_indices = np.argsort(cosine_sim[0])[::-1]\n",
    "        \n",
    "        top_k_related_indices = sorted_indices[:k]\n",
    "        top_k_related_embeddings = documents_embeddings[top_k_related_indices].tolist()\n",
    "        \n",
    "        return top_k_related_embeddings, top_k_related_indices.tolist()\n",
    "\n",
    "    def generate_and_analyze(self, query: str, documents: List[str], max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails, including embedding and document retrieval steps.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # Embed the query and retrieve relevant documents\n",
    "        query_embedding = self.embed_text(query)\n",
    "        documents_embeddings = [self.embed_text(doc, input_type=\"document\") for doc in documents]\n",
    "        _, retrieved_doc_indices = self.retrieve_relevant_documents(query_embedding, documents_embeddings)\n",
    "        \n",
    "        retrieved_docs = [documents[index] for index in retrieved_doc_indices]\n",
    "        retrieved_grounding = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        # First, analyze the prompt using retrieved documents as grounding\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(retrieved_grounding, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # Append retrieved documents to the query\n",
    "        enhanced_query = f\"{query}\\n\\nRelevant information:\\n{retrieved_grounding}\"\n",
    "\n",
    "        # Generate text with the enhanced query\n",
    "        generated_text = self.generate_text(enhanced_query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text using the retrieved documents as grounding\n",
    "        output_passed, output_message, _ = self.analyze_output(retrieved_grounding, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50507b5a-03b2-483e-bc9f-c3a97cfa484a",
   "metadata": {
    "tags": []
   },
   "source": [
    "def main():\n",
    "    query = \"What is the Guaranteed Rate of Return for AB503 Product?\"\n",
    "    documents = [\n",
    "        \"The AG701 Global Growth Fund is currently projecting an annual return of 8.5%, focusing on emerging markets and technology sectors.\",\n",
    "        \"The AB205 Balanced Income Trust offers a steady 4% dividend yield, combining blue-chip stocks and investment-grade bonds.\",\n",
    "        \"The AE309 Green Energy ETF has outperformed the market with a 12% return over the past year, investing in renewable energy companies.\",\n",
    "        \"The AH504 High-Yield Corporate Bond Fund is offering a current yield of 6.75%, targeting BB and B rated corporate debt.\",\n",
    "        \"The AR108 Real Estate Investment Trust focuses on commercial properties and is projecting a 7% annual return including quarterly distributions.\",\n",
    "        \"The AB503 Financial Product is currently offering a non-guaranteed rate of 7%, providing a balance of growth potential and flexible investment options.\"\n",
    "    ]\n",
    "    max_new_tokens = 512\n",
    "    temperature = 0.0\n",
    "\n",
    "    text_gen = TextGenerationWithGuardrails(\n",
    "        endpoint_name=endpoint_name,\n",
    "        guardrail_id=guardrail_id,\n",
    "        guardrail_version=guardrail_version\n",
    "    )\n",
    "\n",
    "    # Bold text function\n",
    "    def bold(text):\n",
    "        return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "    # Embedding the Query\n",
    "    print(bold(\"\\n=== Query Embedding ===\\n\"))\n",
    "    query_embedding = text_gen.embed_text(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Query embedding (first 5 elements): {query_embedding[:5]}...\")\n",
    "    print()\n",
    "\n",
    "    # Embedding the Documents\n",
    "    print(bold(\"\\n=== Document Embedding ===\\n\"))\n",
    "    documents_embeddings = [text_gen.embed_text(doc, input_type=\"document\") for doc in documents]\n",
    "    for i, (doc, embedding) in enumerate(zip(documents, documents_embeddings)):\n",
    "        print(f\"Document {i+1}: {doc[:50]}...\")\n",
    "        print(f\"Embedding (first 5 elements): {embedding[:5]}...\")\n",
    "        print()\n",
    "\n",
    "    # Document Retrieval\n",
    "    print(bold(\"\\n=== Document Retrieval ===\\n\"))\n",
    "    retrieved_emb, retrieved_emb_index = text_gen.retrieve_relevant_documents(query_embedding, documents_embeddings, k=1)\n",
    "    retrieved_doc = [documents[index] for index in retrieved_emb_index]\n",
    "    print(\"Retrieved Document:\")\n",
    "    print(json.dumps(retrieved_doc, indent=2))\n",
    "    print()\n",
    "\n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(retrieved_doc[0], query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: {input_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "        return\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "\n",
    "    # Generate text\n",
    "    print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "    enhanced_query = f\"{query}\\n\\nRelevant information:\\n{retrieved_doc[0]}\"\n",
    "    generated_text = text_gen.generate_text(enhanced_query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    print(f\"Here is what the Model Responded with: {generated_text}\\n\")\n",
    "\n",
    "    # Analyze output\n",
    "    print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "    print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "    output_passed, output_message, output_details = text_gen.analyze_output(retrieved_doc[0], query, generated_text)\n",
    "    if not output_passed:\n",
    "        print(f\"Output Guardrail Intervened. The response to the User is: {output_message}\\n\")\n",
    "        print(\"Full API Response:\")\n",
    "        print(json.dumps(output_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Model Response Passed. The information presented to the user is: {generated_text}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
