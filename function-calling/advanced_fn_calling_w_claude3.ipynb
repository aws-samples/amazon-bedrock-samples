{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function-Calling (Tool Use) with Converse API in Amazon Bedrock - Advanced scenarios\n",
    "\n",
    "In this [Function calling tool use with the Bedrock Converse API](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/function-calling/Function_calling_tool_use_with_Converse_API.ipynb) example notebook, we explored basic tool use, in this notebook we will examine use-cases that _require multiple functions to be performed sequentially in the correct order_  as well as use-cases that can benefit from _parallel function calling_. This notebook uses some of the same code that has been written in [Function calling tool use with the Bedrock Converse API](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/function-calling/Function_calling_tool_use_with_Converse_API.ipynb) notebook so it is **_highly recommended that you run that notebook first and familiarize yourself with the basics of function calling_**.\n",
    "\n",
    "The [Converse or ConverseStream](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) API is a unified structured text action for simplifying the invocations to Bedrock LLMs. It includes the possibility to define tools for implementing external functions that can be called or triggered from the LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import asyncio\n",
    "import logging\n",
    "import inspect\n",
    "from typing import List\n",
    "from pydantic import Field, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using the Claude 3 Sonnet for this notebook\n",
    "MODEL_ID: str = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = boto3.session.Session().region_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator to generate tool config for any function\n",
    "def bedrock_tool(name, description):\n",
    "    def decorator(func):\n",
    "        input_model = create_model(\n",
    "            func.__name__ + \"_input\",\n",
    "            **{\n",
    "                name: (param.annotation, param.default)\n",
    "                for name, param in inspect.signature(func).parameters.items()\n",
    "                if param.default is not inspect.Parameter.empty\n",
    "            },\n",
    "        )\n",
    "\n",
    "        func.bedrock_schema = {\n",
    "            'toolSpec': {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'inputSchema': {\n",
    "                    'json': input_model.schema()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool definition\n",
    "In this notebook we have some simple functions that respond with weather related information. The choice of these functions is such that for some use-cases it would be required to use multiple functions in a given order and it is this orchestration that we are relying on the LLM to perform correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE YOUR TOOLS HERE ###\n",
    "class ToolsList:\n",
    "    @bedrock_tool(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Get weather of a location.\"\n",
    "    )\n",
    "    def get_weather(self, city: str = Field(..., description=\"City of the location\"),\n",
    "                    state: str = Field(..., description=\"State of the location\")):\n",
    "        result = f'Weather in {city, state} is 70F and clear skies.'\n",
    "        return result\n",
    "    \n",
    "    @bedrock_tool(\n",
    "        name=\"get_my_fav_city\",\n",
    "        description=\"Get name of my favorite city.\"\n",
    "    )\n",
    "    def get_my_fav_city(self):\n",
    "        fav_city = 'Georgetown, D.C.'\n",
    "        return fav_city\n",
    "    \n",
    "    @bedrock_tool(\n",
    "        name=\"get_my_fav_month\",\n",
    "        description=\"Get name of my favorite month.\"\n",
    "    )\n",
    "    def get_my_fav_month(self):\n",
    "        fav_month = 'February'\n",
    "        return fav_month\n",
    "    \n",
    "    @bedrock_tool(\n",
    "        name=\"get_weather_in_month\",\n",
    "        description=\"Get weather of a location in a given month.\"\n",
    "    )\n",
    "    def get_weather_in_month(self, city: str = Field(..., description=\"City of the location\"),\n",
    "                    state: str = Field(..., description=\"State of the location\"),\n",
    "                    month: str = Field(..., description=\"Month of interest\")):\n",
    "        result = f'Weather in {city, state, month} is 70F and clear skies.'\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tools': [{'toolSpec': {'name': 'get_weather',\n",
       "    'description': 'Get weather of a location.',\n",
       "    'inputSchema': {'json': {'properties': {'city': {'description': 'City of the location',\n",
       "        'title': 'City',\n",
       "        'type': 'string'},\n",
       "       'state': {'description': 'State of the location',\n",
       "        'title': 'State',\n",
       "        'type': 'string'}},\n",
       "      'required': ['city', 'state'],\n",
       "      'title': 'get_weather_input',\n",
       "      'type': 'object'}}}},\n",
       "  {'toolSpec': {'name': 'get_my_fav_city',\n",
       "    'description': 'Get name of my favorite city.',\n",
       "    'inputSchema': {'json': {'properties': {},\n",
       "      'title': 'get_my_fav_city_input',\n",
       "      'type': 'object'}}}},\n",
       "  {'toolSpec': {'name': 'get_my_fav_month',\n",
       "    'description': 'Get name of my favorite month.',\n",
       "    'inputSchema': {'json': {'properties': {},\n",
       "      'title': 'get_my_fav_month_input',\n",
       "      'type': 'object'}}}},\n",
       "  {'toolSpec': {'name': 'get_weather_in_month',\n",
       "    'description': 'Get weather of a location in a given month.',\n",
       "    'inputSchema': {'json': {'properties': {'city': {'description': 'City of the location',\n",
       "        'title': 'City',\n",
       "        'type': 'string'},\n",
       "       'state': {'description': 'State of the location',\n",
       "        'title': 'State',\n",
       "        'type': 'string'},\n",
       "       'month': {'description': 'Month of interest',\n",
       "        'title': 'Month',\n",
       "        'type': 'string'}},\n",
       "      'required': ['city', 'state', 'month'],\n",
       "      'title': 'get_weather_in_month_input',\n",
       "      'type': 'object'}}}}],\n",
       " 'toolChoice': {'auto': {}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolConfig = {\n",
    "    'tools': [tool.bedrock_schema for tool in ToolsList.__dict__.values() if hasattr(tool, 'bedrock_schema')],\n",
    "    'toolChoice': {'auto': {}}\n",
    "}\n",
    "toolConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Python `asyncio` based concurrency so that we can call multiple functions in parallel\n",
    "\n",
    "For some use-cases the LLM correctly identifies that we need to make multiple calls and these calls can be made in parallel to save on the overall latency. The execution of the functions is actually done by our code so we are in control if we want to invoke these functions sequentially or in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_function(function, tool_class):\n",
    "    logger.info(f\"function={function}\")\n",
    "    function = function[1]\n",
    "    logger.info(f\"step 2, function - Calling tool...{function['name']}\")\n",
    "    tool_name = function['name']\n",
    "    tool_args = function['input'] or {}\n",
    "    tool_response = getattr(tool_class, tool_name)(**tool_args)\n",
    "    logger.info(f\"step 2, function {function['name']}- Got tool response...{tool_response}\")\n",
    "    return dict(tool_response=tool_response, tool_use_id=function['toolUseId'])\n",
    "\n",
    "# Asynchronous wrapper function to allow our function calls to happen concurrently\n",
    "async def async_call_function(function, tool_class):\n",
    "    # Run the call_function function in a separate thread to run each function asynchronously\n",
    "    return await asyncio.to_thread(call_function, function, tool_class)\n",
    "\n",
    "# Final asynchronous function to deploy all of the functions concurrently\n",
    "async def async_call_all_functions(function_calling, tool_class):\n",
    "    \n",
    "    n: int = 4 # max concurrency so as to not get a throttling exception\n",
    "    \n",
    "    ## Split experiments into smaller batches for concurrent deployment\n",
    "    function_calling_splitted = [function_calling[i * n:(i + 1) * n] for i in range((len(function_calling) + n - 1) // n )]\n",
    "    results = []\n",
    "    for function_sublist in function_calling_splitted:\n",
    "        ## do the function calls in batches\n",
    "        result = await asyncio.gather(*[async_call_function(function, \n",
    "                                                            tool_class) for function in enumerate(function_sublist)])\n",
    "        ## Collect and furthermore extend the results from each batch\n",
    "        results.extend(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper functions for the Bedrock `Converse` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_with_tools(modelId, messages, system='', toolConfig=None):\n",
    "    return bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a function call conversation\n",
    "This function uses function call to answer uses questions in 3 steps\n",
    "1. Ask the LLM what to do given the user question and the tools provided.\n",
    "1. Read the LLM response and if the response says use this tool(s) then invoke the\n",
    "   function(s) corresponding to that tool.\n",
    "   1. If the LLM says no tool use is required/available then return that as the final \n",
    "   response\n",
    "1. Provide the entire conversation history including the function call output to the LLM\n",
    "   and ask for a response.\n",
    "1. Repeat steps 2 and 3 until the LLM response says no more tools need to be used, return\n",
    "   that as the final response.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def converse(tool_class, modelId, prompt, system='', toolConfig=None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function uses function call to answer uses questions in 3 steps\n",
    "    1. Ask the LLM what to do given the user question and the tools provided.\n",
    "    2. Read the LLM response and if the response says use this tool(s) then invoke the\n",
    "       function(s) corresponding to that tool.\n",
    "       - If the LLM says no tool use is required/available then return that as the final \n",
    "         response\n",
    "    3. Provide the entire conversation history including the function call output to the LLM\n",
    "       and ask for a response.\n",
    "    4. Repeat steps 2 and 3 until the LLM response says no more tools need to be used, return\n",
    "       that as the final response.   \n",
    "    \"\"\"\n",
    "    # step 1. invoke model for the first time to figure out what tools if any are needed\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    logger.info(f\"step 1. Invoking model...{modelId}\")\n",
    "    output = converse_with_tools(modelId, messages, system, toolConfig)\n",
    "    messages.append(output['output']['message'])\n",
    "    logger.info(f\"step 1 output from model...{json.dumps(output['output'], indent=2, default=str)}\")\n",
    "\n",
    "    while True:\n",
    "        # step 2. check if the model said any tools should be used, invoke the tools that the model\n",
    "        # said need to be used\n",
    "\n",
    "        function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "        if function_calling:\n",
    "            tool_result_message = {\"role\": \"user\", \"content\": []}\n",
    "            logger.info(f\"there are {len(function_calling)} entries in function_calling\")\n",
    "            # async version\n",
    "            s = time.perf_counter()\n",
    "\n",
    "            # Call all functions in parallel\n",
    "            tool_responses_list = await async_call_all_functions(function_calling, tool_class)\n",
    "            elapsed_async = time.perf_counter() - s\n",
    "            logger.info(f\"ran {len(function_calling)} in parallel in {elapsed_async:0.4f} seconds\")\n",
    "            for r in tool_responses_list:\n",
    "                tool_result_message['content'].append({\n",
    "                    'toolResult': {\n",
    "                        'toolUseId': r['tool_use_id'],\n",
    "                        'content': [{\"text\": r['tool_response']}]\n",
    "                    }\n",
    "                })\n",
    "            messages.append(tool_result_message)\n",
    "        else:\n",
    "            logger.info(f\"there are NO functions to call as per the LLM\")\n",
    "            break\n",
    "\n",
    "        # step 3. call the model one final time to put all the tool responses together\n",
    "        # and generate a final response\n",
    "        logger.info(f\"step 3, calling model with the results from calling {len(function_calling)} functions\")\n",
    "        \n",
    "        output = converse_with_tools(modelId, messages, system, toolConfig)\n",
    "        messages.append(output['output']['message'])\n",
    "        logger.info(f\"function calling - Got final answer from step 3, checking if more function calling is needed\")\n",
    "        function_calling = [c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c]\n",
    "        if len(function_calling) == 0:\n",
    "            logger.info(f\"step 3. no more function calling is needed, we have our final answer, exiting\")\n",
    "            break\n",
    "        else:\n",
    "            logger.info(f\"step 3. seems like we still need to call {len(function_calling)} functions, continuing..\")\n",
    "\n",
    "\n",
    "    return messages, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the system prompt\n",
    "The system prompt is set for the overall task that this \"agent\" is expected to perform. Do not make this very specific to one particular use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADJUST YOUR SYSTEM PROMPT HERE - IF DESIRED ###\n",
    "system_prompt: List[str] = [{\"text\": \"You're provided with multiple tools that can help you answer user questions about weather; \\\n",
    "                              only use a tool if required. You can call the tool multiple times in the same response if required. \\\n",
    "                              Given a user input first think about which all tools would be needed and then include them in your function calling response in \\\n",
    "                              the correct order.\\\n",
    "                              Don't make reference to the tools in your final answer.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use-case 1. No tool use\n",
    "Ask an LLM a question that it cannot answer based on the tools provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:17,805] p43052 {1303253139.py:17} INFO - step 1. Invoking model...anthropic.claude-3-sonnet-20240229-v1:0\n",
      "[2024-06-26 22:36:21,378] p43052 {1303253139.py:20} INFO - step 1 output from model...{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Unfortunately, I don't have any tools that can provide information about the top songs or music charts in a specific city like Paris. My tools are focused on providing weather information for different locations. I don't have the capability to look up music chart data.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:21,379] p43052 {1303253139.py:46} INFO - there are NO functions to call as per the LLM\n",
      "[2024-06-26 22:36:21,380] p43052 {2903005534.py:5} INFO - final response = {\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Unfortunately, I don't have any tools that can provide information about the top songs or music charts in a specific city like Paris. My tools are focused on providing weather information for different locations. I don't have the capability to look up music chart data.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:21,381] p43052 {2903005534.py:6} INFO - Output:\n",
      "Unfortunately, I don't have any tools that can provide information about the top songs or music charts in a specific city like Paris. My tools are focused on providing weather information for different locations. I don't have the capability to look up music chart data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### REPLACE WITH YOUR OWN PROMPTS HERE ###\n",
    "prompt: str = \"What is the #1 song in Paris?\"\n",
    "\n",
    "messages, output = await converse(ToolsList(), MODEL_ID, prompt, system_prompt, toolConfig)\n",
    "logger.info(f\"final response = {json.dumps(output['output'], indent=2, default=str)}\")\n",
    "logger.info(f\"Output:\\n{output['output']['message']['content'][0].get('text')}\\n\")\n",
    "#logger.info(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use-case 2: simple use-case call one function\n",
    "Ask the LLM to find the weather in Paris and it says ok call the get weather tool/function, we call the get weather function and provide the tool response and original question to the LLM and ask it again and this time it provides the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:21,393] p43052 {1303253139.py:17} INFO - step 1. Invoking model...anthropic.claude-3-sonnet-20240229-v1:0\n",
      "[2024-06-26 22:36:23,393] p43052 {1303253139.py:20} INFO - step 1 output from model...{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_mXDD6tDvR9SpnxcGqaR-xg\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Paris\",\n",
      "            \"state\": \"NA\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:23,395] p43052 {1303253139.py:29} INFO - there are 1 entries in function_calling\n",
      "[2024-06-26 22:36:23,396] p43052 {3670242087.py:2} INFO - function=(0, {'toolUseId': 'tooluse_mXDD6tDvR9SpnxcGqaR-xg', 'name': 'get_weather', 'input': {'city': 'Paris', 'state': 'NA'}})\n",
      "[2024-06-26 22:36:23,397] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_weather\n",
      "[2024-06-26 22:36:23,398] p43052 {3670242087.py:8} INFO - step 2, function get_weather- Got tool response...Weather in ('Paris', 'NA') is 70F and clear skies.\n",
      "[2024-06-26 22:36:23,400] p43052 {1303253139.py:51} INFO - step 3, calling model with the results from calling 1 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 1 in parallel in 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:24,931] p43052 {1303253139.py:55} INFO - function calling - Got final answer from step 3, checking if more function calling is needed\n",
      "[2024-06-26 22:36:24,932] p43052 {1303253139.py:58} INFO - step 3. no more function calling is needed, we have our final answer, exiting\n",
      "[2024-06-26 22:36:24,933] p43052 {4188552546.py:5} INFO - final response = {\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in Paris is 70\\u00b0F and clear skies.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:24,934] p43052 {4188552546.py:6} INFO - Output:\n",
      "The weather in Paris is 70Â°F and clear skies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the weather in Paris?\"\n",
    "# prompt = \"what is the weather in my favorite city in my favorite month?\"\n",
    "\n",
    "messages, output = await converse(ToolsList(), MODEL_ID, prompt, system_prompt, toolConfig)\n",
    "logger.info(f\"final response = {json.dumps(output['output'], indent=2, default=str)}\")\n",
    "logger.info(f\"Output:\\n{output['output']['message']['content'][0].get('text')}\\n\")\n",
    "#logger.info(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use case 3 - Reason through multiple functions in a sequential order\n",
    "We are for weather in \"my favorite\" city in \"my favorite month\" so the LLM suggests call the my favorite city function first, then we call that function and provide the output to the LLM again adn this time it says find my favorite month, we provide the combined output available at this point to the LLM and this time it says ok call the get weather API and then that is the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:24,948] p43052 {1303253139.py:17} INFO - step 1. Invoking model...anthropic.claude-3-sonnet-20240229-v1:0\n",
      "[2024-06-26 22:36:26,826] p43052 {1303253139.py:20} INFO - step 1 output from model...{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Okay, let me try to gather the required information to answer your question:\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_jzNj8wtFSmWyWcI73es0Qw\",\n",
      "          \"name\": \"get_my_fav_city\",\n",
      "          \"input\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:26,828] p43052 {1303253139.py:29} INFO - there are 1 entries in function_calling\n",
      "[2024-06-26 22:36:26,829] p43052 {3670242087.py:2} INFO - function=(0, {'toolUseId': 'tooluse_jzNj8wtFSmWyWcI73es0Qw', 'name': 'get_my_fav_city', 'input': {}})\n",
      "[2024-06-26 22:36:26,830] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_my_fav_city\n",
      "[2024-06-26 22:36:26,831] p43052 {3670242087.py:8} INFO - step 2, function get_my_fav_city- Got tool response...Georgetown, D.C.\n",
      "[2024-06-26 22:36:26,832] p43052 {1303253139.py:51} INFO - step 3, calling model with the results from calling 1 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 1 in parallel in 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:28,429] p43052 {1303253139.py:55} INFO - function calling - Got final answer from step 3, checking if more function calling is needed\n",
      "[2024-06-26 22:36:28,430] p43052 {1303253139.py:61} INFO - step 3. seems like we still need to call 1 functions, continuing..\n",
      "[2024-06-26 22:36:28,431] p43052 {1303253139.py:29} INFO - there are 1 entries in function_calling\n",
      "[2024-06-26 22:36:28,432] p43052 {3670242087.py:2} INFO - function=(0, {'toolUseId': 'tooluse_WwppVe2iSB2jnyADaDcBZQ', 'name': 'get_my_fav_month', 'input': {}})\n",
      "[2024-06-26 22:36:28,432] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_my_fav_month\n",
      "[2024-06-26 22:36:28,433] p43052 {3670242087.py:8} INFO - step 2, function get_my_fav_month- Got tool response...February\n",
      "[2024-06-26 22:36:28,434] p43052 {1303253139.py:51} INFO - step 3, calling model with the results from calling 1 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 1 in parallel in 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:31,135] p43052 {1303253139.py:55} INFO - function calling - Got final answer from step 3, checking if more function calling is needed\n",
      "[2024-06-26 22:36:31,136] p43052 {1303253139.py:61} INFO - step 3. seems like we still need to call 1 functions, continuing..\n",
      "[2024-06-26 22:36:31,137] p43052 {1303253139.py:29} INFO - there are 1 entries in function_calling\n",
      "[2024-06-26 22:36:31,140] p43052 {3670242087.py:2} INFO - function=(0, {'toolUseId': 'tooluse_85jJPG5kQMiCWTvsN1Y4tA', 'name': 'get_weather_in_month', 'input': {'city': 'Georgetown', 'state': 'D.C.', 'month': 'February'}})\n",
      "[2024-06-26 22:36:31,142] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_weather_in_month\n",
      "[2024-06-26 22:36:31,143] p43052 {3670242087.py:8} INFO - step 2, function get_weather_in_month- Got tool response...Weather in ('Georgetown', 'D.C.', 'February') is 70F and clear skies.\n",
      "[2024-06-26 22:36:31,145] p43052 {1303253139.py:51} INFO - step 3, calling model with the results from calling 1 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 1 in parallel in 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:32,602] p43052 {1303253139.py:55} INFO - function calling - Got final answer from step 3, checking if more function calling is needed\n",
      "[2024-06-26 22:36:32,602] p43052 {1303253139.py:58} INFO - step 3. no more function calling is needed, we have our final answer, exiting\n",
      "[2024-06-26 22:36:32,603] p43052 {309264306.py:4} INFO - final response = {\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in your favorite city Georgetown, D.C. in your favorite month February is 70F and clear skies.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:32,604] p43052 {309264306.py:5} INFO - Output:\n",
      "The weather in your favorite city Georgetown, D.C. in your favorite month February is 70F and clear skies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the weather in my favorite city in my favorite month?\"\n",
    "\n",
    "messages, output = await converse(ToolsList(), MODEL_ID, prompt, system_prompt, toolConfig)\n",
    "logger.info(f\"final response = {json.dumps(output['output'], indent=2, default=str)}\")\n",
    "logger.info(f\"Output:\\n{output['output']['message']['content'][0].get('text')}\\n\")\n",
    "#logger.info(f\"Messages:\\n{json.dumps(messages, indent=2, ensure_ascii=False)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use-case 4 - Parallel function calling\n",
    "\n",
    "We ask for weather in two cities, the LLM results 2 function calls and then we run these functions calls in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:32,618] p43052 {1303253139.py:17} INFO - step 1. Invoking model...anthropic.claude-3-sonnet-20240229-v1:0\n",
      "[2024-06-26 22:36:36,196] p43052 {1303253139.py:20} INFO - step 1 output from model...{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"Okay, let me get the weather information for Paris and Berlin:\"\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_iQNDrl3gR7SsuKzSSmNj4Q\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Paris\",\n",
      "            \"state\": \"France\"\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_pOncmu_ETy-g01TQrPlVqw\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Berlin\",\n",
      "            \"state\": \"Germany\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:36,197] p43052 {1303253139.py:29} INFO - there are 2 entries in function_calling\n",
      "[2024-06-26 22:36:36,198] p43052 {3670242087.py:2} INFO - function=(0, {'toolUseId': 'tooluse_iQNDrl3gR7SsuKzSSmNj4Q', 'name': 'get_weather', 'input': {'city': 'Paris', 'state': 'France'}})\n",
      "[2024-06-26 22:36:36,199] p43052 {3670242087.py:2} INFO - function=(1, {'toolUseId': 'tooluse_pOncmu_ETy-g01TQrPlVqw', 'name': 'get_weather', 'input': {'city': 'Berlin', 'state': 'Germany'}})\n",
      "[2024-06-26 22:36:36,199] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_weather\n",
      "[2024-06-26 22:36:36,201] p43052 {3670242087.py:4} INFO - step 2, function - Calling tool...get_weather\n",
      "[2024-06-26 22:36:36,201] p43052 {3670242087.py:8} INFO - step 2, function get_weather- Got tool response...Weather in ('Paris', 'France') is 70F and clear skies.\n",
      "[2024-06-26 22:36:36,202] p43052 {3670242087.py:8} INFO - step 2, function get_weather- Got tool response...Weather in ('Berlin', 'Germany') is 70F and clear skies.\n",
      "[2024-06-26 22:36:36,207] p43052 {1303253139.py:51} INFO - step 3, calling model with the results from calling 2 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 2 in parallel in 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-26 22:36:39,957] p43052 {1303253139.py:55} INFO - function calling - Got final answer from step 3, checking if more function calling is needed\n",
      "[2024-06-26 22:36:39,958] p43052 {1303253139.py:58} INFO - step 3. no more function calling is needed, we have our final answer, exiting\n",
      "[2024-06-26 22:36:39,959] p43052 {3499383132.py:4} INFO - final response = {\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in Paris, France is 70F and clear skies.\\nThe weather in Berlin, Germany is also 70F and clear skies.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[2024-06-26 22:36:39,960] p43052 {3499383132.py:5} INFO - Output:\n",
      "The weather in Paris, France is 70F and clear skies.\n",
      "The weather in Berlin, Germany is also 70F and clear skies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the weather in Paris and in Berlin?\"\n",
    "\n",
    "messages, output = await converse(ToolsList(), MODEL_ID, prompt, system_prompt, toolConfig)\n",
    "logger.info(f\"final response = {json.dumps(output['output'], indent=2, default=str)}\")\n",
    "logger.info(f\"Output:\\n{output['output']['message']['content'][0].get('text')}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
