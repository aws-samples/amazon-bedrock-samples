{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Reasoning Valid at N Playground\n",
    "\n",
    "This notebook demonstrates how to run a \"Valid at N\" experiment to evaluate how many iterations of response rewriting are needed before the response complies with our policy rules.\n",
    "\n",
    "It includes the following:\n",
    "\n",
    "1. Setting up the Bedrock client with custom API models\n",
    "2. Runs \"Valid at N\" experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the Bedrock client with custom API models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "from policy_definition import get_policy_definition\n",
    "from rewrite import summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bedrock client\n",
    "REGION_NAME=\"us-west-2\" # Fill in the AWS Region\n",
    "my_session = boto3.session.Session()\n",
    "runtime_client = my_session.client('bedrock-runtime', region_name=REGION_NAME)\n",
    "bedrock_client = my_session.client('bedrock', region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy arn for which policy definitions will be retrieved\n",
    "AR_POLICY_ARN=\"<AR_POLICY_ARN>\"\n",
    "\n",
    "# Guardrail id which will be used for calling ApplyGuardrails API\n",
    "GUARDRAIL_ID = \"<GUARDRAIL_ID>\"\n",
    "\n",
    "# Guardrail version which will be used for calling ApplyGuardrails API\n",
    "GUARDRAIL_VERSION = \"<GUARDRAIL_VERSION>\"\n",
    "\n",
    "# Id of the model used by bedrock when generating LLM responses\n",
    "MODEL_ID=\"<MODEL_ID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid at N Experiment\n",
    "\n",
    "## Purpose\n",
    "This code implements a \"Valid at N\" experiment to evaluate how many iterations of response rewriting are needed before the response complies with our policy rules.\n",
    "\n",
    "## Concept\n",
    "- **Valid at N**: N represents the number of iterations required before a response is deemed \"valid\" by the automated reasoning guardrails.\n",
    "- N=1 means the original response was valid without any rewriting\n",
    "- Higher N values indicate more complex policy violations that required multiple rewrites\n",
    "\n",
    "## Methodology\n",
    "1. Start with an initial query and LLM response\n",
    "2. Apply guardrails to check if response is valid\n",
    "3. If valid or too_complex, record N and stop\n",
    "4. If not valid, rewrite the response and repeat the process\n",
    "5. Continue until a valid response is found or max iterations reached\n",
    "\n",
    "## Analysis Value\n",
    "- **Policy Compliance**: Understand how well initial LLM responses comply with policies\n",
    "- **Rewriting Effectiveness**: Measure how efficiently the rewriting process resolves policy violations\n",
    "- **Error Distribution**: Identify common error patterns and their resolution complexity\n",
    "- **Optimization Opportunities**: Determine where to focus prompt engineering efforts\n",
    "\n",
    "### How to Interpret Results\n",
    "\n",
    "- **N=1**: Perfect! The original response is already valid.\n",
    "- **N=2**: Good. Only one rewrite was needed to make the response valid.\n",
    "- **N=3+**: Concerning. Multiple rewrites were needed, suggesting significant misalignment with policy.\n",
    "- **Nâ‰¥max_iterations**: Critical. The response couldn't be made valid within the iteration limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_at_n_experiment(\n",
    "    user_query, \n",
    "    initial_llm_response, \n",
    "    policy_definition, \n",
    "    guardrail_id, \n",
    "    guardrail_version, \n",
    "    bedrock_runtime_client,\n",
    "    model_id=MODEL_ID, \n",
    "    max_iterations=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a 'Valid at N' experiment by repeatedly applying summarize_results\n",
    "    until a valid response is produced.\n",
    "    \"\"\"\n",
    "    # Initialize result tracking\n",
    "    results = {\n",
    "        \"query\": user_query,\n",
    "        \"original_response\": initial_llm_response,\n",
    "        \"iterations\": [],\n",
    "        \"n_value\": None,\n",
    "        \"final_valid_response\": None\n",
    "    }\n",
    "    \n",
    "    # Current response to validate (starts with initial response)\n",
    "    current_response = initial_llm_response\n",
    "    \n",
    "    # Iterate until we find a valid response or reach max iterations\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # Apply summarize_results to get findings and potentially rewrite\n",
    "        iteration_result = summarize_results(\n",
    "            user_query=user_query,\n",
    "            llm_response=current_response,\n",
    "            policy_definition=policy_definition,\n",
    "            guardrail_id=guardrail_id,\n",
    "            guardrail_version=guardrail_version,\n",
    "            bedrock_runtime_client=bedrock_runtime_client,\n",
    "            model_id=model_id\n",
    "        )\n",
    "        \n",
    "        # Store the iteration result\n",
    "        iteration_data = {\n",
    "            \"iteration\": iteration,\n",
    "            \"response\": current_response,\n",
    "            \"findings\": iteration_result[\"findings\"],\n",
    "            \"finding_types\": []\n",
    "        }\n",
    "        \n",
    "        # Extract finding types more precisely\n",
    "        if \"**finding type:** valid\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"valid\")\n",
    "        if \"**finding type:** too_complex\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"too_complex\")\n",
    "        if \"**finding type:** invalid\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"invalid\")\n",
    "        if \"**finding type:** satisfiable\" in iteration_result[\"findings\"].lower() or \"**finding type:** satisfied\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"satisfiable\")\n",
    "        if \"**finding type:** impossible\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"impossible\")\n",
    "        if \"**finding type:** translation_ambiguous\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"translation_ambiguous\")\n",
    "        if \"**finding type:** no_translations\" in iteration_result[\"findings\"].lower():\n",
    "            iteration_data[\"finding_types\"].append(\"no_translations\")\n",
    "        \n",
    "        # Add to results\n",
    "        results[\"iterations\"].append(iteration_data)\n",
    "        \n",
    "        # Check if valid\n",
    "        if \"valid\" in iteration_data[\"finding_types\"] or \"too_complex\" in iteration_data[\"finding_types\"]:\n",
    "            results[\"n_value\"] = iteration\n",
    "            results[\"final_valid_response\"] = current_response\n",
    "            break\n",
    "        \n",
    "        # If not valid and we have a rewritten response, use that for next iteration\n",
    "        if iteration_result[\"rewritten_response\"]:\n",
    "            iteration_data[\"rewritten_to\"] = iteration_result[\"rewritten_response\"]\n",
    "            current_response = iteration_result[\"rewritten_response\"]\n",
    "        else:\n",
    "            # If no rewritten response but not valid, we can't continue\n",
    "            break\n",
    "    \n",
    "    # If we reached max iterations without finding validity\n",
    "    if results[\"n_value\"] is None:\n",
    "        results[\"n_value\"] = f\">= {max_iterations}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_valid_at_n_results(results):\n",
    "    \"\"\"Display the results of a Valid at N experiment in a formatted way.\"\"\"\n",
    "    print(f\"\\n## Valid at N Experiment Results (N = {results['n_value']})\")\n",
    "    print(f\"Query: {results['query']}\")\n",
    "    \n",
    "    print(\"\\n### Original Response\")\n",
    "    print(results['original_response'])\n",
    "    \n",
    "    for i, iteration in enumerate(results['iterations']):\n",
    "        print(f\"\\n### Iteration {iteration['iteration']}\")\n",
    "        print(f\"Finding Types: {', '.join(iteration['finding_types']) if iteration['finding_types'] else 'None'}\")\n",
    "        \n",
    "        if i > 0:  # Skip printing first response again\n",
    "            print(\"\\nResponse:\")\n",
    "            print(iteration['response'])\n",
    "        \n",
    "        print(\"\\nFindings:\")\n",
    "        print(iteration['findings'])\n",
    "        \n",
    "        if 'rewritten_to' in iteration:\n",
    "            print(\"\\nRewritten To:\")\n",
    "            print(iteration['rewritten_to'])\n",
    "    \n",
    "    if results['final_valid_response'] and results['n_value'] != 1:\n",
    "        print(\"\\n### Final Valid Response\")\n",
    "        print(results['final_valid_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"<USER_QUERY>\"\n",
    "llm_response = \"<LLM_RESPONSE>\"\n",
    "\n",
    "policy_definition = get_policy_definition(bedrock_policy_client, AR_POLICY_ARN)\n",
    "\n",
    "# Run the Valid at N experiment\n",
    "results = valid_at_n_experiment(\n",
    "    user_query,\n",
    "    llm_response,\n",
    "    policy_definition,\n",
    "    GUARDRAIL_ID,\n",
    "    GUARDRAIL_VERSION,\n",
    "    bedrock_runtime_client\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_valid_at_n_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
