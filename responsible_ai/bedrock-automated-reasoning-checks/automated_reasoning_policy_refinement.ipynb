{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bdf46cc",
   "metadata": {},
   "source": [
    "# Automated Reasoning Policy Refinement Playground\n",
    "\n",
    "Refining a Automated Reasoning policy is an important step in improving the soundness of Automated Reasoning Checks. Policy refinement involves modifications to rules, variables and types of the Automated Reasoning policy. This can be done through direct updates to the rules, variables or types. \n",
    "It can also be done through natual language feedback or by generating scenarios to test the rules, and then providing feedback from the test results of the generated scenarios.\n",
    "\n",
    "This notebook demonstrates how to generate scenarios from the policy, run tests using the generated scenario, and update rules, types and variables of the automated reasoning policy using the AWS Bedrock APIs. It includes the following functionality:\n",
    "\n",
    "1. Creating annotations\n",
    "    1. Using Scenario Generation to identify which annotations are required through automated testing\n",
    "2. Generating a new Policy Definition using the annotations\n",
    "3. Update the Automated Reasoning Policy with the updated Policy definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860aa339",
   "metadata": {},
   "source": [
    "### Pre-requisites for running this notebook\n",
    "This notebook uses the medical policy created in Lab1 - If you have not set up Automated Reasoning policy for the sample medical policy executed that, first run Lab1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3a0a5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c98220",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d94cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bedrock client\n",
    "my_session = boto3.session.Session()\n",
    "REGION_NAME = my_session.region_name\n",
    "print(f'The region is {REGION_NAME}')\n",
    "\n",
    "runtime_client = my_session.client('bedrock-runtime', region_name=REGION_NAME)\n",
    "bedrock_client = my_session.client('bedrock', region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f299fb",
   "metadata": {},
   "source": [
    "# Automated Reasoning Policy Refinement\n",
    "\n",
    "Policy refinement is an important step in validating that the rules captured in the Automated Reasoning (AR) Policy is accurate, complete and represents the source document/ truth well. This step is crucial in improving the soundness of Automated Reasoning checks.\n",
    "\n",
    "Steps:\n",
    "1. Create an annotation\n",
    "2. Start the automated reasoning workflow with the annotation as a policy repair asset\n",
    "3. Check the result of the workflow and analyse the assets: build log and quality results\n",
    "4. Now we can update the automated reasoning policy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490101d9",
   "metadata": {},
   "source": [
    "#### Before executing this lab, retrieve the ARN of the Policy created and provide it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the policy arn for which you will create a test case\n",
    "# TODO\n",
    "policy_arn=\"\" # Update with the policy created in Lab 1\n",
    "\n",
    "# Retrieve all build workflows associated with the specified policy\n",
    "list_build_workflows_response = bedrock_client.list_automated_reasoning_policy_build_workflows(policyArn=policy_arn)\n",
    "\n",
    "# Extract the build workflow ID from the first workflow in the response\n",
    "# This ID is required to run test cases against the policy\n",
    "build_workflow_id = list_build_workflows_response['automatedReasoningPolicyBuildWorkflowSummaries'][0]['buildWorkflowId']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628d3b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47488ebe",
   "metadata": {},
   "source": [
    "### What are annotations?\n",
    "\n",
    "An annotation is a addition/update/modification performed on the Automated Reasoning Policy through addition/ modification/ deletion of rules, variables and custom types. \n",
    "\n",
    "The allowed annotations are:\n",
    "1. `` addType ``: Add a new type by passing in a name, description and value of the new type\n",
    "2. `` updateType ``: Update a type by passing in the name of the type to be updated, a new name, description and value\n",
    "3. `` deleteType ``: Delete a type by passing in the name of the type to be deleted\n",
    "4. `` addVariable ``: Add a new variable by passing in a name, description and type of the new variable\n",
    "5. `` updateVariable ``: Update a variable by passing in the name of the variable to be updated, a new name and description\n",
    "6. `` addType ``: Add a new type\n",
    "7. `` addType ``: Add a new type\n",
    "8. `` addType ``: Add a new type\n",
    "9. `` addType ``: Add a new type\n",
    "10. `` addType ``: Add a new type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(annotation_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Create an Automated Reasoning Policy Annotation dynamically\n",
    "    \n",
    "    Args:\n",
    "        annotation_type (str): Type of annotation to create\n",
    "        **kwargs: Keyword arguments specific to the annotation type\n",
    "    \n",
    "    Returns:\n",
    "        dict: Annotation dictionary\n",
    "    \n",
    "    Supported Annotation Types:\n",
    "    - 'add_type': Add a new type\n",
    "    - 'update_type': Update an existing type\n",
    "    - 'delete_type': Delete a type\n",
    "    - 'add_variable': Add a new variable\n",
    "    - 'update_variable': Update an existing variable\n",
    "    - 'delete_variable': Delete a variable\n",
    "    - 'add_rule': Add a new rule (SMT)\n",
    "    - 'update_rule': Update an existing rule (SMT)\n",
    "    - 'delete_rule': Delete a rule\n",
    "    - 'add_rule_from_nl': Add a rule from natural language\n",
    "    - 'update_from_rule_feedback': Update rules based on rule feedback\n",
    "    - 'update_from_scenario_feedback': Update rules based on scenario feedback\n",
    "    \"\"\"\n",
    "    annotation_map = {\n",
    "        'add_type': {\n",
    "            'addType': {\n",
    "                'name': kwargs.get('name'),\n",
    "                'description': kwargs.get('description'),\n",
    "                'values': kwargs.get('values', [])\n",
    "            }\n",
    "        },\n",
    "        'update_type': {\n",
    "            'updateType': {\n",
    "                'name': kwargs.get('name'),\n",
    "                'newName': kwargs.get('new_name'),\n",
    "                'description': kwargs.get('description'),\n",
    "                'values': kwargs.get('values', [])\n",
    "            }\n",
    "        },\n",
    "        'delete_type': {\n",
    "            'deleteType': {\n",
    "                'name': kwargs.get('name')\n",
    "            }\n",
    "        },\n",
    "        'add_variable': {\n",
    "            'addVariable': {\n",
    "                'name': kwargs.get('name'),\n",
    "                'type': kwargs.get('type'),\n",
    "                'description': kwargs.get('description')\n",
    "            }\n",
    "        },\n",
    "        'update_variable': {\n",
    "            'updateVariable': {\n",
    "                'name': kwargs.get('name'),\n",
    "                'newName': kwargs.get('new_name'),\n",
    "                'description': kwargs.get('description')\n",
    "            }\n",
    "        },\n",
    "        'delete_variable': {\n",
    "            'deleteVariable': {\n",
    "                'name': kwargs.get('name')\n",
    "            }\n",
    "        },\n",
    "        'add_rule': {\n",
    "            'addRule': {\n",
    "                'expression': kwargs.get('expression')\n",
    "            }\n",
    "        },\n",
    "        'update_rule': {\n",
    "            'updateRule': {\n",
    "                'ruleId': kwargs.get('rule_id'),\n",
    "                'expression': kwargs.get('expression')\n",
    "            }\n",
    "        },\n",
    "        'delete_rule': {\n",
    "            'deleteRule': {\n",
    "                'ruleId': kwargs.get('rule_id')\n",
    "            }\n",
    "        },\n",
    "        'add_rule_from_nl': {\n",
    "            'addRuleFromNaturalLanguage': {\n",
    "                'naturalLanguage': kwargs.get('natural_language')\n",
    "            }\n",
    "        },\n",
    "        'update_from_rule_feedback': {\n",
    "            'updateFromRulesFeedback': {\n",
    "                'ruleIds': kwargs.get('rule_ids', []),\n",
    "                'feedback': kwargs.get('feedback')\n",
    "            }\n",
    "        },\n",
    "        'update_from_scenario_feedback': {\n",
    "            'updateFromScenarioFeedback': {\n",
    "                'ruleIds': kwargs.get('rule_ids', []),\n",
    "                'scenarioExpression': kwargs.get('scenario_expression'),\n",
    "                'feedback': kwargs.get('feedback')\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Validate annotation type\n",
    "    if annotation_type not in annotation_map:\n",
    "        raise ValueError(f\"Invalid annotation type. Choose from {list(annotation_map.keys())}\")\n",
    "    \n",
    "    # Validate required parameters\n",
    "    annotation = annotation_map[annotation_type]\n",
    "    \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc809434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current Automated Reasoning policy\n",
    "def get_policy_definition(policy_arn):\n",
    "    \"\"\"\n",
    "    Get the policy definition.\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the policy.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Policy definition.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock_client.export_automated_reasoning_policy_version(\n",
    "            policyArn=policy_arn\n",
    "        )\n",
    "        \n",
    "        return response.get('policyDefinition', {})\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting policy definition: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "draft_policy = get_policy_definition(\n",
    "    policy_arn=policy_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496929f",
   "metadata": {},
   "source": [
    "Let's check what types exist in the Automated Reasoning Policy for the sample medical policy we created in Lab 1. In the first annotation , let's update one of the types programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_policy['types'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94aaa2a",
   "metadata": {},
   "source": [
    "# Step 1: Create annotations\n",
    "\n",
    "Let's create a new type: PatientAgeGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can provide a rule using natural language as follows:\n",
    "nl_rule = 'Provide patients in low-risk with a 24/7 call-back number and clear instructions on warning signs/symptoms that should prompt them to seek immediate medical attention'\n",
    "nl_rule_annotation = create_annotation('add_rule_from_nl', natural_language=nl_rule)\n",
    "\n",
    "# Let's update the name of the type \"HealthcareFacilityType\" to \"MedicalFacilityType\"\n",
    "update_type_annotation = create_annotation(\n",
    "    'update_type', \n",
    "    name='HealthcareFacilityType', \n",
    "    new_name='MedicalFacilityType', \n",
    "    description='The type of healthcare or medical facility where the readmission risk assessment protocol is being implemented', \n",
    "    values=[{'value': 'ACUTE_CARE_HOSPITAL',\n",
    "   'description': 'An acute care hospital with 25 or more beds'},\n",
    "  {'value': 'CRITICAL_ACCESS_HOSPITAL',\n",
    "   'description': 'A critical access hospital in a rural area'},\n",
    "  {'value': 'ACADEMIC_MEDICAL_CENTER',\n",
    "   'description': 'A university-affiliated medical center that provides education, research, and clinical care'},\n",
    "  {'value': 'SPECIALTY_HOSPITAL',\n",
    "   'description': 'A hospital that specializes in specific medical conditions or treatments'},\n",
    "     {'value': 'EMERGENCY',\n",
    "   'description': 'Emergency healthcare unit to attend to emergencies before transferring to other hospitals'},\n",
    "  {'value': 'FACILITY_TYPE_OTHER',\n",
    "   'description': 'Other types of healthcare facilities not covered by the defined categories'}\n",
    "   ]\n",
    ")\n",
    "\n",
    "# Let's add a new annotation for \"PatientAgeGroup\"\n",
    "add_type_annotation = create_annotation(\n",
    "    'add_type', name='PatientAgeGroup', \n",
    "    description='Age Groups of patients for risk classication', \n",
    "    values = [\n",
    "        {'value': 'NEONATES',\n",
    "         'description': 'New borns who are just born and a maximum of 28 days old'\n",
    "        },\n",
    "        {'value': 'INFANTS',\n",
    "         'description': 'Children who are 29 days to 1 year old'\n",
    "        },\n",
    "        {'value': 'TODDLER',\n",
    "         'description': 'Children who are 1-3 years old'\n",
    "        }\n",
    "    ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb5ba3",
   "metadata": {},
   "source": [
    "# Step 1.1: Scenario Generation\n",
    "\n",
    "Annotations can be created directly to add/ update or delete rules, variables and types. This is done by analysing the Automated Reasoning policy that has been created by formalizing the logic in the source document when you uploaded the document to Automated Reasoning in Lab 1. \n",
    "\n",
    "Scenario generation is a capability that helps you discover which annotations are required through automated testing. In principle, scenario generation replicates the testing process, where you would add a test sample to check if your rules are correct and use that to identify what modifications are required to the Automated Reasoning policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the build workflow ID of the ingest content workflow or any successful workflow which contains the updated policy\n",
    "ar_workflows = bedrock_client.list_automated_reasoning_policy_build_workflows(policyArn=policy_arn)\n",
    "\n",
    "ar_workflow_ingest_content = [a for a in ar_workflows['automatedReasoningPolicyBuildWorkflowSummaries'] if a['buildWorkflowType'] == 'INGEST_CONTENT']\n",
    "build_workflow_id = ar_workflow_ingest_content[0]['buildWorkflowId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a scenario until the test sample is meaningful. \n",
    "# Run this cell as many times as required! Each time you call 'get_automated_reasoning_policy_next_scenario', it generates a new scenario\n",
    "\n",
    "scenario = bedrock_client.get_automated_reasoning_policy_next_scenario(policyArn=policy_arn, buildWorkflowId=build_workflow_id)\n",
    "pprint.pprint(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this scenario to test bench\n",
    "guard_content = scenario['scenario']['alternateExpression']\n",
    "expected_result = 'SATISFIABLE' # Set this value to VALID, INVALID, SATISFIABLE, IMPOSSIBLE based on the plausibility of the scenario\n",
    "\n",
    "created_test_case = bedrock_client.create_automated_reasoning_policy_test_case(\n",
    "    policyArn=policy_arn,\n",
    "    guardContent=guard_content,\n",
    "    expectedAggregatedFindingsResult=expected_result,\n",
    "    clientRequestToken=str(uuid.uuid4()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test case\n",
    "response = bedrock_client.start_automated_reasoning_policy_test_workflow(\n",
    "            policyArn=policy_arn,\n",
    "            buildWorkflowId=build_workflow_id,\n",
    "            testCaseIds=[created_test_case['testCaseId']],\n",
    "            clientRequestToken=str(uuid.uuid4()),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be23f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status to see if the test sample has completed\n",
    "test_response = bedrock_client.get_automated_reasoning_policy_test_result(\n",
    "    policyArn=policy_arn,\n",
    "    buildWorkflowId=build_workflow_id,\n",
    "    testCaseId=created_test_case['testCaseId'],\n",
    "    )\n",
    "\n",
    "if test_response['testResult']['testRunStatus'] == 'COMPLETED':\n",
    "    if test_response['testResult']['testRunResult'] == 'PASSED':\n",
    "        print('Test was successful')\n",
    "    else:\n",
    "        print(f'Test result is {test_response['testResult']['testRunResult']}')\n",
    "else:\n",
    "    print(f'Status of the test is {test_response['testResult']['testRunStatus']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b121e",
   "metadata": {},
   "source": [
    "If the test was not successful, then we can analyse which rules were involved in testing the scenario and understand what annotations need to be created to update the Automated Reasoning Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be793364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ar_policy_rule(policy_arn, rule_id):\n",
    "    policy_response = policy = bedrock_client.get_automated_reasoning_policy(\n",
    "            policyArn=policy_arn\n",
    "        )\n",
    "    policy_status = policy_response.get('status', 'UNKNOWN')\n",
    "\n",
    "    # create AR Policy version if not done so already - It is not possible to retrieve policy if its still in DRAFT mode\n",
    "\n",
    "    # Analyse which rule corresponds to this generated scenario\n",
    "    ar_policy = bedrock_client.export_automated_reasoning_policy_version(\n",
    "                policyArn=policy_arn\n",
    "        )\n",
    "    \n",
    "    rule_expression = [r['alternateExpression'] for r in ar_policy['policyDefinition']['rules'] if r['id']==rule_id]\n",
    "    return rule_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_expressions = []\n",
    "for rule_id in scenario['scenario']['ruleIds']:\n",
    "    rule_expression = get_ar_policy_rule(policy_arn, rule_id)\n",
    "    print(f'RuleID: {rule_id}, Expression: {rule_expression}')\n",
    "    rule_expressions.append(rule_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbb830",
   "metadata": {},
   "source": [
    "#### Exercise: if required, create an annotation based on the rules identified from the scenario generation step (Similar to Step 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ae2ad",
   "metadata": {},
   "source": [
    "# Step 2: Start the workflow build to refine policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ddcf4",
   "metadata": {},
   "source": [
    "There is a current service limit of 2 workflows that can be created per Automated Reasoning policy. TO refine the Automated Reasoning policy through annotations, we will need to create a workflow without exceeding this Service Limit. We also need to ensure we retain one successful workflow which can be used to generate scenarios for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_and_delete_AR_build_workflows():\n",
    "    \"\"\"\n",
    "    List and delete workflows based on specific conditions:\n",
    "    1. Maximum 2 workflows allowed\n",
    "    2. Delete failed workflows\n",
    "    3. For completed workflows, check build log of latest and delete if failed\n",
    "    4. If both completed and build log ok, delete the latest one\n",
    "    \"\"\"\n",
    "    ar_workflows = bedrock_client.list_automated_reasoning_policy_build_workflows(policyArn=policy_arn)\n",
    "    workflows = ar_workflows['automatedReasoningPolicyBuildWorkflowSummaries']\n",
    "    \n",
    "    # Sort workflows by updatedAt timestamp in descending order (latest first)\n",
    "    sorted_workflows = sorted(workflows, key=lambda x: x['updatedAt'], reverse=True)\n",
    "    \n",
    "    for workflow in sorted_workflows:\n",
    "        # Delete any failed workflows\n",
    "        if workflow['status'] == 'FAILED':\n",
    "            bedrock_client.delete_automated_reasoning_policy_build_workflow(\n",
    "                policyArn=workflow['policyArn'],\n",
    "                buildWorkflowId=workflow['buildWorkflowId'],\n",
    "                lastUpdatedAt=workflow['updatedAt']\n",
    "            )\n",
    "            continue\n",
    "            \n",
    "    # After removing failed workflows, get the list again\n",
    "    ar_workflows = bedrock_client.list_automated_reasoning_policy_build_workflows(policyArn=policy_arn)\n",
    "    completed_workflows = [w for w in ar_workflows['automatedReasoningPolicyBuildWorkflowSummaries'] \n",
    "                         if w['status'] == 'COMPLETED']\n",
    "    \n",
    "    # Sort completed workflows by updatedAt timestamp\n",
    "    completed_workflows.sort(key=lambda x: x['updatedAt'], reverse=True)\n",
    "    \n",
    "    if len(completed_workflows) >= 2:\n",
    "        # Get latest workflow's build log\n",
    "        latest_workflow = completed_workflows[0]\n",
    "        build_log = bedrock_client.get_automated_reasoning_policy_build_workflow_result_assets(\n",
    "            policyArn=policy_arn,\n",
    "            buildWorkflowId=latest_workflow['buildWorkflowId'],\n",
    "            assetType='BUILD_LOG'\n",
    "        )\n",
    "        \n",
    "        # Check if build log indicates failure\n",
    "        if 'buildLog' in build_log.get('buildWorkflowAssets', {}) and \\\n",
    "           any(entry.get('status') == 'FAILED' \n",
    "               for entry in build_log['buildWorkflowAssets']['buildLog'].get('entries', [])):\n",
    "            # Delete the failed workflow\n",
    "            bedrock_client.delete_automated_reasoning_policy_build_workflow(\n",
    "                policyArn=latest_workflow['policyArn'],\n",
    "                buildWorkflowId=latest_workflow['buildWorkflowId'],\n",
    "                lastUpdatedAt=latest_workflow['updatedAt']\n",
    "            )\n",
    "        else:\n",
    "            # If build log is ok but we still have 2 completed workflows,\n",
    "            # delete the latest one to maintain the limit\n",
    "            bedrock_client.delete_automated_reasoning_policy_build_workflow(\n",
    "                policyArn=latest_workflow['policyArn'],\n",
    "                buildWorkflowId=latest_workflow['buildWorkflowId'],\n",
    "                lastUpdatedAt=latest_workflow['updatedAt']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_workflow_type = \"REFINE_POLICY\"\n",
    "\n",
    "current_policy = bedrock_client.export_automated_reasoning_policy_version(\n",
    "    policyArn=policy_arn\n",
    ")\n",
    "\n",
    "# We pass in the current policy definition, and the annotation as part of the PolicyRepairAsset\n",
    "# The annotation is applied to the current policy, to build a new policy\n",
    "def apply_annotation_through_policy_repair_asset():\n",
    "    return bedrock_client.start_automated_reasoning_policy_build_workflow(\n",
    "                policyArn=policy_arn,\n",
    "                buildWorkflowType=build_workflow_type,\n",
    "                clientRequestToken=str(uuid.uuid4()),\n",
    "                sourceContent={\n",
    "                    'policyDefinition': {\n",
    "                        'rules': current_policy['policyDefinition']['rules'],\n",
    "                        'variables': current_policy['policyDefinition']['variables'],\n",
    "                        'types': current_policy['policyDefinition']['types'],\n",
    "                        'version': current_policy['policyDefinition']['version']\n",
    "                    },\n",
    "                    'workflowContent': {\n",
    "                        'policyRepairAssets': {\n",
    "                            'annotations': [add_type_annotation]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "    \n",
    "\n",
    "try:\n",
    "    reponse = apply_annotation_through_policy_repair_asset()\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    print('Clearing failed workflows from queue')\n",
    "\n",
    "    list_and_delete_AR_build_workflows()\n",
    "\n",
    "    print('Trying to create annotation through a new workflow')\n",
    "    reponse = apply_annotation_through_policy_repair_asset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the queue of workflows to see if the REFINE_POLICY workflow has been created and is being built\n",
    "ar_workflows = bedrock_client.list_automated_reasoning_policy_build_workflows(policyArn=policy_arn)\n",
    "ar_workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9df9d",
   "metadata": {},
   "source": [
    "### Let's retrieve the Policy Definition from the latest workflow and check if the annotation of PatientAgeGroup exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6172e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_build_workflow(policy_arn):\n",
    "    \"\"\"\n",
    "    Retrieve the latest build workflow for a given policy\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the Automated Reasoning Policy\n",
    "    \n",
    "    Returns:\n",
    "        dict: Latest build workflow details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        workflows_response = bedrock_client.list_automated_reasoning_policy_build_workflows(\n",
    "            policyArn=policy_arn\n",
    "        )\n",
    "        \n",
    "        # Sort workflows by updatedAt timestamp in descending order\n",
    "        sorted_workflows = sorted(\n",
    "            workflows_response['automatedReasoningPolicyBuildWorkflowSummaries'], \n",
    "            key=lambda x: x.get('updatedAt', ''), \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Return the most recent workflow\n",
    "        return sorted_workflows[0] if sorted_workflows else None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving build workflows: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_workflow_result_assets(policy_arn, build_workflow_id, asset_type='POLICY_DEFINITION'):\n",
    "    \"\"\"\n",
    "    Retrieve result assets for a specific build workflow\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the Automated Reasoning Policy\n",
    "        build_workflow_id (str): ID of the build workflow\n",
    "        asset_type (str, optional): Type of asset to retrieve. \n",
    "            Defaults to 'POLICY_DEFINITION'\n",
    "            Options: 'POLICY_DEFINITION', 'QUALITY_REPORT', 'BUILD_LOG'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Workflow result assets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result_assets_response = bedrock_client.get_automated_reasoning_policy_build_workflow_result_assets(\n",
    "            policyArn=policy_arn,\n",
    "            buildWorkflowId=build_workflow_id,\n",
    "            assetType=asset_type\n",
    "        )\n",
    "        \n",
    "        return result_assets_response.get('buildWorkflowAssets', {})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving workflow result assets: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_latest_workflow_results(policy_arn, build_workflow_id=None,asset_type='POLICY_DEFINITION'):\n",
    "    \"\"\"\n",
    "    Comprehensive method to get the latest workflow results\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the Automated Reasoning Policy\n",
    "        build_workflow_id (str, optional): None if not passed in. It gets the latest workflow ID if not passed\n",
    "        asset_type (str, optional): Type of asset to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dict: Latest workflow result assets\n",
    "    \"\"\"\n",
    "    # Get the latest build workflow\n",
    "    if build_workflow_id is None:\n",
    "        latest_workflow = get_latest_build_workflow(policy_arn)\n",
    "    \n",
    "        if not latest_workflow:\n",
    "            print(\"No build workflows found.\")\n",
    "            return None\n",
    "        build_workflow_id = latest_workflow['buildWorkflowId']\n",
    "    # Get workflow result assets\n",
    "    workflow_results = get_workflow_result_assets(\n",
    "        policy_arn, \n",
    "        build_workflow_id, \n",
    "        asset_type\n",
    "    )\n",
    "    \n",
    "    return workflow_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ccf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_definitions = get_latest_workflow_results(\n",
    "    policy_arn=policy_arn, \n",
    "    # build_workflow_id='90aaf8fb-015a-42fb-9720-e87f36854989', \n",
    "    asset_type='POLICY_DEFINITION'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91773cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in policy_definitions['policyDefinition']['types']:\n",
    "    print(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95e36f",
   "metadata": {},
   "source": [
    "### We can see that the type (PatientAgeGroup) we included has been added to the newly built Automated Reasoning Policy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc4cb7",
   "metadata": {},
   "source": [
    "## Quality Report\n",
    "\n",
    "This is a useful report which helps you identify which types, values and variables are unused in the Automated Reasoning Policy. It also shows any conflicting rules or disjoint rule setsto help you identify which rules, variables and types need SME attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3eec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_result = get_latest_workflow_results(\n",
    "    policy_arn=policy_arn, \n",
    "    # build_workflow_id='90aaf8fb-015a-42fb-9720-e87f36854989', \n",
    "    asset_type='QUALITY_REPORT'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c893a8",
   "metadata": {},
   "source": [
    "### Build Log\n",
    "This provides helpful details to identify the status of the annotation that was applied through the REFINE_POLICY workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c144c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_log = get_latest_workflow_results(\n",
    "    policy_arn=policy_arn, \n",
    "    # build_workflow_id='90aaf8fb-015a-42fb-9720-e87f36854989', \n",
    "    asset_type='BUILD_LOG'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92608d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The build log shows the status of the individual annotations and the steps taken to build that annotation\n",
    "build_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784515d",
   "metadata": {},
   "source": [
    "# Step 3: Update the Automated Reasoning Policy with the New Policy generated in Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690810c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now update the policy with the workflow assets that have been created\n",
    "# Update the policy definition with the new annotation\n",
    "response = bedrock_client.update_automated_reasoning_policy(\n",
    "    policyArn=policy_arn,\n",
    "    policyDefinition={\n",
    "        'rules': policy_definitions['policyDefinition']['rules'],\n",
    "        'variables': policy_definitions['policyDefinition']['variables'],\n",
    "        'types': policy_definitions['policyDefinition']['types'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907297b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latest policy and check that the update exists\n",
    "current_policy = bedrock_client.export_automated_reasoning_policy_version(\n",
    "    policyArn=policy_arn\n",
    ")\n",
    "\n",
    "for type in current_policy['policyDefinition']['types']:\n",
    "    print(type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01529018",
   "metadata": {},
   "source": [
    "# Success!\n",
    "We can see that the latest Automated Reasoning Policy contains the type we added - PatientAgeGroup!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
