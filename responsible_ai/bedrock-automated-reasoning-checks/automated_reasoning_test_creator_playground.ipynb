{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Reasoning Test Case Playground\n",
    "\n",
    "This notebook demonstrates how to create and run a test for automated reasoning policies using the AWS Bedrock API. It includes the following functionality:\n",
    "\n",
    "1. Setting up the Bedrock client with custom API models\n",
    "2. Creating an automated reasoning policy test case\n",
    "3. Listing all existing test cases\n",
    "4. Running a test case\n",
    "5. Getting a test case result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the Bedrock client with custom API models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bedrock client\n",
    "my_session = boto3.session.Session()\n",
    "REGION_NAME = my_session.region_name\n",
    "print(f'The region is {REGION_NAME}')\n",
    "runtime_client = my_session.client('bedrock-runtime', region_name=REGION_NAME)\n",
    "bedrock_client = my_session.client('bedrock', region_name=REGION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Case\n",
    "\n",
    "Now, let's create a test case for an automated reasoning policy using the Bedrock API.\n",
    "\n",
    "A test case is a simulation designed to mimic a user interaction, with the main goal of confirming that an automated reasoning policy is functioning as intended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the policy arn for which you will create a test case without the version ID or number\n",
    "policy_arn=\"<POLICY_ARN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_case(policy_arn, guard_content, expected_aggregated_findings_result, query_content=None, confidence_threshold=None):\n",
    "    \"\"\"\n",
    "    Creates a test case for an automated reasoning policy\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the automated reasoning policy to test against\n",
    "        guard_content (str): The LLM answer to be evaluated by the policy\n",
    "        expected_aggregated_findings_result (str): Expected result of the automated reasoning check\n",
    "            Valid values: \"VALID\", \"INVALID\", \"SATISFIABLE\", \"IMPOSSIBLE\", \n",
    "            \"TRANSLATION_AMBIGUOUS\", \"TOO_COMPLEX\", \"NO_TRANSLATION\"\n",
    "        query_content (str, optional): User query to test against the policy\n",
    "        confidence_threshold (float, optional): Confidence threshold for the test (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        dict: Response from the API call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        kwargs = {}\n",
    "        if query_content is not None:\n",
    "            kwargs['queryContent'] = query_content\n",
    "        if confidence_threshold is not None:\n",
    "            kwargs['confidenceThreshold'] = confidence_threshold\n",
    "\n",
    "        return bedrock_client.create_automated_reasoning_policy_test_case(\n",
    "            policyArn=policy_arn,\n",
    "            guardContent=guard_content,\n",
    "            expectedAggregatedFindingsResult=expected_aggregated_findings_result,\n",
    "            clientRequestToken=str(uuid.uuid4()),\n",
    "            **kwargs\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating a test case: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test case for the automated reasoning policy\n",
    "# This example demonstrates creating a test case that expects a VALID result\n",
    "# with maximum confidence threshold (1.0)\n",
    "\n",
    "# Example to test with\n",
    "\n",
    "# query_content=\"\"\" Patient Profile:\n",
    "#     Age: 58 years\n",
    "#     Length of stay: 6 days\n",
    "#     Has chronic kidney disease stage 4\n",
    "#     Two ED visits in last 6 months\n",
    "#     Uninsured status\n",
    "#     New requirement for durable medical equipment\n",
    "#     \"\"\"\n",
    "# guard_content=\"Classification: High Risk\"\n",
    "\n",
    "guard_content=\"<GUARD_CONTENT>\"\n",
    "query_content=\"<QUERY_CONTENT>\"\n",
    "\n",
    "create_test_case_response = create_test_case(\n",
    "    policy_arn=policy_arn,                              # ARN of the policy to test\n",
    "    query_content=query_content,                        # Replace with the user query information that is the request to the LLM\n",
    "    guard_content=guard_content,                        # Replace with actual LLM response to validate\n",
    "    expected_aggregated_findings_result=\"VALID\",        # Expected validation outcome\n",
    "    confidence_threshold=1.0                            # Maximum confidence (100%)\n",
    ")\n",
    "\n",
    "# Extract the test case ID from the response for future operations\n",
    "test_case_id = create_test_case_response['testCaseId']\n",
    "\n",
    "# Display the full API response in JSON format\n",
    "JSON(create_test_case_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all existing test cases. We should be able to see the test case that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_existing_test_cases(policy_arn):\n",
    "    \"\"\"\n",
    "    Returns a list all test cases for an automated reasoning policy\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the policy\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of test cases\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_cases = []\n",
    "        is_first_run = True\n",
    "        pagination_token = None\n",
    "\n",
    "        while is_first_run or pagination_token:\n",
    "            if pagination_token:\n",
    "                response = bedrock_client.list_automated_reasoning_policy_test_cases(\n",
    "                    policyArn=policy_arn,\n",
    "                    nextToken=pagination_token\n",
    "                )\n",
    "            else:\n",
    "                response = bedrock_client.list_automated_reasoning_policy_test_cases(\n",
    "                    policyArn=policy_arn,\n",
    "                )\n",
    "\n",
    "            test_cases.extend(response['testCases'])\n",
    "\n",
    "            is_first_run = False\n",
    "            pagination_token = response.get('nextToken', None)\n",
    "\n",
    "        return test_cases\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing all test cases: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all existing test cases for the specified policy\n",
    "test_cases = list_existing_test_cases(policy_arn=policy_arn)\n",
    "\n",
    "# Convert the test cases list to a pandas DataFrame for better visualization\n",
    "# This creates a tabular format with columns for each test case attribute\n",
    "test_cases_df = pd.DataFrame(test_cases)\n",
    "\n",
    "# Display the DataFrame in a formatted table within the Jupyter notebook\n",
    "# Shows test case IDs, content, expected results, and other metadata\n",
    "display(test_cases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Test Case\n",
    "\n",
    "Prior to running a test case, we need to retrieve a build workflow id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_build_workflows(policy_arn):\n",
    "    \"\"\"\n",
    "    Lists all build workflows\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the policy\n",
    "\n",
    "    Returns:\n",
    "        dict: Response from the API call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bedrock_client.list_automated_reasoning_policy_build_workflows(\n",
    "            policyArn=policy_arn\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing build workflow: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all build workflows associated with the specified policy\n",
    "list_build_workflows_response = list_build_workflows(policy_arn=policy_arn)\n",
    "\n",
    "# Extract the build workflow ID from the first workflow in the response\n",
    "# This ID is required to run test cases against the policy\n",
    "build_workflow_id = list_build_workflows_response['automatedReasoningPolicyBuildWorkflowSummaries'][0]['buildWorkflowId']\n",
    "\n",
    "# Display the complete API response in formatted JSON\n",
    "# Shows all workflow summaries with their status, creation time, and other metadata\n",
    "JSON(list_build_workflows_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the `build_workflow_id` let's run the test case that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_cases(policy_arn, build_workflow_id, test_case_ids):\n",
    "    \"\"\"\n",
    "    Runs a test case\n",
    "\n",
    "    Args:\n",
    "        policy_arn (str): ARN of the policy\n",
    "        build_workflow_id (str): Id of the build workflow\n",
    "        test_case_ids (List[str]): Ids of all test cases\n",
    "\n",
    "    Returns:\n",
    "        dict: Response from the API call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = bedrock_client.start_automated_reasoning_policy_test_workflow(\n",
    "            policyArn=policy_arn,\n",
    "            buildWorkflowId=build_workflow_id,\n",
    "            testCaseIds=test_case_ids,\n",
    "            clientRequestToken=str(uuid.uuid4()),\n",
    "        )\n",
    "\n",
    "        print(f\"Test workflow started successfully!\")\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting a test workflow: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executes test cases for a given policy and build_workflow_id\n",
    "# Note: This call only starts the test case execution asynchronously.\n",
    "# The return does not indicate test completion.\n",
    "# To check test run status and results, we will use get_test_case_result() below.\n",
    "run_test_cases(\n",
    "    policy_arn=policy_arn,               # ARN of the policy to test against\n",
    "    build_workflow_id=build_workflow_id, # ID of the build workflow\n",
    "    test_case_ids=[test_case_id],        # List of test case IDs to run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Test Case Result\n",
    "\n",
    "Now, let's retrieve the test case result that we just started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_case_result(policy_arn, build_workflow_id, test_case_id):\n",
    "    \"\"\"\n",
    "    Returns the test case result\n",
    "    \n",
    "    Args:\n",
    "        policy_arn (str): ARN of the policy\n",
    "        build_workflow_id (str): Id of the build workflow\n",
    "        test_case_id (str): Id of the test case\n",
    "\n",
    "    Returns:\n",
    "        dict: Response from the API call\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bedrock_client.get_automated_reasoning_policy_test_result(\n",
    "            policyArn=policy_arn,\n",
    "            buildWorkflowId=build_workflow_id,\n",
    "            testCaseId=test_case_id,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error returning test case result: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Policy ARN: {policy_arn}')\n",
    "\n",
    "# Create widgets for monitoring\n",
    "status_output = widgets.Output()\n",
    "progress_bar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    orientation='horizontal'\n",
    ")\n",
    "status_text = widgets.HTML(value=\"<b>Status:</b> Initializing...\")\n",
    "\n",
    "# Display the widgets\n",
    "display(status_text)\n",
    "display(progress_bar)\n",
    "display(status_output)\n",
    "\n",
    "# Monitor the test status\n",
    "max_attempts = 30\n",
    "poll_interval = 10  # seconds\n",
    "\n",
    "test_status = None\n",
    "test_run_result = None\n",
    "\n",
    "for attempt in range(max_attempts):\n",
    "    # Check test status\n",
    "    response = get_test_case_result(policy_arn, build_workflow_id, test_case_id)\n",
    "\n",
    "    # Check status\n",
    "    test_status = response.get('testResult', {}).get('testRunStatus', 'UNKNOWN')\n",
    "    test_run_result = response.get('testResult', {}).get('testRunResult', 'UNKNOWN')\n",
    "    \n",
    "    # Calculate progress\n",
    "    progress = 0\n",
    "    if test_status == 'NOT_STARTED':\n",
    "        progress = 0\n",
    "    if test_status == 'SCHEDULED':\n",
    "        progress = 10\n",
    "    elif test_status == 'IN_PROGRESS':\n",
    "        progress = 20\n",
    "    elif test_status == 'TESTING':\n",
    "        progress = 75\n",
    "    elif test_status == 'COMPLETED':\n",
    "        progress = 100\n",
    "    elif test_status == 'FAILED':\n",
    "        progress = 100\n",
    "    \n",
    "    # Update the widgets\n",
    "    progress_bar.value = progress\n",
    "    status_text.value = f\"<b>Status:</b> Test status: {test_status}, Test Case ID {test_case_id}\"\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"Check {attempt + 1}: Test status: {test_status}, Test Case ID: {test_case_id}\")\n",
    "    \n",
    "    # If the test is complete, then we are done\n",
    "    if progress >= 100:\n",
    "        if test_status == 'COMPLETED' and test_run_result == 'PASSED':\n",
    "            progress_bar.bar_style = 'success'\n",
    "        elif test_status == 'FAILED' or test_run_result == 'FAILED':\n",
    "            progress_bar.bar_style = 'danger'\n",
    "            with status_output:\n",
    "                print(f\"Test evaulation failed or was cancelled.\")\n",
    "        break\n",
    "    \n",
    "    # Wait before the next check\n",
    "    if attempt < max_attempts - 1:\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# Final status update\n",
    "if progress >= 100:\n",
    "    if test_status == 'COMPLETED' and test_run_result == 'PASSED':\n",
    "        status_text.value = f\"<b>Status:</b> Test passed!\"\n",
    "    elif test_status == 'FAILED' or test_run_result == 'FAILED':\n",
    "        status_text.value = f\"<b>Status:</b> Test failed!\"\n",
    "    else:\n",
    "        status_text.value = f\"<b>Status:</b> UNKNOWN!\"\n",
    "else:\n",
    "    status_text.value = f\"<b>Status:</b> Test is not yet complete, Test Status: {test_status}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test has not either failed or completed, re-run the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve test case result using policy ARN, build workflow ID and test case ID\n",
    "test_case_result_response = get_test_case_result(\n",
    "    policy_arn=policy_arn,\n",
    "    build_workflow_id=build_workflow_id,\n",
    "    test_case_id=test_case_id,\n",
    ")\n",
    "\n",
    "# Convert test case result response to JSON format for display/processing\n",
    "JSON(test_case_result_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
