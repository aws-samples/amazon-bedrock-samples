{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal RAG using Claude 3 Sonnet provided by Amazon Bedrock\n",
    "Most of documents especially in finance domain, have a blend of content formats, such as textual data and visual representations such as bar charts and line graphs (images).\n",
    "\n",
    "However, the information encapsulated within images is often disregarded in the majority of Retrieval-Augmented Generation (RAG) applications.\n",
    "\n",
    "With the advent of multimodal Large Language Models (LLMs), like `Anthropic Claude 3 Sonnet`, it becomes relevant to explore methodologies for leveraging visual data in RAG applications.\n",
    "\n",
    "## Steps followed in the notebook\n",
    "1. Use a multimodal large language model (LLM), like Claude 3 Sonnet, to generate concise text descriptions from visual inputs (images).\n",
    "\n",
    "2. Convert the textual summaries and raw image data into numerical representations (embeddings) and store them in a retrievable format, maintaining a linkage between the summaries and their corresponding images.\n",
    "\n",
    "3. Feed the raw image data and relevant text chunks into a multimodal LLM, which can jointly process and synthesize information from both modalities to produce an answer.\n",
    "\n",
    "We will use `pypdf` to parse images, and text from documents (PDFs).\n",
    "We will use the [`multi-vector retriever`](#https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector) with Chroma DB to store raw text, tables and images along with their summaries for retrieval.\n",
    "\n",
    "Reference:\n",
    "[Multi-modal RAG](#https://github.com/sudarshan-koirala/youtube-stuffs/blob/main/langchain/LangChain_Multi_modal_RAG.ipynb) authored by Sudarshan Koirala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required packages and import the libraries, which will be used throughout the notebook.\n",
    "Make sure to restart the kernel after installing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install ftfy regex tqdm\n",
    "%pip install chromadb\n",
    "%pip install sqlalchemy==2.0.0\n",
    "%pip install --upgrade langchain>=0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r images\n",
    "!mkdir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://sgp.fas.org/crs/misc/IF10244.pdf\"\n",
    "filename = \"wildfire_stats.pdf\"\n",
    "urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract images and text from the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(f\"{filename}\")\n",
    "print('# of pages: ', len(reader.pages))\n",
    "image_count = 0\n",
    "texts = []\n",
    "\n",
    "# extract and save images\n",
    "for page in reader.pages:\n",
    "    for image in page.images:\n",
    "        image_count+=1\n",
    "        with open(f\"{path}/{image.name}\", 'wb') as f:\n",
    "            f.write(image.data)\n",
    "\n",
    "loader = PyPDFLoader(filename)\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(\"# of text chunks: \", len(docs))\n",
    "print(\"# of images in document: \", image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the images in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "image_paths = []\n",
    "for img_path in os.listdir(\"./images\"):\n",
    "    image_paths.append(str(os.path.join(\"./images\", img_path)))\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if \".png\" in img_path:\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "print(\"Images in the document\")\n",
    "plot_images(image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image summaries using Claude 3 Sonnet\n",
    "We will use Claude 3 Sonnet model from Anthropic to create image summaries, which we will store in the `multi-vector retriever` from Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, lets first set up a connection to the Anthropic Claude multimodal model using the AWS Bedrock service. Imports necessary libraries, configure the AWS Boto3 client, and initialize the Bedrock chat model interface using the LangChain library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "import json\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name = region)\n",
    "print(region)\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define two functions: `encode_image` and `invoke_model_sonnet`. \n",
    "- The `encode_image` function takes an image file path as input and returns the base64 encoded string representation of the image file. \n",
    "- The `invoke_model_sonnet` function takes a base64 encoded image and a prompt as input, initializes a `BedrockChat` instance, creates a message list with the prompt and image, invokes the chat model, and returns the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from langchain.chat_models.bedrock import BedrockChat\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def encode_image(image_path):\n",
    "    ''' Getting the base64 string '''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def invoke_model_sonnet(img_base64,prompt):\n",
    "    ''' Image summary '''\n",
    "    chat = BedrockChat(model_id=modelId,\n",
    "                      client=bedrock_client,\n",
    "                      model_kwargs={\n",
    "                      'temperature': 0.1, \n",
    "                      'top_p': 0.1\n",
    "                  } \n",
    "                      )\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": img_base64}}\n",
    "        ]\n",
    "    message_list = [\n",
    "    {\n",
    "        \"role\": 'user',\n",
    "        \"content\": content\n",
    "    }\n",
    "]\n",
    "    msg = chat.invoke(message_list)\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt = \"\"\"\n",
    "Summarize all the details in the image, focus on the statistics provided in the image such as bar charts and graphs. \n",
    "Make sure to include the time period in the summary, if available. For example, the images shows line graph showing data from 1993 to 2022.\n",
    "\"\"\"\n",
    "\n",
    "# Read images, encode to base64 strings\n",
    "def create_image_summaries(input_files):\n",
    "    img_base64_list = []\n",
    "    image_summaries = []\n",
    "    for file in input_files:\n",
    "        print(file)\n",
    "        base64_image = encode_image(file)\n",
    "        image_summaries.append(invoke_model_sonnet(base64_image,prompt))\n",
    "        img_base64_list.append(base64_image)\n",
    "    return img_base64_list, image_summaries\n",
    "img_base64_list, image_summaries = create_image_summaries(image_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets view the summary of one of the images\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "plt_img_base64(img_base64_list[3])\n",
    "print(\"------------------ Image Summary ---------------\\n\", image_summaries[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data into the Vector Store\n",
    "\n",
    "Now we will set up the necessary components for a [multi-vector retriever](#https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector) system using the Langchain library. We will import required modules, create an instance of the BedrockEmbeddings for text embeddings, initialize a Chroma vector store for indexing document chunks, and set up an in-memory store for parent documents. Finally, we will create a `MultiVectorRetriever` object, which combines the vector store and document store for efficient document retrieval based on vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = f'cohere.embed-english-v3'\n",
    "# instantiate embeddings model.\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_client,\n",
    "    model_id=embedding_model\n",
    ")\n",
    "# define the vector store\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\",\n",
    "                     embedding_function=embedding_model)\n",
    "\n",
    "# define the storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# the retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "texts = []\n",
    "for i, doc in enumerate(docs):\n",
    "    doc.metadata.update({id_key: doc_ids[i]})\n",
    "    texts.append(doc.page_content)\n",
    "retriever.vectorstore.add_documents(docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the retrieved data\n",
    "Now that we have added our text, image summaries and base64 images to the vector store, let's check that the retrieval is giving the relevant documents related to the query by following below steps: \n",
    "- We will first retrieve relevant documents using a retriever object and  write the helper function to split the retrieved documents into base64-encoded images and text chunks. \n",
    "- Finally print the number of retrieved images and text chunks. We will verify the result by displaying the first retrieved image, and printing the dictionary containing the separated images and text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\n",
    "    \"What is the change in wild fires from 1993 to 2022?\"\n",
    ")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64decode\n",
    "def split_image_text_types(docs):\n",
    "    ''' Split base64-encoded images and texts '''\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\n",
    "        \"images\": b64,\n",
    "        \"texts\": text\n",
    "    }\n",
    "docs_by_type = split_image_text_types(docs)\n",
    "print(\"# images retrieved: \", len(docs_by_type[\"images\"]))\n",
    "print(\"# of text chunk retrieved: \", len(docs_by_type[\"texts\"]))\n",
    "print('---------------- Retrieved Image -------------')\n",
    "if docs_by_type[\"images\"]:\n",
    "    plt_img_base64(docs_by_type[\"images\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint.pp(docs_by_type[\"texts\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation workflow\n",
    "Now that we have verified our retrievals, lets test our solution end to end.\n",
    "- We will first define a function `prompt_func` that takes a dictionary as input and returns a list of messages formatted for a `Anthropic Claude 3 Sonnet` model. \n",
    "    - The function concatenates the text content from the input dictionary and formats it along with the `question` as a prompt for the model. It also includes any images from the input dictionary as base64-encoded data in the prompt. \n",
    "- Next, we will set up a pipeline using the LangChain library, to perform retrieval, followed by passing the retrieved data to the `prompt_func`, which sends the formatted prompt to `Claude 3 Sonnet model`, finally parsing the output using a string output parser (`StrOutputParser`). \n",
    "- We will then invoke the pipeline with the question to get the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def prompt_func(dict):\n",
    "    format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
    "    prompt = f'''Answer the question based only on the following context, which can include text and images:\n",
    "    Question: {dict[\"question\"]}\n",
    "    Text:\n",
    "    {format_texts}\n",
    "    '''\n",
    "    content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "    \n",
    "    for image in dict[\"context\"][\"images\"]: \n",
    "        content.append(\n",
    "            {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": image}},\n",
    "        )\n",
    "    # pprint.pp(content)\n",
    "    message_list = [\n",
    "    {\n",
    "        \"role\": 'user',\n",
    "        \"content\": content\n",
    "    }\n",
    "]\n",
    "    \n",
    "    return message_list\n",
    "\n",
    "chat_model = BedrockChat(model_id=modelId,\n",
    "                      client=bedrock_client,\n",
    "                       model_kwargs={\n",
    "                      'temperature': 0.1, \n",
    "                      'top_p': 0.1\n",
    "                  } )\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(split_image_text_types), \"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer = chain.invoke(\n",
    "    \"What is the change in wild fires from 1993 to 2022?\"\n",
    ")\n",
    "pprint.pp(generated_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the above response uses both the retrieved text and images, to generate the final answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we provided the technique to build RAG applications which understand both text and visual data encapsulated in images to provide more relevant and accurate responses to the user. In this approach, we specifically leveraged multimodal Large Language Model (LLM), `Claude 3 Sonnet`, for leveraging both text and visual data for building our RAG application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
