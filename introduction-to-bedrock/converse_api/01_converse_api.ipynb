{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> How to work with Converse API in Amazon Bedrock - Getting Started. </h2>\n",
    "\n",
    "*Note: This notebook has been adapted from the [Getting started with the Converse API in Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/Getting_started_with_Converse_API.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Overview </h2>\n",
    "\n",
    "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
    "\n",
    "To use the Converse API, you call the `Converse` or `ConverseStream` operations to send messages to a model. To call Converse, you require permission for the `bedrock:InvokeModel` operation. To call ConverseStream, you require permission for the `bedrock:InvokeModelWithResponseStream` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites </h2>\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access). \n",
    "    \n",
    "    We have used below Foundation Models in our examples in this Notebook in `us-east-1` (North Virginia) region.\n",
    "      \n",
    "| Provider Name | Foundation Model Name | Model Id |\n",
    "| ------- | ------------- | ------------- |\n",
    "| AI21 Labs | Jamba 1.5 Large | ai21.jamba-1-5-large-v1:0 |\n",
    "| AI21 Labs | Jamba-Instruct  | ai21.jamba-instruct-v1:0 |\n",
    "| Amazon | Nova Pro  | amazon.nova-pro-v1:0 |\n",
    "| Amazon | Nova Lite  | amazon.nova-lite-v1:0 |\n",
    "| Amazon | Nova Micro  | amazon.nova-micro-v1:0 |\n",
    "| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n",
    "| Anthropic | Claude 3 Haiku  | anthropic.claude-3-haiku-20240307-v1:0 |\n",
    "| Cohere | Command R+ | cohere.command-r-plus-v1:0 |\n",
    "| Cohere | Command R | cohere.command-r-v1:0 |\n",
    "| Meta | Llama 3.1 70B Instruct | meta.llama3-1-70b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 8B Instruct | meta.llama3-1-8b-instruct-v1:0 |\n",
    "| Mistral AI | Mistral Large 2 (24.07) | mistral.mistral-large-2407-v1:0 |\n",
    "| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Setup </h2>\n",
    "\n",
    "⚠️ This notebook should work well with the Data Science 3.0 kernel (Python 3.10 runtime) in SageMaker Studio ⚠️\n",
    "\n",
    "Run the cells in this section to install the packages needed by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall boto3\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "from botocore.exceptions import ClientError\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the region and models to use. We can also setup our boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )\n",
    "\n",
    "MODEL_IDS = [\n",
    "    \"ai21.jamba-1-5-large-v1:0\",\n",
    "    \"ai21.jamba-instruct-v1:0\",\n",
    "    \"amazon.nova-pro-v1:0\",\n",
    "    \"amazon.nova-lite-v1:0\",\n",
    "    \"amazon.nova-micro-v1:0\",\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"cohere.command-r-plus-v1:0\",\n",
    "    \"cohere.command-r-v1:0\",\n",
    "    \"meta.llama3-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-8b-instruct-v1:0\",\n",
    "    \"mistral.mistral-large-2402-v1:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Notebook/Code with comments</h2>\n",
    "\n",
    "We're now ready to setup our Converse API action in Bedrock. Note that we use the same syntax for any model, including the messages-formatted prompts, and the inference parameters. Also note that we read the output in the same way independently of the model used.\n",
    "\n",
    "Optionally, we could define additional model specific request fields that are not common across all providers. For more information on this check the [Bedrock Converse API documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n",
    "\n",
    "\n",
    "<h3> Converse for one-shot invocations </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "            #additionalModelRequestFields={\n",
    "            #}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Model invocation error\"\n",
    "    try:\n",
    "        result = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Output parsing error\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our invocation.\n",
    "\n",
    "In this example, we run the same prompt across all the text models supported in Bedrock by the time of writing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
    "    print(f'Model: {i}\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> ConverseStream for streaming invocations </h3>\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    print(f'\\n\\nModel: {i}')\n",
    "    invoke_bedrock_model_stream(bedrock, i, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conversation with Text using Converse API and model specific parameters </h3>\n",
    "\n",
    "In this example we will call the Converse operation with the Anthropic Claude 3 Haiku model. We will send the input text, inference parameters, and additional parameters that are unique to the model. We will start a conversation by asking the model to create a list of songs, then continues the conversation by asking that the songs are by artists from the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Generating message with model {model_id}')\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "\n",
    "    # Base inference parameters to use which are common across all FMs.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "\n",
    "    # Additional inference parameters to use for Anthropic Claude Models.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
    "                    \"Only return song names and the artist.\"}]\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "}\n",
    "message_2 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "}\n",
    "messages = []\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    # Start the conversation with the 1st message.\n",
    "    messages.append(message_1)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    # Add the response message to the conversation.\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Continue the conversation with the 2nd message.\n",
    "    messages.append(message_2)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Show the complete conversation.\n",
    "    for message in messages:\n",
    "        print(f\"Role: {message['role']}\")\n",
    "        for content in message['content']:\n",
    "            print(f\"Text: {content['text']}\")\n",
    "        print()\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Conversation with Image using Converse API </h3>\n",
    "\n",
    "In this example we will send an image as part of a message and requests that the model describe the image. The example uses Converse operation and the Amazon Nova Lite model.\n",
    "\n",
    "Sample image used in this example.\n",
    "\n",
    "![Sample Image](assets/sample_image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          input_text,\n",
    "                          input_image):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_image : The input image.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'jpeg',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": image\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"amazon.nova-lite-v1:0\"\n",
    "input_text = \"What's in this image?\"\n",
    "input_image = \"assets/sample_image.jpg\"\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = image_conversation(\n",
    "        bedrock_client, model_id, input_text, input_image)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conversation with Document using Converse API</h3>\n",
    "\n",
    "\n",
    "In this example, we will send a document as part of a message and requests that the model describe the contents of the document. The example uses Converse operation and the Mistral Large 2 (24.07) Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def document_conversation(bedrock_client,\n",
    "                     model_id,\n",
    "                     input_text,\n",
    "                     input_document):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_document : The input document.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "    \n",
    "    with open(input_document, \"rb\") as f:\n",
    "        doc_bytes = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                \"document\": {\n",
    "                    \"name\": \"MyDocument\",\n",
    "                    \"format\": \"pdf\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": doc_bytes\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "input_text = \"What's in this document?\"\n",
    "input_document = 'assets/2022-Shareholder-Letter.pdf'\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = document_conversation(\n",
    "        bedrock_client, model_id, input_text, input_document)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Next steps</h2>\n",
    "\n",
    "Now that we have seen the Converse API allow us to easily run the invocations with the same syntax across all the models, you can learn\n",
    "\n",
    "\n",
    "- How to do [function calling with the Converse API](../../agents-and-function-calling/function-calling/function_calling_with_converse/function_calling_with_converse.ipynb)\n",
    "- How to work with [Converse API and Amazon Bedrock Guardrails](../../responsible_ai/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Clean up</h2>\n",
    "\n",
    "This notebook does not require any cleanup or additional deletion of resources."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
