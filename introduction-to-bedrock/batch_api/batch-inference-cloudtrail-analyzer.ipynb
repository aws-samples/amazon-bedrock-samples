{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference to analyze CloudTrail logs\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to analyze CloudTrail logs using <b>Amazon Bedrock for batch inference</b> to identify potential security anomalies. \n",
    "\n",
    "## Context\n",
    "\n",
    "The key steps in this process are:\n",
    "\n",
    "1. Data Collection: Retrieve the latest CloudTrail events (default: 20k events)\n",
    "2. Batch Inference: Use Amazon Bedrock batch inference to analyze user activities in batches.\n",
    "3. Summarization: Summarize the results to provide a concise overview of potential security concerns in your AWS environment.\n",
    "\n",
    "The output can be sent to an SNS topic to receive the summary in email for eg.\n",
    "\n",
    "- Amazon Bedrock batch inference works with jsonl files. Each completion to process is a json object with a modelInput and modelOutput.\n",
    "- The minimum number of items in the jsonl file for the batch inference job is 1000.\n",
    "- Observed time to summarize 2000 batch items of 10 cloudtrail events with given prompt is ~15 minutes.\n",
    "- You can check the job status with get_model_invocation_job passing jobArn as parameter.\n",
    "- Final summarisation is performed with Amazon Bedrock invoke_model API.\n",
    "\n",
    "Model chosen for this example is <b>Claude 3 Haiku</b>. This provides a good balance between cost, quality and context size.<br>\n",
    "Other models (Mistal for eg.) can be used as well to lower the cost but would require more requests to be processed due to the smaller context window.\n",
    "\n",
    "Pricing:\n",
    "- Est. Pricing - Input: 20K events est. <b>$3.3</b> with Claude 3 Haiku in us-east-1 as of september 2024\n",
    "- Est. Pricing - Output: 2000 summarizations est. <b>$1.25</b> with Claude 3 Haiku in us-east-1 as of september 2024\n",
    "\n",
    "Assuming:\n",
    "- 20K events ~600 tokens per event\n",
    "- 10 cloudtrail event per batch item\n",
    "- 500 tokens for prompt size\n",
    "- 15-20 final summarizations of 10k tokens\n",
    "- Input tokens ~13M tokens / est. \n",
    "- 500 tokens per summarization\n",
    "- 2000 summaries to generate in batch inference\n",
    "- 15-20 final summarizations to generate\n",
    "- Output ~1M tokens \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Make sure boto3 can access your AWS account\n",
    "- Make sure you have acces to Claude 3 Haiku model in us-east-1\n",
    "- Make sure your credentials allow creation of resources (S3 bucket, SNS topic, IAM role) and access to Bedrock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "import utils\n",
    "import bedrock\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "cloudtrail = boto3.client('cloudtrail')\n",
    "sns = boto3.client('sns')\n",
    "iam = boto3.client('iam')\n",
    "aws_account_number = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bedrock = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "\n",
    "batch_inference_input_file = \"input.jsonl\"\n",
    "s3_bucket_name = f\"cloudtrail-analysis-with-bedrock-{aws_account_number}\"\n",
    "bedrock_role_name = \"CloudTrailAnalyser_BedrockS3AccessRole\"\n",
    "sns_topic_name = \"cloudtrail-summary\"\n",
    "\n",
    "s3_input_uri = f\"s3://{s3_bucket_name}/{batch_inference_input_file}\"\n",
    "s3_output_uri = f\"s3://{s3_bucket_name}/batch_inference_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3 Haiku for a balance between cost and quality\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# chars, not tokens. Adjust this to match your model's context length / performance requirements\n",
    "max_context_length = 50000 \n",
    "# chars, not tokens. Size of prompt to summarize the events\n",
    "prompt_length = 500 \n",
    "# minimum batch entries in jsonl file required for Amazon Bedrockbatch inference\n",
    "min_batch_items_for_bedrock_batch_inference = 1000 \n",
    "# mini batches to keep a balance between summarizing and not losing too much signal\n",
    "events_per_batch_item = 10 \n",
    "# max number of batch item entries in the jsonl file, this is a cost control safety measure\n",
    "max_batch_items = 2000\n",
    "# max tokens in summary to keep a balance between summarizing and not losing too much signal\n",
    "max_tokens_in_summary = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Create S3 bucket, Role, SNS topic\n",
    "\n",
    "- S3 bucket is needed to store intermediate data for the batch inference job.\n",
    "- Role is needed to allow bedrock to access the S3 bucket.\n",
    "- SNS topic is needed to receive the final summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_get_bedrock_s3_access_role(bedrock_role_name: str, s3_bucket_name: str):\n",
    "\n",
    "    try:\n",
    "        response = iam.get_role(RoleName=bedrock_role_name)\n",
    "        print(f\"Role {bedrock_role_name} already exists.\")\n",
    "        return response['Role']['Arn']\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        # Role doesn't exist, create it\n",
    "        trust_policy = {\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": \"bedrock.amazonaws.com\"\n",
    "                    },\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = iam.create_role(\n",
    "                RoleName=bedrock_role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "            )\n",
    "            \n",
    "            # Attach S3 access policy\n",
    "            s3_policy = {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [\n",
    "                    {\n",
    "                        \"Effect\": \"Allow\",\n",
    "                        \"Action\": [\n",
    "                            \"s3:GetObject\",\n",
    "                            \"s3:PutObject\",\n",
    "                            \"s3:ListBucket\"\n",
    "                        ],\n",
    "                        \"Resource\": [\n",
    "                            f\"arn:aws:s3:::{s3_bucket_name}\",\n",
    "                            f\"arn:aws:s3:::{s3_bucket_name}/*\"\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            iam.put_role_policy(\n",
    "                RoleName=bedrock_role_name,\n",
    "                PolicyName=\"S3AccessPolicy\",\n",
    "                PolicyDocument=json.dumps(s3_policy)\n",
    "            )\n",
    "            \n",
    "            print(f\"Role {bedrock_role_name} created successfully.\")\n",
    "            return response['Role']['Arn']\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating role: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def create_s3_bucket_if_not_exists(bucket_name):\n",
    "\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def create_or_get_sns_topic(topic_name):\n",
    "    \n",
    "    try:\n",
    "        # create only if it doesnt exist (function is idempotent)\n",
    "        topic = sns.create_topic(Name=topic_name)\n",
    "        print(f\"SNS topic {topic_name} created successfully (arn: {topic['TopicArn']})\")\n",
    "        return topic['TopicArn']\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating SNS topic {topic_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if not create_s3_bucket_if_not_exists(s3_bucket_name):\n",
    "    print(\"Failed to create or access the S3 bucket.\")\n",
    "\n",
    "role_arn = create_or_get_bedrock_s3_access_role(bedrock_role_name, s3_bucket_name)\n",
    "if not role_arn:\n",
    "    print(\"Failed to create or get the IAM role for Bedrock. Exiting.\")\n",
    "\n",
    "sns_topic_arn = create_or_get_sns_topic(sns_topic_name)\n",
    "if not sns_topic_arn:\n",
    "    print(\"Failed to create or get the SNS topic. Exiting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the batch inference input file:\n",
    "Every line of the input file is a json object with a `modelInput`.<br>\n",
    "`modelInput` is the prompt sent to Claude and contains the list of events to summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_entry_and_add_to_file(events_string, input_file):\n",
    "\n",
    "    # putting words in claude's mouth    \n",
    "    prompt = f\"\"\"Human: Please summarize the following list of AWS CloudTrail events for several users. \n",
    "    Focus on identifying patterns, unusual activities, and potential security concerns. \n",
    "    Here's the list of events:\n",
    "\n",
    "    {events_string}\n",
    "\n",
    "    Provide a concise summary of the user's activities, highlighting any noteworthy or suspicious actions.\n",
    "\n",
    "    Assistant: Certainly! I'll analyze the CloudTrail events several users and provide a summary of their activities, focusing on patterns, unusual activities, and potential security concerns. Here's the summary:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if len(prompt) > max_context_length:\n",
    "        print(f\"Prompt too long: {len(prompt)} chars for max_context_length configured (chars). \\\n",
    "              Process will carry on anyway. You may encounter errors\")\n",
    "\n",
    "    bedrock_batch_json = {\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "            \"max_tokens\": max_tokens_in_summary,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop_sequences\": [],\n",
    "            \"messages\": [ \n",
    "                { \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": prompt \n",
    "                        } \n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(input_file, 'a') as f:\n",
    "        json.dump(bedrock_batch_json, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process CloudTrail events\n",
    "\n",
    "Cloudtrail events can be retrieved with the `lookup_events` API. \n",
    "\n",
    "We paginate through all events in the account and create a jsonl file with a modelInput for each batch item.\n",
    "\n",
    "Max RPS for `lookup_events` is 2. This can lead to throttling exceptions that are automatically retried by boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting CloudTrail event processing...\")\n",
    "\n",
    "if os.path.exists(batch_inference_input_file):\n",
    "    os.remove(batch_inference_input_file)\n",
    "\n",
    "event_count = 0\n",
    "page_count = 0\n",
    "completion_count = 0\n",
    "event_buffer = []\n",
    "\n",
    "paginator = cloudtrail.get_paginator('lookup_events')\n",
    "page_iterator = paginator.paginate(\n",
    "    PaginationConfig={\n",
    "        'MaxItems': None,\n",
    "        'PageSize': 50\n",
    "    }\n",
    ")\n",
    "\n",
    "for page in page_iterator:\n",
    "    \n",
    "    page_count += 1\n",
    "    print(f\"\\rProcessing page {page_count} -> batch entries created : {completion_count}/[{min_batch_items_for_bedrock_batch_inference}(min),{max_batch_items}(max)]\", end=\"\", flush=True)\n",
    "    \n",
    "    for event in page['Events']:\n",
    "\n",
    "        event_count += 1\n",
    "        ct_event = json.loads(event['CloudTrailEvent'])\n",
    "        event_buffer.append(ct_event)\n",
    "\n",
    "        if(len(event_buffer) >= events_per_batch_item):\n",
    "            events_string = ' '.join(json.dumps(event, separators=(',', ':')) for event in event_buffer)\n",
    "\n",
    "            create_batch_entry_and_add_to_file(events_string, batch_inference_input_file)\n",
    "            \n",
    "            event_buffer = []\n",
    "            completion_count += 1\n",
    "    \n",
    "    # stop if we have enough batch items, this limits the cost of the test in case if you have many events\n",
    "    if completion_count >= max_batch_items:\n",
    "        break\n",
    "    \n",
    "    if 'NextToken' not in page:\n",
    "        print(\"Reached the end of available events.\")\n",
    "        break\n",
    "    \n",
    "print(f\"\\nTotal pages processed: {page_count}\")\n",
    "print(f\"Total events processed: {event_count}\")\n",
    "print(f\"Total completion count (items in batch inference): {completion_count}\")\n",
    "\n",
    "if completion_count < min_batch_items_for_bedrock_batch_inference:\n",
    "    print(f\"Bedrock requires a minimum of {completion_count} entries for batch inference. You may not have enough cloudtrail events.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch inference job\n",
    "\n",
    "Batch inference input file is uploaded to S3 and passed as `inputDataConfig` eg. `input_file_name.jsonl`<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"cloudtrail-summary-{int(time.time())}\"\n",
    "\n",
    "s3.upload_file(batch_inference_input_file, s3_bucket_name, batch_inference_input_file)\n",
    "print(f\"Uploaded {batch_inference_input_file} to {s3_input_uri}\")\n",
    "        \n",
    "response = bedrock.create_model_invocation_job(\n",
    "    modelId=model_id,\n",
    "    roleArn=role_arn,\n",
    "    jobName=job_name,\n",
    "    inputDataConfig=({\n",
    "        \"s3InputDataConfig\": {\n",
    "            \"s3Uri\": s3_input_uri\n",
    "        }\n",
    "    }),\n",
    "    outputDataConfig=({\n",
    "        \"s3OutputDataConfig\": {\n",
    "            \"s3Uri\": s3_output_uri\n",
    "        }\n",
    "    })\n",
    ")\n",
    "        \n",
    "job_arn = response.get('jobArn')\n",
    "job_id = job_arn.split('/')[1]\n",
    "\n",
    "print(f\"Batch inference job launched successfully. Job ID: {job_id}\")\n",
    "print(f\"Output will be available at: {s3_output_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the job to complete, est. 10-20min\n",
    "while True:\n",
    "    time.sleep(10)\n",
    "    print(f\"Waiting for job {job_arn} to complete\")\n",
    "    response = bedrock.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    if response['status'] == 'Completed':\n",
    "        print(f\"Done\")\n",
    "        break\n",
    "    elif response['status'] == 'Failed':\n",
    "        raise Exception(f\"Batch inference job failed: {response['failureReason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch inference output\n",
    "\n",
    "Batch inference job output data configuration `outputDataConfig`is a folder where sub-folder `job-id` is created containing a `.out` file eg. `input_file_name.jsonl.out` with the completion results.<br>\n",
    "Each item processed by the batch inference job is a json object containing `modelInput` and `modelOutput` \n",
    "\n",
    "NOTE: a `manifest.json.out` is also generated and includes statistics on the batch job. eg. input tokens, output tokens.\n",
    "\n",
    "In this example, we download the `.out` file locally to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"output.jsonl\"\n",
    "\n",
    "print(f\"Downloading batch output from {s3_bucket_name} for job {job_id}\")\n",
    "s3.download_file(s3_bucket_name, f'batch_inference_output/{job_id}/{batch_inference_input_file}.out', output_file)\n",
    "print(f\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "summaries = []\n",
    "\n",
    "print(f\"Processing {len(lines)} lines to get the summaries\")\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    summary = data['modelOutput'][\"content\"][0][\"text\"]\n",
    "    summaries.append(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing final summary\n",
    "\n",
    "To compute the final summary we will group the summaries in prompt maxing out the configured context length.\n",
    "\n",
    "Direct calls to bedrock are made to summarize the groups of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility method to invoke bedrock and get result\n",
    "def invoke_bedrock(prompt):\n",
    "\n",
    "    native_request = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    response_text = model_response[\"content\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt sent to bedrockto summarize a block of summaries generated by batch inference\n",
    "def summarize_block(to_summarize):\n",
    "\n",
    "    final_prompt = f\"\"\"Human: Please summarize the following summaries of AWS CloudTrail events. \n",
    "        Focus on identifying patterns, unusual activities, and potential security concerns. \n",
    "        Here's the list of summaries:\n",
    "\n",
    "        {to_summarize}\n",
    "\n",
    "        Provide a concise summary of the user's activities, highlighting any noteworthy or suspicious actions.\n",
    "\n",
    "        Assistant: Certainly! I'll analyze the summaries and provide a final summary of their activities, focusing on patterns, unusual activities, and potential security concerns. Here's the final summary:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    summary = invoke_bedrock(final_prompt)\n",
    "\n",
    "    print(f\"Summarized {len(to_summarize)} chars with bedrock in {len(summary)} chars\")\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes the list of summaries to generate a final summary\n",
    "def summarize_list(summaries):\n",
    "\n",
    "    print(f\"Summarizing {len(summaries)} summaries\")\n",
    "\n",
    "    context_length = 0\n",
    "    summaries_of_summaries = []\n",
    "    final_summary = \"\"\n",
    "    to_summarize = \"\"\n",
    "    count_summaries = 0\n",
    "\n",
    "    for summary in summaries:     \n",
    "        \n",
    "        count_summaries += 1\n",
    "        \n",
    "        context_length += len(summary)\n",
    "        to_summarize += \"\\n\" + summary\n",
    "\n",
    "        # we split summarization task by max_context_length given, \n",
    "        if context_length > max_context_length - prompt_length:\n",
    "            print(f\"Processing summaries {count_summaries} of {len(summaries)}\")\n",
    "            summaries_of_summaries.append(summarize_block(to_summarize))\n",
    "            to_summarize = \"\"\n",
    "            context_length = 0\n",
    "\n",
    "    if len(to_summarize) > 0:\n",
    "        summaries_of_summaries.append(summarize_block(to_summarize))       \n",
    "\n",
    "    if len(summaries_of_summaries) > 1:\n",
    "        final_summary = summarize_list(summaries_of_summaries)\n",
    "    else:\n",
    "        final_summary = summaries_of_summaries[0]\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = summarize_list(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output 1\n",
    "\n",
    "Based on the provided CloudTrail event summaries, the key observations regarding the user's activities are:\n",
    "\n",
    "1. Repeated CloudTrail API Calls by User \"vivien\":\n",
    "   - The IAM user \"--redacted--\" made multiple consecutive \"LookupEvents\" API calls to the CloudTrail service within a short timeframe, potentially indicating an attempt to retrieve a large amount of CloudTrail data.\n",
    "   - This user accessed the CloudTrail service from a non-AWS IP address (x.x.x.x) and used consistent user agent information (Boto3/1.35.8), suggesting an automated or scripted activity.\n",
    "   - The repeated API calls and resulting throttling exceptions raise concerns about the user's intentions and the potential risk of unauthorized access or data exfiltration.\n",
    "\n",
    "2. Broad Role Permissions:\n",
    "   - Some of the assumed roles, such as the \"--redacted---\" and the \"--redacted---\", have broad permissions (e.g., \"Allow *\" on all resources).\n",
    "   - This level of broad access should be reviewed to ensure it is necessary and not overly permissive, as it could potentially lead to security risks if the roles are compromised.\n",
    "\n",
    "3. Security Audit Activities:\n",
    "   - The user with the assumed role \"--redacted---\" performed a DescribeRegions operation, which is likely part of a security audit or monitoring activity.\n",
    "   - However, this user also attempted to retrieve the bucket policy for the \"--redacted---\" bucket, but received a \"NoSuchBucketPolicy\" error, indicating a potential permission issue.\n",
    "   - Additionally, the user accessed a bucket named \"--redacted---\", which is an unusual and potentially suspicious bucket name. This bucket access should be investigated further.\n",
    "\n",
    "In summary, the key concerns identified in the CloudTrail event summaries are the repeated CloudTrail API calls by the user \"--redacted--\", which could indicate unauthorized access or data exfiltration.\n",
    "\n",
    "### Sample output 2\n",
    "\n",
    "Based on the analysis of the provided CloudTrail event summaries, the key findings are:\n",
    "\n",
    "1. Routine CloudTrail Monitoring: The majority of the events are related to the CloudTrail service accessing S3 buckets to monitor and log API activities, which is a standard security practice.\n",
    "\n",
    "2. Assumed Roles by AWS Services: Various AWS services, such as Kinesis Analytics, SageMaker, Lightsail, and Pipes, are assuming specific IAM roles to perform their operations. This is a common and expected behavior for AWS services.\n",
    "\n",
    "3. Potential Security Concern: One event stands out where the SageMaker service assumed the \"--redacted--\" role, which grants broad permissions (\"*\" on all resources). While assuming roles is a normal practice, the broad permissions granted to this role could be a potential security concern and should be reviewed to ensure the role has the minimum required permissions.\n",
    "\n",
    "4. Unusual IAM User Activity: The IAM user \"--redacted--\" is performing a high volume of \"InvokeModel\" API calls on the \"anthropic.claude-3-haiku-20240307-v1:0\" model within a short timeframe. This pattern of repeated model invocations from a single user account could indicate potential unauthorized or abusive use of the AI/ML service.\n",
    "\n",
    "5. Potential Security Concern with Source IP: The source IP address \"--redacted--\" used by the \"--redacted--\" user is not a typical AWS IP range, which raises a potential security concern and warrants further investigation to ensure the legitimacy of the user's activities.\n",
    "\n",
    "Overall, the CloudTrail event summaries do not indicate any clearly malicious or suspicious activities, aside from the unusual pattern of model invocations by the \"--redacted--\" user and the broad permissions granted to the SageMaker execution role. It is recommended to closely monitor the user's activities, review their permissions, investigate the source IP address, and ensure the principle of least privilege is followed for all IAM roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish to SNS topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to sns to receive the summary in email for eg.\n",
    "sns.publish(TopicArn=sns_topic_arn, Message=final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "This sample is a baseline for anyone looking to enhance their security posture by analyzing CloudTrail logs using Amazon Bedrock. By following the steps outlined in the notebook, you can quickly set up and execute batch inference jobs that help you identify potential security threats in your AWS environment. Furthermore, the techniques demonstrated here can be applied to a wide range of other AWS services and use cases, making it a versatile addition to your cloud security toolkit.\n",
    "We encourage you to explore its capabilities, adapt it to your specific needs, and share your experiences with the community.\n",
    "\n",
    "- parralelize the final summary processing to reduce latency of last step\n",
    "- filter out principals or services that are not believed to be interesting\n",
    "- break down by user, service, region\n",
    "- Integrate code in step function / lambda to to be able to trigger it on schedule\n",
    "- use IaC to create the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Optionaly, you can delete the resources created in the setup section through AWS console or CLI.\n",
    "\n",
    "- Empty S3 bucket and delete the bucket\n",
    "- Delete the SNS topic\n",
    "- Delete the role\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudtrail-batch-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
