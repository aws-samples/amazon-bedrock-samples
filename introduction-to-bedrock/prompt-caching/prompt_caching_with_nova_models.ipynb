{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b87af2-6de5-4055-b410-237fff246f98",
   "metadata": {},
   "source": [
    "## Prompt Caching with Amazon Nova models\n",
    "\n",
    "Prompt caching, now generally available on Amazon Bedrock with Anthropicâ€™s Claude 3.5 Haiku and Claude 3.7 Sonnet, along with Nova Micro, Nova Lite, and Nova Pro models. This notebook demonstrates how to work with prompt caching with Amazon Nova models using both the Bedrock Converse API and InvokeModel API.\n",
    "\n",
    "### Understanding Prompt Caching Benefits\n",
    "Prompt caching allows you to cache frequently used context across multiple model invocations, which is especially valuable for:\n",
    "\n",
    "* Document Q&A systems where users ask multiple questions about the same document\n",
    "* Coding assistants that maintain context about code files\n",
    "* Applications with long, repeated prompts\n",
    "* \n",
    "The cached context remains available for up to 5 minutes after each access, with each cache hit resetting this countdown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a87ff-672a-4307-a367-e49c26d2b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "# initialize bedrock runtime client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "boto3_bedrock = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff75e2-2831-4285-b2ea-ebbdb20bcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use nova models in this notebook please make sure to enable the access to Nova models\n",
    "nova_models = ['us.amazon.nova-micro-v1:0','us.amazon.nova-lite-v1:0','us.amazon.nova-pro-v1:0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423052e-d2bf-4576-bc5e-5c264b1f7d74",
   "metadata": {},
   "source": [
    "### Use case: Chat with document\n",
    "\n",
    "#### Prompt structure\n",
    "For document chat applications, the optimal caching approach separates static and dynamic content:\n",
    "\n",
    "- **Static content** (cache these):\n",
    "  - Instructions (system prompt)\n",
    "  - Document content (messages)\n",
    "- **Dynamic content** (don't cache):\n",
    "  - User queries\n",
    "\n",
    "This separation maximizes cache efficiency while maintaining flexibility for varied user inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77409a9-1e59-46df-aa94-d04a46c1ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document topics \n",
    "topics = [\n",
    "    'https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-preview-prompt-caching',\n",
    "    'https://aws.amazon.com/about-aws/whats-new/2025/03/amazon-nova-models-govcloud/',\n",
    "]\n",
    "\n",
    "# Sample user queries\n",
    "questions = [\n",
    "    'what is it about?',\n",
    "    'what are the use cases?'\n",
    "]\n",
    "\n",
    "# Model output instructions\n",
    "instructions = (\n",
    "    \"I will provide you with a document, followed by a question about its content. \"\n",
    "    \"Your task is to analyze the document, extract relevant information, and provide \"\n",
    "    \"a comprehensive answer to the question. Please follow these detailed instructions:\"\n",
    "\n",
    "    \"\\n\\n1. Identifying Relevant Quotes:\"\n",
    "    \"\\n   - Carefully read through the entire document.\"\n",
    "    \"\\n   - Identify sections of the text that are directly relevant to answering the question.\"\n",
    "    \"\\n   - Select quotes that provide key information, context, or support for the answer.\"\n",
    "    \"\\n   - Quotes should be concise and to the point, typically no more than 2-3 sentences each.\"\n",
    "    \"\\n   - Choose a diverse range of quotes if multiple aspects of the question need to be addressed.\"\n",
    "    \"\\n   - Aim to select between 2 to 5 quotes, depending on the complexity of the question.\"\n",
    "\n",
    "    \"\\n\\n2. Presenting the Quotes:\"\n",
    "    \"\\n   - List the selected quotes under the heading 'Relevant quotes:'\"\n",
    "    \"\\n   - Number each quote sequentially, starting from [1].\"\n",
    "    \"\\n   - Present each quote exactly as it appears in the original text, enclosed in quotation marks.\"\n",
    "    \"\\n   - If no relevant quotes can be found, write 'No relevant quotes' instead.\"\n",
    "    \"\\n   - Example format:\"\n",
    "    \"\\n     Relevant quotes:\"\n",
    "    \"\\n     [1] \\\"This is the first relevant quote from the document.\\\"\"\n",
    "    \"\\n     [2] \\\"This is the second relevant quote from the document.\\\"\"\n",
    "\n",
    "    \"\\n\\n3. Formulating the Answer:\"\n",
    "    \"\\n   - Begin your answer with the heading 'Answer:' on a new line after the quotes.\"\n",
    "    \"\\n   - Provide a clear, concise, and accurate answer to the question based on the information in the document.\"\n",
    "    \"\\n   - Ensure your answer is comprehensive and addresses all aspects of the question.\"\n",
    "    \"\\n   - Use information from the quotes to support your answer, but do not repeat them verbatim.\"\n",
    "    \"\\n   - Maintain a logical flow and structure in your response.\"\n",
    "    \"\\n   - Use clear and simple language, avoiding jargon unless it's necessary and explained.\"\n",
    "\n",
    "    \"\\n\\n4. Referencing Quotes in the Answer:\"\n",
    "    \"\\n   - Do not explicitly mention or introduce quotes in your answer (e.g., avoid phrases like 'According to quote [1]').\"\n",
    "    \"\\n   - Instead, add the bracketed number of the relevant quote at the end of each sentence or point that uses information from that quote.\"\n",
    "    \"\\n   - If a sentence or point is supported by multiple quotes, include all relevant quote numbers.\"\n",
    "    \"\\n   - Example: 'The company's revenue grew by 15% last year. [1] This growth was primarily driven by increased sales in the Asian market. [2][3]'\"\n",
    "\n",
    "    \"\\n\\n5. Handling Uncertainty or Lack of Information:\"\n",
    "    \"\\n   - If the document does not contain enough information to fully answer the question, clearly state this in your answer.\"\n",
    "    \"\\n   - Provide any partial information that is available, and explain what additional information would be needed to give a complete answer.\"\n",
    "    \"\\n   - If there are multiple possible interpretations of the question or the document's content, explain this and provide answers for each interpretation if possible.\"\n",
    "\n",
    "    \"\\n\\n6. Maintaining Objectivity:\"\n",
    "    \"\\n   - Stick to the facts presented in the document. Do not include personal opinions or external information not found in the text.\"\n",
    "    \"\\n   - If the document presents biased or controversial information, note this objectively in your answer without endorsing or refuting the claims.\"\n",
    "\n",
    "    \"\\n\\n7. Formatting and Style:\"\n",
    "    \"\\n   - Use clear paragraph breaks to separate different points or aspects of your answer.\"\n",
    "    \"\\n   - Employ bullet points or numbered lists if it helps to organize information more clearly.\"\n",
    "    \"\\n   - Ensure proper grammar, punctuation, and spelling throughout your response.\"\n",
    "    \"\\n   - Maintain a professional and neutral tone throughout your answer.\"\n",
    "\n",
    "    \"\\n\\n8. Length and Depth:\"\n",
    "    \"\\n   - Provide an answer that is sufficiently detailed to address the question comprehensively.\"\n",
    "    \"\\n   - However, avoid unnecessary verbosity. Aim for clarity and conciseness.\"\n",
    "    \"\\n   - The length of your answer should be proportional to the complexity of the question and the amount of relevant information in the document.\"\n",
    "\n",
    "    \"\\n\\n9. Dealing with Complex or Multi-part Questions:\"\n",
    "    \"\\n   - For questions with multiple parts, address each part separately and clearly.\"\n",
    "    \"\\n   - Use subheadings or numbered points to break down your answer if necessary.\"\n",
    "    \"\\n   - Ensure that you've addressed all aspects of the question in your response.\"\n",
    "\n",
    "    \"\\n\\n10. Concluding the Answer:\"\n",
    "    \"\\n    - If appropriate, provide a brief conclusion that summarizes the key points of your answer.\"\n",
    "    \"\\n    - If the question asks for recommendations or future implications, include these based strictly on the information provided in the document.\"\n",
    "\n",
    "    \"\\n\\nRemember, your goal is to provide a clear, accurate, and well-supported answer based solely on the content of the given document. \"\n",
    "    \"Adhere to these instructions carefully to ensure a high-quality response that effectively addresses the user's query.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81940d4f-059a-434e-93b9-933986d039d6",
   "metadata": {},
   "source": [
    "#### Implementation with InvokeModel API\n",
    "\n",
    "The first example is to use InvokeModel API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0162905-9bbe-45f4-b3f8-37cae0720a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_chat_with_document(system_prompt, document, user_query, model_id):\n",
    "    \n",
    "    document_content =  f\"## document:\\n{document} \"\n",
    "\n",
    "    # Define your system prompt(s).\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": system_prompt,\n",
    "            \"cachePoint\": {\n",
    "                \"type\": \"default\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": document_content,\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": user_query,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configure the inference parameters.\n",
    "    inf_params = {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 20,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    native_request = {\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(native_request),\n",
    "        modelId=model_id,\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    print(json.dumps(response_body, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82953127-e91b-4c3a-ad8b-09582eeb1e09",
   "metadata": {},
   "source": [
    "**First invocation**: cache write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b6fc1-79d3-47ef-af52-cd4977cb4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topics[0])\n",
    "blog = response.text\n",
    "invoke_chat_with_document(instructions, blog, questions[0], nova_models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c319855-729a-4986-9b83-c70630ebcaec",
   "metadata": {},
   "source": [
    "**Subsequent invocation**: cache read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb5f48-3ea7-492a-980b-15d182a959cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_chat_with_document(instructions, blog, questions[1], nova_models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e506874-017c-4c3f-add0-a18d9f7b73e4",
   "metadata": {},
   "source": [
    "#### Implementation with Converse API\n",
    "\n",
    "Converse API on Bedrock provides unified API experience across models. The following example implement the same use case with Converse API on Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97c8ca-94ad-4411-8894-4af743633cd1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> You will need boto3 > 1.37.26, which includes prompt caching in the Converse API.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de92cec-5d46-4633-992f-36f62560e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc35f0-03ad-45db-b38a-6cc58c750fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converse_with_document(system_prompt, document, user_query, model_id):\n",
    "\n",
    "    document_content =  f\"## document:\\n{document} \"\n",
    "\n",
    "    system_list = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"cachePoint\": {\n",
    "                \"type\": \"default\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    message_list = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'text': document_content\n",
    "                },\n",
    "                {\n",
    "                    \"cachePoint\": {\n",
    "                        \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'text': user_query\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inference_config = {\n",
    "        'maxTokens': 500,\n",
    "        'temperature': 0,\n",
    "        'topP': 1\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.converse(\n",
    "        system=system_list,\n",
    "        messages=message_list,\n",
    "        modelId=model_id,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "    response_text = output_message[\"content\"][0][\"text\"]\n",
    "\n",
    "    print(\"Response text:\")\n",
    "    print(response_text)\n",
    "\n",
    "    print(\"Usage:\")\n",
    "    print(json.dumps(response[\"usage\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf760e24-7464-4218-8193-2f9b84bb72a1",
   "metadata": {},
   "source": [
    "**Same instructions, different document**: cache read on instructions, cache write on document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b02eb6-e1a1-49df-92a0-3451b7b7164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(topics[1])\n",
    "blog = response.text\n",
    "converse_with_document(instructions, blog, questions[0], nova_models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d86ba-ee12-4e7d-901e-8c716aa289c1",
   "metadata": {},
   "source": [
    "**Subsequent invocation**: cache read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6d355-ffdd-472e-8fe4-44916219b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "converse_with_document(instructions, blog, questions[1], nova_models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc255ffe-7914-4745-8b95-7f73a6046056",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook expolored Amazon Bedrock's prompt caching feature with Amazon Nova models.\n",
    "\n",
    "For more information about working with prompt caching on Amazon Bedrock, see the [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883d778-6fe6-4c0f-8615-65b129b90516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
