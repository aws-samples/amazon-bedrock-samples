{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Cross-region Inference in Amazon Bedrock\n",
    "\n",
    "Inference profiles via Amazon Bedrock allow you to perform cross-region inference without the need to setup anything in a fully-managed, secure and private manner. Inference profiles is a managed feature built on Amazon Bedrock, offering generative AI application builders a seamless solution for managing traffic bursts and ensuring optimal availability and performance. By leveraging this feature, builders no longer have to spend time and effort building complex resiliency structure. Instead, inference profiles intelligently routes traffic across multiple opted-in regions, automatically adapting to peak utilization surges. Any inferences carried out through inference profiles will dynamically utilize on-demand capacity available from any of the configured regions, maximizing availability and throughput.\n",
    "\n",
    "## Key Features & Benefits\n",
    "\n",
    "Some of the key features include:\n",
    "\n",
    "* Access to capacity in different regions allowing generative AI workloads to scale with demand.\n",
    "* Access to region-agnostic models to achieve higher throughput.\n",
    "* Become resilient to any traffic bursts.\n",
    "* Ability to select between the pre-defined region sets suiting application needs.\n",
    "* Compatible with existing APIs.\n",
    "* No additional cost.\n",
    "* Same model pricing as the source region.\n",
    "\n",
    "The end-user experience is not impacted as the model behind cross-region inference remains the same, and builders can focus on writing logic for a differentiated application. \n",
    "\n",
    "Bedrock is the easiest way to build and scale generative AI applications. Cross region inference further enhances the usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by installing the dependencies to ensure we have a recent version\n",
    "#!pip install --quiet --upgrade --force-reinstall boto3 botocore awscli\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods \n",
    "\n",
    "1. To allow assume role capability if needed\n",
    "2. Leverage the profile set up for boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "def get_boto_client_tmp_cred(\n",
    "    retry_config = None,\n",
    "    target_region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        aws_session_token=os.getenv('AWS_SESSION_TOKEN',\"\"),\n",
    "\n",
    "    )\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client    \n",
    "\n",
    "def get_boto_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\", None)\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        signature_version = 'v4',\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "    else: # use temp credentials -- add to the client kwargs\n",
    "        print(f\"  Using temp credentials\")\n",
    "\n",
    "        return get_boto_client_tmp_cred(retry_config=retry_config,target_region=target_region, runtime=runtime, service_name=service_name)\n",
    "\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)\n",
    "\n",
    "#os.environ[\"AWS_PROFILE\"] = 'default'\n",
    "boto3_client =  get_boto_client(region='us-east-1', runtime=False) # switch to the region of your choice\n",
    "boto3_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ IMPORTANT‚ùóüî¥\n",
    "\n",
    "*Note:* <span style=\"color:red\">This notebook sample uses `us-east-1` region and the inference profiles available there as example.\n",
    "Change the `region_name` and inference profile ids in the cells below depending on which region you are in and which inference profiles are available.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have AWS credentials or AWS profile setup before running this cell\n",
    "\n",
    "#boto3_client =  get_bedrock_client(region='us-east-1', runtime=False)\n",
    "\n",
    "region_name = 'us-east-1' # switch to the region of your choice\n",
    "account_id = get_boto_client(service_name='sts', region=region_name).get_caller_identity().get('Account')\n",
    "bedrock_client = get_boto_client(service_name='bedrock', region=region_name)\n",
    "bedrock_runtime = get_boto_client(service_name='bedrock-runtime', region=region_name)\n",
    "\n",
    "print(account_id, bedrock_client, bedrock_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Inference Profiles\n",
    "\n",
    "The cross-region inference feature comes with two additional APIs in the Bedrock client SDK:\n",
    "\n",
    "* `list_inference_profiles()` -> This API tells you all the models that are configured behind Inference Profiles for you.\n",
    "* `get_inference_profile(inferenceProfileIdentifier)` -> This API give you specific details of a certain Inference Profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.list_inference_profiles()['inferenceProfileSummaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that cross-region inference offers two forms:\n",
    "\n",
    "**Foundation model in source region**\n",
    "\n",
    "In this mode a Inference Profile is only configured for a model which exists in the source region. For such model(s) a failover mechanism would exist allowing requests to be re-routed to fulfilment regions configured in Inference Profile. By default the request will go to source region resources and only if the region is busy or you hit your quota limit, the request is routed to another region. In order to determine, which region the request should go to Amazon Bedrock intelligently checks in real-time which region has redundant capacity available. The on-demand principle still applies if none of the region has capacity to handle the request, then it will be throttled.\n",
    "\n",
    "In this mode, If the source region doesn't have a certain model available, you will not be able to access it in a fulfilment region via cross-region inference feature.\n",
    "\n",
    "**Available via Inference Profiles**\n",
    "\n",
    "Select list of models are made available via cross-region inference, where Amazon Bedrock abstracts away the regional details and manages the hosting and inference routing automatically. Such foundation models then exist across the pre-defined region sets and the builder can build applications agnostic of setting up a region. This allows reliable throughput, access to leading foundation models as well as scalability in terms of throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5c82c01d-9dfe-485c-8917-954e7a519da1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 06 Nov 2024 19:02:32 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '667',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '5c82c01d-9dfe-485c-8917-954e7a519da1'},\n",
       "  'RetryAttempts': 0},\n",
       " 'inferenceProfileName': 'US Anthropic Claude 3.5 Sonnet',\n",
       " 'models': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0'}],\n",
       " 'description': 'Routes requests to Anthropic Claude 3.5 Sonnet in us-east-1 and us-west-2.',\n",
       " 'createdAt': datetime.datetime(2024, 8, 26, 0, 0, tzinfo=tzutc()),\n",
       " 'updatedAt': datetime.datetime(2024, 9, 26, 14, 9, tzinfo=tzutc()),\n",
       " 'inferenceProfileArn': 'arn:aws:bedrock:us-east-1:622343165275:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'inferenceProfileId': 'us.anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'status': 'ACTIVE',\n",
       " 'type': 'SYSTEM_DEFINED'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.get_inference_profile(\n",
    "    inferenceProfileIdentifier='us.anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `get_inference_profile` API you can observe the `status` if a Inference Profile is `ACTIVE` or not and also which regions are configured for inference routing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Inference Profiles\n",
    "\n",
    "An Inference Profile is used in the same way as a foundation model using the `modelId` or `arn` of the model. Inference profile also comes with it's own id and arn, where the difference is in the prefix. For the inference profile you can expect a regional prefix such as `us.` or `eu.` behind the model id for it to be recognized as an inference profile. Also in the `arn`, you can find the difference from `:<region>::foundation-model/<model-id>` to `:<region>::inference-profile/<region-set-prefix>.<model-id>`\n",
    "\n",
    "You can use both the `arn` and the `modelId` with `Converse` API whereas only the `modelId` with `InvokeModel` API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converse API\n",
    "\n",
    "Amazon Bedrock now supports a unified messaging API for seamless application building experience. Read about all the models supported via this API [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n",
    "\n",
    "Let's send a request to both the foundation model as well as the Inference Profile to observe change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 12 second(s)\n",
      "::model:id:anthropic.claude-3-haiku-20240307-v1:0::Response time: 12 second(s)\n",
      "::Foundation Model::Response output: [{'text': \"For cost-effective applications, Amazon S3 is generally a better option than Amazon EFS for storing documents.\\n\\nHere's why:\\n\\n1. Storage Costs:\\n   - S3 has a lower storage cost per gigabyte compared to EFS, especially for large datasets.\\n   - S3 charges based on the actual amount of data stored, while EFS has a minimum storage charge per file system.\\n\\n2. Data Access Patterns:\\n   - S3 is designed for object storage and is optimized for infrequent or batch-style data access.\\n   - EFS is a file storage service that provides low-latency, high-throughput access to files, which is better suited for applications that require frequent and random file access.\\n\\n3. Scalability:\\n   - S3 can easily scale to store large amounts of data, as it is designed to handle billions of objects and exabytes of data.\\n   - EFS can also scale, but it may have higher costs for large datasets compared to S3.\\n\\n4. Durability and Availability:\\n   - S3 provides high durability and availability, with built-in redundancy and replication across multiple Availability Zones.\\n   - EFS also provides high durability and availability, but it may have slightly higher costs for these features.\\n\\nIn summary, for cost-effective applications that primarily require storing and accessing documents in a batch or infrequent manner, Amazon S3 is generally the more cost-effective choice compared to Amazon EFS. S3 offers lower storage costs and better scalability for large datasets.\\n\\nHowever, if your application requires frequent and random file access, with low-latency requirements, then Amazon EFS might be the better option, even though it may have higher costs.\\n\\nIt's important to evaluate your specific use case, data access patterns, and cost requirements to determine the most suitable storage solution for your application.\"}]\n",
      "::Inference Profile::Response time: 8 second(s)\n",
      "::model:id:us.anthropic.claude-3-5-sonnet-20240620-v1:0::Response time: 8 second(s)\n",
      "::Inference Profile::Response output: [{'text': \"For cost-effective document storage in most applications, Amazon S3 (Simple Storage Service) is generally the better choice compared to Amazon EFS (Elastic File System). Here's why:\\n\\n1. Cost: S3 is typically less expensive than EFS, especially for storing large amounts of data that isn't frequently accessed.\\n\\n2. Scalability: S3 offers virtually unlimited scalability, while EFS has limits on the number of files it can store efficiently.\\n\\n3. Access patterns: S3 is ideal for object storage and works well for documents that don't require frequent modifications.\\n\\n4. Durability: S3 provides 99.999999999% (11 9's) durability, which is higher than EFS.\\n\\n5. Storage classes: S3 offers various storage classes (e.g., Standard, Infrequent Access, Glacier) to optimize costs based on access patterns.\\n\\n6. Integration: S3 integrates easily with other AWS services and has broad support for static website hosting, content delivery, and data analytics.\\n\\nHowever, EFS might be more suitable in specific scenarios:\\n\\n1. When you need a shared file system across multiple EC2 instances.\\n2. For applications requiring file locking or POSIX compliance.\\n3. When you need high-throughput access to frequently changing data.\\n\\nIn most cases, for document storage in cost-effective applications, Amazon S3 is the recommended choice due to its lower cost, scalability, and flexibility.\"}]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "system_prompt = \"You are an expert on AWS services and always provide correct and concise answers.\"\n",
    "input_message = \"Should I be storing documents in Amazon S3 or EFS for cost effective applications?\"\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-haiku-20240307-v1:0')\n",
    "#inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-haiku-20240307-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-5-sonnet-20240620-v1:0')\n",
    "\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=id,\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": input_message}]\n",
    "        }]\n",
    "    )\n",
    "    end = time()\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::model:id:{id}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response['output']['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that using the same API and only the change in model ID you can expect similar behavior.\n",
    "\n",
    "It is also possible to use a full ARN in place of the model ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_profile_id = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\" #\"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inference_profile_arn,\n",
    "    system=[{\"text\": system_prompt}],\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_message}]\n",
    "    }]\n",
    ")\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InvokeModel API\n",
    "\n",
    "Most of the generative AI applications in production are already built on top of `InvokeModel` API, even the third-party tools also use this API for using the models. The cross-region inference feature is also compatible with this API. Where the Converse API only supports select models, all models available in Amazon Bedrock can leverage InvokeModel API.\n",
    "\n",
    "Let's send a request via both the foundation model id as well as the inference profile id to observe change via this API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Foundation Model::Response time: 22 second(s)\n",
      "::Foundation Model::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, archives, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability through data replication across multiple Availability Zones.\n",
      "- Access to S3 is over the internet, which may not be suitable for low-latency or high-throughput workloads.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file storage service that provides a file system interface and file system access semantics.\n",
      "- It is designed for use cases that require shared access to the same data from multiple compute resources (EC2 instances, Lambda functions, etc.) within the same AWS region and VPC.\n",
      "- EFS is well-suited for applications that require a traditional file system interface, such as content management systems, web servers, data analytics, and media processing workflows.\n",
      "- EFS provides high performance and low latency, making it suitable for workloads that require frequent access to shared data.\n",
      "- EFS automatically scales file system capacity and performance as your needs grow, without disrupting applications.\n",
      "- EFS is more expensive than S3 for storing large amounts of data, especially for infrequently accessed data.\n",
      "\n",
      "In general, if you need to store and retrieve large amounts of unstructured data cost-effectively, with high durability and availability, S3 is the better choice. However, if you have applications that require shared access to the same data with low latency and high throughput, and you don't mind paying a higher cost for that performance, EFS would be the better option.\n",
      "\n",
      "For cost-effective applications that don't require frequent access to shared data, S3 is likely the more cost-effective choice. However, if your application requires shared access to data with low latency, EFS may be worth the additional cost.\n",
      "::Inference Profile::Response time: 11 second(s)\n",
      "::Inference Profile::Response output: The choice between Amazon S3 (Simple Storage Service) and Amazon EFS (Elastic File System) for storing documents depends on your specific use case and requirements. Here's a general comparison to help you decide:\n",
      "\n",
      "**Amazon S3**:\n",
      "- S3 is an object storage service designed for storing and retrieving any amount of data from anywhere on the internet.\n",
      "- It is highly scalable, durable, and cost-effective for storing large amounts of data.\n",
      "- S3 is ideal for storing static files, backups, archives, media files, and other types of unstructured data.\n",
      "- Data is stored as objects (files) within buckets, and you can set different storage classes based on access patterns to optimize costs.\n",
      "- S3 provides high availability and durability through data replication across multiple Availability Zones.\n",
      "- Access to S3 is over the internet, which may not be suitable for low-latency or high-throughput workloads.\n",
      "\n",
      "**Amazon EFS**:\n",
      "- EFS is a fully-managed file storage service that provides a simple and scalable file system interface.\n",
      "- It is designed for use with AWS cloud services and on-premises resources, providing a traditional file system access over the Network File System (NFS) protocol.\n",
      "- EFS is well-suited for workloads that require shared access to the same data from multiple instances or servers, such as content management systems, web serving, and data analytics.\n",
      "- EFS file systems can automatically grow and shrink as files are added or removed, eliminating the need to provision and manage storage capacity.\n",
      "- EFS provides high availability and durability through data replication across multiple Availability Zones within a region.\n",
      "- Access to EFS is over the AWS network, which provides low-latency and high-throughput performance.\n",
      "\n",
      "In terms of cost-effectiveness, S3 is generally more cost-effective for storing large amounts of unstructured data, especially if you can take advantage of different storage classes (e.g., S3 Glacier for archiving). However, if you need shared file system access with low latency and high throughput, EFS may be more suitable, despite being more expensive than S3 for storage alone.\n",
      "\n",
      "Ultimately, the choice depends on your specific requirements for data access patterns, performance, and the type of data you're storing. If you're dealing with large amounts of unstructured data that doesn't require frequent or shared access, S3 is likely the more cost-effective option. If you need a shared file system with low-latency access, EFS may be the better choice, despite potentially higher costs.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": system_prompt,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{input_message}\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "modelId = ('Foundation Model', 'anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "inferenceProfileId = ('Inference Profile', 'us.anthropic.claude-3-sonnet-20240229-v1:0')\n",
    "for inference_type, id in [modelId, inferenceProfileId]:\n",
    "    start = time()\n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=id, accept=accept, contentType=contentType)\n",
    "    end = time()\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    print(f\"::{inference_type}::Response time: {int(end-start)} second(s)\")\n",
    "    print(f\"::{inference_type}::Response output: {response_body['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "\n",
    "Below you can find integration on how to use the new cross-region inference feature with one of the popular open-source frameworks [LangChain](https://python.langchain.com/v0.2/docs/integrations/platforms/aws/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet langchain_aws langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LangChain](https://python.langchain.com/v0.2/docs/introduction/) with the help of integrations implements [`langchain-aws`](https://github.com/langchain-ai/langchain-aws) which provides support  `Converse` API via [`ChatBedrockConverse`](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/#beta-bedrock-converse-api). This allows you to use the latest models such as Anthropic Claude 3 Sonnet via this implementation. Below you can see an example of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like poetry,\n",
      "Pixels dance with ecstasy.\n",
      "Logic's rhythm, my heart's key,\n",
      "Programming, my true reverie.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_profile_id = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=inference_profile_arn,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the ChatBedrock with Model id as the inference profile model id instead of the ARN\n",
    "\n",
    "**Option 1**\n",
    "\n",
    "1. We set the converse api flag to be true and that invokes the ChatBedrockConverse class which will invoke the model using the converse api class\n",
    "2. `Region` needs to be explictly passed in for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence \"I love programming\" shaped into a poetic form:\n",
      "\n",
      "Code flows like verses, \n",
      "Pixels dance at my command.\n",
      "Programming, a passion,\n",
      "Igniting joy, line by line.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True,  \n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_profile_id = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id=inference_profile_arn,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True,  \n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1',\n",
    "    provider='anthropic'\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Model parameters\n",
    "\n",
    "Pass in the `Model inference parameters` at run time for model invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poetic rendering of \"I love programming because...\":\n",
      "\n",
      "Code dances on the screen, a symphony\n",
      "Of logic's rhythm, pixels set free\n",
      "I love programming because it lets me\n",
      "Bring ideas to life, creativity's key\n",
      "\n",
      "Each line a brush stroke, each function a hue\n",
      "Painting realities, visions anew\n",
      "I love programming for the thrill it imbues\n",
      "Solving puzzles, conquering the blue\n",
      "\n",
      "From zeroes an\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Generate a potery from this cue\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming because......\"),\n",
    "]\n",
    "#model_parameter = {\"temperature\": 0.1, \"top_p\": .5, \"max_tokens\": 1000  } #\"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    #model_kwargs=model_parameter,\n",
    "    beta_use_converse_api=True,\n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "# - these will take precedence even if we have similiar params when creating the class\n",
    "runtime_parameter = {\"temperature\": 0.9,  \"max_tokens\": 100 }\n",
    "print(llm.invoke(messages, **runtime_parameter).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming with LangChain\n",
    "\n",
    "Demostrate the streaming capabilities with LangChain. We also demonstrate that run time parameters take precedence. Say if you have temperature when creating the ChatBedrock class and also during the invocation, it will use the invocation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a poetic take on \"I love programming because...\":\n",
      "\n",
      "Code dances on the screen, a symphony\n",
      "Of logic's brilliant choreography\n",
      "I love programming because it frees\n",
      "My mind to build, create, and seize\n",
      "The boundless potential that technology\n",
      "Unveils with each new line's philosophy\n",
      "\n",
      "Solving puzzles, one by one\n",
      "Finding elegance when the code is done\n",
      "I love programming for the thrill\n",
      "Of conquering challenges with skill"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Generate a potery from this cue\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming because......\"),\n",
    "]\n",
    "model_parameter = {\"temperature\": 0.1, \"top_p\": .5, \"max_tokens\": 1000  } #\"max_tokens_to_sample\": 200}\n",
    "llm = ChatBedrock(\n",
    "    model_id='us.anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    model_kwargs=model_parameter,\n",
    "    beta_use_converse_api=True,\n",
    "    client=bedrock_runtime,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# These would take precedence \n",
    "runtime_parameter = {\"temperature\": 0.9,  \"max_tokens\": 100 }\n",
    "response = llm.stream(messages, **runtime_parameter)\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "for chunk in response:\n",
    "    if chunk and len(chunk.content) > 0:\n",
    "        sys.stdout.write(chunk.content[0].get('text',\"\"))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2 where we have the model provider**\n",
    "\n",
    "This does not use the converse api behind the scenes and calls the invoke api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentence translated into poetic form:\n",
      "\n",
      "Coding's my passion, a love so true,\n",
      "Crafting lines of logic, a dance anew.\n",
      "Algorithms and syntax, a symphony to behold,\n",
      "Transforming ideas into digital gold.\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "inference_profile_id = \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "inference_profile_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{inference_profile_id}\"\n",
    "print(inference_profile_arn)\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=inference_profile_arn,\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the invoke and the prescribed message format for Claude models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the user's sentence translated into poetic form:\n",
      "\n",
      "Digits dance, code unfurls,\n",
      "Algorithms weave their whirls.\n",
      "Syntax shines, a language's art,\n",
      "Programming, my beating heart.\n"
     ]
    }
   ],
   "source": [
    "messages = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"system\": 'You are a helpful assistant that shapes sentences into poetic form. Translate the user sentence.',\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"I love programming.\",\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5}\n",
    "llm = ChatBedrock(\n",
    "    provider=\"anthropic\",\n",
    "    model_id=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=model_parameter, \n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring, Logging, and Metrics\n",
    "\n",
    "Using cross-region inference might route your request to a different region, based on the selected inference profile.\n",
    "\n",
    "If a request gets re-routed, the [Bedrock model invocation log](https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html) will mention the used region in the `inferenceRegion` element of the JSON log entry.\n",
    "\n",
    "The below code snippet will enable the model invocation logging to a CloudWatch log group and then create a CloudWatch metric filter to select and count all model invocations that get routed to a fulfilment region. You can adapt this code to build a metric for a specific target region and to monitor latency based on region.\n",
    "\n",
    "This code snippet creates a new IAM role with appropriate permissions to enable the model invocation log delivered into CloudWatch Logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# see https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html\n",
    "\n",
    "log_group_name = 'bedrock-model-invocation-logging'\n",
    "role_name = 'AmazonBedrockModelInvocationCWDeliveryRole'\n",
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": account_id,\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": f\"arn:aws:bedrock:{region_name}:{account_id}:*\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client = boto3.client('iam', region_name=region_name)\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    )\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    pass\n",
    "policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\",\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:logs:{region_name}:{account_id}:log-group:{log_group_name}:log-stream:aws/bedrock/modelinvocations\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "iam_client.put_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyName=role_name,\n",
    "    PolicyDocument=json.dumps(policy_doc),\n",
    ")\n",
    "\n",
    "logs_client = boto3.client('logs', region_name=region_name)\n",
    "try:\n",
    "    logs_client.create_log_group(\n",
    "        logGroupName=log_group_name,\n",
    "    )\n",
    "except logs_client.exceptions.ResourceAlreadyExistsException:\n",
    "    pass\n",
    "logs_client.put_retention_policy(\n",
    "    logGroupName=log_group_name,\n",
    "    retentionInDays=365\n",
    ")\n",
    "\n",
    "response = bedrock_client.put_model_invocation_logging_configuration(\n",
    "    loggingConfig={\n",
    "        'cloudWatchConfig': {\n",
    "            'logGroupName': log_group_name,\n",
    "            'roleArn': f\"arn:aws:iam::{account_id}:role/{role_name}\",\n",
    "        },\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'imageDataDeliveryEnabled': False,\n",
    "        'embeddingDataDeliveryEnabled': False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Model Invocation Log enabled and configured for CloudWatch Logs, you can now create a CloudWatch metric filter from the ingested JSON log entries by filtering for `inferenceRegion` to match / mis-match against your source or target region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_client.put_metric_filter(\n",
    "    logGroupName=log_group_name,\n",
    "    filterName='bedrock_cross_region_inference_rerouted',\n",
    "    filterPattern=f'{{ $.inferenceRegion != \"{region_name}\" }}',\n",
    "    metricTransformations=[\n",
    "        {\n",
    "            'metricNamespace': 'Custom',\n",
    "            'metricName': 'bedrock_cross_region_inference_rerouted',\n",
    "            'metricValue': '1',\n",
    "            'defaultValue': 0,\n",
    "            'unit': 'Count',\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cross-region inference provides the ability to manage bursts and spiky traffic patterns across a variety of generative AI workloads and disparate request shapes.With this feature you can easily scale your workloads in production without the need of heavy-lifting, lengthy migrations and overhead of infrastructure management. Amazon Bedrock handles the routing securely, reliably and in a transparent manner while giving you the control you need.\n",
    "\n",
    "### Key considerations\n",
    "While building or migrating applications to use of cross-region inference, it is important to keep in mind the following:\n",
    "- **Latency** - If your application is latency sensitive, it is advised to properly test the use of cross-region inference prior to fully relying on it since the routing to different regions could lead to higher latency numbers thus impacting your application behavior.\n",
    "- **Compliance** - Cross-region inference comes with pre-defined region sets, if these sets contain a region where you can't operate or goes against your policy, then it is advised not to use Inference Profiles, instead utilize Foundation Models directly.\n",
    "- **Determinism** - If you need to have control over where your requests are (or will be) re-routed (other than source region), it is better to consider just rely on Foundation Model directly. Also the model exclusive to cross-region inference exclusive models should be opted for carefully.\n",
    "- **Variety** - Inference Profiles do not provide access to multiple different models from different regions, it either acts as a failover mechanism for models in source region or provides Inference Profile exclusive models where the construct of a region is abstracted away. If your business demands to span across variety of foundation models from multiple regions, it is wise to consider building your own architecture via VPC Peering/Transit Gateway or other architectural patterns.\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "If you ran the **Monitoring, Logging, and Metrics** code, consider disabling the Model Invocation Log to avoid incuring associated cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
