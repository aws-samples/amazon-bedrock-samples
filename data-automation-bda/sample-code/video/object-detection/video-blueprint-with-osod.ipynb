{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Visual Intelligence with Amazon Bedrock Data Automation\n",
    "\n",
    "Welcome to our specialized workshop on video object detection using Amazon Bedrock Data Automation (BDA). This notebook focuses on BDA's powerful object detection capabilities that transform raw video content into structured, searchable, and analyzable data.\n",
    "\n",
    "## Why Object Detection Matters for Video\n",
    "\n",
    "Video content presents unique opportunities and challenges:\n",
    "\n",
    "- A single minute of video contains approximately 1,800 frames (at 30 fps)\n",
    "- Organizations possess vast libraries of video content with limited metadata\n",
    "- Manual object tagging is prohibitively expensive at $15-25 per minute of processed content \n",
    "- Traditional approaches detect objects in isolated frames without understanding temporal context\n",
    "- Only 1-2% of video content is typically leveraged in business intelligence systems\n",
    "\n",
    "Amazon Bedrock Data Automation transforms this landscape by enabling intelligent object detection across video content with business-friendly field naming and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "Let's begin by installing required libraries and importing dependencies. We'll be using our specialized utility functions for object detection visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install \"boto3\" \"matplotlib\" \"moviepy\" \"pandas\" \"seaborn\" \"wordcloud\" --upgrade -qq\n",
    "\n",
    "# Import necessary libraries\n",
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Video, clear_output, HTML, display, Markdown\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import utilities\n",
    "from bda_object_detection_utils import BDAObjectDetectionUtils\n",
    "\n",
    "# Initialize our utility class\n",
    "bda_utils = BDAObjectDetectionUtils()\n",
    "print(f\"Setup complete. BDA utilities initialized for region: {bda_utils.current_region}\")\n",
    "print(f\"Using S3 bucket: {bda_utils.bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Sample Video\n",
    "\n",
    "First, we'll download a sample video and upload it to S3 for processing with BDA. We'll use a short video that contains various objects that BDA can detect and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample video using our enhanced utility function\n",
    "sample_video = 'movie-demo.mp4'\n",
    "source_url = 'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/NetflixMeridian.mp4'\n",
    "\n",
    "# Download the video with enhanced error handling\n",
    "try:\n",
    "    bda_utils.download_video(source_url, sample_video)\n",
    "    print(f\"Successfully downloaded video to {sample_video}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading video: {e}\")\n",
    "\n",
    "# Display the video in the notebook for preview\n",
    "display(Video(sample_video, width=800))\n",
    "\n",
    "# Upload to S3 for BDA processing\n",
    "s3_key = f'{bda_utils.data_prefix}/{sample_video}'\n",
    "s3_uri = bda_utils.upload_to_s3(sample_video, s3_key)\n",
    "print(f\"Uploaded video to S3: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Enhanced Blueprint for Object Detection\n",
    "\n",
    "Now we'll define a custom blueprint for object detection. This blueprint uses business-friendly field names that better reflect the purpose of each detection type, making the schema more intuitive and self-documenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the enhanced blueprint for object detection with business-friendly field names\n",
    "# and detailed comments about each field based on AWS documentation\n",
    "\n",
    "blueprint = {\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"description\": \"This blueprint enhances the searchability and discoverability of video content by providing comprehensive object detection and scene analysis.\",\n",
    "  \"class\": \"media_search_video_analysis\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    # Targeted Object Detection: Identifies visually prominent objects in the video with bounding boxes\n",
    "    # Set granularity to chapter level for more precise object detection\n",
    "    \"targeted-object-detection\": {\n",
    "      \"items\": {\n",
    "        \"$ref\": \"bedrock-data-automation#/definitions/Entity\"\n",
    "      },\n",
    "      \"type\": \"array\",\n",
    "      \"instruction\": \"Please detect all the visually prominent objects in the video\",\n",
    "      \"granularity\": [\"chapter\"]  # Chapter-level granularity provides per-scene object detection\n",
    "    },\n",
    "    \n",
    "    # Category-Based Detection: Groups objects by categories for better organization with bounding boxes\n",
    "    # This allows for detecting objects belonging to specific categories (e.g., furniture)\n",
    "    \"category-based-detection\": {\n",
    "      \"items\": {\n",
    "        \"$ref\": \"bedrock-data-automation#/definitions/Entity\"\n",
    "      },\n",
    "      \"type\": \"array\",\n",
    "      \"instruction\": \"Detect all the furniture items in the video\",\n",
    "      \"granularity\": [\"chapter\"]  # Per-scene category detection\n",
    "    },\n",
    "    \n",
    "    # Visual Importance Analysis: Determines the most significant visual elements with bounding boxes\n",
    "    # This helps identify what draws viewer attention in each scene\n",
    "    \"visual-importance-analysis\": {\n",
    "      \"items\": {\n",
    "        \"$ref\": \"bedrock-data-automation#/definitions/Entity\"\n",
    "      },\n",
    "      \"type\": \"array\",\n",
    "      \"instruction\": \"Find and detect the most visually important elements in the video\",\n",
    "      \"granularity\": [\"chapter\"]  # Analyze visual importance per chapter\n",
    "    },\n",
    "    \n",
    "    # Contextual Object Detection: Finds objects within specific contexts with bounding boxes\n",
    "    # Allows for more complex detection scenarios like \"people riding motorcycles\"\n",
    "    \"contextual-object-detection\": {\n",
    "      \"items\": {\n",
    "        \"$ref\": \"bedrock-data-automation#/definitions/Entity\"\n",
    "      },\n",
    "      \"type\": \"array\",\n",
    "      \"instruction\": \"Detect the people driving a car in the video\",\n",
    "      \"granularity\": [\"chapter\"]  # Per-chapter contextual object detection\n",
    "    },\n",
    "    \n",
    "    # Object Verification: Confirms presence/absence of specific objects\n",
    "    # Video-level granularity checks across the entire video\n",
    "    \"object-verification\": {\n",
    "      \"type\": \"boolean\",\n",
    "      \"inferenceType\": \"inferred\",  # Uses inference rather than direct extraction\n",
    "      \"instruction\": \"Are there zebras in this video? Respond with false if none are present.\",\n",
    "      \"granularity\": [\"video\"]  # Video-level verification spans the entire content\n",
    "    },\n",
    "    \n",
    "    # Verification Explanation: Provides reasoning for object verification\n",
    "    # Helps users understand why the model determined objects were present/absent\n",
    "    \"verification-explanation\": {\n",
    "      \"type\": \"string\",\n",
    "      \"inferenceType\": \"inferred\",\n",
    "      \"instruction\": \"Explain why you believe zebras are or are not present in the video\",\n",
    "      \"granularity\": [\"video\"]  # Video-level explanation\n",
    "    },\n",
    "    \n",
    "    # Total Objects Count: Aggregates unique objects across the video\n",
    "    # Provides a high-level metric of object diversity\n",
    "    \"total-objects-count\": {\n",
    "      \"type\": \"number\",\n",
    "      \"inferenceType\": \"inferred\",\n",
    "      \"instruction\": \"Count the total number of distinct objects detected in the video\",\n",
    "      \"granularity\": [\"video\"]  # Video-level aggregate count\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Generate a unique blueprint name to avoid naming conflicts\n",
    "unique_id = str(uuid.uuid4())[0:6]\n",
    "blueprint_name = f\"bda-video-enhanced-blueprint-{unique_id}\"\n",
    "\n",
    "print(f\"Creating blueprint with name: {blueprint_name}\")\n",
    "\n",
    "# Create the blueprint in BDA\n",
    "try:\n",
    "    bp_response = bda_utils.bda_client.create_blueprint(\n",
    "        blueprintName=blueprint_name,\n",
    "        type='VIDEO',  # Specify this is for video analysis\n",
    "        blueprintStage='LIVE',  # Use development stage for workshop\n",
    "        schema=json.dumps(blueprint),  # Convert blueprint dict to JSON string\n",
    "    )\n",
    "    \n",
    "    blueprint_arn = bp_response.get(\"blueprint\", {}).get(\"blueprintArn\")\n",
    "    print(f\"Blueprint created successfully with ARN: {blueprint_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating blueprint: {e}\")\n",
    "    blueprint_arn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define BDA Configuration and Create Project\n",
    "\n",
    "Now we'll define the standard output configuration for video analysis and create a BDA project. This configuration determines what information BDA will extract from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard output configuration for video processing\n",
    "standard_output_config = {\n",
    "    'video': {\n",
    "        'extraction': {\n",
    "            'category': {\n",
    "                'state': 'ENABLED',\n",
    "                'types': [\n",
    "                    'CONTENT_MODERATION',  # Detect inappropriate content\n",
    "                    'TEXT_DETECTION',      # Extract text from the video\n",
    "                    'TRANSCRIPT',          # Generate transcript of spoken content\n",
    "                    'LOGOS'                # Identify brand logos\n",
    "                ]\n",
    "            },\n",
    "            'boundingBox': {\n",
    "                'state': 'ENABLED'         # Include bounding boxes for detected elements\n",
    "            }\n",
    "        },\n",
    "        'generativeField': {\n",
    "            'state': 'ENABLED',\n",
    "            'types': [\n",
    "                'VIDEO_SUMMARY',           # Generate overall video summary\n",
    "                'CHAPTER_SUMMARY',         # Generate summaries for each chapter\n",
    "                'IAB'                      # Classify into IAB categories\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a BDA project with our standard output configuration\n",
    "print(\"Creating BDA project for object detection...\")\n",
    "response = bda_utils.bda_client.create_data_automation_project(\n",
    "    projectName=f'bda-workshop-object-detection-project-{str(uuid.uuid4())[0:4]}',\n",
    "    projectDescription='BDA workshop object detection project',\n",
    "    projectStage='LIVE',\n",
    "    standardOutputConfiguration=standard_output_config,\n",
    "    customOutputConfiguration={\n",
    "        'blueprints': [\n",
    "            {\n",
    "                'blueprintArn': blueprint_arn,\n",
    "                'blueprintStage': 'LIVE'\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get the project ARN\n",
    "video_project_arn = response.get(\"projectArn\")\n",
    "print(f\"BDA project created with ARN: {video_project_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Video with BDA\n",
    "\n",
    "Now we'll use the `invoke_data_automation_async` API to process our video with BDA. BDA operates asynchronously due to the complexity and processing time required for video analysis and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke BDA to process the video\n",
    "print(f\"Processing video: {s3_uri}\")\n",
    "print(f\"Results will be stored at: s3://{bda_utils.bucket_name}/{bda_utils.output_prefix}\")\n",
    "\n",
    "# Call the invoke_data_automation_async API\n",
    "response = bda_utils.bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': s3_uri  # The S3 location of our video\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{bda_utils.bucket_name}/{bda_utils.output_prefix}'  # Where to store results\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': video_project_arn,  # The project we created\n",
    "        'stage': 'LIVE'                          # Must match the project stage\n",
    "    },\n",
    "    dataAutomationProfileArn=f'arn:aws:bedrock:{bda_utils.current_region}:{bda_utils.account_id}:data-automation-profile/us.data-automation-v1'\n",
    ")\n",
    "\n",
    "# Get the invocation ARN\n",
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(f\"Invocation ARN: {invocation_arn}\")\n",
    "\n",
    "# Wait for processing to complete using our enhanced pattern\n",
    "status_response = bda_utils.wait_for_completion(\n",
    "    get_status_function=bda_utils.bda_runtime_client.get_data_automation_status,\n",
    "    status_kwargs={'invocationArn': invocation_arn},\n",
    "    completion_states=['Success'],\n",
    "    error_states=['ClientError', 'ServiceError'],\n",
    "    status_path_in_response='status',\n",
    "    max_iterations=40,  # Video might take longer than other modalities\n",
    "    delay=10\n",
    ")\n",
    "\n",
    "# Check if processing was successful\n",
    "if status_response['status'] == 'Success':\n",
    "    output_config_uri = status_response.get(\"outputConfiguration\", {}).get(\"s3Uri\")\n",
    "    print(f\"\\nVideo processing completed successfully!\")\n",
    "    print(f\"Output configuration: {output_config_uri}\")\n",
    "else:\n",
    "    print(f\"\\nVideo processing failed with status: {status_response['status']}\")\n",
    "    if 'error_message' in status_response:\n",
    "        print(f\"Error message: {status_response['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve and Explore BDA Results\n",
    "\n",
    "Now that the video has been processed, let's retrieve the results from S3 and explore the object detection insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job metadata\n",
    "config_data = bda_utils.read_json_from_s3(output_config_uri)\n",
    "\n",
    "# Get custom output path\n",
    "custom_output_path = config_data[\"output_metadata\"][0][\"segment_metadata\"][0][\"custom_output_path\"]\n",
    "result_data = bda_utils.read_json_from_s3(custom_output_path)\n",
    "\n",
    "# Save the result data to the bda-results directory\n",
    "with open('bda_results.json', 'w') as f:\n",
    "    json.dump(result_data, f)\n",
    "    \n",
    "print(f\"Saved video object detection results to: bda_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Video Metadata and Object Verification\n",
    "\n",
    "Let's first look at the video metadata and object verification results to understand what BDA detected across the entire video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video metadata and object verification results\n",
    "print(\"=== Video Metadata and Object Verification ===\\n\")\n",
    "metadata = result_data.get(\"metadata\", {})\n",
    "inference_result = result_data.get(\"inference_result\", {})\n",
    "\n",
    "print(f\"Video Type: {inference_result.get('video-type', 'N/A')}\")\n",
    "print(f\"Genre: {inference_result.get('genre', 'N/A')}\")\n",
    "\n",
    "if \"object-verification\" in inference_result:\n",
    "    verification = inference_result[\"object-verification\"]\n",
    "    explanation = inference_result.get(\"verification-explanation\", \"No explanation provided\")\n",
    "    print(f\"\\nObject Verification Query: Are there zebras in this video?\")\n",
    "    print(f\"Result: {'Present' if verification else 'Not Present'}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "\n",
    "if \"total-objects-count\" in inference_result:\n",
    "    print(f\"\\nTotal Unique Objects Detected: {int(inference_result['total-objects-count'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chapter-Based Object Detection Analysis\n",
    "\n",
    "Let's analyze the objects detected across different chapters of the video. This provides insights into what objects appear in each scene and how they relate to the narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze objects detected across video chapters\n",
    "# Call the method from our bda_utils instance instead of as a standalone function\n",
    "bda_utils.analyze_chapter_objects(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Object Detection with Bounding Boxes\n",
    "\n",
    "Now we'll visualize the objects detected in a specific chapter with their bounding boxes. This provides precise spatial information about where objects appear in the video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a chapter to analyze (index starts at 0)\n",
    "chapter_index = 2\n",
    "\n",
    "# Visualize objects with bounding boxes for the selected chapter\n",
    "bda_utils.visualize_objects_with_bounding_boxes(sample_video, result_data, chapter_index, confidence_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: The Business Value of Object Detection\n",
    "\n",
    "Amazon Bedrock Data Automation's object detection capabilities transform raw video assets into structured, searchable data that delivers significant business value across industries:\n",
    "\n",
    "### Media & Entertainment\n",
    "- **Content Discovery**: Enable search and discovery by specific objects, improving viewer engagement\n",
    "- **Automated Metadata**: Generate rich object-based metadata without manual tagging\n",
    "- **Content Recommendations**: Match content based on similar visual elements and objects\n",
    "- **Smart Navigation**: Allow viewers to jump directly to scenes containing objects of interest\n",
    "\n",
    "### Marketing & Advertising\n",
    "- **Product Placement**: Track product appearances and measure screen time\n",
    "- **Competitive Analysis**: Analyze competitors' video content for featured products and objects\n",
    "- **Brand Monitoring**: Automatically detect logo and product appearances\n",
    "- **Contextual Targeting**: Place ads alongside content featuring similar objects\n",
    "\n",
    "### Retail & E-Commerce\n",
    "- **Shoppable Content**: Tag products in videos for direct purchasing\n",
    "- **Visual Merchandising**: Analyze store layout and product placement\n",
    "- **Product Detection**: Identify products in user-generated content\n",
    "- **Visual Search**: Enable search for products seen in videos\n",
    "\n",
    "### Security & Compliance\n",
    "- **Object Verification**: Confirm presence or absence of required safety equipment\n",
    "- **Suspicious Object Detection**: Flag potentially dangerous items\n",
    "- **Prohibited Content**: Identify banned or restricted objects\n",
    "- **Safety Monitoring**: Ensure compliance with safety regulations\n",
    "\n",
    "By leveraging object detection with business-friendly visualizations, organizations can unlock the full value of their video content libraries and create more engaging, discoverable, and monetizable video experiences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
