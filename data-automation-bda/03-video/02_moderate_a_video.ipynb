{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a899bfb2-fa00-482e-ab5a-f97daa18d876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T23:22:21.289317Z",
     "iopub.status.busy": "2025-01-20T23:22:21.288559Z",
     "iopub.status.idle": "2025-01-20T23:22:21.296763Z",
     "shell.execute_reply": "2025-01-20T23:22:21.295537Z",
     "shell.execute_reply.started": "2025-01-20T23:22:21.289288Z"
    }
   },
   "source": [
    "# Video Content Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259074a6-a277-4247-a8d1-faa44d0020bf",
   "metadata": {},
   "source": [
    "Video content moderation is a common use case across various industries, including Social Media, Media & Entertainment, Advertising, Education and more. Customers need to prevent unsafe or toxic content that could damage brand safety, or violate regional regulations. \n",
    "\n",
    "BDA Video offers features that help customers detect unsafe content in both the visual and audio components of videos. With blueprints, customers can extend their moderation policies to flexibly support their business needs.\n",
    "\n",
    "In this lab, we will use BDA to analyze a sample video containing unsafe and toxic content:\n",
    "\n",
    "![video moderation](../static/bda-video-cm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87659b5-90c4-40a4-9c03-931d70ab8955",
   "metadata": {},
   "source": [
    "## Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf31c6e-182c-4998-83ce-f60ed4f53b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install \"boto3>=1.35.76\" itables==2.2.4 PyPDF2==3.0.1 --upgrade -qq\n",
    "%pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d3478-84a7-4c29-a5e6-4a3f6c686d4b",
   "metadata": {},
   "source": [
    "For a self-hosted workshop, we recommend creating a new S3 bucket in the same region where you plan to run the workshop. You can name it `bda-workshop-YOUR_ACCOUNT_ID-YOUR_REGION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc3186-7df3-4e5f-ae7d-2522ce275209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_bucket = \"<Enter your bucket name here>\"\n",
    "data_prefix = \"bad-workshop/video\"\n",
    "output_prefix = \"bad-workshop/video/ouput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94080f3-7a23-4b04-898d-57be31c638a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import utils\n",
    "\n",
    "bda_client = boto3.client('bedrock-data-automation')\n",
    "bda_runtime_client = boto3.client('bedrock-data-automation-runtime')\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa554a34-ae81-4802-9ca5-b4a40e759f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T00:31:53.506949Z",
     "iopub.status.busy": "2025-01-21T00:31:53.506589Z",
     "iopub.status.idle": "2025-01-21T00:31:53.513035Z",
     "shell.execute_reply": "2025-01-21T00:31:53.511550Z",
     "shell.execute_reply.started": "2025-01-21T00:31:53.506925Z"
    }
   },
   "source": [
    "## Create a BDA project\n",
    "To start a BDA job, you need a BDA project, which organizes both standard and custom output configurations. This project is reusable, allowing you to apply the same configuration to process multiple videos that share the same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eeea0-12c1-4cd7-8e40-f8c3d8b3f8be",
   "metadata": {},
   "source": [
    "In the code snippet below, we create a BDA project with standard output configurations for video modality. These configurations can be tailored to extract only the specific information you need. In this lab, we will enable the below video outputs:\n",
    "- Full video summary\n",
    "- Content moderation (visual and audio)\n",
    "- Text in video (without bounding-boxes)\n",
    "\n",
    "For a complete API reference for creating a BDA project, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation/client/create_data_automation_project.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2510f5-f4f2-4fdd-ac96-4d9edbeeba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_client.create_data_automation_project(\n",
    "    projectName=f'bda-workshop-video-project-moderation-{str(uuid.uuid4())[0:4]}',\n",
    "    projectDescription='BDA workshop video sample project - content moderation',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration={\n",
    "        'video': {\n",
    "            'extraction': {\n",
    "                'category': {\n",
    "                    'state': 'ENABLED',\n",
    "                    'types': ['CONTENT_MODERATION','TEXT_DETECTION','TRANSCRIPT']\n",
    "                },\n",
    "                'boundingBox': {\n",
    "                    'state': 'DISABLED'\n",
    "                }\n",
    "            },\n",
    "            'generativeField': {\n",
    "                'state': 'ENABLED',\n",
    "                'types': [\n",
    "                    'VIDEO_SUMMARY',\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfd78-a072-4375-b970-d2650909f0de",
   "metadata": {},
   "source": [
    "The `create_data_automation_project` API will return the project ARN, which we will use it to invoke the video analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ab6c-5249-4364-927c-c5def65a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_project_arn = response.get(\"projectArn\")\n",
    "print(\"BDA video project ARN:\", video_project_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b81791-e597-457f-a375-265bc0711525",
   "metadata": {},
   "source": [
    "## Start an asynchronous BDA task to extract and analyze a video\n",
    "In this section, we will use a sample video contains unsafe content, and extract and analyze it using BDA, applying the configuration defined in the BDA project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5d08-0be9-495e-bdc3-27e2b68c0e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T01:16:19.221297Z",
     "iopub.status.busy": "2025-01-21T01:16:19.220673Z",
     "iopub.status.idle": "2025-01-21T01:16:19.225680Z",
     "shell.execute_reply": "2025-01-21T01:16:19.224891Z",
     "shell.execute_reply.started": "2025-01-21T01:16:19.221270Z"
    }
   },
   "source": [
    "### Prepare the sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad7889-a3a7-4c82-9766-eb5ecb94acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample video\n",
    "sample_video_path = 'content-moderation-demo.mp4'\n",
    "source_url = f'https://d1xvhy22zmw77y.cloudfront.net/tmp/{sample_video_path}'\n",
    "\n",
    "!curl {source_url} --output {sample_video_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b8628-c692-4385-9725-48c4ea3b70cc",
   "metadata": {},
   "source": [
    "Let's display the video. This short sample contains unsafe visual scenes, including alcohol, tobacco, and suggestive content, as well as toxic audio components. It also features text in the visual portion, which is considered unsafe. Our mission is to extract and analyze this information for content moderation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd61670-4a65-471a-a75b-14525d923af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(sample_video_path, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cf849-11ec-4c37-a3dc-e1bb5c51c92e",
   "metadata": {},
   "source": [
    "To analyze the video using BDA, we need to upload it to an S3 bucket that BDA can access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717c142-df4d-4d34-8a98-8f36a5671fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = f'{data_prefix}/{sample_video_path.split(\"/\")[-1]}'\n",
    "s3_client.upload_file(sample_video_path, data_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b33c9-a23c-4dd3-8f66-22d169e598b7",
   "metadata": {},
   "source": [
    "### Start BDA task\n",
    "We will now invoke the BDA API to process the uploaded video. You need to provide the BDA project ARN that we created at the beginning of the lab and specify an S3 location where BDA will store the output results.\n",
    "\n",
    "For a complete API reference for invoke a BDA async task, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation-runtime/client/invoke_data_automation_async.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681293ae-0023-4eac-9970-af61ec934e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{s3_key}'\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{output_prefix}'\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationArn': video_project_arn,\n",
    "        'stage': 'DEVELOPMENT'\n",
    "    },\n",
    "    notificationConfiguration={\n",
    "        'eventBridgeConfiguration': {\n",
    "            'eventBridgeEnabled': False\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8b65a-1e4e-4dc1-b011-254e7d1cc941",
   "metadata": {},
   "source": [
    "The `invoke_data_automation_async` API is asynchronous. It returns an invocation task identifier, `invocationArn`. We can then use another API `get_data_automation_status` to monitor the task's status until it completes.\n",
    "\n",
    "> In production workloads, an event-driven pattern is recommended. Allow BDA to trigger the next step once the task is complete. This can be achieved by configuring the notificationConfiguration in the invoke task, which will send a notification to a subscribed AWS service, such as a Lambda function. Alternatively, you can set up an S3 trigger on the bucket where BDA will drop the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61061730-ded4-4852-bbf4-d18ab1168c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(\"BDA task started:\", invocation_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13518bf-15eb-4e83-bc4b-955b66974457",
   "metadata": {},
   "source": [
    "In this lab, we will use the loop below to monitor the task by calling the `get_data_automation_status` API every 5 seconds until the task is complete.\n",
    "\n",
    "This video will take less than 5 minutes to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9c48c-9347-43ba-8148-1ea9c29479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "status, status_response = None, None\n",
    "while status not in [\"Success\",\"ServiceError\",\"ClientError\"]:\n",
    "    status_response = bda_runtime_client.get_data_automation_status(\n",
    "        invocationArn=invocation_arn\n",
    "    )\n",
    "    status = status_response.get(\"status\")\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} : BDA video task: {status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "output_config = status_response.get(\"outputConfiguration\",{}).get(\"s3Uri\")\n",
    "print(\"Ouput configureation file:\", output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c617c2-3a71-43dd-b996-d7b9a4d86f08",
   "metadata": {},
   "source": [
    "## Access the BDA analysis result\n",
    "The `get_data_automation_status` API returns an S3 URI containing the result configuration, which provides the S3 location where BDA outputs the extraction results. We will then parse this file to retrieve the result path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1adb7-1620-491f-adb1-b616451d62be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_data = utils.read_json_on_s3(output_config,s3_client)\n",
    "print(json.dumps(config_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc0557-b53d-4dc4-ae1c-05de559093e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As shown above, the BDA output configuration file contains metadata about the BDA result, including the job ID, status, modality, and the S3 location of the actual result JSON. We will now download this result file to verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93edb4-e459-4c00-aeb6-31577240aabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "result_uri = config_data[\"output_metadata\"][0][\"segment_metadata\"][0][\"standard_output_path\"]\n",
    "result_data = utils.read_json_on_s3(result_uri,s3_client)\n",
    "\n",
    "JSON(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2b413-3f1c-4225-8f60-01c00a256110",
   "metadata": {},
   "source": [
    "## Review the result\n",
    "The BDA video analysis results provide a detailed breakdown of information, organized by video and scene levels. \n",
    "> A video scene is a sequence of shots that form a coherent unit of action or narrative within the video.\n",
    "\n",
    "In this lab, we are using a short sample video that contains only one scene, as our focus will be on reviewing the moderation analysis output. For more detailed instructions on metadata on scene and frame level, such as summary, IAB, and bounding boxes, please refer to the previous lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfee39-d157-4b55-bc6b-6011b82579d5",
   "metadata": {},
   "source": [
    "### Full video transcription\n",
    "At the video level, we receive the full transcript based on the video's audio, with speakers identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bee0b-c95c-4082-9109-0bbf1fe69b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result_data[\"video\"][\"transcript\"][\"representation\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d7a98-648d-4277-9ec7-8717b55211c4",
   "metadata": {},
   "source": [
    "### Audio transcripts with toxicity analysis\n",
    "You can find two fields at the scene level containing audio transcripts and audio moderation data: `audio_segments` and `content_moderation`. The sequence of moderation (toxicity) analysis corresponds to the transcripts. Below, we display them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f2687-79fd-4ef9-be2a-e86da3f4faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for scene in result_data[\"scenes\"]:\n",
    "    for idx in range(0,len(scene[\"audio_segments\"])):\n",
    "        print(f'[{scene[\"audio_segments\"][idx][\"start_timestamp_millis\"]/1000} - {scene[\"audio_segments\"][idx][\"end_timestamp_millis\"]/1000}] {scene[\"audio_segments\"][idx][\"text\"]}')\n",
    "        \n",
    "        data = scene[\"content_moderation\"][idx]\n",
    "        # Extract category names and their confidence values\n",
    "        categories = [item[\"category\"] for item in data[\"moderation_categories\"]]\n",
    "        confidence_values = [item[\"confidence\"] for item in data[\"moderation_categories\"]]\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(categories, confidence_values, color='skyblue')\n",
    "        \n",
    "        # Add title and labels\n",
    "        plt.title(f'Moderation Confidence by Category - overall confidence {data[\"confidence\"]*100}%')\n",
    "        plt.xlabel('Moderation Categories')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Display the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250b163-e118-4fd3-938f-07770a0d07f9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The bar chart above represents the audio segment with its start and end timestamps. It visualizes the toxicity analysis of the audio transcript, where the x-axis shows the toxicity categories and the y-axis displays the corresponding confidence scores. \n",
    "\n",
    "BDA video toxicity analysis support the below built-in toxicity categories with confidence score between 0 and 1:\n",
    "- profanity\n",
    "- hate_speech\n",
    "- sexual\n",
    "- insult\n",
    "- violence_or_threat\n",
    "- graphic\n",
    "- harassment_or_abuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1c24d-1d6a-4192-862b-ec3be2060d17",
   "metadata": {},
   "source": [
    "### Visual content moderation the frame level with confidence score\n",
    "BDA also analyzes the visual portion of the video and provides content moderation labels at the frame level. You can find the visual moderation output for each scene under the respective frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005fd12-21b9-4faf-b9bc-f381450f537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with VideoFileClip(sample_video_path) as video_clip:\n",
    "    for scene in result_data[\"scenes\"]:\n",
    "        for frame in scene[\"frames\"]:\n",
    "            if frame.get(\"content_moderation\"):\n",
    "                for cm in frame[\"content_moderation\"]:\n",
    "                    timestamp = frame[\"timestamp_millis\"]/1000\n",
    "                    img_frame = video_clip.get_frame(timestamp)  \n",
    "                    frame_image = Image.fromarray(img_frame)\n",
    "    \n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.imshow(frame_image)\n",
    "                    plt.title(f\"Frame at {timestamp} seconds, {cm['category']}  ({cm['confidence']*100}%) \")\n",
    "                    plt.axis(\"off\")\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de933514-e57b-45c7-9a7a-af12a528604b",
   "metadata": {},
   "source": [
    "### Frame level text extraction and confidence scores\n",
    "Text extraction, along with confidence scores, is available at the frame level. In the output JSON structure, frames are organized under each scene with defined start and end times. If text is detected at a given frame timestamp, you can find `text_words` and `text_lines` included at the frame level.\n",
    "\n",
    "Let's plot the frames with detected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ed953-7356-4d09-a1ec-a941908446c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with VideoFileClip(sample_video_movie) as video_clip:\n",
    "    for scene in result_data[\"scenes\"]:\n",
    "        for frame in scene[\"frames\"]:\n",
    "            txts = []\n",
    "            if frame.get(\"text_lines\"):\n",
    "                for tl in frame[\"text_lines\"]:\n",
    "                    txts.append(tl[\"text\"])\n",
    "            if txts:\n",
    "                img_frame = video_clip.get_frame(timestamp)  \n",
    "                frame_image = Image.fromarray(img_frame)\n",
    "\n",
    "                timestamp = frame[\"timestamp_millis\"]/1000\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(frame_image)\n",
    "                plt.title(f\"Frame at {timestamp} seconds: {'; '.join(txts)}\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced63a5-b37e-403a-9602-0b78b2d03429",
   "metadata": {},
   "source": [
    "### Blueprint analyze the extract text - TODO when feature GA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebc676-e867-4155-b136-556943b7a4b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we use BDA to extract and analyze a sample video containing unsafe and toxic content. The BDA output includes visual and audio moderation detections, as specified in the BDA project configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e1336-3ae1-49b5-8982-811637e22fe7",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Delete the BDA project, blueprint, image, and result from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245e595-6a2f-41b2-9d7c-4961a18ba121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete BDA project\n",
    "response = bda_client.delete_data_automation_project(\n",
    "    projectArn=video_project_arn\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0b6ab-cc34-40ed-9191-ae9a0e395262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete uploaded image from S3\n",
    "s3_client.delete_object(Bucket=data_bucket, Key=s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb89fb-ef71-48b3-a3e8-534ccfa37acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete results from S3\n",
    "utils.delete_s3_folder(data_bucket, output_config.replace(\"job_metadata.json\",\"\") ,s3_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
