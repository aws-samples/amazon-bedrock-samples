{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28739f83-678f-4e76-bb40-af12caf8fac2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Integrate with Amazon Bedrock Knowledge Bases:\n",
    "After processed the audio and video files with a BDA project, next it is time to integrate with Bedrock KB.\n",
    "## Steps involved in this integration: \n",
    "- Set up a knowledge base to parse documents using Amazon Bedrock Data Automation as the parser.\n",
    "- Ingest the processed data into the knowledge base for retrieval and response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2eeba-193f-4f23-bf51-761001661720",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please run this notebook after you finish running the first notebook: 01_data_prep_using_bda.ipynb, the notebook cell one at a time instead of using \"Run All Cells\" option.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16843e-0586-4c58-991f-ca6898e192e2",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "Please make sure to enable `Anthropic Claude 3.5 Haiku` , `Amazon Nova Micro` and  `Titan Text Embeddings V2` model access in Amazon Bedrock Console\n",
    "\n",
    "You need to have suitable IAM role permission to run this notebook. For IAM role, choose either an existing IAM role in your account or create a new role. The role must the necessary permissions to invoke the BDA, Bedrock KB, create IAM roles, SageMaker and S3 APIs.\n",
    "\n",
    "Note: The AdministratorAccess IAM policy can be used, if allowed by security policies at your organization.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please run the notebook cell one at a time instead of using \"Run All Cells\" option.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ac402-0ae3-4041-9cb6-8b2c253c58d3",
   "metadata": {},
   "source": [
    "# Setup notebook and boto3 clients\n",
    "\n",
    "In this step, we will import some necessary libraries that will be used throughout this notebook. To use Amazon Bedrock Data Automation (BDA) with boto3, you'll need to ensure you have the latest version of the AWS SDK for Python (boto3) installed. Version Boto3 1.35.96 of later is required.\n",
    "\n",
    "Note: At time of Public Preview launch, BDA is available in us-west-2 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bfcb2-1f39-4288-a6b7-d3604cff85bc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt --no-deps --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecada3-f537-4099-b0a8-2c8204ee1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dd502-5210-4567-99ce-d2300c5f43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66d474-3dfd-49ea-b132-8122b3675530",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> In this workshop, a new S3 bucket following the naming convention \"kb-bda-multimodal-datasource-{account_id}\" will be used, and the input and output will be saved under a folder called \"bda\" in the default bucket.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb5216-a0d3-4a6d-a57f-ec95c98aa2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "import pprint\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from IPython.display import JSON, IFrame, Audio, display, clear_output\n",
    "import IPython.display as display\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import sagemaker\n",
    "\n",
    "from utils.knowledge_base import BedrockKnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf698514-4a29-4427-b009-9b0ee7071a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clients\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket_name = session.default_bucket()\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "bucket_name_kb = f'bedrock-kb-{suffix}-1' # replace it with your first bucket name.\n",
    "region_name = \"us-west-2\" \n",
    "region = region_name\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region_name)\n",
    "\n",
    "bda_client = boto3.client('bedrock-data-automation', region_name=region_name)\n",
    "bda_runtime_client = boto3.client('bedrock-data-automation-runtime', region_name=region_name)\n",
    "\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime') \n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa312565-ca13-46dc-91b1-c5f54ecebcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy local BDA output files to a S3 bucket for KB integration\n",
    "# Function to check if the bucket exists, if not, create the data_bucket\n",
    "from utils.knowledge_base_operators import bucket_exists\n",
    "suffix = random.randrange(200, 900)\n",
    "bucket_name_kb = f'bedrock-bda-kb-{suffix}-1' \n",
    "           \n",
    "# Create S3 bucket for the KB if it doesn't exist\n",
    "if not bucket_exists(bucket_name_kb):\n",
    "    print(f\"Bucket '{bucket_name_kb}' does not exist. Creating it now...\")\n",
    "    if region == \"us-east-1\":\n",
    "        s3_client.create_bucket(Bucket=bucket_name_kb)\n",
    "    else:\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name_kb,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "        )\n",
    "    print(f\"Bucket '{bucket_name_kb}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Bucket '{bucket_name_kb}' already exists.\")\n",
    "\n",
    "\n",
    "obj_audio = 'bda/dataset/result_aud.json'  \n",
    "s3_client.upload_file('result_aud.json', bucket_name_kb, obj_audio)\n",
    "\n",
    "obj_video = 'bda/dataset/result_vid.json'  \n",
    "s3_client.upload_file('result_vid.json', bucket_name_kb, obj_video)\n",
    "\n",
    "# copy pdf file and image file to bda_kb_bucket_name\n",
    "file_name_doc = 'examples/bedrock-ug.pdf'\n",
    "obj_doc = f\"bda/dataset/{file_name_doc}\"\n",
    "\n",
    "file_name_img = 'examples/bda-idp.png'\n",
    "obj_img = f\"bda/dataset/{file_name_img}\"\n",
    "\n",
    "s3_client.upload_file(file_name_doc, bucket_name_kb, obj_doc )\n",
    "s3_client.upload_file(file_name_img, bucket_name_kb, obj_img )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bb87f-52ad-43b3-82a3-23cac4da5a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "knowledge_base_name = f\"bedrock-multi-modal-kb-{suffix}\"\n",
    "knowledge_base_description = \"Multi-modal RAG knowledge base.\"\n",
    "\n",
    "foundation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7a09f-0f26-47fe-b3fc-ec392bda9bcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Start the Knowledge Base creation \n",
    "\n",
    "In this notebook, the process of creating a KB is simplified by using a wrapper function from the knowledge_base.py file in \"utils\" folder of this notebook. The whole process of creating data source, creating a KB, creating an embedding index, saving the index in a vector data store is simplified by using this function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad0767-573d-4f89-9611-2469e80b0d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please uncomment the data sources that you want to add and update the placeholder values accordingly.\n",
    "\n",
    "#data=[{\"type\": \"S3\", \"bucket_name\": bucket_name, \"inclusionPrefixes\": [\"bda/dataset/\"]}]\n",
    "data=[{\"type\": \"S3\", \"bucket_name\": bucket_name_kb}]\n",
    "\n",
    "\n",
    "                # {\"type\": \"SHAREPOINT\", \"tenantId\": \"888d0b57-69f1-4fb8-957f-e1f0bedf64de\", \"domain\": \"yourdomain\",\n",
    "                #   \"authType\": \"OAUTH2_CLIENT_CREDENTIALS\",\n",
    "                #  \"credentialsSecretArn\": f\"arn:aws::secretsmanager:{region_name}:secret:<<your_secret_name>>\",\n",
    "                #  \"siteUrls\": [\"https://yourdomain.sharepoint.com/sites/mysite\"]\n",
    "                # },\n",
    "    \n",
    "                \n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba453a-e46a-47c0-ac88-12a5e6f52a63",
   "metadata": {},
   "source": [
    "### Step 1 - Create Knowledge Base with Multi modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13cfd2-09a6-4c83-aab8-595441b50e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multi-modal RAG While instantiating BedrockKnowledgeBase, pass multi_modal= True and choose the parser you want to use\n",
    "\n",
    "knowledge_base = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data,\n",
    "    multi_modal= True,\n",
    "    parser= 'BEDROCK_DATA_AUTOMATION', #'BEDROCK_Data Automation service is used'\n",
    "    chunking_strategy = \"FIXED_SIZE\", \n",
    "    suffix = f'{suffix}-f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572dcd5-7236-4c7e-a01b-facdbc0d185a",
   "metadata": {},
   "source": [
    "### Step 2 - Start data ingestion job to KB\n",
    "\n",
    "Once the KB and data source(s) created, we can start the ingestion job for each data source. During the ingestion job, KB will fetch the documents from the data source, Parse the document to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS.\n",
    "\n",
    "NOTE: Currently, you can only kick-off one ingestion job at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e9483-3462-42ce-b8c9-d400300714fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c95f-0ccb-48d9-a25f-b8b1d79293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the kb_id for invocation later in the invoke request\n",
    "kb_id = knowledge_base.get_knowledge_base_id()\n",
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb09e7-74f9-4d5a-a15f-8f9417698045",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 3 -  Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = Give me the summary of the AWS Rethink podcast hosted by Nolan Chen and Malini Chatterjee?\n",
    "\n",
    "The right response for this query is expected to fetch from a the audio transcript ingested in Knowledge Bases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739ab05-b9bb-428e-b44b-3352420e4b96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Step 4: Query Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56001da-07e6-4188-8c50-928f410fc477",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Give me the summary of the AWS Rethink podcast hosted by Nolan Chen and Malini Chatterjee?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf2fc3-fc44-48be-9c5e-843147ae4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "foundation_model = \"amazon.nova-micro-v1:0\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dfe0d4-2702-4f7c-a167-29dd94896109",
   "metadata": {},
   "source": [
    "### Directly play the audio file as part of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88e8f0-8f6c-4c37-98c4-4b65883ec15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio path and timestamps from the response\n",
    "from utils.knowledge_base_operators import extract_audio_path_and_timestamps\n",
    "audio_s3_info, timestamps = extract_audio_path_and_timestamps(response)\n",
    "\n",
    "audio_s3_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16296882-80ba-4d33-9dab-e96536427d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information for all audio segments first, and play audio file\n",
    "from utils.knowledge_base_operators import play_audio_segment\n",
    "\n",
    "    \n",
    "# Display single audio player after all segments\n",
    "print(\"\\nAudio Player (click to play):\")\n",
    "print(\"Note: Please use the time ranges above as reference points in the audio.\")\n",
    "play_audio_segment(audio_s3_info, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54670ba9-09c7-4e19-a100-3212e74e764e",
   "metadata": {},
   "source": [
    "### Step 5: Query Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd9325-1d7f-4852-99ae-62fa18c42ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you find a promotional video containing BDA key features?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cad0c-6c38-42d2-b99c-f0f3dba35787",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae2d63-e746-46a7-b973-3d3f1da5830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract video path and timestamps from the response\n",
    "from IPython.display import HTML\n",
    "from utils.knowledge_base_operators import parse_response_and_get_s3_info\n",
    "from utils.knowledge_base_operators import get_video_from_metadata\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Parse the response\n",
    "    video_info = parse_response_and_get_s3_info(response)\n",
    "\n",
    "    if video_info and video_info['s3_uri']:\n",
    "        if video_info['timestamps']:\n",
    "            print(f\"\\nFound {len(video_info['timestamps'])} Video Segments:\")\n",
    "            for ts in video_info['timestamps']:\n",
    "                print(f\"\\nShot {ts['shot_index']}:\")\n",
    "                print(f\"Time Range: {ts['start_timecode']} - {ts['end_timecode']}\")\n",
    "                print(f\"Duration: {ts['duration']/1000:.2f} seconds\")\n",
    "        \n",
    "        if video_info['summary']:\n",
    "            print(\"\\nVideo Summary:\")\n",
    "            print(video_info['summary'])\n",
    "            \n",
    "        # Get and play the video\n",
    "        print(\"\\nLoading video player...\")\n",
    "\n",
    "        local_video_path = get_video_from_metadata(\n",
    "            video_info['s3_uri']['bucket'],\n",
    "            video_info['s3_uri']['key']\n",
    "        )\n",
    "    else:\n",
    "        print(\"Could not find video information in response\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in main execution: {e}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe24c15-aaf1-4913-b458-bd2ecebc4111",
   "metadata": {},
   "source": [
    "### Step 6: Query Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c55ffd-3541-4dd8-965d-a29be8e24ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me an architecture design of an IDP workflow using Bedrock Data Automation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f38553a-ff1b-4cb5-beda-9d8beb4af0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "# foundation_model = \"amazon.nova-micro-v1:0\"\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b8684-431f-410e-adf4-33f6138598f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import s3fs\n",
    "from IPython.display import display\n",
    "import mimetypes\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "## Function to print retrieved response\n",
    "\n",
    "def print_response(response):\n",
    "#structure 'retrievalResults': list of contents. Each list has ['ResponseMetadata', 'citations', 'output', 'sessionId']\n",
    "    print( f'OUTPUT: {response[\"output\"][\"text\"]} \\n')\n",
    "    \n",
    "    print(f'CITATION DETAILS: \\n')\n",
    "    \n",
    "    for num, chunk in enumerate(response['citations']):\n",
    "        print(f'CHUNK {num}',end='\\n'*1)\n",
    "        print(\"========\")\n",
    "        print(f'\\t Generated  Response Text: ')\n",
    "        print(f'\\t ------------------------- ')\n",
    "        print(f'\\t Generated  Response Text: ',chunk['generatedResponsePart']['textResponsePart']['text'],end='\\n'*2)\n",
    "        for i, ref in enumerate (chunk['retrievedReferences']):\n",
    "            print(f'\\t Retrieved References: ')\n",
    "            print(f'\\t ---------------------', )\n",
    "            print(f'\\n\\t\\t --> Location:', ref['location'])\n",
    "            print(f'\\t\\n\\t\\t --> Metadata: \\n\\t\\t\\t ---> Source', ref['metadata']['x-amz-bedrock-kb-source-uri'])\n",
    "            # print(f'\\t\\n\\t\\t\\n\\t\\t\\t ---> x-amz-bedrock-kb-description', ref['metadata']['x-amz-bedrock-kb-description'])\n",
    "            \n",
    "            # Check if byte-content-source exists in metadata\n",
    "            if 'x-amz-bedrock-kb-byte-content-source' in ref['metadata']:\n",
    "                print(f'\\t\\n\\t\\t\\n\\t\\t\\t ---> x-amz-bedrock-kb-byte-content-source', \n",
    "                      ref['metadata']['x-amz-bedrock-kb-byte-content-source'])\n",
    "                print(\"\")\n",
    "            else:\n",
    "                print(f\"No image... skip chunk\")\n",
    "                continue\n",
    "                \n",
    "            # Get the file extension and check if it's an image\n",
    "            file_path = ref['metadata']['x-amz-bedrock-kb-byte-content-source']\n",
    "            mime_type, _ = mimetypes.guess_type(file_path)\n",
    "            \n",
    "            if mime_type and mime_type.startswith('image/'):\n",
    "                try:\n",
    "                    with fs.open(file_path) as f:\n",
    "                        display(Image.open(f).resize((400, 400)))\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process image: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"No image... skip chunk\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee61d8-aa19-4937-8c38-fc945f687a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339176b4-0bee-4900-9f50-d3656f86b93e",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Let's delete the sample files that were uploaded to S3 and Bedrock Knowledge Base created using BDA as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def delete_folder_and_contents(bucket_name, folder_prefix):\n",
    "   \n",
    "    try:\n",
    "        # Initialize S3 client\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Ensure folder_prefix ends with '/'\n",
    "        if not folder_prefix.endswith('/'):\n",
    "            folder_prefix += '/'\n",
    "            \n",
    "        # List all objects within the folder\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        objects_to_delete = []\n",
    "        \n",
    "        # Paginate through all objects including the folder itself\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=folder_prefix):\n",
    "            if 'Contents' in page:\n",
    "                # Collect objects for deletion\n",
    "                objects_to_delete.extend([\n",
    "                    {'Key': obj['Key']} \n",
    "                    for obj in page['Contents']\n",
    "                ])\n",
    "        \n",
    "        # Add the folder itself to objects_to_delete\n",
    "        objects_to_delete.append({'Key': folder_prefix})\n",
    "        \n",
    "        if objects_to_delete:\n",
    "            # S3 allows maximum 1000 objects per delete operation\n",
    "            chunk_size = 1000\n",
    "            for i in range(0, len(objects_to_delete), chunk_size):\n",
    "                chunk = objects_to_delete[i:i + chunk_size]\n",
    "                response = s3_client.delete_objects(\n",
    "                    Bucket=bucket_name,\n",
    "                    Delete={\n",
    "                        'Objects': chunk,\n",
    "                        'Quiet': True\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Check for errors\n",
    "                if 'Errors' in response:\n",
    "                    for error in response['Errors']:\n",
    "                        logging.error(f\"Error deleting {error['Key']}: {error['Message']}\")\n",
    "                        \n",
    "            logging.info(f\"Successfully deleted folder {folder_prefix} and {len(objects_to_delete)-1} objects\")\n",
    "            return True\n",
    "            \n",
    "        logging.info(f\"No objects found in {folder_prefix}\")\n",
    "        return True\n",
    "        \n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def is_folder_deleted(bucket_name, folder_prefix):\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Ensure folder_prefix ends with '/'\n",
    "        if not folder_prefix.endswith('/'):\n",
    "            folder_prefix += '/'\n",
    "            \n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name,\n",
    "            Prefix=folder_prefix,\n",
    "            MaxKeys=1\n",
    "        )\n",
    "        \n",
    "        # If there are no contents, the folder doesn't exist\n",
    "        return 'Contents' not in response\n",
    "        \n",
    "    except ClientError as e:\n",
    "        logging.error(f\"Error checking folder existence: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete S3 Files\n",
    "## Delete folder and its contents\n",
    "folder_path='bda/'\n",
    "success = delete_folder_and_contents(bucket_name_kb, folder_path)\n",
    "\n",
    "if success:\n",
    "    # Verify deletion\n",
    "    if is_folder_deleted(bucket_name, folder_path):\n",
    "        print(f\"Successfully deleted folder {folder_path} and its contents\")\n",
    "    else:\n",
    "        print(f\"Deletion operation completed but folder may still exist\")\n",
    "else:\n",
    "    print(f\"Failed to delete folder {folder_path}\")\n",
    "\n",
    "success = delete_folder_and_contents(bucket_name, folder_path)\n",
    "\n",
    "if success:\n",
    "    # Verify deletion\n",
    "    if is_folder_deleted(bucket_name, folder_path):\n",
    "        print(f\"Successfully deleted folder {folder_path} and its contents\")\n",
    "    else:\n",
    "        print(f\"Deletion operation completed but folder may still exist\")\n",
    "else:\n",
    "    print(f\"Failed to delete folder {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebefcf2-c020-41da-9770-35fe9cfdc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Knowledge Base\n",
    "\n",
    "knowledge_base.delete_kb(delete_s3_bucket=True, delete_iam_roles_and_policies=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e875d-5ac0-4d6e-b9aa-a802bbd6d041",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By following this guide, you can effectively harness the power of Amazon Bedrock’s features to build a robust Multimodal RAG application tailored to your specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
