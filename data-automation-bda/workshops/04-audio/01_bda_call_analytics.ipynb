{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a899bfb2-fa00-482e-ab5a-f97daa18d876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T23:22:21.289317Z",
     "iopub.status.busy": "2025-01-20T23:22:21.288559Z",
     "iopub.status.idle": "2025-01-20T23:22:21.296763Z",
     "shell.execute_reply": "2025-01-20T23:22:21.295537Z",
     "shell.execute_reply.started": "2025-01-20T23:22:21.289288Z"
    }
   },
   "source": [
    "# Podcast Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259074a6-a277-4247-a8d1-faa44d0020bf",
   "metadata": {},
   "source": [
    "The tool used in this notebook, Bedrock Data Automation, provides the following rich features for audio data processing: Audio metadata (including speaker counts), Audio summarization, Full transcript with speaker identification, Chapter segmentation, and Content moderation results. Its out of box audio summarization will help to provide future knowledge base application. By the end, you will have a clear understanding of how to handle audio content end-to-end, preparing it for tasks such as topic modeling, question answering, or even generating summaries for quick reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87659b5-90c4-40a4-9c03-931d70ab8955",
   "metadata": {},
   "source": [
    "## Prerequisits\n",
    "You need to have suitable IAM role permission to run this notebook. For IAM role, choose either an existing IAM role in your account or create a new role. The role must the necessary permissions to invoke the BDA, SageMaker and S3 APIs.\n",
    "\n",
    "These IAM policies can be assigned to the role: AmazonBedrockFullAccess, AmazonS3FullAccess, AmazonSageMakerFullAccess, IAMReadOnlyAccess\n",
    "\n",
    "Note: The AdministratorAccess IAM policy can be used, if allowed by security policies at your organization.\n",
    "\n",
    "## Note\n",
    "\n",
    "It is important to run the cells below in order. If you need to re-start the workbook, and have not sucessfully run step 8 to cleanup resources, you will need to login to the AWS Console and delete the project and blueprints created in this workbook. \n",
    "\n",
    "If you get out of order, and unexpected results, you can 'Restart Kernel' from the SageMaker studio Kernel menu. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1289e-d961-4db0-9eef-c2f7ba9d7cfe",
   "metadata": {},
   "source": [
    "## Setup notebook and boto3 clients\n",
    "\n",
    "In this step, we will import some necessary libraries that will be used throughout this notebook. \n",
    "To use Amazon Bedrock Data Automation (BDA) with boto3, you'll need to ensure you have the latest version of the AWS SDK for Python (boto3) installed. Version Boto3 1.35.96 of later is required. \n",
    "\n",
    "Note: At time of Public Preview launch, BDA is available in us-west-2 only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf31c6e-182c-4998-83ce-f60ed4f53b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.37.4 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d3478-84a7-4c29-a5e6-4a3f6c686d4b",
   "metadata": {},
   "source": [
    "In this workshop, a default S3 bucket will be used, and the input and output will be saved under a folder called \"bda\" in the default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc3186-7df3-4e5f-ae7d-2522ce275209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3, json, uuid\n",
    "from time import sleep\n",
    "from IPython.display import JSON, IFrame, Audio, display\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "default_bucket = session.default_bucket()\n",
    "print(f\"Bucket_name: {default_bucket}\")\n",
    "\n",
    "region_name = \"us-west-2\" # can be removed ones BDA is GA and available in other regions.\n",
    "\n",
    "s3 = boto3.client('s3', region_name=region_name)\n",
    "bda_client = boto3.client('bedrock-data-automation', region_name=region_name)\n",
    "bda_runtime_client = boto3.client('bedrock-data-automation-runtime', region_name=region_name)\n",
    "sts_client = boto3.client('sts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa554a34-ae81-4802-9ca5-b4a40e759f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T00:31:53.506949Z",
     "iopub.status.busy": "2025-01-21T00:31:53.506589Z",
     "iopub.status.idle": "2025-01-21T00:31:53.513035Z",
     "shell.execute_reply": "2025-01-21T00:31:53.511550Z",
     "shell.execute_reply.started": "2025-01-21T00:31:53.506925Z"
    }
   },
   "source": [
    "## Create a BDA project\n",
    "To start a BDA job, you need a BDA project, which organizes both standard and custom output configurations. This project is reusable, allowing you to apply the same configuration to process multiple videos that share the same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eeea0-12c1-4cd7-8e40-f8c3d8b3f8be",
   "metadata": {},
   "source": [
    "In the code snippet below, we create a BDA project with standard output configurations for audio modality. These configurations can be tailored to extract only the specific information you need. In this lab, we will enable the below video outputs:\n",
    "- Full audio summary\n",
    "- Chapter segmentation/summary\n",
    "- Transcript\n",
    "- Content moderation (audio)\n",
    "- metadata\n",
    "- Statistics\n",
    "\n",
    "For a complete API reference for creating a BDA project, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation/client/create_data_automation_project.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a03c70-8262-49ad-8298-22d38d094ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name= f'bda-workshop-audio-project-{str(uuid.uuid4())[0:4]}'\n",
    "\n",
    "# delete project if it already exists\n",
    "projects_existing = [project for project in bda_client.list_data_automation_projects()[\"projects\"] if project[\"projectName\"] == project_name]\n",
    "if len(projects_existing) >0:\n",
    "    print(f\"Deleting existing project: {projects_existing[0]}\")\n",
    "    bda_client.delete_data_automation_project(projectArn=projects_existing[0][\"projectArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2510f5-f4f2-4fdd-ac96-4d9edbeeba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_client.create_data_automation_project(\n",
    "    projectName = project_name,\n",
    "    projectDescription='BDA workshop audio sample project',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration={\n",
    "      \"audio\": {\n",
    "    \"extraction\": {\n",
    "      \"category\": {\n",
    "        \"state\": \"ENABLED\", \n",
    "        \"types\": [\"AUDIO_CONTENT_MODERATION\", \"TOPIC_CONTENT_MODERATION\", \"TRANSCRIPT\"]\n",
    "      }\n",
    "    },\n",
    "    \"generativeField\": {\n",
    "      \"state\": \"ENABLED\",\n",
    "      \"types\": [\"AUDIO_SUMMARY\", \"TOPIC_SUMMARY\", \"IAB\"]\n",
    "    }\n",
    "  }\n",
    " }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfd78-a072-4375-b970-d2650909f0de",
   "metadata": {},
   "source": [
    "The `create_data_automation_project` API will return the project ARN, which we will use it to invoke the audio analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ab6c-5249-4364-927c-c5def65a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_project_arn = response.get(\"projectArn\")\n",
    "print(\"BDA audio project ARN:\", audio_project_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b81791-e597-457f-a375-265bc0711525",
   "metadata": {},
   "source": [
    "## Start an asynchronous BDA task to extract and analyze a audio file from podcast\n",
    "In this section, we will use a sample podcast audio, and extract and analyze it using BDA, applying the configuration defined in the BDA project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5d08-0be9-495e-bdc3-27e2b68c0e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T01:16:19.221297Z",
     "iopub.status.busy": "2025-01-21T01:16:19.220673Z",
     "iopub.status.idle": "2025-01-21T01:16:19.225680Z",
     "shell.execute_reply": "2025-01-21T01:16:19.224891Z",
     "shell.execute_reply.started": "2025-01-21T01:16:19.221270Z"
    }
   },
   "source": [
    "### Download and store a sample audio\n",
    "we use the document path to download the document and store it a S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2958eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'podcastdemo.mp3'\n",
    "source_url = 'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/podcastdemo.mp3'\n",
    "\n",
    "!curl {source_url} --output {file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad7889-a3a7-4c82-9766-eb5ecb94acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload an audio file\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "object_name = f'bda/input/{file_name}'\n",
    "\n",
    "default_bucket\n",
    "s3.upload_file(file_name, default_bucket, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b8628-c692-4385-9725-48c4ea3b70cc",
   "metadata": {},
   "source": [
    "Let's display the audio. This short sample contains a short audio about a customer call. Our mission is to extract and summarize this call so the audio file can be accurately represented with text information, related with the call summary and speaker information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd61670-4a65-471a-a75b-14525d923af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and play an MP3 file\n",
    "display(Audio(file_name, autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b33c9-a23c-4dd3-8f66-22d169e598b7",
   "metadata": {},
   "source": [
    "### Start BDA task\n",
    "We will now invoke the BDA API to process the uploaded audio. You need to provide the BDA project ARN that we created at the beginning of the lab and specify an S3 location where BDA will store the output results.\n",
    "\n",
    "For a complete API reference for invoke a BDA async task, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation-runtime/client/invoke_data_automation_async.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681293ae-0023-4eac-9970-af61ec934e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "output_name = f'bda/output/' \n",
    "response = bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={'s3Uri':  f\"s3://{default_bucket}/{object_name}\"},\n",
    "    outputConfiguration={'s3Uri': f\"s3://{default_bucket}/{output_name}\"},\n",
    "    dataAutomationProfileArn= f'arn:aws:bedrock:us-west-2:{account_id}:data-automation-profile/us.data-automation-v1',\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': audio_project_arn,\n",
    "        'stage': 'DEVELOPMENT'\n",
    "    })\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8b65a-1e4e-4dc1-b011-254e7d1cc941",
   "metadata": {},
   "source": [
    "The `invoke_data_automation_async` API is asynchronous. It returns an invocation task identifier, `invocationArn`. We can then use another API `get_data_automation_status` to monitor the task's status until it completes.\n",
    "\n",
    "> In production workloads, an event-driven pattern is recommended. Allow BDA to trigger the next step once the task is complete. This can be achieved by configuring the notificationConfiguration in the invoke task, which will send a notification to a subscribed AWS service, such as a Lambda function. Alternatively, you can set up an S3 trigger on the bucket where BDA will drop the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61061730-ded4-4852-bbf4-d18ab1168c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(\"BDA task started:\", invocation_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13518bf-15eb-4e83-bc4b-955b66974457",
   "metadata": {},
   "source": [
    "In this lab, we will use the loop below to monitor the task by calling the `get_data_automation_status` API every 5 seconds until the task is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9c48c-9347-43ba-8148-1ea9c29479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "status, status_response = None, None\n",
    "while status not in [\"Success\",\"ServiceError\",\"ClientError\"]:\n",
    "    status_response = bda_runtime_client.get_data_automation_status(\n",
    "        invocationArn=invocation_arn\n",
    "    )\n",
    "    status = status_response.get(\"status\")\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} : BDA audio task: {status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "output_config = status_response.get(\"outputConfiguration\",{}).get(\"s3Uri\")\n",
    "print(\"Ouput configureation file:\", output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c617c2-3a71-43dd-b996-d7b9a4d86f08",
   "metadata": {},
   "source": [
    "## Access the BDA analysis result\n",
    "The `get_data_automation_status` API returns an S3 URI containing the result configuration, which provides the S3 location where BDA outputs the extraction results. We will then parse this file to retrieve the result path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee6613-2233-4d5f-abb6-db7795568bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_loc = status_response['outputConfiguration']['s3Uri'].split(\"/job_metadata.json\", 1)[0].split(default_bucket+\"/\")[1]\n",
    "out_loc += \"/0/standard_output/0/result.json\"\n",
    "s3.download_file(default_bucket, out_loc, 'result.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc0557-b53d-4dc4-ae1c-05de559093e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As shown above, the BDA output configuration file contains metadata about the BDA result, including the job ID, status, modality, and the S3 location of the actual result JSON. We will now download this result file to verify the output.\n",
    "\n",
    "We will display the JSON of the Standard Output next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93edb4-e459-4c00-aeb6-31577240aabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "data = json.load(open('result.json'))\n",
    "JSON(data, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f612888-1581-4303-a434-eeacc7aef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(json.dumps(data['statistics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2b413-3f1c-4225-8f60-01c00a256110",
   "metadata": {},
   "source": [
    "## Review the result\n",
    "The BDA audio analysis results provide a detailed breakdown of information, organized by whole audio summary and chapter summary. \n",
    "> A chapter is a sequence of audio section that form a coherent unit of action or narrative within the audio. The segment is generated by BDA service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfee39-d157-4b55-bc6b-6011b82579d5",
   "metadata": {},
   "source": [
    "### Full audio transcription\n",
    "At the audio level, we receive the full transcript based on the audio, with speakers identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26f7df-9abf-457b-b72c-e0bf59f8915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"audio\"][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bee0b-c95c-4082-9109-0bbf1fe69b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[\"audio\"][\"transcript\"][\"representation\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d7a98-648d-4277-9ec7-8717b55211c4",
   "metadata": {},
   "source": [
    "### Audio transcripts with toxicity analysis\n",
    "You can find two fields at the scene level containing audio transcripts and audio moderation data: `audio_segments` and `content_moderation`. The sequence of moderation (toxicity) analysis corresponds to the transcripts. Below, we display them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f2687-79fd-4ef9-be2a-e86da3f4faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_content_moderation(json_data):\n",
    "    # Get the content moderation array\n",
    "    moderation_results = json_data.get('audio', {}).get('content_moderation', [])\n",
    "    \n",
    "    high_risk_segments = []\n",
    "    \n",
    "    for segment in moderation_results:\n",
    "        start_time = segment.get('start_timestamp_millis', 0) / 1000  # Convert to seconds\n",
    "        end_time = segment.get('end_timestamp_millis', 0) / 1000\n",
    "        \n",
    "        # Check each moderation category\n",
    "        for category in segment.get('moderation_categories', []):\n",
    "            if category['confidence'] > 0.5:\n",
    "                high_risk_segments.append({\n",
    "                    'time_range': f\"{start_time:.2f}s - {end_time:.2f}s\",\n",
    "                    'category': category['category'],\n",
    "                    'confidence': category['confidence']\n",
    "                })\n",
    "    \n",
    "    if high_risk_segments:\n",
    "        print(\"⚠️ High Risk Content Detected:\")\n",
    "        for segment in high_risk_segments:\n",
    "            print(f\"Time Range: {segment['time_range']}\")\n",
    "            print(f\"Category: {segment['category']}\")\n",
    "            print(f\"Confidence Score: {segment['confidence']:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"✅ No high-risk content detected (all scores below 0.5)\")\n",
    "        \n",
    "    # Optional: Show overall statistics\n",
    "    print(\"\\nOverall Content Safety Summary:\")\n",
    "    total_segments = len(moderation_results)\n",
    "    print(f\"Total segments analyzed: {total_segments}\")\n",
    "    print(f\"Segments with high risk content: {len(high_risk_segments)}\")\n",
    "analyze_content_moderation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250b163-e118-4fd3-938f-07770a0d07f9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The above message represents the audio segment with its start and end timestamps, and content moderation score for each section. The low content moderation score means the audio content is safe. \n",
    "\n",
    "BDA audio toxicity analysis support the below built-in toxicity categories with confidence score between 0 and 1:\n",
    "- profanity\n",
    "- hate_speech\n",
    "- sexual\n",
    "- insult\n",
    "- violence_or_threat\n",
    "- graphic\n",
    "- harassment_or_abuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b1c24d-1d6a-4192-862b-ec3be2060d17",
   "metadata": {},
   "source": [
    "### Visual content moderation score\n",
    "The following section shows how to visulize the output store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005fd12-21b9-4faf-b9bc-f381450f537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_moderation_summary(json_data):\n",
    "    moderation_results = json_data.get('audio', {}).get('content_moderation', [])\n",
    "    \n",
    "    # Initialize category averages\n",
    "    category_scores = {\n",
    "        'profanity': [],\n",
    "        'hate_speech': [],\n",
    "        'sexual': [],\n",
    "        'insult': [],\n",
    "        'violence_or_threat': [],\n",
    "        'graphic': [],\n",
    "        'harassment_or_abuse': []\n",
    "    }\n",
    "    \n",
    "    # Collect all scores for each category\n",
    "    for segment in moderation_results:\n",
    "        for category in segment.get('moderation_categories', []):\n",
    "            category_scores[category['category']].append(category['confidence'])\n",
    "    \n",
    "    # Calculate averages and prepare data for plotting\n",
    "    categories = []\n",
    "    averages = []\n",
    "    max_scores = []\n",
    "    \n",
    "    for category, scores in category_scores.items():\n",
    "        if scores:\n",
    "            avg_score = sum(scores) / len(scores)\n",
    "            max_score = max(scores)\n",
    "            categories.append(category.replace('_', ' ').title())\n",
    "            averages.append(avg_score)\n",
    "            max_scores.append(max_score)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bars\n",
    "    x = range(len(categories))\n",
    "    bars = plt.bar(x, averages)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title('Average Content Moderation Scores by Category')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Average Confidence Score')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(x, categories, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add a horizontal line at 0.5 to show the threshold\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Risk Threshold (0.5)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set y-axis limits to include some padding\n",
    "    plt.ylim(0, max(max(averages) * 1.2, 0.6))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "generate_moderation_summary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec5d60",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Let's delete the sample files that were downloaded locally and that uploaded to S3. 1. Delete the BDA project you created. 2. Delete files you uploaded to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebc676-e867-4155-b136-556943b7a4b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we use BDA to extract and analyze a sample podcast audio. The BDA output includes audio summary, speaker identification and moderation detections, as specified in the BDA project configuration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
