{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a899bfb2-fa00-482e-ab5a-f97daa18d876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T23:22:21.289317Z",
     "iopub.status.busy": "2025-01-20T23:22:21.288559Z",
     "iopub.status.idle": "2025-01-20T23:22:21.296763Z",
     "shell.execute_reply": "2025-01-20T23:22:21.295537Z",
     "shell.execute_reply.started": "2025-01-20T23:22:21.289288Z"
    }
   },
   "source": [
    "# Extract and analyze a movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259074a6-a277-4247-a8d1-faa44d0020bf",
   "metadata": {},
   "source": [
    "Industries such as Media & Entertainment, Advertising and Sports manage vast inventories of professionally produced videos, including TV shows, movies, news, sports events, documentaries, and more. To effectively extract insights from this type of video content, users require information such as video summaries, chapter-level analysis, IAB classifications for ad targeting, and speaker identification.\n",
    "\n",
    "> [IAB categories](https://smartclip.tv/adtech-glossary/iab-categories/) are standard classifications for web content that are developed by the Interactive Advertising Bureau (IAB). These categories are used to sort advertisers into industries and segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cba547-3ef6-4194-a2e6-315e5abe7963",
   "metadata": {},
   "source": [
    "In this lab, we will use BDA Video to extract and analyze a sample open-source movie [Meridian](https://en.wikipedia.org/wiki/Meridian_(film)), walking through the process and exploring the generated outputs.\n",
    "![video moderation](../static/bda-video-chapter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87659b5-90c4-40a4-9c03-931d70ab8955",
   "metadata": {},
   "source": [
    "## Prerequisits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf31c6e-182c-4998-83ce-f60ed4f53b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install \"boto3>=1.37.4\" \"moviepy==2.1.2\" --upgrade -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94080f3-7a23-4b04-898d-57be31c638a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import utils\n",
    "import sagemaker\n",
    "\n",
    "bda_client = boto3.client('bedrock-data-automation')\n",
    "bda_runtime_client = boto3.client('bedrock-data-automation-runtime')\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d3478-84a7-4c29-a5e6-4a3f6c686d4b",
   "metadata": {},
   "source": [
    "For a self-hosted workshop, we recommend creating a new S3 bucket in the same region where you plan to run the workshop. You can name it `bda-workshop-YOUR_ACCOUNT_ID-YOUR_REGION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc3186-7df3-4e5f-ae7d-2522ce275209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_bucket = sagemaker.Session().default_bucket() # Enter your bucket name here\n",
    "data_prefix = \"bda-workshop/image\"\n",
    "output_prefix = \"bda-workshop/image/ouput\"\n",
    "\n",
    "print('Workshop S3 bucket:', data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8600f82-92d1-46e7-8962-e62a93874c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current AWS account Id and region\n",
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f'Current AWS account Id: {account_id}, region name: {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa554a34-ae81-4802-9ca5-b4a40e759f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T00:31:53.506949Z",
     "iopub.status.busy": "2025-01-21T00:31:53.506589Z",
     "iopub.status.idle": "2025-01-21T00:31:53.513035Z",
     "shell.execute_reply": "2025-01-21T00:31:53.511550Z",
     "shell.execute_reply.started": "2025-01-21T00:31:53.506925Z"
    }
   },
   "source": [
    "## Create a BDA project with a standard output configuration for videos\n",
    "To start a BDA job, you need a BDA project, which organizes both standard and custom output configurations. This project is reusable, allowing you to apply the same configuration to process multiple videos that share the same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eeea0-12c1-4cd7-8e40-f8c3d8b3f8be",
   "metadata": {},
   "source": [
    "In the code snippet below, we create a BDA project with standard output configurations for video modality. These configurations can be tailored to extract only the specific information you need. In this lab, we will enable the below video outputs:\n",
    "- Full video summary\n",
    "- Chapter summaries\n",
    "- IAB categories on the chapter level\n",
    "- Full audio transcript\n",
    "- Text in video with bounding-boxes\n",
    "- Brand & logo in video with bounding-boxes\n",
    "\n",
    "For a complete API reference for creating a BDA project, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation/client/create_data_automation_project.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2510f5-f4f2-4fdd-ac96-4d9edbeeba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_client.create_data_automation_project(\n",
    "    projectName=f'bda-workshop-video-project-{str(uuid.uuid4())[0:4]}',\n",
    "    projectDescription='BDA workshop video sample project',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration={\n",
    "        'video': {\n",
    "            'extraction': {\n",
    "                'category': {\n",
    "                    'state': 'ENABLED',\n",
    "                    'types': ['TEXT_DETECTION','TRANSCRIPT','LOGOS'],\n",
    "                },\n",
    "                'boundingBox': {\n",
    "                    'state': 'ENABLED',\n",
    "                }\n",
    "            },\n",
    "            'generativeField': {\n",
    "                'state': 'ENABLED',\n",
    "                'types': ['VIDEO_SUMMARY','CHAPTER_SUMMARY','IAB'],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfd78-a072-4375-b970-d2650909f0de",
   "metadata": {},
   "source": [
    "The create_data_automation_project API will return the project ARN, which we will use it to invoke the video analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ab6c-5249-4364-927c-c5def65a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_project_arn = response.get(\"projectArn\")\n",
    "print(\"BDA video project ARN:\", video_project_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b81791-e597-457f-a375-265bc0711525",
   "metadata": {},
   "source": [
    "## Start an asynchronous BDA task to extract and analyze a movie\n",
    "In this section, we will use a open-source movie Meridian, and extract and analyze it using BDA, applying the configuration defined in the BDA project. We will then review the output to gain a deeper understanding of how BDA performs video extraction and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5d08-0be9-495e-bdc3-27e2b68c0e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T01:16:19.221297Z",
     "iopub.status.busy": "2025-01-21T01:16:19.220673Z",
     "iopub.status.idle": "2025-01-21T01:16:19.225680Z",
     "shell.execute_reply": "2025-01-21T01:16:19.224891Z",
     "shell.execute_reply.started": "2025-01-21T01:16:19.221270Z"
    }
   },
   "source": [
    "### Prepare the sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad7889-a3a7-4c82-9766-eb5ecb94acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample video\n",
    "sample_video_movie = 'NetflixMeridian.mp4'\n",
    "source_url = f'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/NetflixMeridian.mp4'\n",
    "\n",
    "!curl {source_url} --output {sample_video_movie}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b8628-c692-4385-9725-48c4ea3b70cc",
   "metadata": {},
   "source": [
    "Let's display the video. [Meridian](https://en.wikipedia.org/wiki/Meridian_(film)) is a test movie from Netflix, we use it to showcase how BDA works with video extraction. As you can see, it is a classic-style movie composed of multiple chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd61670-4a65-471a-a75b-14525d923af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(sample_video_movie, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cf849-11ec-4c37-a3dc-e1bb5c51c92e",
   "metadata": {},
   "source": [
    "To analyze the video using BDA, we need to upload it to an S3 bucket that BDA can access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717c142-df4d-4d34-8a98-8f36a5671fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = f'{data_prefix}/{sample_video_movie.split(\"/\")[-1]}'\n",
    "s3_client.upload_file(sample_video_movie, data_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b33c9-a23c-4dd3-8f66-22d169e598b7",
   "metadata": {},
   "source": [
    "### Start BDA task\n",
    "We will now invoke the BDA API to process the uploaded video. You need to provide the BDA project ARN that we created at the beginning of the lab and specify an S3 location where BDA will store the output results.\n",
    "\n",
    "For a complete API reference for invoke a BDA async task, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation-runtime/client/invoke_data_automation_async.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681293ae-0023-4eac-9970-af61ec934e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{s3_key}'\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{output_prefix}'\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': video_project_arn,\n",
    "        'stage': 'DEVELOPMENT'\n",
    "    },\n",
    "    notificationConfiguration={\n",
    "        'eventBridgeConfiguration': {\n",
    "            'eventBridgeEnabled': False\n",
    "        }\n",
    "    },\n",
    "    dataAutomationProfileArn=f'arn:aws:bedrock:{region}:{account_id}:data-automation-profile/us.data-automation-v1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8b65a-1e4e-4dc1-b011-254e7d1cc941",
   "metadata": {},
   "source": [
    "The `invoke_data_automation_async` API is asynchronous. It returns an invocation task identifier, `invocationArn`. We can then use another API `get_data_automation_status` to monitor the task's status until it completes.\n",
    "\n",
    "> In production workloads, an event-driven pattern is recommended. Allow BDA to trigger the next step once the task is complete. This can be achieved by configuring the notificationConfiguration in the invoke task, which will send a notification to a subscribed AWS service, such as a Lambda function. Alternatively, you can set up an S3 trigger on the bucket where BDA will drop the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61061730-ded4-4852-bbf4-d18ab1168c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(\"BDA task started:\", invocation_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13518bf-15eb-4e83-bc4b-955b66974457",
   "metadata": {},
   "source": [
    "In this lab, we will use the loop below to monitor the task by calling the `get_data_automation_status` API every 5 seconds until the task is complete.\n",
    "\n",
    "This video will take ~5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9c48c-9347-43ba-8148-1ea9c29479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "status, status_response = None, None\n",
    "while status not in [\"Success\",\"ServiceError\",\"ClientError\"]:\n",
    "    status_response = bda_runtime_client.get_data_automation_status(\n",
    "        invocationArn=invocation_arn\n",
    "    )\n",
    "    status = status_response.get(\"status\")\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} : BDA video task: {status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "output_config = status_response.get(\"outputConfiguration\",{}).get(\"s3Uri\")\n",
    "print(\"Ouput configureation file:\", output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c617c2-3a71-43dd-b996-d7b9a4d86f08",
   "metadata": {},
   "source": [
    "## Access the BDA analysis result\n",
    "The `get_data_automation_status` API returns an S3 URI containing the result configuration, which provides the S3 location where BDA outputs the extraction results. We will then parse this file to retrieve the result path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1adb7-1620-491f-adb1-b616451d62be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_data = utils.read_json_on_s3(output_config,s3_client)\n",
    "print(json.dumps(config_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc0557-b53d-4dc4-ae1c-05de559093e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As shown above, the BDA output configuration file contains metadata about the BDA result, including the job ID, status, modality, and the S3 location of the actual result JSON. We will now download this result file to verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93edb4-e459-4c00-aeb6-31577240aabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "result_uri = config_data[\"output_metadata\"][0][\"segment_metadata\"][0][\"standard_output_path\"]\n",
    "result_data = utils.read_json_on_s3(result_uri,s3_client)\n",
    "\n",
    "JSON(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2b413-3f1c-4225-8f60-01c00a256110",
   "metadata": {},
   "source": [
    "## Review the result\n",
    "The BDA video analysis result contains a detailed breakdown of information, organized by video and chapter level.\n",
    "> A video chapter is a sequence of shots that form a coherent unit of action or narrative within the video. This feature breaks down the video into meaningful segments based on visual and audible cues, provides timestamps for those segments, and summarizes each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9aaec-d8a1-4b0d-9cb1-fdfcd85cb694",
   "metadata": {},
   "source": [
    "### Full video summary\n",
    "\n",
    "Let's take a look at the video level summary - it distills the key themes, events, and information presented throughout the video into a concise summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293b8fe-0db9-455a-9dfa-dd06a9063d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result_data[\"video\"][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfee39-d157-4b55-bc6b-6011b82579d5",
   "metadata": {},
   "source": [
    "### Full video transcription\n",
    "At the video level, we also receive the full transcript based on the video's audio, with speakers identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bee0b-c95c-4082-9109-0bbf1fe69b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(result_data[\"video\"][\"transcript\"][\"representation\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af915615-73b4-47c7-9ac4-c35637d028af",
   "metadata": {},
   "source": [
    "### Chapter time ranges, summaries, and IAB categories \n",
    "BDA also generates a chapter-level summary, as specified in the project configuration. Additionally, we get more metadata, including the start and end times of each chapter, as well as the [IAB](https://en.wikipedia.org/wiki/Interactive_Advertising_Bureau) categories classified based on the chapter content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fc8b5-fda1-47c9-99a6-d9cf9e5dcd24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for chapter in result_data[\"chapters\"]:\n",
    "    iabs = []\n",
    "    if chapter.get(\"iab_categories\"):\n",
    "        for iab in chapter[\"iab_categories\"]:\n",
    "            iabs.append(iab[\"category\"])\n",
    "        \n",
    "    print(f'[{chapter[\"start_timecode_smpte\"]} - {chapter[\"end_timecode_smpte\"]}] {\", \".join(iabs)}')\n",
    "    print(chapter[\"summary\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d7a98-648d-4277-9ec7-8717b55211c4",
   "metadata": {},
   "source": [
    "### Audio transcript segements\n",
    "Granular transcripts are also available at the chatper level. Under each chatper, you can find a list named `audio_segments` with associated timestamps. This can support additional downstream analysis that requires detailed transcript information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2337c1-2a0b-40a9-81b7-1f3dfe1270bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for chapter in result_data[\"chapters\"]:\n",
    "    for trans in chapter[\"audio_segments\"]:\n",
    "        print(f'[{trans[\"start_timestamp_millis\"]/1000} - {trans[\"end_timestamp_millis\"]/1000}] {trans[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de933514-e57b-45c7-9a7a-af12a528604b",
   "metadata": {},
   "source": [
    "### Frame level text extraction with bounding-boxes and confidence scores\n",
    "Text extraction, along with bounding boxes and confidence scores, is available at the frame level. In the output JSON structure, frames are organized under each chatper with captured timestamp. If text is detected at a given frame, you can find text_words and text_lines included at the frame level.\n",
    "\n",
    "Let's plot the frames for a given chapter with detected text, including their bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ed953-7356-4d09-a1ec-a941908446c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all frames with boundingbox in the given chapter\n",
    "chapter_index = 1\n",
    "utils.plot_text(sample_video_movie, result_data, chapter_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a524581-f396-4696-9b15-ad4f2f94d06f",
   "metadata": {},
   "source": [
    "### Frame level logo detection with bounding-boxes and confidence scores\n",
    "Logo detection is available at the frame level, in a format similar to the text detection output. The metadata is provided at the timestamp level, including bounding boxes and confidence scores.\n",
    "\n",
    "Let's plot the frames for a given chapter with detected logos, including their bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3659c-fb2f-4b9e-b1d0-3e29701b0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all frames with boundingbox in the given chapter\n",
    "chapter_index = 14\n",
    "utils.plot_logo(sample_video_movie, result_data, chapter_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e20d2-dbfe-4332-b1d4-d0aa04145ccc",
   "metadata": {},
   "source": [
    "### Shots in video\n",
    "The BDA video returns a list of shots, each with a start and end time.\n",
    "> A shot is a series of interrelated consecutive pictures taken contiguously by a single camera and representing a continuous action in time and space. \n",
    "\n",
    "In the following cells, we will display screenshots of all the shots, based on the start timestamps from the BDA output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c405936-cce5-4099-beb1-17f5263569b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate shot images\n",
    "images = utils.generate_shot_images(sample_video_movie, result_data, image_width=120)\n",
    "\n",
    "# Ploat the shot images\n",
    "utils.plot_shots(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebc676-e867-4155-b136-556943b7a4b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "BDA video returns highly detailed metadata managed by the configuration. In this lab, we have enabled standard outputs required for media video analysis, using a movie as an example. You can explore the output JSON to discover more details. This lab does not cover moderation detection and analysis; for that, you can refer to the next lab, which uses a social media-style video as an example to better understand the moderation analysis offered by BDA video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f626c2f-2994-4d5e-86f8-1084c319e9ea",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Delete the BDA project, blueprint, image, and result from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31814f0d-5ae7-40d0-8344-4b50050e2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete BDA project\n",
    "response = bda_client.delete_data_automation_project(\n",
    "    projectArn=video_project_arn\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d9f2c-74a7-4427-9632-836a9be25018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete uploaded image from S3\n",
    "s3_client.delete_object(Bucket=data_bucket, Key=s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4953074-9a5c-461b-893b-c5cbf4872b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete results from S3\n",
    "utils.delete_s3_folder(data_bucket, output_config.replace(\"job_metadata.json\",\"\") ,s3_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
