{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt flow RAG example\n",
    "---\n",
    "\n",
    "Amazon Bedrock Flows accelerates the creation, testing, and deployment of user-defined workflows for generative AI applications through an intuitive visual builder. Using Bedrock prompt flows, users can drag, drop and link Prompts, existing Agents, Knowledge Bases, Guardrails and other AWS services. This enables generative AI teams to build a business logic via workflow creations. \n",
    "\n",
    "In this example, we will be building a simple RAG application. We will follow the following steps:\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example focuses on building a basic **RAG (Retrieval-Augmented Generation)** application. The high-level steps are:\n",
    "\n",
    "1. **Create a RAG prompt** and store it in **Bedrock prompt management**.  \n",
    "   - Bedrock prompt management simplifies the creation, evaluation, versioning, and sharing of prompts, ensuring you can easily reuse and maintain them.\n",
    "   - In this example we create two prompts, one for RAG and one as a router prompt. The router prompt is powered by an SLM (`haiku`) to route requests and figure\n",
    "   out whether a question is of one type versus another\n",
    "   \n",
    "1. **Apply a condition** via the router, to route requests to different Knowledge Bases based on different user questions.\n",
    "\n",
    "1. **Create two knowledge bases** that contains sample information about AWS services. One of the knowledge bases contains information about basic AWS services and the other contains information about specific generative AI services.\n",
    "\n",
    "3. **Create a prompt flow** that:\n",
    "   - Takes a user-provided question.\n",
    "   - Routes the request to the right KB based on the condition.\n",
    "   - Retrieve relevant chunks from the knowledge base.\n",
    "   - Sends the retrieved context and user question to a foundation model for an answer along with a RAG prompt stored in the prompt management library.\n",
    "   - Return the final output to the user.\n",
    "\n",
    "\n",
    "[Amazon Bedrock Prompt Management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html) streamlines the creation, evaluation, deployment, and sharing of prompts in the Amazon Bedrock console and via APIs in the SDK. This feature helps developers and business users obtain the best responses from foundation models for their specific use cases.\n",
    "\n",
    "[Amazon Bedrock Prompt Flows](https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html) allows you to easily link multiple foundation models (FMs), prompts, and other AWS services, reducing development time and effort. It introduces a visual builder in the Amazon Bedrock console and a new set of APIs in the SDK, that simplifies the creation of complex generative AI workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making sure we have the lastest version of the Amazon Bedrock SDK, importing the libraries, and setting-up the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time...\n",
    "!pip3 install boto3 botocore matplotlib -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# This is the model that will be used for standard text generation for our\n",
    "# RAG application use case\n",
    "TEXT_GEN_MODEL: str = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# Router model\n",
    "ROUTER_MODEL: str = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "# S3 bucket that contains information on the PDF file that is stored within the Knowledge base\n",
    "FOUNDATIONAL_S3_INPUT_BUCKET: str = \"foundational-aws-frameworks-2039\"\n",
    "GENAI_S3_INPUT_BUCKET: str = \"ai-new-kb-bucket-2039\"\n",
    "# These are the links of the PDF files that will stored within the Knowledge bases\n",
    "URLs: List[str] = ['https://d0.awsstatic.com/whitepapers/aws-overview.pdf', 'https://docs.aws.amazon.com/pdfs/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.pdf']\n",
    "# Local directory where the data is saved\n",
    "DATA_DIR: str = \"data\"\n",
    "# PDF file name to be saved and put in S3\n",
    "PDF_FILE_NAME: str = \"aws-overview.pdf\"\n",
    "PDF_FILE_NAME2: str = \"aws-overview-genai.pdf\"\n",
    "# This is the name of the Knowledge base that will be created\n",
    "KB_NAME_1: str = \"aws-overview-kb\"\n",
    "KB_DESC_1: str = \"\"\"Use this KB to retrieve information about AWS services that the user is asking about.\"\"\"\n",
    "\n",
    "# This is the second knowledge base that we will add a routing condition to\n",
    "KB_NAME_2: str = \"aws-genai-kb-2039\"\n",
    "KB_DESC_2: str = \"\"\"Use this KB to retrieve information about specifically the genAI AWS services that the user is asking about.\"\"\"\n",
    "\n",
    "# This is the lambda function that will be used to fetch the top 5 chunks from the knowledge base\n",
    "KB_LAMBDA_FUNCTION_NAME: str = 'demo-lambda-function'\n",
    "KB_LAMBDA_FUNCTION: str = 'kb-lambda-function.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the file locally, put it in S3, and then sync it with the Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map URLs to their corresponding file names and buckets\n",
    "url_to_file_and_bucket = {\n",
    "    URLs[0]: {\"filename\": PDF_FILE_NAME, \"bucket\": FOUNDATIONAL_S3_INPUT_BUCKET},\n",
    "    URLs[1]: {\"filename\": PDF_FILE_NAME2, \"bucket\": GENAI_S3_INPUT_BUCKET}\n",
    "}\n",
    "\n",
    "# Download the files\n",
    "for url, file_info in url_to_file_and_bucket.items():\n",
    "    filename = file_info[\"filename\"]\n",
    "    logger.info(f\"Downloading file from {url}\")\n",
    "    try:\n",
    "        if not os.path.exists(DATA_DIR):\n",
    "            os.makedirs(DATA_DIR)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save the file with its corresponding name\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logger.info(f\"File successfully downloaded to {file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error downloading file: {e}\")\n",
    "\n",
    "# Upload files to their respective S3 buckets\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for url, file_info in url_to_file_and_bucket.items():\n",
    "    filename = file_info[\"filename\"]\n",
    "    bucket = file_info[\"bucket\"]\n",
    "    try:\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            file_path,           \n",
    "            bucket,     # Use the corresponding bucket for each file\n",
    "            filename   # Upload to root of bucket\n",
    "        )\n",
    "        logger.info(f\"Successfully uploaded {filename} to {bucket}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the current AWS region\n",
    "region = boto3.client('sts').meta.region_name # change to another region if you do not want\n",
    "# the region to be dynamically fetched\n",
    "logger.info(f\"Current AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application Prompt\n",
    "\n",
    "Let's create our sample RAG application prompt by leveraging on Prompt Management for Amazon Bedrock. Here, you can adjust the sample prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"aws-expert-assistant\",\n",
    "    description = \"Prompt template for AWS expert assistant to analyze content and answer user questions\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "            \"text\": {\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0,\n",
    "            }\n",
    "            },\n",
    "            \"modelId\": TEXT_GEN_MODEL,\n",
    "            \"name\": \"variantOne\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"question\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"context\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": \"\"\"\n",
    "<system>You are an expert AWS assistant with deep knowledge of AWS services, architectures, and best practices. Your role is to carefully analyze provided content and answer user questions with accurate, detailed, and technically precise information.</system>\n",
    "\n",
    "Refer to the user question below in the user_question xml tags:\n",
    "<user_question>\n",
    "{{question}}\n",
    "</user_question>\n",
    "\n",
    "Here is the content to analyze to answer the user question in the content xml tags:\n",
    "<content>xw\n",
    "{{context}}\n",
    "</content>\n",
    "\n",
    "<assistant>I'll help you by analyzing the content provided and answering your specific question about AWS. Let me break this down systematically:\n",
    "\n",
    "1. First, I'll carefully examine your question and the provided content\n",
    "2. Then, I'll provide a detailed, technically accurate answer\n",
    "3. I'll ensure to include relevant AWS-specific details and best practices\n",
    "4. If any information is unclear or missing, I'll note that in my response\n",
    "\n",
    "Here's my answer:</assistant>\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variantOne\"\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "promptTextGenId = response[\"id\"]\n",
    "promptTextGenArn = response[\"arn\"]\n",
    "promptTextGenName = response[\"name\"]\n",
    "logger.info(f\"Prompt ID: {promptTextGenId}\\nPrompt ARN: {promptTextGenArn}\\nPrompt Name: {promptTextGenName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a draft prompt, we can create a version from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptTextGenId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt router. This prompt is powered by an SLM and assists in giving a condition to the type of question asked\n",
    "---\n",
    "\n",
    "This prompt is invoked using `llama3.2 1b` and it determines whether the question is about foundational AWS services or generative AI services. Based on either, a condition will be applied that routes the request to the respective KB to retrieve and generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"aws-router-assistant\",\n",
    "    description = \"Prompt template for routing user questions based on foundational and AI aws services\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "            \"text\": {\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0,\n",
    "            }\n",
    "            },\n",
    "            \"modelId\": ROUTER_MODEL,\n",
    "            \"name\": \"variantOne\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"question\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"context\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": \"\"\"\n",
    "Based on the user question, determine if the question is more related to foundational AWS knowledge, \n",
    "or more tailored to ai/ml or generative ai. If it is tailored to foundational knowledge, answer with only one word: \"foundational\". \n",
    "If it is tailored to AI, then answer with only one word: \"ai\". Regardless of the question, you have to answer with one word of the two. \n",
    "\n",
    "Refer to the question below:\n",
    "{{question}}\n",
    "\n",
    "If your response is \"foundational\", it should be in lower case only and same for \"ai\". Only respond with \"ai\" if the question is about SageMaker, Bedrock, \n",
    "or any compute question for machine learning workloads.\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variantOne\"\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "promptRouterId = response[\"id\"]\n",
    "promptRouterArn = response[\"arn\"]\n",
    "promptRouterName = response[\"name\"]\n",
    "logger.info(f\"Prompt ID: {promptRouterId}\\nPrompt ARN: {promptRouterArn}\\nPrompt Name: {promptRouterName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptRouterId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Knowledge Base\n",
    "---\n",
    "\n",
    "In this portion of the notebook, we will create a knowledge base and store information about AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from utils import *\n",
    "from utils.knowledge_base_helper import (\n",
    "    KnowledgeBasesForAmazonBedrock\n",
    ")\n",
    "from utils.lambda_utils import *\n",
    "\n",
    "# Initialize the KB class\n",
    "kb = KnowledgeBasesForAmazonBedrock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a generic knowledge base for all the basic AWS information\n",
    "---\n",
    "\n",
    "This is the first knowledge base which will answer basic user questions on foundational AWS questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    generic_aws_kb_id, generic_aws_ds_id = kb.create_or_retrieve_knowledge_base(\n",
    "        KB_NAME_1,\n",
    "        KB_DESC_1,\n",
    "        FOUNDATIONAL_S3_INPUT_BUCKET,\n",
    "    )\n",
    "    logger.info(f\"Knowledge Base ID: {generic_aws_kb_id}\")\n",
    "    logger.info(f\"Data Source ID: {generic_aws_ds_id}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred while creating the KB: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a specific GenAI knowledge base for all the advanced, generative AI related questions\n",
    "---\n",
    "\n",
    "This is the second knowledge base which will answer 300-400 level type questions on Generative AI services at AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "try:\n",
    "    genai_kb_id, genai_ds_id = kb.create_or_retrieve_knowledge_base(\n",
    "        KB_NAME_2,\n",
    "        KB_DESC_2,\n",
    "        GENAI_S3_INPUT_BUCKET,\n",
    "        \"amazon.titan-embed-text-v2:0\",\n",
    "        # adding a parameter to make new policies for the new kb\n",
    "        'newpolicies'\n",
    "    )\n",
    "    logger.info(f\"Knowledge Base ID: {genai_kb_id}\")\n",
    "    logger.info(f\"Data Source ID: {genai_ds_id}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred while creating the KB: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronize the Knowledge Base with Data in `S3`\n",
    "---\n",
    "\n",
    "Now we will sync the data from the S3 bucket into the AWS expert knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync knowledge base for both knowledge bases\n",
    "kb.synchronize_data(generic_aws_kb_id, generic_aws_ds_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb.synchronize_data(genai_kb_id, genai_ds_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the knowledge base in a lambda function\n",
    "---\n",
    "\n",
    "Next, we will wrap the function above in a lambda function and then invoke the lambda function to get the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the Lambda. This lambda will return the top 5 chunks \n",
    "# from the knowledge base that contains information on the data that\n",
    "# is inputted\n",
    "function_arn = create_kb_lambda(\n",
    "    lambda_function_name=KB_LAMBDA_FUNCTION_NAME,\n",
    "    source_code_file=os.path.join('utils', KB_LAMBDA_FUNCTION),\n",
    "    region=region,\n",
    "    kb_id=generic_aws_kb_id\n",
    ")\n",
    "\n",
    "logger.info(f\"Lambda function ARN: {function_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Application Flow\n",
    "---\n",
    "\n",
    "Now, we have created a RAG Application prompt, a knowledge base that contains information about AWS services, and have wrapped the knowledge base in a lambda function so that when the user asks a question, the top 5 chunks from the knowledge base are retrieved. Once that is done, the chunks and the user question are fed into a customized prompt and the answer is given back to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need an AWS IAM role for creating the Prompt Flow in Amazon Bedrock. If you already have a role with your permissions you can directly replace the ```flowRole``` variable with your role's ARN.\n",
    "\n",
    "For simplicity in this example we'll create a new role and attach the ```AmazonBedrockFullAccess``` policy to it. In general, it's recommended that you further limit the policies with conditions.\n",
    "\n",
    "You can check further details in the [How Prompt Flows for Amazon Bedrock works](https://docs.aws.amazon.com/bedrock/latest/userguide/flows-how-it-works.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Create the role if it doesn't exist\n",
    "try:\n",
    "    response = iam.create_role(\n",
    "        RoleName='MyBedrockFlowsRole',\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                        \"Service\": [\n",
    "                            \"bedrock.amazonaws.com\",\n",
    "                            \"lambda.amazonaws.com\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    )\n",
    "    flowRole = response['Role']['Arn']\n",
    "except iam.exceptions.EntityAlreadyExistsException:\n",
    "    flowRole = f\"arn:aws:iam::{boto3.client('sts').get_caller_identity()['Account']}:role/MyBedrockFlowsRole\"\n",
    "\n",
    "# Attach necessary policies\n",
    "policies_to_attach = [\n",
    "    'arn:aws:iam::aws:policy/AmazonBedrockFullAccess',\n",
    "    'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "]\n",
    "\n",
    "for policy in policies_to_attach:\n",
    "    try:\n",
    "        iam.attach_role_policy(\n",
    "            RoleName='MyBedrockFlowsRole',\n",
    "            PolicyArn=policy\n",
    "        )\n",
    "    except iam.exceptions.NoSuchEntityException:\n",
    "        print(f\"Role doesn't exist for policy: {policy}\")\n",
    "    except iam.exceptions.LimitExceededException:\n",
    "        print(f\"Policy already attached: {policy}\")\n",
    "\n",
    "# Create inline policy for Lambda invocation\n",
    "lambda_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                function_arn\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    iam.put_role_policy(\n",
    "        RoleName='MyBedrockFlowsRole',\n",
    "        PolicyName='LambdaInvokePolicy',\n",
    "        PolicyDocument=json.dumps(lambda_policy)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error attaching Lambda invoke policy: {e}\")\n",
    "\n",
    "logger.info(f'Using flowRole: {flowRole}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = bedrock_agent.create_flow(\n",
    "    name=f\"RAGFlow-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    description=\"RAG Application Flow that uses KB and custom prompt to answer questions\",\n",
    "    executionRoleArn=flowRole,\n",
    "    definition={\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"name\": \"FlowInputNode\",\n",
    "                \"type\": \"Input\",\n",
    "                \"configuration\": {\"input\": {}},\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"document\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GetKBChunks\",\n",
    "                \"type\": \"LambdaFunction\",\n",
    "                \"configuration\": {\n",
    "                    \"lambdaFunction\": {\n",
    "                        \"lambdaArn\": function_arn\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\n",
    "                        \"expression\": \"$.data\",\n",
    "                        \"name\": \"input\",\n",
    "                        \"type\": \"String\"\n",
    "                    }\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"functionResponse\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GenerateResponse\",\n",
    "                \"type\": \"Prompt\",\n",
    "                \"configuration\": {\n",
    "                    \"prompt\": {\n",
    "                        \"sourceConfiguration\": {\n",
    "                            \"resource\": {\n",
    "                                \"promptArn\": promptTextGenArn\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"question\", \"type\": \"String\"},\n",
    "                    {\"expression\": \"$.data\", \"name\": \"context\", \"type\": \"String\"}\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"modelCompletion\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Prompt_1\",\n",
    "                \"type\": \"Prompt\",\n",
    "                \"configuration\": {\n",
    "                    \"prompt\": {\n",
    "                        \"sourceConfiguration\": {\n",
    "                            \"resource\": {\n",
    "                                \"promptArn\": promptRouterArn\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"question\", \"type\": \"String\"}\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"modelCompletion\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"genai_kb\",\n",
    "                \"type\": \"KnowledgeBase\",\n",
    "                \"configuration\": {\n",
    "                    \"knowledgeBase\": {\n",
    "                        \"knowledgeBaseId\": genai_kb_id,\n",
    "                        \"modelId\": TEXT_GEN_MODEL\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"retrievalQuery\", \"type\": \"String\"}\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"outputText\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Prompt_genAI\",\n",
    "                \"type\": \"Prompt\",\n",
    "                \"configuration\": {\n",
    "                    \"prompt\": {\n",
    "                        \"sourceConfiguration\": {\n",
    "                            \"resource\": {\n",
    "                                \"promptArn\": promptTextGenArn\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"question\", \"type\": \"String\"},\n",
    "                    {\"expression\": \"$.data\", \"name\": \"context\", \"type\": \"String\"}\n",
    "                ],\n",
    "                \"outputs\": [\n",
    "                    {\"name\": \"modelCompletion\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ConditionNode\",\n",
    "                \"type\": \"Condition\",\n",
    "                \"configuration\": {\n",
    "                    \"condition\": {\n",
    "                        \"conditions\": [\n",
    "                            {\n",
    "                                \"expression\": \"conditionInput == \\\"foundational\\\"\",\n",
    "                                \"name\": \"Condition2\"\n",
    "                            },\n",
    "                            {\"name\": \"default\"}\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"conditionInput\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"End\",\n",
    "                \"type\": \"Output\",\n",
    "                \"configuration\": {\"output\": {}},\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"document\", \"type\": \"String\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FlowOutputNode_1\",\n",
    "                \"type\": \"Output\",\n",
    "                \"configuration\": {\"output\": {}},\n",
    "                \"inputs\": [\n",
    "                    {\"expression\": \"$.data\", \"name\": \"document\", \"type\": \"String\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"connections\": [\n",
    "            {\n",
    "                \"name\": \"FlowInputNodeToGetKBChunks\",\n",
    "                \"source\": \"FlowInputNode\",\n",
    "                \"target\": \"GetKBChunks\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"input\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FlowInputNodeToGenerateResponse\",\n",
    "                \"source\": \"FlowInputNode\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GetKBChunksToGenerateResponse\",\n",
    "                \"source\": \"GetKBChunks\",\n",
    "                \"target\": \"GenerateResponse\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"functionResponse\",\n",
    "                        \"targetInput\": \"context\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"GenerateResponseToEnd\",\n",
    "                \"source\": \"GenerateResponse\",\n",
    "                \"target\": \"End\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"modelCompletion\",\n",
    "                        \"targetInput\": \"document\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FlowInputNodeToPrompt_1\",\n",
    "                \"source\": \"FlowInputNode\",\n",
    "                \"target\": \"Prompt_1\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Prompt_1ToConditionNode\",\n",
    "                \"source\": \"Prompt_1\",\n",
    "                \"target\": \"ConditionNode\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"modelCompletion\",\n",
    "                        \"targetInput\": \"conditionInput\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FlowInputNodeTogenai_kb\",\n",
    "                \"source\": \"FlowInputNode\",\n",
    "                \"target\": \"genai_kb\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"retrievalQuery\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"genai_kbToPrompt_genAI\",\n",
    "                \"source\": \"genai_kb\",\n",
    "                \"target\": \"Prompt_genAI\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"outputText\",\n",
    "                        \"targetInput\": \"context\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FlowInputNodeToPrompt_genAI\",\n",
    "                \"source\": \"FlowInputNode\",\n",
    "                \"target\": \"Prompt_genAI\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"document\",\n",
    "                        \"targetInput\": \"question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"Prompt_genAIToFlowOutputNode_1\",\n",
    "                \"source\": \"Prompt_genAI\",\n",
    "                \"target\": \"FlowOutputNode_1\",\n",
    "                \"type\": \"Data\",\n",
    "                \"configuration\": {\n",
    "                    \"data\": {\n",
    "                        \"sourceOutput\": \"modelCompletion\",\n",
    "                        \"targetInput\": \"document\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ConditionNodeDefaultToGenai_kb\",\n",
    "                \"source\": \"ConditionNode\",\n",
    "                \"target\": \"genai_kb\",\n",
    "                \"type\": \"Conditional\",\n",
    "                \"configuration\": {\n",
    "                    \"conditional\": {\n",
    "                        \"condition\": \"default\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ConditionNodeCondition2ToGetKBChunks\",\n",
    "                \"source\": \"ConditionNode\",\n",
    "                \"target\": \"GetKBChunks\",\n",
    "                \"type\": \"Conditional\",\n",
    "                \"configuration\": {\n",
    "                    \"conditional\": {\n",
    "                        \"condition\": \"Condition2\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "flowId = response[\"id\"]\n",
    "flowArn = response[\"arn\"]\n",
    "flowName = response[\"name\"]\n",
    "\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "logger.info(f\"Flow ID: {flowId}\\nFlow ARN: {flowArn}\\nFlow Name: {flowName}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your Bedrock prompt flow should look like on the console:\n",
    "\n",
    "![./bedrock-prompt-flow](prompt_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare teh flow, so that it builds and validates our flow creation\n",
    "response = bedrock_agent.prepare_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.get_flow(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_version(\n",
    "    flowIdentifier = flowId\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create flow alises, so that we can point our application front-ends and any other integrations to these. \n",
    "# This allows creating new versions without impacting our service.\n",
    "response = bedrock_agent.create_flow_alias(\n",
    "    flowIdentifier = flowId,\n",
    "    name = flowName,\n",
    "    description = \"Alias for my test flow in the customer service use case\",\n",
    "    routingConfiguration = [\n",
    "        {\n",
    "            \"flowVersion\": \"1\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "logger.info(json.dumps(response, indent=2, default=str))\n",
    "flowAliasId = response['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke a Flow\n",
    "---\n",
    "\n",
    "Now that we have learned how to create and manage flows, we can test these with invocations.\n",
    "\n",
    "**Note**: You can invoke flows from any application front-end or your own systems as required. It effectively exposes all the logic of your flow through an Agent Endpoint API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime = boto3.client(service_name = 'bedrock-agent-runtime', region_name = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon SageMaker is a fully-managed machine learning service from AWS that allows developers and data scientists to quickly build, train, and deploy machine learning models at any scale.\n",
      "\n",
      "The key features and capabilities of Amazon SageMaker include:\n",
      "\n",
      "- Removes the heavy lifting of building machine learning models by providing modules that can be used together or independently for data labeling, model training, tuning, deployment, and inference.\n",
      "\n",
      "- Integrates with Amazon SageMaker Ground Truth which helps build accurate training datasets through easy access to public and private human labelers as well as automatic labeling using machine learning.\n",
      "\n",
      "- Provides managed compute resources like GPU instances for training models, removing the need to provision hardware.\n",
      "\n",
      "- Supports all the popular machine learning frameworks like TensorFlow, PyTorch, Apache MXNet, Chainer, Scikit-learn, etc.\n",
      "\n",
      "- Enables one-click deployment of trained models to production with automatic scaling and provisioning.\n",
      "\n",
      "- Offers tools for monitoring model performance and managing drift over time.\n",
      "\n",
      "- Provides pre-built algorithms and pre-trained models for common use cases like image classification, text analysis, etc.\n",
      "\n",
      "In summary, SageMaker abstracts away the heavy lifting of building and operationalizing machine learning workflows, allowing developers to focus on the ML problem they want to solve rather than the complex infrastructure management.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_agent_runtime.invoke_flow(\n",
    "    flowIdentifier = flowId,\n",
    "    flowAliasIdentifier = flowAliasId,\n",
    "    inputs = [\n",
    "        { \n",
    "            \"content\": { \n",
    "                \"document\": \"Hi, What is Amazon SageMaker?\"\n",
    "            },\n",
    "            \"nodeName\": \"FlowInputNode\",\n",
    "            \"nodeOutputName\": \"document\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the event stream\n",
    "for event in response[\"responseStream\"]:\n",
    "    if \"flowOutputEvent\" in event:\n",
    "        # Extract just the answer content\n",
    "        answer = event[\"flowOutputEvent\"][\"content\"][\"document\"]\n",
    "        # Remove the <answer> tags if present\n",
    "        answer = answer.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "        print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Simple Notification Service (Amazon SNS) is a highly available, durable, secure, fully managed pub/sub messaging service provided by AWS. Its main purpose is to decouple and scale microservices, distributed systems, and serverless applications.\n",
      "\n",
      "With SNS, you can send individual messages or fan-out messages to a large number of subscribers through topics. These subscribers can be other AWS services like SQS queues, Lambda functions, HTTP/S webhooks, or even mobile/email notifications to end users.\n",
      "\n",
      "Some key use cases and features of SNS include:\n",
      "\n",
      "- Microservices communication: SNS allows your decoupled services to publish and consume messages asynchronously with high throughput.\n",
      "\n",
      "- Fan-out messaging: Publish once to an SNS topic and fan-out messages in parallel to multiple subscriber systems/endpoints.\n",
      "\n",
      "- Mobile push notifications: You can send push notifications to mobile apps through SNS integration with mobile platforms.\n",
      "\n",
      "- SMS and email: SNS supports sending SMS text messages and email notifications directly.\n",
      "\n",
      "- Fully managed: SNS is a serverless service, so AWS handles provisioning, scaling, patching of the messaging infrastructure.\n",
      "\n",
      "- High availability and durability: Messages are redundantly stored across multi-AZ deployment for reliability.\n",
      "\n",
      "SNS integrates closely with other AWS services like SQS, Lambda, CloudWatch for monitoring, and IAM for access control. It supports multiple protocols like HTTP/S, AWS SQS, AWS Lambda, SMS, email, and mobile push. Overall, SNS provides a scalable, flexible, and managed pub/sub messaging service for modern distributed architectures.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_agent_runtime.invoke_flow(\n",
    "    flowIdentifier = flowId,\n",
    "    flowAliasIdentifier = flowAliasId,\n",
    "    inputs = [\n",
    "        { \n",
    "            \"content\": { \n",
    "                \"document\": \"I have no idea what Amazon SNS is for. Could you tell me?\"\n",
    "            },\n",
    "            \"nodeName\": \"FlowInputNode\",\n",
    "            \"nodeOutputName\": \"document\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process the event stream\n",
    "for event in response[\"responseStream\"]:\n",
    "    if \"flowOutputEvent\" in event:\n",
    "        # Extract just the answer content\n",
    "        answer = event[\"flowOutputEvent\"][\"content\"][\"document\"]\n",
    "        # Remove the <answer> tags if present\n",
    "        answer = answer.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "        print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
