{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Prompt Caching and Routing Workshop\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates Amazon Bedrock's prompt caching and routing capabilities using the latest Claude models. You'll learn how to reduce latency and costs through intelligent prompt caching and how to route requests to optimal models based on your specific needs.\n",
    "\n",
    "**Key Learning Outcomes:**\n",
    "- Implement prompt caching to reduce costs and latency\n",
    "- Use prompt routing for intelligent model selection\n",
    "- Understand best practices for Bedrock API integration\n",
    "- Monitor performance and usage statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context or Details about feature/use case\n",
    "\n",
    "### Prompt Caching\n",
    "Prompt caching allows you to cache frequently used prompts, reducing both latency and costs for subsequent requests. This is particularly useful for:\n",
    "- Document analysis workflows\n",
    "- Multi-turn conversations\n",
    "- Repetitive query patterns\n",
    "\n",
    "### Prompt Routing\n",
    "Prompt routing intelligently directs requests to the most appropriate model based on:\n",
    "- Query complexity\n",
    "- Cost optimization requirements\n",
    "- Performance needs\n",
    "- Model capabilities\n",
    "\n",
    "### Supported Models\n",
    "- **Claude Haiku 4.5**: Fast, cost-effective for simple tasks\n",
    "- **Claude Sonnet 4.5**: Balanced performance and cost\n",
    "- **Claude Opus 4.1**: Most capable for complex reasoning\n",
    "- **Amazon Nova Models**: Latest AWS-native models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **AWS Account** with appropriate permissions\n",
    "2. **Amazon Bedrock access** with Claude models enabled\n",
    "3. **AWS CLI configured** with credentials\n",
    "4. **Python 3.8+** installed\n",
    "5. **Required Python packages** (installed in Setup section)\n",
    "\n",
    "### Required AWS Permissions\n",
    "Your AWS credentials need the following permissions:\n",
    "- `bedrock:InvokeModel`\n",
    "- `bedrock:ListFoundationModels`\n",
    "- `bedrock:GetModelInvocationLoggingConfiguration`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 streamlit pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "print(\"âœ… Setup complete! Bedrock client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code with comments starts here\n",
    "\n",
    "### Model Manager Class\n",
    "\n",
    "First, let's create a model manager to handle different Claude models and their configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manages Bedrock model selection and configuration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'haiku': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            'sonnet': 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
    "            'opus': 'anthropic.claude-3-opus-20240229-v1:0'\n",
    "        }\n",
    "        \n",
    "    def get_model_id(self, model_name: str) -> str:\n",
    "        \"\"\"Get the full model ID for a given model name\"\"\"\n",
    "        return self.models.get(model_name.lower(), self.models['sonnet'])\n",
    "    \n",
    "    def list_available_models(self) -> List[str]:\n",
    "        \"\"\"List all available model names\"\"\"\n",
    "        return list(self.models.keys())\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager()\n",
    "print(f\"Available models: {model_manager.list_available_models()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Service Class\n",
    "\n",
    "Now let's create a service class to handle Bedrock API interactions with caching capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockService:\n",
    "    \"\"\"Service class for Bedrock API interactions with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.cache = {}  # Simple in-memory cache\n",
    "        self.cache_stats = {'hits': 0, 'misses': 0}\n",
    "    \n",
    "    def _generate_cache_key(self, model_id: str, prompt: str) -> str:\n",
    "        \"\"\"Generate a cache key for the prompt\"\"\"\n",
    "        return f\"{model_id}:{hash(prompt)}\"\n",
    "    \n",
    "    def invoke_model_with_cache(self, model_id: str, prompt: str, use_cache: bool = True) -> Dict:\n",
    "        \"\"\"Invoke model with optional caching\"\"\"\n",
    "        cache_key = self._generate_cache_key(model_id, prompt)\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache and cache_key in self.cache:\n",
    "            self.cache_stats['hits'] += 1\n",
    "            print(\"ðŸŽ¯ Cache HIT - Using cached response\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Cache miss - make API call\n",
    "        self.cache_stats['misses'] += 1\n",
    "        print(\"ðŸ”„ Cache MISS - Making API call\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare request body\n",
    "        body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Make API call\n",
    "        response = self.client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body)\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        \n",
    "        # Add timing information\n",
    "        response_body['latency_ms'] = round((time.time() - start_time) * 1000, 2)\n",
    "        response_body['timestamp'] = datetime.now().isoformat()\n",
    "        \n",
    "        # Cache the response\n",
    "        if use_cache:\n",
    "            self.cache[cache_key] = response_body\n",
    "        \n",
    "        return response_body\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        total = self.cache_stats['hits'] + self.cache_stats['misses']\n",
    "        hit_rate = (self.cache_stats['hits'] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cache_hits': self.cache_stats['hits'],\n",
    "            'cache_misses': self.cache_stats['misses'],\n",
    "            'hit_rate_percent': round(hit_rate, 2),\n",
    "            'cached_items': len(self.cache)\n",
    "        }\n",
    "\n",
    "# Initialize Bedrock service\n",
    "bedrock_service = BedrockService(bedrock_client)\n",
    "print(\"âœ… Bedrock service initialized with caching capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Router Class\n",
    "\n",
    "Let's create a prompt router that intelligently selects the best model based on query characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptRouter:\n",
    "    \"\"\"Intelligent prompt routing based on query characteristics\"\"\"\n",
    "    \n",
    "    def __init__(self, model_manager: ModelManager):\n",
    "        self.model_manager = model_manager\n",
    "        self.routing_stats = {}\n",
    "    \n",
    "    def analyze_query_complexity(self, prompt: str) -> str:\n",
    "        \"\"\"Analyze prompt complexity and return complexity level\"\"\"\n",
    "        word_count = len(prompt.split())\n",
    "        \n",
    "        # Simple heuristics for complexity\n",
    "        complex_keywords = ['analyze', 'compare', 'evaluate', 'reasoning', 'complex', 'detailed']\n",
    "        simple_keywords = ['summarize', 'list', 'what is', 'define', 'simple']\n",
    "        \n",
    "        has_complex = any(keyword in prompt.lower() for keyword in complex_keywords)\n",
    "        has_simple = any(keyword in prompt.lower() for keyword in simple_keywords)\n",
    "        \n",
    "        if word_count > 100 or has_complex:\n",
    "            return 'complex'\n",
    "        elif word_count < 20 or has_simple:\n",
    "            return 'simple'\n",
    "        else:\n",
    "            return 'medium'\n",
    "    \n",
    "    def route_prompt(self, prompt: str, priority: str = 'balanced') -> Tuple[str, str]:\n",
    "        \"\"\"Route prompt to optimal model based on complexity and priority\"\"\"\n",
    "        complexity = self.analyze_query_complexity(prompt)\n",
    "        \n",
    "        # Routing logic\n",
    "        if priority == 'cost':\n",
    "            model_name = 'haiku'  # Always use cheapest\n",
    "        elif priority == 'performance':\n",
    "            model_name = 'opus'   # Always use most capable\n",
    "        else:  # balanced\n",
    "            if complexity == 'simple':\n",
    "                model_name = 'haiku'\n",
    "            elif complexity == 'complex':\n",
    "                model_name = 'opus'\n",
    "            else:\n",
    "                model_name = 'sonnet'\n",
    "        \n",
    "        # Track routing decisions\n",
    "        if model_name not in self.routing_stats:\n",
    "            self.routing_stats[model_name] = 0\n",
    "        self.routing_stats[model_name] += 1\n",
    "        \n",
    "        model_id = self.model_manager.get_model_id(model_name)\n",
    "        \n",
    "        print(f\"ðŸŽ¯ Routing Decision: {complexity} complexity â†’ {model_name} model\")\n",
    "        \n",
    "        return model_id, model_name\n",
    "    \n",
    "    def get_routing_stats(self) -> Dict:\n",
    "        \"\"\"Get routing statistics\"\"\"\n",
    "        return self.routing_stats.copy()\n",
    "\n",
    "# Initialize prompt router\n",
    "prompt_router = PromptRouter(model_manager)\n",
    "print(\"âœ… Prompt router initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Prompt Caching in Action\n",
    "\n",
    "Let's demonstrate how prompt caching works by making repeated requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample document for caching demo\n",
    "sample_document = \"\"\"\n",
    "Amazon Web Services (AWS) is a comprehensive cloud computing platform provided by Amazon. \n",
    "It offers over 200 fully featured services from data centers globally. AWS serves millions \n",
    "of customers including startups, large enterprises, and government agencies. Key services \n",
    "include compute power, database storage, content delivery, and machine learning capabilities.\n",
    "\"\"\"\n",
    "\n",
    "# First request - will be cached\n",
    "prompt1 = f\"Based on this document: {sample_document}\\n\\nQuestion: What is AWS?\"\n",
    "model_id = model_manager.get_model_id('sonnet')\n",
    "\n",
    "print(\"=== First Request (Cache Miss Expected) ===\")\n",
    "response1 = bedrock_service.invoke_model_with_cache(model_id, prompt1, use_cache=True)\n",
    "print(f\"Response: {response1['content'][0]['text'][:100]}...\")\n",
    "print(f\"Latency: {response1['latency_ms']}ms\")\n",
    "print()\n",
    "\n",
    "# Second identical request - should hit cache\n",
    "print(\"=== Second Identical Request (Cache Hit Expected) ===\")\n",
    "response2 = bedrock_service.invoke_model_with_cache(model_id, prompt1, use_cache=True)\n",
    "print(f\"Response: {response2['content'][0]['text'][:100]}...\")\n",
    "print(f\"Latency: {response2['latency_ms']}ms\")\n",
    "print()\n",
    "\n",
    "# Display cache statistics\n",
    "cache_stats = bedrock_service.get_cache_stats()\n",
    "print(\"=== Cache Performance ===\")\n",
    "for key, value in cache_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Intelligent Prompt Routing\n",
    "\n",
    "Now let's see how the prompt router selects different models based on query complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",  # Simple\n",
    "    \"Explain the differences between supervised and unsupervised learning algorithms, including their use cases and performance characteristics.\",  # Complex\n",
    "    \"List the main AWS compute services.\",  # Simple\n",
    "    \"Analyze the trade-offs between microservices and monolithic architectures in cloud-native applications.\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"=== Prompt Routing Demonstration ===\")\n",
    "results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n--- Query {i} ---\")\n",
    "    print(f\"Query: {query[:60]}...\")\n",
    "    \n",
    "    # Route the prompt\n",
    "    model_id, model_name = prompt_router.route_prompt(query, priority='balanced')\n",
    "    \n",
    "    # Make the request (without caching for this demo)\n",
    "    response = bedrock_service.invoke_model_with_cache(model_id, query, use_cache=False)\n",
    "    \n",
    "    results.append({\n",
    "        'query': query[:50] + '...',\n",
    "        'model': model_name,\n",
    "        'latency_ms': response['latency_ms'],\n",
    "        'response_length': len(response['content'][0]['text'])\n",
    "    })\n",
    "    \n",
    "    print(f\"Selected Model: {model_name}\")\n",
    "    print(f\"Latency: {response['latency_ms']}ms\")\n",
    "    print(f\"Response: {response['content'][0]['text'][:100]}...\")\n",
    "\n",
    "# Display routing statistics\n",
    "print(\"\\n=== Routing Statistics ===\")\n",
    "routing_stats = prompt_router.get_routing_stats()\n",
    "for model, count in routing_stats.items():\n",
    "    print(f\"{model}: {count} requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3: Performance Comparison\n",
    "\n",
    "Let's compare performance with and without caching, and across different routing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "test_prompt = \"Explain the benefits of cloud computing for small businesses.\"\n",
    "model_id = model_manager.get_model_id('sonnet')\n",
    "\n",
    "# Test without caching\n",
    "print(\"=== Performance Comparison ===\")\n",
    "print(\"\\n1. Without Caching:\")\n",
    "times_no_cache = []\n",
    "for i in range(3):\n",
    "    response = bedrock_service.invoke_model_with_cache(model_id, test_prompt, use_cache=False)\n",
    "    times_no_cache.append(response['latency_ms'])\n",
    "    print(f\"  Request {i+1}: {response['latency_ms']}ms\")\n",
    "\n",
    "# Test with caching\n",
    "print(\"\\n2. With Caching:\")\n",
    "times_with_cache = []\n",
    "for i in range(3):\n",
    "    response = bedrock_service.invoke_model_with_cache(model_id, test_prompt, use_cache=True)\n",
    "    times_with_cache.append(response['latency_ms'])\n",
    "    print(f\"  Request {i+1}: {response['latency_ms']}ms\")\n",
    "\n",
    "# Calculate savings\n",
    "avg_no_cache = sum(times_no_cache) / len(times_no_cache)\n",
    "avg_with_cache = sum(times_with_cache) / len(times_with_cache)\n",
    "savings_percent = ((avg_no_cache - avg_with_cache) / avg_no_cache) * 100\n",
    "\n",
    "print(f\"\\n=== Performance Summary ===\")\n",
    "print(f\"Average latency without cache: {avg_no_cache:.2f}ms\")\n",
    "print(f\"Average latency with cache: {avg_with_cache:.2f}ms\")\n",
    "print(f\"Performance improvement: {savings_percent:.1f}%\")\n",
    "\n",
    "# Final cache statistics\n",
    "final_stats = bedrock_service.get_cache_stats()\n",
    "print(f\"\\n=== Final Cache Statistics ===\")\n",
    "for key, value in final_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Considerations or Advanced section or Best Practices\n",
    "\n",
    "### Best Practices for Prompt Caching\n",
    "\n",
    "1. **Cache Key Design**: Use meaningful cache keys that include model version and relevant parameters\n",
    "2. **TTL Management**: Implement time-to-live (TTL) for cache entries to ensure freshness\n",
    "3. **Memory Management**: Monitor cache size and implement eviction policies for production use\n",
    "4. **Cache Warming**: Pre-populate cache with frequently used prompts\n",
    "\n",
    "### Best Practices for Prompt Routing\n",
    "\n",
    "1. **Complexity Analysis**: Develop sophisticated heuristics for query complexity\n",
    "2. **Cost Monitoring**: Track costs across different models to optimize routing decisions\n",
    "3. **Performance Metrics**: Monitor latency and quality metrics for each model\n",
    "4. **Fallback Strategies**: Implement fallback models for high availability\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Persistent Caching**: Use Redis or DynamoDB for distributed caching\n",
    "- **Monitoring**: Implement comprehensive logging and metrics\n",
    "- **Security**: Ensure sensitive data is not cached inappropriately\n",
    "- **Rate Limiting**: Implement proper rate limiting for API calls\n",
    "- **Error Handling**: Add robust error handling and retry logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of production-ready cache with TTL\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "class ProductionCache:\n",
    "    \"\"\"Production-ready cache with TTL and size limits\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):\n",
    "        self.cache = {}\n",
    "        self.timestamps = {}\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "    \n",
    "    def get(self, key: str) -> Optional[dict]:\n",
    "        \"\"\"Get item from cache if not expired\"\"\"\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        \n",
    "        # Check if expired\n",
    "        if time.time() - self.timestamps[key] > self.default_ttl:\n",
    "            del self.cache[key]\n",
    "            del self.timestamps[key]\n",
    "            return None\n",
    "        \n",
    "        return self.cache[key]\n",
    "    \n",
    "    def set(self, key: str, value: dict) -> None:\n",
    "        \"\"\"Set item in cache with size management\"\"\"\n",
    "        # Evict oldest items if at capacity\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            oldest_key = min(self.timestamps.keys(), key=lambda k: self.timestamps[k])\n",
    "            del self.cache[oldest_key]\n",
    "            del self.timestamps[oldest_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.timestamps[key] = time.time()\n",
    "\n",
    "# Example usage\n",
    "prod_cache = ProductionCache(max_size=100, default_ttl=1800)  # 30 minutes TTL\n",
    "print(\"âœ… Production cache example created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've learned the basics of prompt caching and routing, here are some next steps to explore:\n",
    "\n",
    "### 1. Advanced Routing Strategies\n",
    "- Implement machine learning-based routing decisions\n",
    "- Add user preference learning\n",
    "- Develop cost-aware routing algorithms\n",
    "\n",
    "### 2. Integration Patterns\n",
    "- Build a REST API wrapper around these capabilities\n",
    "- Create a Streamlit web application for interactive use\n",
    "- Integrate with existing applications and workflows\n",
    "\n",
    "### 3. Monitoring and Analytics\n",
    "- Set up CloudWatch metrics for cache performance\n",
    "- Implement cost tracking across different models\n",
    "- Create dashboards for routing decision analysis\n",
    "\n",
    "### 4. Scale and Production\n",
    "- Deploy using AWS Lambda for serverless scaling\n",
    "- Implement distributed caching with ElastiCache\n",
    "- Add comprehensive error handling and logging\n",
    "\n",
    "### 5. Explore Additional Features\n",
    "- Multi-modal prompt routing (text, images, documents)\n",
    "- Streaming responses with caching\n",
    "- Custom model fine-tuning integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's clean up any resources and display final statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final statistics\n",
    "print(\"=== Workshop Summary ===\")\n",
    "print(f\"Total API calls made: {bedrock_service.cache_stats['hits'] + bedrock_service.cache_stats['misses']}\")\n",
    "print(f\"Cache hits: {bedrock_service.cache_stats['hits']}\")\n",
    "print(f\"Cache misses: {bedrock_service.cache_stats['misses']}\")\n",
    "print(f\"Cache hit rate: {bedrock_service.get_cache_stats()['hit_rate_percent']}%\")\n",
    "print(f\"Items in cache: {len(bedrock_service.cache)}\")\n",
    "\n",
    "print(\"\\n=== Model Usage ===\")\n",
    "routing_stats = prompt_router.get_routing_stats()\n",
    "for model, count in routing_stats.items():\n",
    "    print(f\"{model}: {count} requests\")\n",
    "\n",
    "# Clear cache to free memory\n",
    "bedrock_service.cache.clear()\n",
    "bedrock_service.cache_stats = {'hits': 0, 'misses': 0}\n",
    "\n",
    "print(\"\\nâœ… Cleanup complete! Cache cleared and statistics reset.\")\n",
    "print(\"\\nðŸŽ‰ Workshop completed successfully!\")\n",
    "print(\"\\nYou've learned how to:\")\n",
    "print(\"- Implement prompt caching for cost and latency optimization\")\n",
    "print(\"- Use intelligent prompt routing for model selection\")\n",
    "print(\"- Monitor performance and usage statistics\")\n",
    "print(\"- Apply best practices for production deployments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}