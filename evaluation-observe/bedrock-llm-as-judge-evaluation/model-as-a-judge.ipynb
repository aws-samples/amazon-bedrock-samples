{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model-as-a-Judge Evaluation Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock's Model-as-a-Judge feature for systematic model evaluation. The Model-as-a-Judge approach uses a foundation model to score another model's responses and provide explanations for the scores. The guide covers creating evaluation datasets, running evaluations, and comparing different foundation models.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Dataset Generation](#dataset)\n",
    "3. [S3 Integration](#s3)\n",
    "4. [Single Model Evaluation](#single)\n",
    "5. [Model Selection and Comparison](#comparison)\n",
    "6. [Monitoring and Results](#monitoring)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with Bedrock access\n",
    "- Appropriate IAM roles and permissions\n",
    "- Access to supported evaluator models (Claude 3 Haiku, Claude 3.5 Sonnet, Mistral Large, or Meta Llama 3.1)\n",
    "- An S3 bucket for storing evaluation data\n",
    "\n",
    "Let's begin with updating boto3 to latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# AWS Configuration\n",
    "REGION = \"<YOUR_REGION>\"\n",
    "ROLE_ARN = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n",
    "PREFIX = \"<YOUR_BUCKET_PREFIX>\"\n",
    "dataset_custom_name = \"dummy-data\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock', region_name=REGION)\n",
    "s3_client = boto3.client('s3', region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation <a name=\"dataset\"></a>\n",
    "\n",
    "We'll create a simple dataset of mathematical reasoning problems. These problems test:\n",
    "- Basic arithmetic\n",
    "- Logical reasoning\n",
    "- Natural language understanding\n",
    "\n",
    "The dataset follows the required JSONL format for Bedrock evaluation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def generate_shopping_problems(num_problems=50):\n",
    "    \"\"\"Generate shopping-related math problems with random values.\"\"\"\n",
    "    problems = []\n",
    "    items = [\"apples\", \"oranges\", \"bananas\", \"books\", \"pencils\", \"notebooks\"]\n",
    "    \n",
    "    for _ in range(num_problems):\n",
    "        # Generate random values\n",
    "        item = random.choice(items)\n",
    "        quantity = random.randint(3, 20)\n",
    "        price_per_item = round(random.uniform(1.5, 15.0), 2)\n",
    "        discount_percent = random.choice([10, 15, 20, 25, 30])\n",
    "        \n",
    "        # Calculate the answer\n",
    "        total_price = quantity * price_per_item\n",
    "        discount_amount = total_price * (discount_percent / 100)\n",
    "        final_price = round(total_price - discount_amount, 2)\n",
    "        \n",
    "        # Create the problem\n",
    "        problem = {\n",
    "            \"prompt\": f\"If {item} cost \\${price_per_item} each and you buy {quantity} of them with a {discount_percent}% discount, how much will you pay in total?\",\n",
    "            \"category\": \"Shopping Math\",\n",
    "            \"referenceResponse\": f\"The total price will be \\${final_price}. Original price: \\${total_price} minus {discount_percent}% discount (\\${discount_amount})\"\n",
    "        }\n",
    "        \n",
    "        problems.append(problem)\n",
    "    \n",
    "    return problems\n",
    "\n",
    "def save_to_jsonl(problems, output_file):\n",
    "    \"\"\"Save the problems to a JSONL file.\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for problem in problems:\n",
    "            f.write(json.dumps(problem) + '\\n')\n",
    "\n",
    "SAMPLE_SIZE = 30\n",
    "problems = generate_shopping_problems(SAMPLE_SIZE)\n",
    "save_to_jsonl(problems, f\"{dataset_custom_name}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Integration <a name=\"s3\"></a>\n",
    "\n",
    "After generating our sample dataset, we need to upload it to S3 for use in the evaluation job. \n",
    "We'll use the boto3 S3 client to upload our JSONL file.\n",
    "\n",
    "> **Note**: Make sure your IAM role has appropriate S3 permissions (s3:PutObject) for the target bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file: str, bucket: str, s3_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with error handling.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"✓ Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error uploading to S3: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Upload dataset\n",
    "s3_key = f\"{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "upload_success = upload_to_s3(f\"{dataset_custom_name}.jsonl\", BUCKET_NAME, s3_key)\n",
    "\n",
    "if not upload_success:\n",
    "    raise Exception(\"Failed to upload dataset to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Job Configuration\n",
    "\n",
    "Configure the LLM-as-Judge evaluation with comprehensive metrics for assessing model performance:\n",
    "\n",
    "| Metric Category | Description |\n",
    "|----------------|-------------|\n",
    "| Quality | Correctness, Completeness, Faithfulness |\n",
    "| User Experience | Helpfulness, Coherence, Relevance |\n",
    "| Instructions | Following Instructions, Professional Style |\n",
    "| Safety | Harmfulness, Stereotyping, Refusal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_llm_judge_evaluation(\n",
    "    client,\n",
    "    job_name: str,\n",
    "    role_arn: str,\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    evaluator_model_id: str,\n",
    "    generator_model_id: str,\n",
    "    dataset_name: str = None,\n",
    "    task_type: str = \"General\" # must be General for LLMaaJ\n",
    "):    \n",
    "    # All available LLM-as-judge metrics\n",
    "    llm_judge_metrics = [\n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.Completeness\", \n",
    "        \"Builtin.Faithfulness\",\n",
    "        \"Builtin.Helpfulness\",\n",
    "        \"Builtin.Coherence\",\n",
    "        \"Builtin.Relevance\",\n",
    "        \"Builtin.FollowingInstructions\",\n",
    "        \"Builtin.ProfessionalStyleAndTone\",\n",
    "        \"Builtin.Harmfulness\",\n",
    "        \"Builtin.Stereotyping\",\n",
    "        \"Builtin.Refusal\"\n",
    "    ]\n",
    "\n",
    "    # Configure dataset\n",
    "    dataset_config = {\n",
    "        \"name\": dataset_name or \"CustomDataset\",\n",
    "        \"datasetLocation\": {\n",
    "            \"s3Uri\": input_s3_uri\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.create_evaluation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            applicationType=\"ModelEvaluation\",\n",
    "            evaluationConfig={\n",
    "                \"automated\": {\n",
    "                    \"datasetMetricConfigs\": [\n",
    "                        {\n",
    "                            \"taskType\": task_type,\n",
    "                            \"dataset\": dataset_config,\n",
    "                            \"metricNames\": llm_judge_metrics\n",
    "                        }\n",
    "                    ],\n",
    "                    \"evaluatorModelConfig\": {\n",
    "                        \"bedrockEvaluatorModels\": [\n",
    "                            {\n",
    "                                \"modelIdentifier\": evaluator_model_id\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            inferenceConfig={\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        \"bedrockModel\": {\n",
    "                            \"modelIdentifier\": generator_model_id\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3Uri\": output_s3_uri\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Model Evaluation <a name=\"single\"></a>\n",
    "\n",
    "First, let's run a single evaluation job using Claude 3 Haiku as both generator and evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Job Configuration\n",
    "evaluator_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "generator_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "job_name = f\"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# S3 Paths\n",
    "input_data = f\"s3://{BUCKET_NAME}/{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}\"\n",
    "\n",
    "# Create evaluation job\n",
    "try:\n",
    "    llm_as_judge_response = create_llm_judge_evaluation(\n",
    "        client=bedrock_client,\n",
    "        job_name=job_name,\n",
    "        role_arn=ROLE_ARN,\n",
    "        input_s3_uri=input_data,\n",
    "        output_s3_uri=output_path,\n",
    "        evaluator_model_id=evaluator_model,\n",
    "        generator_model_id=generator_model,\n",
    "        task_type=\"General\"\n",
    "    )\n",
    "    print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create evaluation job: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = llm_as_judge_response['jobArn']\n",
    "\n",
    "# Check job status\n",
    "check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) \n",
    "print(f\"Job Status: {check_status['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Comparison <a name=\"comparison\"></a>\n",
    "\n",
    "Now, let's evaluate multiple generator models to find the optimal model for our use case. We'll compare different foundation models while using a consistent evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Available Generator Models\n",
    "GENERATOR_MODELS = [\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"amazon.nova-micro-v1:0\"\n",
    "]\n",
    "\n",
    "# Consistent Evaluator\n",
    "EVALUATOR_MODEL = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def run_model_comparison(\n",
    "    generator_models: List[str],\n",
    "    evaluator_model: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    evaluation_jobs = []\n",
    "    \n",
    "    for generator_model in generator_models:\n",
    "        job_name = f\"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        \n",
    "        try:\n",
    "            response = create_llm_judge_evaluation(\n",
    "                client=bedrock_client,\n",
    "                job_name=job_name,\n",
    "                role_arn=ROLE_ARN,\n",
    "                input_s3_uri=input_data,\n",
    "                output_s3_uri=f\"{output_path}/{job_name}/\",\n",
    "                evaluator_model_id=evaluator_model,\n",
    "                generator_model_id=generator_model,\n",
    "                task_type=\"General\"\n",
    "            )\n",
    "            \n",
    "            job_info = {\n",
    "                \"job_name\": job_name,\n",
    "                \"job_arn\": response[\"jobArn\"],\n",
    "                \"generator_model\": generator_model,\n",
    "                \"evaluator_model\": evaluator_model,\n",
    "                \"status\": \"CREATED\"\n",
    "            }\n",
    "            evaluation_jobs.append(job_info)\n",
    "            \n",
    "            print(f\"✓ Created job: {job_name}\")\n",
    "            print(f\"  Generator: {generator_model}\")\n",
    "            print(f\"  Evaluator: {evaluator_model}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {generator_model}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return evaluation_jobs\n",
    "\n",
    "# Run model comparison\n",
    "evaluation_jobs = run_model_comparison(GENERATOR_MODELS, EVALUATOR_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Results <a name=\"monitoring\"></a>\n",
    "\n",
    "Track the progress of all evaluation jobs and display their current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to check job status\n",
    "def check_jobs_status(jobs, client):\n",
    "    \"\"\"Check and update status for all evaluation jobs\"\"\"\n",
    "    for job in jobs:\n",
    "        try:\n",
    "            response = client.get_evaluation_job(\n",
    "                jobIdentifier=job[\"job_arn\"]\n",
    "            )\n",
    "            job[\"status\"] = response[\"status\"]\n",
    "        except Exception as e:\n",
    "            job[\"status\"] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Check initial status\n",
    "updated_jobs = check_jobs_status(evaluation_jobs, bedrock_client)\n",
    "\n",
    "# Display status summary\n",
    "for job in updated_jobs:\n",
    "    print(f\"Job: {job['job_name']}\")\n",
    "    print(f\"Status: {job['status']}\")\n",
    "    print(f\"Generator: {job['generator_model']}\")\n",
    "    print(f\"Evaluator: {job['evaluator_model']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's Correlation Analysis Between Multiple Generator Models\n",
    "\n",
    "* To calculate the Spearman's rank correlation between generator models, first read the evaluation results from S3 using the path structure:\n",
    "```s3://[output-path]/[job-name]/[job-uuid]/models/[model-id]/taskTypes/[task-type]/datasets/dataset/[file-uuid]_output.jsonl```\n",
    "- Each file contains evaluation scores across different metrics (Correctness, Completeness, Helpfulness, Coherence, and Faithfulness).\n",
    "\n",
    "* Use scipy.stats to compute the correlation coefficient between pairs of generator models, filtering out any constant values or error messages. \n",
    "\n",
    "* The resulting correlation matrix helps identify which models produce similar outputs and where they differ significantly in their response patterns. Higher correlation coefficients (closer to 1.0) indicate stronger agreement between models' responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def read_and_organize_metrics_from_s3(bucket_name, file_key):\n",
    "    s3_client = boto3.client('s3')\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        for line in content.strip().split('\\n'):\n",
    "            if line:\n",
    "                data = json.loads(line)\n",
    "                if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n",
    "                    for score in data['automatedEvaluationResult']['scores']:\n",
    "                        metric_name = score['metricName']\n",
    "                        if 'result' in score:\n",
    "                            metric_value = score['result']\n",
    "                            if metric_name not in metrics_dict:\n",
    "                                metrics_dict[metric_name] = []\n",
    "                            metrics_dict[metric_name].append(metric_value)\n",
    "        return metrics_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_spearmanr_correlation(scores1, scores2):\n",
    "    if len(set(scores1)) == 1 or len(set(scores2)) == 1:\n",
    "        return \"undefined (constant scores)\", \"undefined\"\n",
    "    \n",
    "    try:\n",
    "        result = stats.spearmanr(scores1, scores2)\n",
    "        return round(float(result.statistic), 4), round(float(result.pvalue), 4)\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\", \"undefined\"\n",
    "\n",
    "# Extract metrics\n",
    "bucket_name = \"<EVALUATION_OUTPUT_BUCKET>\"\n",
    "file_key1 = \"<EVALUATION_FILE_KEY1>\"\n",
    "file_key2 = \"<EVALUATION_FILE_KEY2>\"\n",
    "\n",
    "metrics1 = read_and_organize_metrics_from_s3(bucket_name, file_key1)\n",
    "metrics2 = read_and_organize_metrics_from_s3(bucket_name, file_key2)\n",
    "\n",
    "# Calculate correlations for common metrics\n",
    "common_metrics = set(metrics1.keys()) & set(metrics2.keys())\n",
    "\n",
    "for metric_name in common_metrics:\n",
    "    scores1 = metrics1[metric_name]\n",
    "    scores2 = metrics2[metric_name]\n",
    "    \n",
    "    if len(scores1) == len(scores2):\n",
    "        correlation, p_value = get_spearmanr_correlation(scores1, scores2)\n",
    "        \n",
    "        print(f\"\\nMetric: {metric_name}\")\n",
    "        print(f\"Number of samples: {len(scores1)}\")\n",
    "        print(f\"Unique values in Model 1 scores: {len(set(scores1))}\")\n",
    "        print(f\"Unique values in Model 2 scores: {len(set(scores2))}\")\n",
    "        print(f\"Model 1 scores range: [{min(scores1)}, {max(scores1)}]\")\n",
    "        print(f\"Model 2 scores range: [{min(scores2)}, {max(scores2)}]\")\n",
    "        print(f\"Spearman correlation coefficient: {correlation}\")\n",
    "        print(f\"P-value: {p_value}\")\n",
    "    else:\n",
    "        print(f\"\\nMetric: {metric_name}\")\n",
    "        print(\"Error: Different number of samples between models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running the evaluation job:\n",
    "1. Monitor the job status in the Bedrock console or through `get_evaluation_job` API\n",
    "2. Review the report card for:\n",
    "   - Score distributions across different metrics\n",
    "   - Detailed explanations for scoring provided by the judge model\n",
    "   - Overall performance analysis\n",
    "3. Access full results in your specified S3 bucket\n",
    "\n",
    "> **Note**: The evaluation results will help you understand your model's strengths and areas for improvement across multiple dimensions of performance."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
