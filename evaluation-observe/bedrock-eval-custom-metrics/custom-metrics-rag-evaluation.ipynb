{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56726f49-50c0-4e99-9481-4775547c5335",
   "metadata": {},
   "source": [
    "# RAG Evaluation with Custom Metrics on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock Evaluations now supports Custom Metrics for RAG (Retrieval-Augmented Generation) systems, enabling you to define specialized evaluation criteria tailored to your specific needs. This notebook demonstrates how to create and implement custom metrics for your RAG evaluation jobs, allowing you to measure unique aspects of your RAG system's performance beyond the built-in metrics.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Creating custom metrics for RAG evaluation with full configuration control\n",
    "- Implementing retrieve-and-generate evaluation jobs with custom metrics\n",
    "- Defining numerical and categorical scoring systems for your custom metrics\n",
    "- Analyzing evaluation results with your specialized metrics alongside built-in metrics\n",
    "- Monitoring evaluation progress and interpreting custom metric results\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "### AWS Account and Model Access\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- [Selected evaluator and generator models are enabled](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) in Amazon Bedrock (verify on the [Model access page](https://console.aws.amazon.com/bedrock/home#/modelaccess) of the Amazon Bedrock console)\n",
    "- Confirmed [AWS Regions where the models are available](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) and their [quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)\n",
    "\n",
    "### IAM and S3 Configuration\n",
    "- [An IAM role with necessary permissions](https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html) for S3 and Bedrock\n",
    "- Configured [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) with appropriate permissions for accessing and writing output data\n",
    "- [Enabled CORS on your S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enabling-cors-examples.html)\n",
    "\n",
    "### Additional Requirements\n",
    "- A dataset formatted according to the [RAG evaluation requirements](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-evaluation-prompt.html)\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Make sure these are enabled in your account.\n",
    "\n",
    "## Custom Metrics for RAG Evaluation\n",
    "\n",
    "Custom metrics allow you to evaluate specific dimensions of your RAG system's performance beyond the default metrics. For example, you might want to evaluate:\n",
    "- Information comprehensiveness\n",
    "- Knowledge integration fidelity\n",
    "- Information relevance\n",
    "- Brand voice consistency\n",
    "- Domain-specific accuracy criteria\n",
    "\n",
    "Let's implement these custom evaluations using the Amazon Bedrock SDK.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160db6b-f65a-4ce9-a09f-ccc4b2c1318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed4a13-df43-47c0-8c31-9d82bc9c3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73466974-ee03-432b-958e-6e7564b4e16f",
   "metadata": {},
   "source": [
    "To use the Python SDK for creating an RAG evaluation job with your own inference responses, use the following steps. First, set up the required configurations, which should include your model identifier for the evaluator, IAM role with appropriate permissions, S3 paths for input data containing your inference responses, and output location for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16117b-4cc1-49e5-a839-289886bf4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate unique name for the job\n",
    "job_name = f\"rag-evaluation-custom-metrics-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "knowledge_base_id = \"<YOUR_KB_ID>\"\n",
    "evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "generator_model = \"amazon.nova-lite-v1:0\"\n",
    "custom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = f\"s3://{BUCKET_NAME}/evaluation_data/input.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n",
    "\n",
    "# Configure retrieval settings\n",
    "num_results = 10\n",
    "search_type = \"HYBRID\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd77cf-8efa-49bf-8fd2-9db716d9e699",
   "metadata": {},
   "source": [
    "## Creating a Retrieval and Generation Evaluation Job with Custom Metrics\n",
    "\n",
    "For this evaluation job, we'll use three key built-in metrics:\n",
    "- `Builtin.Correctness`: Evaluates factual accuracy of generated responses\n",
    "- `Builtin.Completeness`: Assesses if all relevant information is included  \n",
    "- `Builtin.Helpfulness`: Measures how useful the response is\n",
    "\n",
    "Additionally, we'll implement our custom metric:\n",
    "- `information_comprehensiveness`: Evaluates how thoroughly the response utilizes retrieved information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9913ae1-97ce-4abd-a3b3-29bca0d9721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our custom information_comprehensiveness metric\n",
    "information_comprehensiveness_metric = {\n",
    "    \"customMetricDefinition\": {\n",
    "        \"name\": \"information_comprehensiveness\",\n",
    "        \"instructions\": \"\"\"\n",
    "        Your role is to evaluate how comprehensively the response addresses the query using the retrieved information. \n",
    "        Assess whether the response provides a thorough treatment of the subject by effectively utilizing the available retrieved passages.\n",
    "\n",
    "Carefully evaluate the comprehensiveness of the RAG response for the given query against all specified criteria. \n",
    "Assign a single overall score that best represents the comprehensiveness, and provide a brief explanation justifying your rating, referencing specific strengths and weaknesses observed.\n",
    "\n",
    "When evaluating response comprehensiveness, consider the following rubrics:\n",
    "- Coverage: Does the response utilize the key relevant information from the retrieved passages?\n",
    "- Depth: Does the response provide sufficient detail on important aspects from the retrieved information?\n",
    "- Context utilization: How effectively does the response leverage the available retrieved passages?\n",
    "- Information synthesis: Does the response combine retrieved information to create a thorough treatment?\n",
    "\n",
    "Evaluate using the following:\n",
    "\n",
    "Query: {{prompt}}\n",
    "\n",
    "Retrieved passages: {{context}}\n",
    "\n",
    "Response to evaluate: {{prediction}}\n",
    "\"\"\",\n",
    "        \"ratingScale\": [\n",
    "            {\n",
    "                \"definition\": \"Very comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 3\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Moderately comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 2\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Minimally comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Not at all comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 0\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17e5dc-af58-4ebd-8c7d-c3766907e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation job\n",
    "retrieve_generate_job_name = f\"rag-evaluation-generate-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=retrieve_generate_job_name,\n",
    "    jobDescription=\"Evaluate retrieval and generation with custom metric\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"RagEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"ragConfigs\": [{\n",
    "            \"knowledgeBaseConfig\": {\n",
    "                \"retrieveAndGenerateConfig\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseId\": knowledge_base_id,\n",
    "                        \"modelArn\": generator_model,\n",
    "                        \"retrievalConfiguration\": {\n",
    "                            \"vectorSearchConfiguration\": {\n",
    "                                \"numberOfResults\": num_results\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"General\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"RagDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.Correctness\",\n",
    "                    \"Builtin.Completeness\",\n",
    "                    \"Builtin.Helpfulness\",\n",
    "                    \"information_comprehensiveness\"\n",
    "                ]\n",
    "            }],\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            },\n",
    "            \"customMetricConfig\": {\n",
    "                \"customMetrics\": [\n",
    "                    information_comprehensiveness_metric\n",
    "                ],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": custom_metrics_evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation job: {retrieve_generate_job_name}\")\n",
    "print(f\"Job ID: {retrieve_generate_job['jobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be3a9d-cb4b-4fe4-a64c-9e6fd6078c31",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c3d15-1c0a-41bb-aa03-b506d216e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = retrieve_generate_job['jobArn']  # or retrieve_generate_job['jobArn']\n",
    "\n",
    "# Check job status\n",
    "response = bedrock_client.get_evaluation_job(\n",
    "    jobIdentifier=evaluation_job_arn \n",
    ")\n",
    "print(f\"Job Status: {response['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eaf6ba-3519-48fc-9554-7e848a4117ef",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This guide demonstrated how to implement Custom Metrics for RAG Evaluation on Amazon Bedrock. This feature allows organizations to:\n",
    "- Create tailored evaluation criteria beyond standard metrics\n",
    "- Define specialized scoring systems for unique business requirements\n",
    "- Combine custom and built-in metrics for comprehensive RAG assessment\n",
    "  \n",
    "With these capabilities, you can systematically evaluate and optimize your RAG applications according to the dimensions that matter most for your specific use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
