{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89f8ebe2-88e6-4a87-a723-abca3d513f4c",
   "metadata": {},
   "source": [
    "# LLM as a Judge Model Evaluation with Custom Metrics on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock provides robust capabilities for evaluating foundation models using custom metrics, allowing you to assess model performance based on criteria specific to your use case. This notebook demonstrates how to implement custom evaluation metrics for foundation models on Amazon Bedrock, enabling you to measure unique aspects of model performance beyond standard metrics.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Creating custom metrics for foundation model evaluation\n",
    "- Implementing model evaluation jobs with your specialized metrics\n",
    "- Defining numerical and categorical scoring systems tailored to your requirements\n",
    "- Analyzing evaluation results with your custom metrics alongside built-in metrics\n",
    "- Monitoring evaluation progress and interpreting results\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, make sure you have:\n",
    "\n",
    "### AWS Account and Model Access\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- [Selected evaluator and generator models are enabled](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) in Amazon Bedrock (verify on the [Model access page](https://console.aws.amazon.com/bedrock/home#/modelaccess) of the Amazon Bedrock console)\n",
    "- Confirmed [AWS Regions where the models are available](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) and their [quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html)\n",
    "\n",
    "### IAM and S3 Configuration\n",
    "- [An IAM role with necessary permissions](https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html) for S3 and Bedrock\n",
    "- Configured [S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) with appropriate permissions for accessing and writing output data\n",
    "- [Enabled CORS on your S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enabling-cors-examples.html)\n",
    "\n",
    "### Additional Requirements\n",
    "- A dataset formatted according to the [model evaluation requirements](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-judge.html)\n",
    "\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Ensure these are enabled in your account.\n",
    "\n",
    "## Custom Metrics for Model Evaluation\n",
    "\n",
    "Custom metrics allow you to evaluate specific dimensions of model performance that standard metrics might not capture. For example, you might want to evaluate:\n",
    "- Response creativity and originality\n",
    "- Domain-specific accuracy\n",
    "- Style and tone consistency\n",
    "- Task-specific requirements\n",
    "- Business-aligned performance indicators\n",
    "\n",
    "Let's implement these custom evaluations using the Amazon Bedrock SDK.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73db4d0-3700-47df-90f5-872ac84150bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70aad68-b474-463f-82a2-950f35755a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae70831-88d4-4995-927a-4b2f27495e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "generator_model = \"amazon.nova-lite-v1:0\"\n",
    "custom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = f\"s3://{BUCKET_NAME}/evaluation_data/input.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30db1ef-2928-40d3-b3c6-69d93fe030f0",
   "metadata": {},
   "source": [
    "## Creating a Model Evaluation Job with Custom Metrics\n",
    "\n",
    "For this evaluation job, we'll use several key built-in metrics:\n",
    "- `Builtin.Correctness`: Evaluates factual accuracy of model responses\n",
    "- `Builtin.Completeness`: Assesses if all relevant information is included\n",
    "- `Builtin.Coherence`: Measures how logical and well-structured the response is\n",
    "- `Builtin.Relevance`: Assesses if the response directly addresses the input prompt\n",
    "- `Builtin.FollowingInstructions`: Evaluates how well the model follows given instructions\n",
    "\n",
    "Additionally, we'll implement our custom metric:\n",
    "- `comprehensiveness`: Evaluates how thorough and complete the model's response is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438d1c2-1efd-46c0-88c5-34e7f565d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensiveness_metric ={\n",
    "    \"customMetricDefinition\": {\n",
    "        \"name\": \"comprehensiveness\",\n",
    "        \"instructions\": \"\"\"Your role is to judge the comprehensiveness of an answer based on the question and the prediction. Assess the quality, accuracy, and helpfulness of language model response, and use these to judge how comprehensive the response is. Award higher scores to responses that are detailed and thoughtful.\n",
    "\n",
    "Carefully evaluate the comprehensiveness of the LLM response for the given query (prompt) against all specified criteria. Assign a single overall score that best represents the comprehensivenss, and provide a brief explanation justifying your rating, referencing specific strengths and weaknesses observed.\n",
    "\n",
    "When evaluating the response quality, consider the following rubrics:\n",
    "- Accuracy: Factual correctness of information provided\n",
    "- Completeness: Coverage of important aspects of the query\n",
    "- Clarity: Clear organization and presentation of information\n",
    "- Helpfulness: Practical utility of the response to the user\n",
    "\n",
    "Evaluate the following:\n",
    "\n",
    "Query:\n",
    "{{prompt}}\n",
    "\n",
    "Response to evaluate:\n",
    "{{prediction}}\"\"\",\n",
    "        \"ratingScale\": [\n",
    "            {\n",
    "                \"definition\": \"Very comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 10\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Mildly comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 3\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Not at all comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 1\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc8c5f-0c82-4961-b4a6-528be9eaba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model evaluation job\n",
    "model_eval_job_name = f\"model-evaluation-custom-metrics{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "model_eval_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=model_eval_job_name,\n",
    "    jobDescription=\"Evaluate model performance with custom comprehensiveness metric\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"ModelEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"models\": [{\n",
    "            \"bedrockModel\": {\n",
    "                \"modelIdentifier\": generator_model\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"General\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"ModelEvalDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.Correctness\",\n",
    "                    \"Builtin.Completeness\",\n",
    "                    \"Builtin.Coherence\",\n",
    "                    \"Builtin.Relevance\",\n",
    "                    \"Builtin.FollowingInstructions\",\n",
    "                    \"comprehensiveness\"\n",
    "                ]\n",
    "            }],\n",
    "            \"customMetricConfig\": {\n",
    "                \"customMetrics\": [\n",
    "                    comprehensiveness_metric\n",
    "                ],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": custom_metrics_evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            },\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created model evaluation job: {model_eval_job_name}\")\n",
    "print(f\"Job ID: {model_eval_job['jobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee8bb5-42fd-423b-a27d-abcf5095d53e",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e48dd-918b-4174-bb20-ca1f0912a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = model_eval_job['jobArn']  # or retrieve_generate_job['jobArn']\n",
    "\n",
    "# Check job status\n",
    "response = bedrock_client.get_evaluation_job(\n",
    "    jobIdentifier=evaluation_job_arn \n",
    ")\n",
    "print(f\"Job Status: {response['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46961e92-4bbb-436a-8929-926e99c5073a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This guide demonstrated how to implement Custom Metrics for Model Evaluation on Amazon Bedrock. This powerful feature allows organizations to:\n",
    "\n",
    "- Create custom evaluation metrics beyond standard benchmarks\n",
    "- Define specialized scoring systems aligned with specific business requirements\n",
    "- Combine custom and built-in metrics for comprehensive model assessment\n",
    "- Evaluate models based on domain-specific criteria that matter to your use case\n",
    "- Generate consistent, comparable results to track model improvements over time\n",
    "\n",
    "With these capabilities, you can systematically evaluate and optimize your foundation models according to the dimensions that matter most for your specific applications, ensuring they meet your quality standards and business objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636400b-848c-4b93-b32d-24ffc4df6487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
