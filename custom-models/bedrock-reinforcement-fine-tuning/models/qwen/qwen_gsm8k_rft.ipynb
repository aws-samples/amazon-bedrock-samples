{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Reinforcement Fine-Tuning with OpenAI compatible APIs - Qwen3 32B\n",
    "\n",
    "With OpenAI-compatible fine-tuning APIs, Amazon Bedrock can now help you train popular open-weight models, including OpenAI GPT-OSS and Qwen models using reinforcement fine-tuning (RFT).  \n",
    "\n",
    "\n",
    "## What's RFT?\n",
    "\n",
    "Traditional fine-tuning shows a model examples and says \"produce outputs like this.\" RFT takes a different approach: it lets the model generate its own responses, then uses a reward signal to reinforce good outputs and discourage bad ones. Think of it like training with a coach who gives feedback rather than just copying from a textbook.\n",
    "\n",
    "For math problems, this works particularly well because we can automatically verify if an answer is correct‚Äîno human labeling needed.\n",
    "\n",
    "## What's GSM8K?\n",
    "\n",
    "GSM8K (Grade School Math 8K) is a dataset of ~8,000 grade-school math word problems. Each problem requires multi-step reasoning to solve. It's become a standard benchmark for testing whether language models can actually \"think\" through problems rather than just pattern-match.\n",
    "\n",
    "Example problem:\n",
    "> *Janet's ducks lay 16 eggs per day. She eats three for breakfast and bakes muffins with four. She sells the rest at $2 each. How much does she make daily?*\n",
    "\n",
    "This notebook demonstrates all fine-tuning API operations using the OpenAI SDK using the GSM8K dataset.\n",
    "\n",
    "**API Operations Covered:**\n",
    "1. Upload and list files\n",
    "2. List submitted fine-tuning jobs\n",
    "3. Create fine-tuning job (RFT)\n",
    "4. Describe the submitted job\n",
    "5. Optionally Cancel a job \n",
    "6. List events\n",
    "7. List checkpoints\n",
    "\n",
    "**Prerequisites:**\n",
    "- SageMaker notebook with IAM role\n",
    "- Training file `rft_train_data.jsonl` present in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "\n",
    "# Install required packages\n",
    "!pip install --upgrade boto3 botocore\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade httpx\n",
    "!pip install --upgrade colorama tiktoken aws-bedrock-token-generator\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import boto3\n",
    "import openai\n",
    "import httpx\n",
    "\n",
    "\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"openai version: {openai.__version__}\")\n",
    "print(f\"httpx version: {httpx.__version__}\")\n",
    "print(\"\\n‚úÖ All imports successful! Tested with boto3 (v1.42.49), OpenAI (v2.21.0), httpx (v0.28.1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock API Keys\n",
    "\n",
    "Before we can proceed, please use the following documentation to generate a short- or long-term Bedrock API Key:\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys.html\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys-generate.html#api-keys-generate-console\n",
    "\n",
    "Here we will be using the Bedrock token generator library to create a shiort term key.\n",
    "\n",
    "### Fine tuning role\n",
    "\n",
    "Create a role (or edit your sagemaker execution role to have the following permissions):\n",
    "\n",
    "- For Lambda invocation, the following shows an example policy you can use:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\t\t \t \t \n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:lambda:*:*:function:reward-function-name\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "- For RL using AI feedback, you will need to add specific permissions to invoke foundation models to the Lambda execution role. In your lambda role, you can configure these managed policies for LLMs for grading. See `AmazonBedrockLimitedAccess` .\n",
    "\n",
    "The following is an example for invoking Amazon Bedrock foundation models as judge using the Invoke API:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\t\t \t \t \n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:bedrock:*:*:foundation-model/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Next, we will create the reward function required for our training the GPT-OSS model using RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda reward function creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add the project root (two levels up) to the Python path so we can import helpers\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..')))\n",
    "\n",
    "from helpers import (\n",
    "    create_lambda_deployment_package,\n",
    "    cleanup_lambda_deployment_package\n",
    ")\n",
    "\n",
    "# ============== UPDATE THESE VALUES ==============\n",
    "AWS_REGION = \"us-west-2\"\n",
    "S3_BUCKET = \"subshreyevals\"\n",
    "AWS_PROFILE = None  # Set to your profile name, or None for default credentials\n",
    "# =================================================\n",
    "\n",
    "# Create session\n",
    "session = boto3.Session(profile_name=AWS_PROFILE, region_name=AWS_REGION) if AWS_PROFILE else boto3.Session(region_name=AWS_REGION)\n",
    "AWS_ACCOUNT_ID = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"gsm8k\"\n",
    "HF_DATASET = \"openai/gsm8k\"\n",
    "LOCAL_DATA_DIR = \"../../tmp-data\"\n",
    "\n",
    "assert S3_BUCKET != \"your-bucket-name\", \"Please update S3_BUCKET with your own bucket name\"\n",
    "\n",
    "# S3 paths\n",
    "S3_TRAINING_DATA = f\"s3://{S3_BUCKET}/rft-data/datasets/{DATASET_NAME}/train.jsonl\"\n",
    "S3_VALIDATION_DATA = f\"s3://{S3_BUCKET}/rft-data/datasets/{DATASET_NAME}/val.jsonl\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/rft-output/\"\n",
    "\n",
    "# Resource names\n",
    "LAMBDA_FUNCTION_NAME = f\"{DATASET_NAME}-reward-function\"\n",
    "LAMBDA_ROLE_NAME = f\"{DATASET_NAME.upper()}-Lambda-Role\"\n",
    "BEDROCK_ROLE_NAME = \"BedrockRFTRole\"\n",
    "REWARD_FUNCTION_FILE = f\"../../reward-functions/{DATASET_NAME}_gptoss_rew_func.py\"\n",
    "REWARD_FUNCTION_MODULE = f\"{DATASET_NAME}_rew_func\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda execution role\n",
    "s3_client = session.client('s3', region_name=\"us-west-2\")\n",
    "bedrock_client = session.client('bedrock', region_name=\"us-west-2\")\n",
    "lambda_client = session.client('lambda', region_name=\"us-west-2\")\n",
    "iam_client = session.client('iam', region_name=\"us-west-2\")\n",
    "\n",
    "print(\"Creating Lambda execution role...\")\n",
    "\n",
    "lambda_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=LAMBDA_ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(lambda_trust_policy),\n",
    "        Description=f\"Execution role for {DATASET_NAME} reward function\"\n",
    "    )\n",
    "    lambda_role_arn = response['Role']['Arn']\n",
    "    iam_client.attach_role_policy(RoleName=LAMBDA_ROLE_NAME, PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole')\n",
    "    print(f\"‚úì Created role: {LAMBDA_ROLE_NAME}\")\n",
    "    print(\"Waiting 10s for role propagation...\")\n",
    "    time.sleep(10)\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    lambda_role_arn = iam_client.get_role(RoleName=LAMBDA_ROLE_NAME)['Role']['Arn']\n",
    "    print(f\"‚úì Using existing role: {LAMBDA_ROLE_NAME}\")\n",
    "\n",
    "# Package and deploy Lambda\n",
    "lambda_zip_content = create_lambda_deployment_package(\n",
    "    source_file=REWARD_FUNCTION_FILE,\n",
    "    zip_filename=\"lambda_deployment.zip\",\n",
    "    archive_name=f\"{REWARD_FUNCTION_MODULE}.py\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDeploying Lambda: {LAMBDA_FUNCTION_NAME}...\")\n",
    "try:\n",
    "    lambda_client.get_function(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "    lambda_client.update_function_code(FunctionName=LAMBDA_FUNCTION_NAME, ZipFile=lambda_zip_content)\n",
    "    waiter = lambda_client.get_waiter('function_updated_v2')\n",
    "    waiter.wait(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "    print(\"‚úì Updated existing function\")\n",
    "except lambda_client.exceptions.ResourceNotFoundException:\n",
    "    lambda_client.create_function(\n",
    "        FunctionName=LAMBDA_FUNCTION_NAME,\n",
    "        Runtime='python3.11',\n",
    "        Role=lambda_role_arn,\n",
    "        Handler=f\"{REWARD_FUNCTION_MODULE}.lambda_handler\",\n",
    "        Code={'ZipFile': lambda_zip_content},\n",
    "        Timeout=300,\n",
    "        MemorySize=512\n",
    "    )\n",
    "    print(\"‚úì Created new function\")\n",
    "\n",
    "waiter = lambda_client.get_waiter('function_active_v2')\n",
    "waiter.wait(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "lambda_arn = lambda_client.get_function(FunctionName=LAMBDA_FUNCTION_NAME)['Configuration']['FunctionArn']\n",
    "print(f\"‚úì Lambda ready: {lambda_arn}\")\n",
    "\n",
    "# Create Bedrock role\n",
    "print(f\"\\nCreating Bedrock role: {BEDROCK_ROLE_NAME}...\")\n",
    "\n",
    "bedrock_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "\n",
    "bedrock_permissions = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"], \"Resource\": [f\"arn:aws:s3:::{S3_BUCKET}/*\", f\"arn:aws:s3:::{S3_BUCKET}\"]},\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": f\"arn:aws:s3:::{S3_BUCKET}/rft-output/*\"},\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"lambda:InvokeFunction\", \"Resource\": lambda_arn}\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=BEDROCK_ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(bedrock_trust_policy),\n",
    "        Description=\"Execution role for Bedrock RFT\"\n",
    "    )\n",
    "    bedrock_role_arn = response['Role']['Arn']\n",
    "    print(f\"‚úì Created role: {BEDROCK_ROLE_NAME}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    bedrock_role_arn = iam_client.get_role(RoleName=BEDROCK_ROLE_NAME)['Role']['Arn']\n",
    "    print(f\"‚úì Using existing role: {BEDROCK_ROLE_NAME}\")\n",
    "\n",
    "iam_client.put_role_policy(RoleName=BEDROCK_ROLE_NAME, PolicyName='BedrockRFTPermissions', PolicyDocument=json.dumps(bedrock_permissions))\n",
    "print(f\"‚úì Bedrock role ready: {bedrock_role_arn}\")\n",
    "\n",
    "cleanup_lambda_deployment_package()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ROLE_ARN = BEDROCK_ROLE_NAME\n",
    "TARGET_ACCOUNT_ID =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "AWS_REGION = \"us-west-2\" \n",
    "MANTLE_ENDPOINT = \"https://bedrock-mantle.us-west-2.api.aws\"\n",
    "\n",
    "from aws_bedrock_token_generator import provide_token\n",
    "\n",
    "ST_BEDROCK_API_KEY = provide_token(region=\"us-west-2\")\n",
    "print(f\"Token: {ST_BEDROCK_API_KEY}\")\n",
    "\n",
    "# Fine-tuning configuration\n",
    "MODEL_ID = \"qwen.qwen3-32b-v1:0\"  # Change to your model\n",
    "TRAINING_FILE_PATH = \"rft_train_data.jsonl\"  # Training data file\n",
    "\n",
    "print(f\"Target Role: {TARGET_ROLE_ARN}\")\n",
    "print(f\"Account ID: {TARGET_ACCOUNT_ID}\")\n",
    "print(f\"Region: {AWS_REGION}\")\n",
    "print(f\"Endpoint: {MANTLE_ENDPOINT}\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Training File: {TRAINING_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Create OpenAI Client with Bedrock API Key Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import urllib3\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "def create_openai_client(endpoint: str,\n",
    "                        region: str,\n",
    "                        account_id: Optional[str] = None,\n",
    "                        verify_ssl: bool = False) -> OpenAI:\n",
    "    \"\"\"\n",
    "    Create OpenAI client with custom SigV4-signing transport.\n",
    "    \"\"\"\n",
    "\n",
    "    return OpenAI(\n",
    "        base_url=f\"{endpoint}/v1\",\n",
    "        api_key=ST_BEDROCK_API_KEY,  # Required by SDK but not used with SigV4\n",
    "    )\n",
    "\n",
    "# Suppress SSL warnings if not verifying SSL\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Create the client\n",
    "client = create_openai_client(\n",
    "    endpoint=MANTLE_ENDPOINT,\n",
    "    region=AWS_REGION,\n",
    "    account_id=TARGET_ACCOUNT_ID,\n",
    "    verify_ssl=False  # Set to True for production\n",
    ")\n",
    "\n",
    "print(\"‚úÖ OpenAI client created successfully!\")\n",
    "print(f\"   Base URL: {MANTLE_ENDPOINT}/v1\")\n",
    "print(f\"   Account ID: {TARGET_ACCOUNT_ID}\")\n",
    "print(f\"   Region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4: Learn about OpenAI compatible Fine-Tuning API Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 1: List Fine-Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìã API Operation 1: List Fine-Tuning Jobs\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# List fine-tuning jobs\n",
    "response = client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(response.model_dump(), indent=2))\n",
    "print()\n",
    "print(\"List Fine Tune job call completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 2: File Operations\n",
    "\n",
    "### 2.a. Upload Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üì§ API Operation 2: Upload Training File\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Upload training file: {TRAINING_FILE_PATH}\")\n",
    "print()\n",
    "\n",
    "# Upload training file\n",
    "with open(TRAINING_FILE_PATH, 'rb') as f:\n",
    "    file_response = client.files.create(\n",
    "        file=f,\n",
    "        purpose='fine-tune'\n",
    "    )\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(file_response.model_dump(), indent=2))\n",
    "print()\n",
    "\n",
    "# Store file ID for next steps\n",
    "training_file_id = file_response.id\n",
    "print(f\"‚úÖ Training file uploaded successfully: {training_file_id}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. List files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìÅ List Files\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# List files\n",
    "files_response = client.files.list(purpose='fine-tune')\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(files_response.model_dump(), indent=2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c. Retrieve file details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìÑ Retrieve File Details\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Get details about file: {training_file_id}\")\n",
    "print()\n",
    "\n",
    "# Retrieve file details\n",
    "file_details = client.files.retrieve(training_file_id)\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(file_details.model_dump(), indent=2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d. Delete file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete a file\n",
    "#delete_response = client.files.delete(\"14c68926-ecdb-4d13-aad8-8076208fdbfa\")\n",
    "#print(json.dumps(delete_response.model_dump(), indent=2))\n",
    "\n",
    "import warnings\n",
    "warnings.warn(\"This code snippet is provided for educational purposes. Here, we will continue the training without deleting the file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 3: Create Fine-Tuning Job (RFT)\n",
    "\n",
    "**Note**: Update the Lambda ARN with your actual Lambda function for RFT jobs.\n",
    "For Supervised Fine-Tuning (SFT), omit the `extra_body` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üèóÔ∏è  API Operation 3: Create Fine-Tuning Job (RFT)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Create fine-tuning job with RFT method\n",
    "job_response = client.fine_tuning.jobs.create(\n",
    "    model=MODEL_ID,\n",
    "    training_file=training_file_id,\n",
    "    # Suffix field is not supported so commenting for now.\n",
    "    # suffix=\"rft-example\",  # Optional: suffix for fine-tuned model name\n",
    "    extra_body={\n",
    "        \"method\": {\n",
    "            \"type\": \"reinforcement\",  # Use \"supervised\" for SFT\n",
    "            \"reinforcement\": {\n",
    "                \"grader\": {\n",
    "                    \"type\": \"lambda\",\n",
    "                    \"lambda\": {\n",
    "                        \"function\": lambda_arn  # Replace with your Lambda ARN\n",
    "                    }\n",
    "                },\n",
    "                \"hyperparameters\": {\n",
    "                    \"n_epochs\": 1,  # Number of training epochs\n",
    "                    \"batch_size\": 4,  # Batch size\n",
    "                    \"learning_rate_multiplier\": 1.0  # Learning rate multiplier\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(job_response.model_dump(), indent=2))\n",
    "print()\n",
    "\n",
    "# Store job ID for next steps\n",
    "job_id = job_response.id\n",
    "print(f\"‚úÖ Fine-tuning job created successfully: {job_id}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 4: List Jobs with Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìã API Operation 4: List Jobs (Filtered)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# List jobs with limit and pagination\n",
    "response = client.fine_tuning.jobs.list(\n",
    "    limit=20  # Maximum number of jobs to return\n",
    ")\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(response.model_dump(), indent=2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 5: Describe Specific Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üîç API Operation 5: Describe Specific Job\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Get detailed information about job: {job_id}\")\n",
    "print()\n",
    "\n",
    "# Retrieve specific job details\n",
    "job_details = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(job_details.model_dump(), indent=2))\n",
    "print(f\"RFT job is currently: {job_details.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 6: List Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä API Operation 6: List Events\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"List all events for job: {job_id}\")\n",
    "print()\n",
    "\n",
    "# List events for the fine-tuning job\n",
    "events_response = client.fine_tuning.jobs.list_events(\n",
    "    fine_tuning_job_id=job_id,\n",
    "    limit=100  # Maximum number of events to return\n",
    ")\n",
    "\n",
    "# Print raw response\n",
    "print(json.dumps(events_response.model_dump(), indent=2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics from job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt                                                                                                                \n",
    "\n",
    "data = events_response                                                                                                                                   \n",
    "# Extract metric events                                                                                                                        \n",
    "metrics = [e.data for e in events_response.data if e.type == \"metrics\"]                                                                        \n",
    "   \n",
    "steps = [m[\"step\"] for m in metrics]                                                                                                           \n",
    "fields = [\"critic_rewards_mean\", \"actor_pg_loss\", \"actor_entropy\",\n",
    "        \"actor_grad_norm\", \"critic_advantages_mean\", \"response_length_mean\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Fine-Tuning Metrics\", fontsize=14)\n",
    "\n",
    "for ax, field in zip(axes.flat, fields):\n",
    "  values = [m[field] for m in metrics]\n",
    "  ax.plot(steps, values, linewidth=1.2)\n",
    "  ax.set_title(field)\n",
    "  ax.set_xlabel(\"Step\")\n",
    "  ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to read these metrics\n",
    "Here we basically plot the \"metrics\" parts of the emitted events from the job. An example of a metric is shown below:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "      \"id\": \"ftevent-c3c14785-4a3b-4dab-99a5-a15aeb6c0742\",\n",
    "      \"created_at\": 1771442218,\n",
    "      \"level\": \"info\",\n",
    "      \"message\": \"Step 4/67: training metrics\",\n",
    "      \"object\": \"fine_tuning.job.event\",\n",
    "      \"data\": {\n",
    "        \"total_steps\": 67,\n",
    "        \"actor_grad_norm\": 0.0008667297661304474,\n",
    "        \"response_length_mean\": 519.09375,\n",
    "        \"step\": 4,\n",
    "        \"actor_pg_loss\": 0.10153239965438844,\n",
    "        \"critic_rewards_mean\": 0.4375,\n",
    "        \"actor_entropy\": 0.6235736012458801,\n",
    "        \"critic_advantages_mean\": 0.013622610829770563\n",
    "      },\n",
    "      \"type\": \"metrics\"\n",
    "```\n",
    "\n",
    "Let's discuss what these mean:\n",
    "\n",
    "| Metric | Meaning |                                                                                                                                          \n",
    "  |---|---|                                                                                                                     \n",
    "  | **step** / **total_steps** | Current training step / out of total  |                                                                                 \n",
    "  | **critic_rewards_mean** | Average reward score across the batch (0.4375 means ~44% of responses got correct answers from your grader). This is the primary metric to watch ‚Äî you want it trending up. |                                                                                                                  \n",
    "  | **actor_pg_loss** | Policy gradient loss. This is the objective being optimized ‚Äî how much the model's policy is being pushed toward higher-reward responses. Fluctuates naturally; no single \"good\" value. |\n",
    "  | **actor_entropy** | How spread out the model's token probability distribution is. Higher = more exploratory/diverse outputs. If it collapses toward 0, the model is becoming too deterministic (mode collapse). You want it to decrease gradually, not crash. |\n",
    "  | **actor_grad_norm** | Magnitude of the gradient update to the actor (the model). Large spikes can indicate training instability. Yours is very small (0.0009), which suggests stable, conservative updates. |\n",
    "  | **critic_advantages_mean** | Average advantage estimate ‚Äî how much better/worse a response was compared to the critic's baseline prediction. Near-zero (0.014) means the critic is well-calibrated. Large positive values mean the model is doing much better than expected; large negative means worse. |\n",
    "  | **response_length_mean** | Average token length of generated responses (519). Worth monitoring ‚Äî if it grows unboundedly, the model may be gaming length for reward. |\n",
    "\n",
    "  **What to watch for during training:**\n",
    "  - `critic_rewards_mean` trending upward = model is learning\n",
    "  - `actor_entropy` collapsing to 0 = mode collapse (bad)\n",
    "  - `actor_grad_norm` spiking = instability\n",
    "  - `response_length_mean` exploding = reward hacking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Operation 7: List Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ API Operation 7: List Checkpoints\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"List all checkpoints for job: {job_id}\")\n",
    "print()\n",
    "\n",
    "# List checkpoints for the fine-tuning job\n",
    "try:\n",
    "    checkpoints_response = client.fine_tuning.jobs.checkpoints.list(\n",
    "        fine_tuning_job_id=job_id\n",
    "    )\n",
    "    \n",
    "    # Print raw response\n",
    "    print(json.dumps(checkpoints_response.model_dump(), indent=2))\n",
    "    print()\n",
    "    \n",
    "    if checkpoints_response.data:\n",
    "        print(f\"‚úÖ Found {len(checkpoints_response.data)} checkpoint(s)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No checkpoints available yet (job may still be running)\")\n",
    "    print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error listing checkpoints: {e}\")\n",
    "    print(\"   Note: Checkpoints are only available after the job starts training\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Additional API Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel a Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to cancel a job\n",
    "#cancel_response = client.fine_tuning.jobs.cancel(job_id)\n",
    "#print(json.dumps(cancel_response.model_dump(), indent=2))\n",
    "\n",
    "warnings.warn(\"To cancel a job, uncomment the code above and replace job_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Fine-Tuned Model\n",
    "\n",
    "Get the fine-tuned model ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = client.fine_tuning.jobs.retrieve(job_id)\n",
    "job_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once your job completes, you can run inference like this:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if job_details.status == 'succeeded' and job_details.fine_tuned_model:\n",
    "    fine_tuned_model = job_details.fine_tuned_model\n",
    "    print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
    "    print()\n",
    "    \n",
    "    # Run inference\n",
    "    inference_response = client.chat.completions.create(\n",
    "        model=fine_tuned_model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Write a 100 word essay on Euclid's contributions.\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(json.dumps(inference_response.model_dump(), indent=2))\n",
    "    print()\n",
    "else:\n",
    "    print(f\"Job status: {job_details.status}\")\n",
    "    print(\"Job must be in 'succeeded' status to run inference\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"Uncomment the code above to run inference after job completes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test Streaming\n",
    "from openai import OpenAI\n",
    "\n",
    "example1 = \"\"\"Gina chooses what she and her sister will watch on Netflix three times as often as her sister does. If her sister watches a total of 24 shows on Netflix per week, and each show is 50 minutes long, how many minutes of Netflix does Gina get to choose? Let's think step by step and output the final answer after '####'.\"\"\"\n",
    "\n",
    "example2 = \"\"\"In the honey shop, the bulk price of honey is $5 per pound and the minimum spend is $40 before tax. The honey is taxed at $1 per pound. If Penny has paid $240 for honey, by how many pounds has Penny‚Äôs purchase exceed the minimum spend? Let's think step by step and output the final answer after \"####\".\"\"\"\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "stream = client.responses.create(\n",
    "    model= fine_tuned_model, \n",
    "    # model = MODEL_ID, # Base model\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": example1,\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    "    reasoning = {\"effort\":\"low\"}\n",
    ")\n",
    "\n",
    "for event in stream:\n",
    "    # Each event has a 'type' and 'data'\n",
    "    \n",
    "    # print(event.type)\n",
    "    if event.type in ['response.reasoning_part.added','response.reasoning_part.done']:\n",
    "        print(Fore.GREEN + Style.DIM + \"\\n<thinking>\\n\")\n",
    "    if event.type == 'response.reasoning_text.delta':\n",
    "        print(Fore.GREEN + Style.DIM + event.delta, end=\"\", flush=True)\n",
    "    if event.type in ['response.output_text.delta',]:\n",
    "        print(Fore.BLACK + Style.RESET_ALL + event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showed end-to-end reinforcement fine-tuning (RFT) of GPT-OSS 20B on GSM8K math problems via Bedrock's OpenAI-compatible APIs. A quick recap of the steps we performed: we set up a Lambda-based reward function that scores model responses by extracting answers and comparing them to ground truth, then created the necessary IAM roles for both Lambda and Bedrock. Using a short-term Bedrock API key and the OpenAI SDK, we uploaded training data, kicked off an RFT job with a single epoch, and monitored its progress through events and checkpoints. Once training completed (67 steps, 4 checkpoints), we ran inference against the fine-tuned model using both chat completions and streaming with reasoning, and benchmarked it against the base model on latency and throughput.\n",
    "\n",
    "For more information, please visit the documentation here - https://docs.aws.amazon.com/bedrock/latest/userguide/fine-tuning-openai-apis.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Benchmarking base vs. fine-tuned model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand how fine-tuning affects not just model accuracy but also inference performance. Reinforcement fine-tuning modifies the model's weights to favor higher-reward responses, but this can also change generation characteristics ‚Äî response length, reasoning depth, and token distributions all shift. The benchmarking snippet below compares the base GPT-OSS 20B model against the fine-tuned version you just created across three key dimensions: time to first token (TTFT), output throughput (tokens per second), and total latency. This helps you evaluate whether the accuracy gains from RFT come with any inference cost trade-offs, and informs decisions about deployment readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                                                                                                                                    \n",
    "import tiktoken                                      \n",
    "import matplotlib.pyplot as plt                                                                                                                \n",
    "import matplotlib.patches as mpatches                                                                                                          \n",
    "                                                                                                                                             \n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")                                                                                                     \n",
    "\n",
    "def benchmark_model(client, model_id, prompt, label=\"model\"):\n",
    "  stream = client.responses.create(\n",
    "      model=model_id,\n",
    "      input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "      stream=True,\n",
    "      reasoning={\"effort\": \"low\"}\n",
    "  )\n",
    "\n",
    "  start = time.perf_counter()\n",
    "  ttft = None\n",
    "  first_output_token_time = None\n",
    "  reasoning_text = \"\"\n",
    "  output_text = \"\"\n",
    "\n",
    "  for event in stream:\n",
    "      now = time.perf_counter()\n",
    "      if event.type == 'response.reasoning_text.delta':\n",
    "          if ttft is None:\n",
    "              ttft = now - start\n",
    "          reasoning_text += event.delta\n",
    "      elif event.type == 'response.output_text.delta':\n",
    "          if first_output_token_time is None:\n",
    "              first_output_token_time = now - start\n",
    "          output_text += event.delta\n",
    "\n",
    "  end = time.perf_counter()\n",
    "  total_time = end - start\n",
    "\n",
    "  reasoning_tokens = len(enc.encode(reasoning_text))\n",
    "  output_tokens = len(enc.encode(output_text))\n",
    "  total_tokens = reasoning_tokens + output_tokens\n",
    "\n",
    "  gen_duration = total_time - (ttft if ttft else 0)\n",
    "  output_duration = total_time - (first_output_token_time if first_output_token_time else 0)\n",
    "\n",
    "  result = {\n",
    "      \"label\": label,\n",
    "      \"ttft\": ttft or 0,\n",
    "      \"time_to_first_output\": first_output_token_time or 0,\n",
    "      \"total_time\": total_time,\n",
    "      \"reasoning_tokens\": reasoning_tokens,\n",
    "      \"output_tokens\": output_tokens,\n",
    "      \"total_tokens\": total_tokens,\n",
    "      \"total_tokens_per_sec\": total_tokens / gen_duration if gen_duration > 0 else 0,\n",
    "      \"output_tokens_per_sec\": output_tokens / output_duration if output_duration > 0 else 0,\n",
    "  }\n",
    "\n",
    "  print(f\"\\n{'='*60}\")\n",
    "  print(f\"  {label}\")\n",
    "  print(f\"{'='*60}\")\n",
    "  print(f\"  TTFT (first reasoning token):  {result['ttft']:.3f}s\")\n",
    "  print(f\"  Time to first output token:    {result['time_to_first_output']:.3f}s\")\n",
    "  print(f\"  Total time:                    {result['total_time']:.3f}s\")\n",
    "  print(f\"  Reasoning tokens:              {result['reasoning_tokens']}\")\n",
    "  print(f\"  Output tokens:                 {result['output_tokens']}\")\n",
    "  print(f\"  Total tokens:                  {result['total_tokens']}\")\n",
    "  print(f\"  Total tokens/sec:              {result['total_tokens_per_sec']:.1f}\")\n",
    "  print(f\"  Output tokens/sec:             {result['output_tokens_per_sec']:.1f}\")\n",
    "  print(f\"{'='*60}\\n\")\n",
    "\n",
    "  return result\n",
    "\n",
    "# --- Run benchmarks ---\n",
    "prompt = \"Write a 1000-word essay on the history and future of artificial intelligence.\"\n",
    "base_result = benchmark_model(client, MODEL_ID, prompt, label=\"Base Model\")\n",
    "ft_result = benchmark_model(client, fine_tuned_model, prompt, label=\"Fine-Tuned Model\")\n",
    "\n",
    "# --- Plot ---\n",
    "plt.rcParams.update({\n",
    "  'font.family': 'sans-serif',\n",
    "  'font.size': 11,\n",
    "  'axes.spines.top': False,\n",
    "  'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "fig.suptitle(\"Base vs Fine-Tuned Model ‚Äî Inference Performance\",\n",
    "           fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "colors = [\"#2563EB\", \"#F97316\"]\n",
    "labels = [\"Base\", \"Fine-Tuned\"]\n",
    "\n",
    "def pct_change(base, ft):\n",
    "  if base == 0:\n",
    "      return 0\n",
    "  return ((ft - base) / base) * 100\n",
    "\n",
    "def annotate_change(ax, base_val, ft_val, y_offset_frac=0.15, higher_is_better=False):\n",
    "  change = pct_change(base_val, ft_val)\n",
    "  is_improvement = (change > 0 and higher_is_better) or (change < 0 and not higher_is_better)\n",
    "  color = \"#16A34A\" if is_improvement else \"#DC2626\"\n",
    "  arrow_symbol = \"‚ñ≤\" if change > 0 else \"‚ñº\"\n",
    "  sign = \"+\" if change > 0 else \"\"\n",
    "\n",
    "  max_val = max(base_val, ft_val)\n",
    "  y_pos = max_val + max_val * y_offset_frac\n",
    "\n",
    "  ax.annotate(\n",
    "      f\"{arrow_symbol} {sign}{change:.1f}%\",\n",
    "      xy=(0.5, y_pos), fontsize=12, fontweight='bold',\n",
    "      color=color, ha='center', va='bottom',\n",
    "      bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.12,\n",
    "                edgecolor=color, linewidth=1.2)\n",
    "  )\n",
    "  ax.plot([0, 1], [y_pos, y_pos], color=color, linewidth=1.5, alpha=0.6)\n",
    "  ax.plot([0, 0], [base_val + max_val*0.01, y_pos], color=color, linewidth=1, alpha=0.4, linestyle='--')\n",
    "  ax.plot([1, 1], [ft_val + max_val*0.01, y_pos], color=color, linewidth=1, alpha=0.4, linestyle='--')\n",
    "\n",
    "# Panel 1: TTFT\n",
    "vals = [base_result[\"ttft\"], ft_result[\"ttft\"]]\n",
    "bars = axes[0].bar(labels, vals, color=colors, width=0.5, edgecolor='white', linewidth=1.5)\n",
    "axes[0].set_title(\"Time to First Token\", fontweight='bold', pad=12)\n",
    "axes[0].set_ylabel(\"Seconds (lower is better)\", fontsize=10, color=\"#666\")\n",
    "for bar, v in zip(bars, vals):\n",
    "  axes[0].text(bar.get_x() + bar.get_width()/2, v + max(vals)*0.01,\n",
    "               f\"{v:.3f}s\", ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "annotate_change(axes[0], vals[0], vals[1], higher_is_better=False)\n",
    "\n",
    "# Panel 2: Output Tokens/sec\n",
    "vals = [base_result[\"output_tokens_per_sec\"], ft_result[\"output_tokens_per_sec\"]]\n",
    "bars = axes[1].bar(labels, vals, color=colors, width=0.5, edgecolor='white', linewidth=1.5)\n",
    "axes[1].set_title(\"Output Throughput\", fontweight='bold', pad=12)\n",
    "axes[1].set_ylabel(\"Tokens/sec (higher is better)\", fontsize=10, color=\"#666\")\n",
    "for bar, v in zip(bars, vals):\n",
    "  axes[1].text(bar.get_x() + bar.get_width()/2, v + max(vals)*0.01,\n",
    "               f\"{v:.1f}\", ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "annotate_change(axes[1], vals[0], vals[1], higher_is_better=True)\n",
    "\n",
    "# Panel 3: Total Time\n",
    "vals = [base_result[\"total_time\"], ft_result[\"total_time\"]]\n",
    "bars = axes[2].bar(labels, vals, color=colors, width=0.5, edgecolor='white', linewidth=1.5)\n",
    "axes[2].set_title(\"Total Latency\", fontweight='bold', pad=12)\n",
    "axes[2].set_ylabel(\"Seconds (lower is better)\", fontsize=10, color=\"#666\")\n",
    "for bar, v in zip(bars, vals):\n",
    "  axes[2].text(bar.get_x() + bar.get_width()/2, v + max(vals)*0.01,\n",
    "               f\"{v:.2f}s\", ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "annotate_change(axes[2], vals[0], vals[1], higher_is_better=False)\n",
    "\n",
    "for ax in axes:\n",
    "  ymin, ymax = ax.get_ylim()\n",
    "  ax.set_ylim(0, ymax * 1.4)\n",
    "  ax.tick_params(axis='x', labelsize=11)\n",
    "\n",
    "legend_elements = [mpatches.Patch(facecolor=colors[0], label='Base Model'),\n",
    "                 mpatches.Patch(facecolor=colors[1], label='Fine-Tuned Model')]\n",
    "fig.legend(handles=legend_elements, loc='upper left', frameon=True,\n",
    "         fontsize=10, edgecolor='#ccc', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
