{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7e17da",
   "metadata": {},
   "source": [
    "# Reinforcement Fine-Tuning Amazon Nova 2.0 Lite with PandaLM\n",
    "\n",
    "This notebook walks through training an Amazon Nova model using Reinforcement Fine-Tuning (RFT) on the [PandaLM](https://github.com/WeOpenML/PandaLM) evaluation dataset.\n",
    "\n",
    "## What's RFT?\n",
    "\n",
    "Traditional fine-tuning shows a model examples and says \"produce outputs like this.\" RFT takes a different approach: it lets the model generate its own responses, then uses a reward signal to reinforce good outputs and discourage bad ones.\n",
    "\n",
    "## What's PandaLM?\n",
    "\n",
    "PandaLM is a dataset designed to train and evaluate LLM-as-a-judge models. Each example contains:\n",
    "\n",
    "- **Instruction**: A task description\n",
    "- **Input**: Optional context for the task\n",
    "- **Response 1 & 2**: Two model responses to compare\n",
    "- **Human annotations**: Which response is better (1, 2, or tie)\n",
    "\n",
    "The goal is to train a model that can accurately evaluate and compare LLM outputs—essentially creating an AI judge for RLAIF (Reinforcement Learning from AI Feedback).\n",
    "\n",
    "> *Example:*\n",
    "> \n",
    "> **Instruction:** Rewrite the sentence to make it clearer and more concise.\n",
    "> \n",
    "> **Input:** \"If you have any questions about my rate or if you find it necessary to increase or decrease the scope...\"\n",
    "> \n",
    "> **Response 1:** \"If you have any questions about my rate, please let me know.\"\n",
    "> \n",
    "> **Response 2:** \"If you have any questions, please let me know.\"\n",
    "> \n",
    "> **Label:** Response 2 is better (more concise)\n",
    "\n",
    "## What we'll build\n",
    "\n",
    "1. Prepare PandaLM data in the format Bedrock RFT expects\n",
    "2. Deploy a Lambda function that uses LLM-as-judge to score model evaluations\n",
    "3. Kick off an RFT training job on Amazon Bedrock\n",
    "4. Monitor the job until completion\n",
    "\n",
    "By the end, you'll have a Nova model that's better at evaluating LLM outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531fca5",
   "metadata": {},
   "source": [
    "## Prerequisites: SageMaker Role Permissions\n",
    "\n",
    "**NOTE:** If you are running this notebook using an AWS Profile with Admin you can skip this cell...\n",
    "\n",
    "....otherwise this Jupyter notebook requires your SageMaker execution role to have these IAM permissions:\n",
    "\n",
    "| Service | Actions | Resources | Why |\n",
    "|---------|---------|-----------|-----|\n",
    "| **S3** | `PutObject`, `GetObject`, `ListBucket`, `DeleteObject` | `arn:aws:s3:::YOUR-BUCKET/*` and `arn:aws:s3:::YOUR-BUCKET` | Upload/download training data |\n",
    "| **IAM** | `CreateRole`, `GetRole`, `AttachRolePolicy`, `PutRolePolicy`, **`PassRole`** | `arn:aws:iam::ACCOUNT:role/PANDALM-Lambda-Role`, `arn:aws:iam::ACCOUNT:role/BedrockRFT-pandalm-Role` | Create Lambda & Bedrock roles |\n",
    "| **Lambda** | `CreateFunction`, `GetFunction`, `UpdateFunctionCode`, `InvokeFunction` | `arn:aws:lambda:REGION:ACCOUNT:function:pandalm-reward-function` | Deploy reward function |\n",
    "| **Bedrock** | `CreateModelCustomizationJob`, `GetModelCustomizationJob`, `InvokeModel` | `*` | Start/monitor training, LLM-as-judge |\n",
    "| **STS** | `GetCallerIdentity` | `*` | Get account info |\n",
    "\n",
    "**Critical**: The Lambda function needs `bedrock:InvokeModel` permission to call the LLM judge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c1cec",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU boto3 botocore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e989f8d",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Data Prep\n",
    "\n",
    "First, set your AWS region, S3 bucket, and profile. Then we'll pull PandaLM from GitHub, format it for Bedrock RFT, and upload to S3.\n",
    "\n",
    "For this RLAIF use-case, we format each example as an evaluation task where the model must judge which response is better and provide reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77851e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "\n",
    "from helpers import (\n",
    "    create_lambda_deployment_package,\n",
    "    cleanup_lambda_deployment_package\n",
    ")\n",
    "\n",
    "# ============== UPDATE THESE VALUES ==============\n",
    "AWS_REGION = \"us-east-1\"\n",
    "S3_BUCKET = \"your-bucket-name\"\n",
    "AWS_PROFILE = None  # Set to your profile name, or None for default credentials\n",
    "# =================================================\n",
    "\n",
    "# Create session\n",
    "session = boto3.Session(profile_name=AWS_PROFILE, region_name=AWS_REGION) if AWS_PROFILE else boto3.Session(region_name=AWS_REGION)\n",
    "AWS_ACCOUNT_ID = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"pandalm\"\n",
    "PANDALM_URL = \"https://raw.githubusercontent.com/WeOpenML/PandaLM/main/data/testset-v1.json\"\n",
    "TOTAL_SAMPLES = None  # Set to None to use all available data, or an integer to limit\n",
    "LOCAL_DATA_DIR = \"../../tmp-data\"\n",
    "\n",
    "assert S3_BUCKET != \"your-bucket-name\", \"Please update S3_BUCKET with your own bucket name\"\n",
    "S3_OUTPUT_PATH = f\"s3://{S3_BUCKET}/rft-output/\"\n",
    "\n",
    "# Resource names\n",
    "LAMBDA_FUNCTION_NAME = f\"{DATASET_NAME}-reward-function\"\n",
    "LAMBDA_ROLE_NAME = f\"{DATASET_NAME.upper()}-Lambda-Role\"\n",
    "BEDROCK_ROLE_NAME = f\"BedrockRFT-{DATASET_NAME}-Role\"\n",
    "REWARD_FUNCTION_FILE = f\"../../reward-functions/{DATASET_NAME}_rew_func.py\"\n",
    "REWARD_FUNCTION_MODULE = f\"{DATASET_NAME}_rew_func\"\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL_ID = f\"arn:aws:bedrock:{AWS_REGION}::foundation-model/amazon.nova-2-lite-v1:0:256k\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3_client = session.client('s3')\n",
    "bedrock_client = session.client('bedrock')\n",
    "lambda_client = session.client('lambda')\n",
    "iam_client = session.client('iam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_size(n):\n",
    "    \"\"\"Format sample count as human-readable string (e.g., 7k, 1.2k).\"\"\"\n",
    "    if n >= 1000:\n",
    "        return f\"{n/1000:.0f}k\" if n % 1000 == 0 else f\"{n/1000:.1f}k\"\n",
    "    return str(n)\n",
    "\n",
    "def preprocess_pandalm(url, total_samples, output_dir, train_ratio=0.8, val_ratio=0.1):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading {url}...\")\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        data = json.loads(response.read().decode())\n",
    "\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    available = len(data)\n",
    "    total = min(total_samples, available) if total_samples else available\n",
    "\n",
    "    train_size = int(total * train_ratio)\n",
    "    val_size = int(total * val_ratio)\n",
    "    test_size = total - train_size - val_size\n",
    "\n",
    "    def get_majority_label(item):\n",
    "        \"\"\"Get majority vote from annotators (1=resp1 better, 2=resp2 better, 0=tie).\"\"\"\n",
    "        votes = [item.get(\"annotator1\", 0), item.get(\"annotator2\", 0), item.get(\"annotator3\", 0)]\n",
    "        return max(set(votes), key=votes.count)\n",
    "\n",
    "    def label_to_text(label):\n",
    "        if label == 1:\n",
    "            return \"Response 1 is better\"\n",
    "        elif label == 2:\n",
    "            return \"Response 2 is better\"\n",
    "        return \"Both responses are equally good (tie)\"\n",
    "\n",
    "    def format_row(item, idx, split):\n",
    "        label = get_majority_label(item)\n",
    "        instruction = item.get(\"instruction\", \"\")\n",
    "        input_text = item.get(\"input\", \"\")\n",
    "        response1 = item.get(\"response1\", \"\")\n",
    "        response2 = item.get(\"response2\", \"\")\n",
    "\n",
    "        # Build the evaluation task prompt\n",
    "        task = f\"Instruction: {instruction}\"\n",
    "        if input_text:\n",
    "            task += f\"\\n\\nInput: {input_text}\"\n",
    "        task += f\"\\n\\nResponse 1:\\n{response1}\\n\\nResponse 2:\\n{response2}\"\n",
    "\n",
    "        user_content = f\"\"\"You are an expert evaluator comparing two AI responses.\n",
    "\n",
    "{task}\n",
    "\n",
    "Evaluate both responses and determine which is better. Provide:\n",
    "1. Your judgment: Which response is better (Response 1, Response 2, or Tie)\n",
    "2. Your reasoning: Explain why, considering accuracy, helpfulness, and quality\n",
    "\n",
    "Format your answer as:\n",
    "JUDGMENT: [Response 1 / Response 2 / Tie]\n",
    "REASONING: [Your detailed explanation]\"\"\"\n",
    "\n",
    "        ground_truth = f\"JUDGMENT: {label_to_text(label)}\\nREASONING: Based on the task requirements, {'Response 1' if label == 1 else 'Response 2' if label == 2 else 'both responses'} better addresses the instruction.\"\n",
    "\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert AI evaluator who compares model responses and provides detailed judgments with reasoning.\"},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "            \"reference_answer\": {\n",
    "                \"label\": label,\n",
    "                \"question\": task,\n",
    "                \"ground_truth\": ground_truth\n",
    "            },\n",
    "            \"task_id\": f\"pandalm_{split}_{idx}\",\n",
    "            \"domain\": \"evaluation\",\n",
    "            \"data_source\": \"pandalm\"\n",
    "        }\n",
    "\n",
    "    def write_split(data, start_idx, size, filename, split_name):\n",
    "        with open(f\"{output_dir}/{filename}\", \"w\") as f:\n",
    "            for i, item in enumerate(data[start_idx:start_idx + size]):\n",
    "                f.write(json.dumps(format_row(item, i, split_name)) + \"\\n\")\n",
    "        print(f\"✓ Created {output_dir}/{filename} ({size} samples)\")\n",
    "\n",
    "    write_split(data, 0, train_size, \"train.jsonl\", \"train\")\n",
    "    write_split(data, train_size, val_size, \"val.jsonl\", \"val\")\n",
    "    write_split(data, train_size + val_size, test_size, \"test.jsonl\", \"test\")\n",
    "\n",
    "    return train_size, val_size, test_size\n",
    "\n",
    "print(\"Preprocessing PandaLM dataset...\")\n",
    "train_size, val_size, test_size = preprocess_pandalm(PANDALM_URL, TOTAL_SAMPLES, LOCAL_DATA_DIR)\n",
    "\n",
    "# S3 paths with sample counts in filenames\n",
    "S3_TRAINING_DATA = f\"s3://{S3_BUCKET}/rft-data/datasets/{DATASET_NAME}/train-{format_size(train_size)}.jsonl\"\n",
    "S3_VALIDATION_DATA = f\"s3://{S3_BUCKET}/rft-data/datasets/{DATASET_NAME}/val-{format_size(val_size)}.jsonl\"\n",
    "S3_TEST_DATA = f\"s3://{S3_BUCKET}/rft-data/datasets/{DATASET_NAME}/test-{format_size(test_size)}.jsonl\"\n",
    "\n",
    "print(\"\\nUploading to S3...\")\n",
    "for local_file, s3_uri in [\n",
    "    (\"train.jsonl\", S3_TRAINING_DATA),\n",
    "    (\"val.jsonl\", S3_VALIDATION_DATA),\n",
    "    (\"test.jsonl\", S3_TEST_DATA)\n",
    "]:\n",
    "    s3_key = '/'.join(s3_uri.split('/')[3:])\n",
    "    s3_client.upload_file(f\"{LOCAL_DATA_DIR}/{local_file}\", S3_BUCKET, s3_key)\n",
    "    print(f\"✓ Uploaded {s3_uri.split('/')[-1]}\")\n",
    "\n",
    "print(f\"\\n✓ Ready | {train_size} train / {val_size} val / {test_size} test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"\\nCleaning up temporary files...\")\n",
    "if os.path.exists(LOCAL_DATA_DIR):\n",
    "    shutil.rmtree(LOCAL_DATA_DIR)\n",
    "    print(f\"✓ Removed {LOCAL_DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"✓ No temporary files to clean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440ba0e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Deploy the Reward Function\n",
    "\n",
    "For PandaLM, we use an LLM-as-judge approach (RLAIF). The Lambda function calls a Bedrock model to evaluate how well the training model's judgment matches the ground truth human annotations.\n",
    "\n",
    "This is different from RLVR (like GSM8K/FinQA) where we can programmatically verify correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf53e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lambda execution role with Bedrock permissions\n",
    "print(\"Creating Lambda execution role...\")\n",
    "\n",
    "lambda_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"lambda.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "\n",
    "lambda_permissions = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\"], \"Resource\": \"arn:aws:logs:*:*:*\"},\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"bedrock:InvokeModel\", \"bedrock:Converse\"], \"Resource\": \"*\"}  # For LLM-as-judge\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=LAMBDA_ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(lambda_trust_policy),\n",
    "        Description=f\"Execution role for {DATASET_NAME} reward function\"\n",
    "    )\n",
    "    lambda_role_arn = response['Role']['Arn']\n",
    "    iam_client.put_role_policy(RoleName=LAMBDA_ROLE_NAME, PolicyName='LambdaBedrockPolicy', PolicyDocument=json.dumps(lambda_permissions))\n",
    "    print(f\"✓ Created role: {LAMBDA_ROLE_NAME}\")\n",
    "    print(\"Waiting 10s for role propagation...\")\n",
    "    time.sleep(10)\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    lambda_role_arn = iam_client.get_role(RoleName=LAMBDA_ROLE_NAME)['Role']['Arn']\n",
    "    iam_client.put_role_policy(RoleName=LAMBDA_ROLE_NAME, PolicyName='LambdaBedrockPolicy', PolicyDocument=json.dumps(lambda_permissions))\n",
    "    print(f\"✓ Using existing role: {LAMBDA_ROLE_NAME}\")\n",
    "\n",
    "# Package and deploy Lambda\n",
    "lambda_zip_content = create_lambda_deployment_package(\n",
    "    source_file=REWARD_FUNCTION_FILE,\n",
    "    zip_filename=\"lambda_deployment.zip\",\n",
    "    archive_name=f\"{REWARD_FUNCTION_MODULE}.py\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDeploying Lambda: {LAMBDA_FUNCTION_NAME}...\")\n",
    "try:\n",
    "    lambda_client.get_function(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "    lambda_client.update_function_code(FunctionName=LAMBDA_FUNCTION_NAME, ZipFile=lambda_zip_content)\n",
    "    waiter = lambda_client.get_waiter('function_updated_v2')\n",
    "    waiter.wait(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "    print(\"✓ Updated existing function\")\n",
    "except lambda_client.exceptions.ResourceNotFoundException:\n",
    "    lambda_client.create_function(\n",
    "        FunctionName=LAMBDA_FUNCTION_NAME,\n",
    "        Runtime='python3.11',\n",
    "        Role=lambda_role_arn,\n",
    "        Handler=f\"{REWARD_FUNCTION_MODULE}.lambda_handler\",\n",
    "        Code={'ZipFile': lambda_zip_content},\n",
    "        Timeout=300,  # Longer timeout for LLM calls\n",
    "        MemorySize=512\n",
    "    )\n",
    "    print(\"✓ Created new function\")\n",
    "\n",
    "waiter = lambda_client.get_waiter('function_active_v2')\n",
    "waiter.wait(FunctionName=LAMBDA_FUNCTION_NAME)\n",
    "lambda_arn = lambda_client.get_function(FunctionName=LAMBDA_FUNCTION_NAME)['Configuration']['FunctionArn']\n",
    "print(f\"✓ Lambda ready: {lambda_arn}\")\n",
    "\n",
    "# Create Bedrock role\n",
    "print(f\"\\nCreating Bedrock role: {BEDROCK_ROLE_NAME}...\")\n",
    "\n",
    "bedrock_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"bedrock.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]\n",
    "}\n",
    "\n",
    "bedrock_permissions = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\"Effect\": \"Allow\", \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"], \"Resource\": [f\"arn:aws:s3:::{S3_BUCKET}/*\", f\"arn:aws:s3:::{S3_BUCKET}\"]},\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": f\"arn:aws:s3:::{S3_BUCKET}/rft-output/*\"},\n",
    "        {\"Effect\": \"Allow\", \"Action\": \"lambda:InvokeFunction\", \"Resource\": lambda_arn}\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=BEDROCK_ROLE_NAME,\n",
    "        AssumeRolePolicyDocument=json.dumps(bedrock_trust_policy),\n",
    "        Description=\"Execution role for Bedrock RFT\"\n",
    "    )\n",
    "    bedrock_role_arn = response['Role']['Arn']\n",
    "    print(f\"✓ Created role: {BEDROCK_ROLE_NAME}\")\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    bedrock_role_arn = iam_client.get_role(RoleName=BEDROCK_ROLE_NAME)['Role']['Arn']\n",
    "    print(f\"✓ Using existing role: {BEDROCK_ROLE_NAME}\")\n",
    "\n",
    "iam_client.put_role_policy(RoleName=BEDROCK_ROLE_NAME, PolicyName='BedrockRFTPermissions', PolicyDocument=json.dumps(bedrock_permissions))\n",
    "print(f\"✓ Bedrock role ready: {bedrock_role_arn}\")\n",
    "\n",
    "cleanup_lambda_deployment_package()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09f5f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Test the Reward Function\n",
    "\n",
    "Before kicking off training, let's verify the LLM-as-judge reward function works correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing reward function...\")\n",
    "\n",
    "test_payload = [{\n",
    "    \"id\": \"test_001\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Evaluate these two responses...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"JUDGMENT: Response 2 is better\\nREASONING: Response 2 is more concise and directly addresses the task without unnecessary repetition.\"}\n",
    "    ],\n",
    "    \"reference_answer\": {\n",
    "        \"label\": 2,\n",
    "        \"question\": \"Which response better rewrites the sentence?\",\n",
    "        \"ground_truth\": \"JUDGMENT: Response 2 is better\\nREASONING: Response 2 is more concise.\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=LAMBDA_FUNCTION_NAME,\n",
    "    InvocationType='RequestResponse',\n",
    "    Payload=json.dumps(test_payload)\n",
    ")\n",
    "\n",
    "result = json.loads(response['Payload'].read())\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "if 'errorMessage' in result:\n",
    "    print(f\"\\n✗ Error: {result['errorMessage']}\")\n",
    "elif isinstance(result, list) and result[0].get('aggregate_reward_score', 0) > 0:\n",
    "    print(\"\\n✓ Reward function working correctly!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Unexpected result - check the output above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c392f",
   "metadata": {},
   "source": [
    "### Test with Real Training & Validation Data\n",
    "\n",
    "Let's verify the reward function works correctly with actual samples from our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples_from_s3(s3_uri, n=5):\n",
    "    bucket = s3_uri.split('/')[2]\n",
    "    key = '/'.join(s3_uri.split('/')[3:])\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    lines = obj['Body'].read().decode('utf-8').strip().split('\\n')\n",
    "    return [json.loads(line) for line in random.sample(lines, min(n, len(lines)))]\n",
    "\n",
    "def simulate_correct_response(sample):\n",
    "    \"\"\"Add an assistant message with a correct-ish judgment.\"\"\"\n",
    "    label = sample['reference_answer']['label']\n",
    "    judgment = \"Response 1 is better\" if label == 1 else \"Response 2 is better\" if label == 2 else \"Tie\"\n",
    "    sample_copy = sample.copy()\n",
    "    sample_copy['messages'] = sample['messages'] + [\n",
    "        {'role': 'assistant', 'content': f'JUDGMENT: {judgment}\\nREASONING: Based on the evaluation criteria, this response better addresses the task requirements.'}\n",
    "    ]\n",
    "    return sample_copy\n",
    "\n",
    "print('Loading samples from S3...')\n",
    "train_samples = load_samples_from_s3(S3_TRAINING_DATA, n=5)\n",
    "val_samples = load_samples_from_s3(S3_VALIDATION_DATA, n=5)\n",
    "\n",
    "test_payloads = [simulate_correct_response(s) for s in train_samples + val_samples]\n",
    "\n",
    "print(f'Testing {len(test_payloads)} samples (5 train + 5 val)...')\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName=LAMBDA_FUNCTION_NAME,\n",
    "    InvocationType='RequestResponse',\n",
    "    Payload=json.dumps(test_payloads)\n",
    ")\n",
    "results = json.loads(response['Payload'].read())\n",
    "\n",
    "print('\\nResults:')\n",
    "for r in results:\n",
    "    score = r.get('aggregate_reward_score', 0)\n",
    "    status = '✓' if score >= 0.5 else '⚠'\n",
    "    print(f\"  {status} {r['id']}: {score:.2f}\")\n",
    "\n",
    "avg_score = sum(r.get('aggregate_reward_score', 0) for r in results) / len(results)\n",
    "print(f'\\nAverage score: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6145d8",
   "metadata": {},
   "source": [
    "### Analyze Dataset for Hyperparameter Selection\n",
    "\n",
    "**Key hyperparameters to consider:**\n",
    "\n",
    "| Parameter | What it controls | Trade-off |\n",
    "|-----------|-----------------|-----------|\n",
    "| `maxPromptLength` | Max tokens for input prompts | Higher = more context, but more memory & slower training |\n",
    "| `inferenceMaxTokens` | Max tokens the model can generate per response | Higher = longer reasoning, but slower & more expensive |\n",
    "| `trainingSamplePerPrompt` | Number of response samples per prompt | More samples = better reward estimation, but slower |\n",
    "| `batchSize` | Samples per training batch | Larger = more stable gradients, but more memory |\n",
    "\n",
    "**For PandaLM specifically:**\n",
    "- Prompts include instruction + input + two full responses (can be long)\n",
    "- Model needs to output judgment + detailed reasoning\n",
    "- LLM-as-judge scoring is slower than programmatic verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tiktoken\n",
    "import tiktoken\n",
    "import statistics\n",
    "\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "print('Analyzing training data...')\n",
    "obj = s3_client.get_object(Bucket=S3_BUCKET, Key='/'.join(S3_TRAINING_DATA.split('/')[3:]))\n",
    "samples = [json.loads(line) for line in obj['Body'].read().decode('utf-8').strip().split('\\n')]\n",
    "\n",
    "prompt_tokens = []\n",
    "for s in samples:\n",
    "    prompt_text = ' '.join(m['content'] for m in s['messages'])\n",
    "    prompt_tokens.append(count_tokens(prompt_text))\n",
    "\n",
    "print(f'\\nDataset Statistics ({len(samples)} samples)')\n",
    "print(f'\\nPrompt tokens (input):')\n",
    "print(f'  Min: {min(prompt_tokens)}, Max: {max(prompt_tokens)}, Mean: {statistics.mean(prompt_tokens):.0f}')\n",
    "print(f'  P95: {sorted(prompt_tokens)[int(len(prompt_tokens)*0.95)]}, P99: {sorted(prompt_tokens)[int(len(prompt_tokens)*0.99)]}')\n",
    "\n",
    "recommended_prompt_len = sorted(prompt_tokens)[int(len(prompt_tokens)*0.99)] * 2\n",
    "print(f'\\nRecommended hyperparameters:')\n",
    "print(f'  maxPromptLength: {recommended_prompt_len} (2x P99 prompt length)')\n",
    "print(f'  inferenceMaxTokens: 500 (judgment + reasoning)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b239225",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Start the RFT Training Job\n",
    "\n",
    "Now we'll create a model customization job using RLAIF (LLM-as-judge) to train the model to be a better evaluator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5060c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating RFT training job...\")\n",
    "\n",
    "from datetime import datetime\n",
    "date_str = datetime.now().strftime('%Y%m%d')\n",
    "hp_suffix = f\"e{1}_bs{16}_lr{5e-5}\".replace('.', '').replace('-', '')\n",
    "CUSTOM_MODEL_NAME = f\"{DATASET_NAME}-nova-rft-{date_str}-{hp_suffix}\"\n",
    "JOB_NAME = f\"{DATASET_NAME}-rft-{date_str}-{int(time.time())}\"\n",
    "\n",
    "print(f\"  Job: {JOB_NAME}\")\n",
    "print(f\"  Model: {CUSTOM_MODEL_NAME}\")\n",
    "print(f\"  Base: {BASE_MODEL_ID}\")\n",
    "\n",
    "response = bedrock_client.create_model_customization_job(\n",
    "    jobName=JOB_NAME,\n",
    "    customModelName=CUSTOM_MODEL_NAME,\n",
    "    roleArn=bedrock_role_arn,\n",
    "    baseModelIdentifier=BASE_MODEL_ID,\n",
    "    customizationType='REINFORCEMENT_FINE_TUNING',\n",
    "    trainingDataConfig={'s3Uri': S3_TRAINING_DATA},\n",
    "    validationDataConfig={'validators': [{'s3Uri': S3_VALIDATION_DATA}]},\n",
    "    outputDataConfig={'s3Uri': S3_OUTPUT_PATH},\n",
    "    customizationConfig={\n",
    "        'rftConfig': {\n",
    "            'graderConfig': {'lambdaGrader': {'lambdaArn': lambda_arn}},\n",
    "            'hyperParameters': {\n",
    "                'batchSize': 16,  # Smaller batch for LLM-as-judge (slower reward computation)\n",
    "                'epochCount': 1,  # Start with 1; increase if validation rewards still rising\n",
    "                'evalInterval': 10,  # Eval every 10 steps\n",
    "                'inferenceMaxTokens': 500,  # Room for judgment + detailed reasoning\n",
    "                'learningRate': 0.00005,\n",
    "                'maxPromptLength': 2500,  # PandaLM prompts include two full responses - adjust based on analysis\n",
    "                'reasoningEffort': 'high',\n",
    "                'trainingSamplePerPrompt': 2  # Fewer samples since LLM-as-judge is expensive\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Job created: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657ff09",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Monitor Training Progress\n",
    "\n",
    "Run this cell periodically to check on your training job. Status will progress through: `InProgress` → `Completed` (or `Failed`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.get_model_customization_job(jobIdentifier=JOB_NAME)\n",
    "print(f\"Job: {JOB_NAME}\")\n",
    "print(f\"Status: {response['status']}\")\n",
    "\n",
    "if response['status'] == 'Completed' and 'outputModelArn' in response:\n",
    "    print(f\"\\n✓ Training complete!\")\n",
    "    print(f\"  Model ARN: {response['outputModelArn']}\")\n",
    "elif response['status'] == 'Failed':\n",
    "    print(f\"\\n✗ Training failed: {response.get('failureMessage', 'Unknown error')}\")\n",
    "elif response['status'] == 'InProgress':\n",
    "    print(\"\\nStill training... run this cell again to check progress\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2d4d4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations, you've successfully launched a Reinforcement Fine-Tuning job for Amazon Nova on the PandaLM evaluation dataset.\n",
    "\n",
    "### What You've Built\n",
    "\n",
    "- **Preprocessed PandaLM dataset** into Bedrock RFT format for evaluation tasks\n",
    "- **Deployed an LLM-as-judge Lambda** that scores model evaluations using RLAIF\n",
    "- **Created IAM roles** for Lambda and Bedrock execution\n",
    "- **Started an RFT training job** with customized hyperparameters\n",
    "\n",
    "### Key Differences from RLVR (GSM8K/FinQA)\n",
    "\n",
    "| Aspect | RLVR (Math) | RLAIF (PandaLM) |\n",
    "|--------|-------------|-----------------|\n",
    "| Reward Signal | Programmatic verification | LLM-as-judge |\n",
    "| Speed | Fast | Slower (LLM calls) |\n",
    "| Cost | Lower | Higher |\n",
    "| Use Case | Verifiable answers | Subjective quality |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Once your training job completes:\n",
    "\n",
    "1. **Test your fine-tuned model** as an evaluator on held-out examples\n",
    "2. **Compare judgments** against human annotations\n",
    "3. **Use the model** for RLAIF in other training pipelines\n",
    "4. **Experiment with hyperparameters** for better performance\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [Amazon Bedrock RFT Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/reinforcement-fine-tuning.html)\n",
    "- [PandaLM Paper](https://arxiv.org/abs/2306.05087)\n",
    "- [RLAIF: Scaling Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2309.00267)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc6cd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
