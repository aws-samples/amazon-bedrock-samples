{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune and Evaluate Amazon Nova Micro model provided by Amazon Bedrock: End-to-End\n",
    "\n",
    "In this notebook we demonstrate using Boto3 sdk for the fine-tuning and provisioning of [Nova Micro](https://aws.amazon.com/bedrock/nova/) model in Bedrock. You can also do this through the Bedrock Console.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Warning:</b> This module cannot be executed in Workshop Studio Accounts, and you will have to run this notebook in your own account.\n",
    "\n",
    "</div>\n",
    "\n",
    "### A Summarization Use Case\n",
    "\n",
    "In this notebook, we build an end-to-end workflow for fine-tuning and evaluating the Foundation Models (FMs) in Amazon Bedrock. We choose [Amazon Nova Micro](https://aws.amazon.com/bedrock/nova/) as our FM to perform the customization through fine-tuning, we then create on demand inference deployment of the fine-tuned model, and test the on demand invocation using various API methods.\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`**, **`Python 3`**, and **`ml.c5.2xlarge`** kernel in SageMaker Studio*\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    " - Make sure you have executed `01_setup.ipynb` notebook.\n",
    " - Make sure you are using the same kernel and instance as `01_setup.ipynb` notebook.\n",
    "\n",
    "In this notebook we demonstrate using Boto3 sdk for the fine-tuning and provisioning of [Nova Micro](https://aws.amazon.com/bedrock/nova/) model in Bedrock. You can also do this through the Bedrock Console.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Warning:</b> This notebook will create provisioned throughput for testing the fine-tuned model. Therefore, please make sure to delete the provisioned throughput as mentioned in the last section of the notebook, otherwise you will be charged for it, even if you are not using it.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import all the needed libraries and dependencies to complete this notebook.\n",
    "\n",
    "Please ignore error messages related to pip's dependency resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install the fmeval package for foundation model evaluation\n",
    "\n",
    "!rm -Rf ~/.cache/pip/*\n",
    "!pip install tokenizers==0.12.1\n",
    "!pip install -qU fmeval==0.3.0\n",
    "!pip install awscurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Tips:\n",
    "\n",
    "⚠️ ⚠️ ⚠️ If you have trouble installing fmeval, please make sure you have the dependencies installed correctly. See full list of dependencies [here](https://github.com/aws/fmeval/blob/main/poetry.lock). ⚠️ ⚠️ ⚠️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel for packages to take effect\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching varialbes from `00_setup.ipynb` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r role_arn\n",
    "%store -r s3_train_uri\n",
    "%store -r s3_validation_uri\n",
    "%store -r s3_test_uri\n",
    "%store -r bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pp(role_arn)\n",
    "pprint.pp(s3_train_uri)\n",
    "pprint.pp(s3_validation_uri)\n",
    "pprint.pp(s3_test_uri)\n",
    "pprint.pp(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = \"us-east-1\" # Hardcoded to us-west-2 region\n",
    "sts_client = boto3.client('sts')\n",
    "s3_client = boto3.client('s3')\n",
    "aws_account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=region, endpoint_url='https://preprod.us-east-1.controlplane.bedrock.aws.dev')\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url='https://preprod.us-east-1.dataplane.bedrock.aws.dev/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = \"test-cnn-10.jsonl\"\n",
    "data_folder = \"fine-tuning-datasets-nova\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Fine-Tuning Job\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Note:</b> Fine-tuning job will take around 60mins to complete with 5K records.</div>\n",
    "\n",
    "Amazon Nova Micro customization hyperparameters: \n",
    "\n",
    "- `epochs`: The number of iterations through the entire training dataset and can take up any integer values in the range of 1-10, with a default value of 2.\n",
    "\n",
    "- `batchSize`: The number of samples processed before updating model parametersand can take up any integer values in the range of 1-64, with a default value of 1.\n",
    "\n",
    "- `learningRate`:\tThe rate at which model parameters are updated after each batch\twhich can take up a float value betweek 0.0-1.0 with a default value set to\t1.00E-5.\n",
    "\n",
    "\n",
    "For guidelines on setting hyper-parameters refer to the guidelines provided [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Choose the foundation model you want to customize and provide ModelId(find more about model reference at https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-reference.html)\n",
    "base_model_id = \"amazon.nova-micro-v1:0:128k\"\n",
    "\n",
    "# Select the customization type from \"FINE_TUNING\" or \"CONTINUED_PRE_TRAINING\". \n",
    "customization_type = \"FINE_TUNING\"\n",
    "\n",
    "# Specify the roleArn for your customization job\n",
    "customization_role = role_arn\n",
    "\n",
    "# Create a customization job name\n",
    "customization_job_name = f\"amazon-nova-micro-custom-job-{ts}\"\n",
    "\n",
    "# Create a customized model name for your fine-tuned Nova Micro model\n",
    "custom_model_name = f\"amazon-nova-micro-custom-model-{ts}\"\n",
    "\n",
    "# Define the hyperparameters for fine-tuning Nova Micro model\n",
    "hyper_parameters = {\n",
    "        \"epochCount\": \"2\",\n",
    "        \"batchSize\": \"1\",\n",
    "        \"learningRate\": \"0.00005\",\n",
    "    }\n",
    "\n",
    "# Specify your data path for training, validation(optional) and output\n",
    "training_data_config = {\"s3Uri\": s3_train_uri}\n",
    "\n",
    "# # uncomment the below section if you have validation dataset and provide the s3 uri for it. \n",
    "\n",
    "validation_data_config = {\n",
    "        \"validators\": [{\n",
    "            \"s3Uri\": s3_validation_uri\n",
    "        }]\n",
    "    }\n",
    "\n",
    "output_data_config = {\"s3Uri\": f's3://{bucket_name}/outputs/output-{custom_model_name}'}\n",
    "\n",
    "# # Create the customization job\n",
    "bedrock.create_model_customization_job(\n",
    "    customizationType=customization_type,\n",
    "    jobName=customization_job_name,\n",
    "    customModelName=custom_model_name,\n",
    "    roleArn=customization_role,\n",
    "    baseModelIdentifier=base_model_id,\n",
    "    hyperParameters=hyper_parameters,\n",
    "    trainingDataConfig=training_data_config,\n",
    "    validationDataConfig=validation_data_config,\n",
    "    outputDataConfig=output_data_config\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Customization Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "fine_tune_job = bedrock.get_model_customization_job(jobIdentifier=customization_job_name)[\"status\"]\n",
    "print(fine_tune_job)\n",
    "\n",
    "while fine_tune_job == \"InProgress\":\n",
    "    time.sleep(60)\n",
    "    fine_tune_job = bedrock.get_model_customization_job(jobIdentifier=customization_job_name)[\"status\"]\n",
    "    print (fine_tune_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Custom Model\n",
    "\n",
    "Once the customization job is finished, you can check your existing custom model(s) and retrieve the modelArn of your fine-tuned Nova Micro model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can list your custom models using the command below\n",
    "bedrock.list_custom_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>Note:</b> Please make sure your customization job status is \"completed\" before proceeding to retrieve the modelArn, otherwise you will run into errors. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the modelArn of the fine-tuned model\n",
    "fine_tune_job = bedrock.get_custom_model(modelIdentifier=custom_model_name)\n",
    "custom_model_id = fine_tune_job['modelArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_job_name = \"model-customization-job-\"+fine_tune_job['jobArn'].split('/')[-1]\n",
    "output_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training and Validation Loss\n",
    "\n",
    "Now that we have completed fine-tuning job, lets visualize our results to see if our job is not underfitting or overfitting. \n",
    "Download model customization job metrics from S3 and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics_path = f\"fine-tuning-datasets-nova/{output_job_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $output_metrics_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_s3_prefix=f'outputs/output-{custom_model_name}/{output_job_name}/training_artifacts/step_wise_training_metrics.csv'\n",
    "validation_metrics_s3_prefix=f'outputs/output-{custom_model_name}/{output_job_name}/validation_artifacts/post_fine_tuning_validation/validation/validation_metrics.csv'\n",
    "train_metrics_name='train_metrics.csv'\n",
    "validation_metrics_name='validation_metrics.csv'\n",
    "train_file_name_local=output_metrics_path+'/'+train_metrics_name\n",
    "validation_file_name_local=output_metrics_path+'/'+validation_metrics_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(bucket_name, train_metrics_s3_prefix, train_file_name_local)\n",
    "s3_client.download_file(bucket_name, validation_metrics_s3_prefix, validation_file_name_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_file_name_local)\n",
    "'''The training loss is at an iteration level. To calculate loss at the epoch level,\n",
    "\n",
    "    average the iteration-level loss for each epoch'''\n",
    "train_metrics_epoch=train_data.groupby('epoch_number').mean()\n",
    "validation_metrics_epoch=pd.read_csv(validation_file_name_local)\n",
    "plt.plot(validation_metrics_epoch.epoch_number, validation_metrics_epoch.validation_loss,label='validation')\n",
    "plt.plot(train_metrics_epoch.index, train_metrics_epoch.training_loss,label='training')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Fine-tuned Model\n",
    "\n",
    "After fine-tuning is complete, we need to deploy the model to make it available for inference. The following command creates a deployment for our fine-tuned Nova Micro model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awscurl -i --service bedrock -X POST  https://preprod.us-east-1.controlplane.bedrock.aws.dev/model-customization/custom-model-deployments --data '{\"modelDeploymentName\":\"novafinetuned\",\"modelArn\":\"arn:aws:bedrock:us-east-1:656681812069:custom-model/amazon.nova-micro-v1:0:128k/3q67l58zazx9\",\"description\":\"foo\",\"clientRequestToken\":\"foo\",\"tags\":[{\"key\":\"test\",\"value\":\"test\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Deployed Model\n",
    "\n",
    "Now that we have deployed our fine-tuned Nova Micro model, we can test it using various API methods provided by Amazon Bedrock. Each method offers different capabilities for interacting with the model.\n",
    "\n",
    "### Converse API\n",
    "The Converse API allows for synchronous conversation with the model, receiving the complete response at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "brt = boto3.client(service_name='bedrock-runtime', region_name = 'us-east-1', endpoint_url='https://preprod.us-east-1.dataplane.bedrock.aws.dev/')\n",
    "\n",
    "try:\n",
    "    response = brt.converse(\n",
    "        modelId='arn:aws:bedrock:us-east-1:656681812069:custom-model-deployment/uholep6rb2hc',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {\n",
    "                        'text': 'tell me a joke in 100 words',\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Request ID:\", response['ResponseMetadata']['RequestId'])\n",
    "    result = response.get('output')\n",
    "    print(result)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(result['message']['content'][0]['text'])\n",
    "\n",
    "except Exception as e:\n",
    "    # Print the full error response for debugging\n",
    "    print(\"Error:\", e)\n",
    "    # Extract and print the Request ID from the error response\n",
    "    if 'ResponseMetadata' in e.response:\n",
    "        print(\"Request ID:\", e.response['ResponseMetadata']['RequestId'])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converse Stream API\n",
    "The Converse Stream API provides the response as a stream of tokens, allowing for faster time-to-first-token and progressive rendering of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "brt = boto3.client(service_name='bedrock-runtime', region_name = 'us-east-1', endpoint_url='https://preprod.us-east-1.dataplane.bedrock.aws.dev/')\n",
    "\n",
    "try:\n",
    "    response = brt.converse_stream(\n",
    "        \n",
    "        modelId='arn:aws:bedrock:us-east-1:656681812069:custom-model-deployment/uholep6rb2hc',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {\n",
    "                        'text': 'tell me a joke in 100 words',\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for chunk in response[\"stream\"]:\n",
    "        if \"contentBlockDelta\" in chunk:\n",
    "            text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "            print(text, end=\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Print the full error response for debugging\n",
    "    print(\"Error:\", e)\n",
    "    # Extract and print the Request ID from the error response\n",
    "    if 'ResponseMetadata' in e.response:\n",
    "        print(\"Request ID:\", e.response['ResponseMetadata']['RequestId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API\n",
    "The Invoke API allows for direct model invocation with custom system prompts and inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "brt = boto3.client(service_name='bedrock-runtime', region_name = 'us-east-1', endpoint_url='https://preprod.us-east-1.dataplane.bedrock.aws.dev/')\n",
    "\n",
    "# Define your system prompt(s).\n",
    "system_list = [\n",
    "            {\n",
    "                \"text\": \"Act as a creative writing assistant. When the user provides you with a topic, write a short story about that topic.\"\n",
    "            }\n",
    "]\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [{\"role\": \"user\", \"content\": [{\"text\": \"A camping trip\"}]}]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 500, \"topP\": 0.9, \"topK\": 20, \"temperature\": 0.7}\n",
    "request_body = {\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params\n",
    "\n",
    "}\n",
    "\n",
    "body = json.dumps(request_body)\n",
    "try:\n",
    "    response = brt.invoke_model(\n",
    "        modelId='arn:aws:bedrock:us-east-1:656681812069:custom-model-deployment/uholep6rb2hc',\n",
    "        body=body\n",
    "\n",
    "    )\n",
    "    print(\"Request ID:\", response['ResponseMetadata']['RequestId'])\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except Exception as e:\n",
    "    # Print the full error response for debugging\n",
    "    print(\"Error:\", e)\n",
    "    # Extract and print the Request ID from the error response\n",
    "    if 'ResponseMetadata' in e.response:\n",
    "        print(\"Request ID:\", e.response['ResponseMetadata']['RequestId'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Stream API\n",
    "Similar to the Converse Stream API, this provides streaming responses but with the Invoke API's flexibility for custom configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "brt = boto3.client(service_name='bedrock-runtime', region_name = 'us-east-1', endpoint_url='https://preprod.us-east-1.dataplane.bedrock.aws.dev//')\n",
    "\n",
    "system_list = [\n",
    "            {\n",
    "                \"text\": \"Act as a creative writing assistant. When the user provides you with a topic, write a short story about that topic.\"\n",
    "            }\n",
    "]\n",
    "# Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "message_list = [{\"role\": \"user\", \"content\": [{\"text\": \"A camping trip\"}]}]\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inf_params = {\"maxTokens\": 500, \"topP\": 0.9, \"topK\": 20, \"temperature\": 0.7}\n",
    "\n",
    "request_body = {\n",
    "    \"schemaVersion\": \"messages-v1\",\n",
    "    \"messages\": message_list,\n",
    "    \"system\": system_list,\n",
    "    \"inferenceConfig\": inf_params,\n",
    "}\n",
    "\n",
    "try:\n",
    "    start_time = datetime.now()\n",
    "    chunk_count = 0\n",
    "    time_to_first_token = None\n",
    "    response = brt.invoke_model_with_response_stream(\n",
    "        modelId='arn:aws:bedrock:us-east-1:656681812069:custom-model-deployment/uholep6rb2hc',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "\n",
    "    stream = response.get(\"body\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get(\"chunk\")\n",
    "            if chunk:\n",
    "                # Print the response chunk\n",
    "                chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "                # Pretty print JSON\n",
    "                # print(json.dumps(chunk_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "                content_block_delta = chunk_json.get(\"contentBlockDelta\")\n",
    "                if content_block_delta:\n",
    "                    if time_to_first_token is None:\n",
    "                        time_to_first_token = datetime.now() - start_time\n",
    "                        print(f\"Time to first token: {time_to_first_token}\")\n",
    "    \n",
    "                    chunk_count += 1\n",
    "                    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:%f\")\n",
    "                    # print(f\"{current_time} - \", end=\"\")\n",
    "                    print(content_block_delta.get(\"delta\").get(\"text\"), end=\"\")\n",
    "\n",
    "        print(f\"Total chunks: {chunk_count}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No response stream received.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Print the full error response for debugging\n",
    "    print(\"Error:\", e)\n",
    "    # Extract and print the Request ID from the error response\n",
    "    if 'ResponseMetadata' in e.response:\n",
    "        print(\"Request ID:\", e.response['ResponseMetadata']['RequestId'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
