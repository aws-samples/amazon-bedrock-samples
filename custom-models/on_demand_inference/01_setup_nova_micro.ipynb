{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for running customization notebooks for fine-tuning using Amazon Bedrock with Nova Micro\n",
    "\n",
    "In this notebook, we will create a set of roles and an S3 bucket which will be used for Nova Micro fine-tuning. We'll also prepare the dataset in the required format for Nova Micro.\n",
    "\n",
    "> This notebook should work well with the **`Data Science 3.0`**, **`Python 3`**, and **`ml.t3.medium`** kernel in SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Custom job role\n",
    "\n",
    "The notebook allows you to either create a Bedrock role for running customization jobs in the **Create IAM customisation job role** section or you can skip this section and create Bedrock Service role for customization jobs following [instructions on managing permissions for customization jobs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html). If you want to use an existing custom job role please edit the variable **customization_role** and also ensure it has access to the S3 bucket which is created containing the dataset.\n",
    "\n",
    "#### Create IAM Pre-requisites\n",
    "\n",
    "This notebook requires permissions to:\n",
    "- create and delete Amazon IAM roles\n",
    "- create, update and delete Amazon S3 buckets\n",
    "- access Amazon Bedrock\n",
    "\n",
    "If you are running this notebook without an Admin role, make sure that your role includes the following managed policies:\n",
    "- IAMFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonBedrockFullAccess\n",
    "\n",
    "- You can also create a custom model in the Bedrock console following the instructions [here](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import all the needed libraries and dependencies to complete this notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Please ignore error messages related to pip's dependency resolver.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n",
    "\n",
    "!pip install -qU --force-reinstall langchain typing_extensions pypdf urllib3==2.1.0\n",
    "!pip install -qU ipywidgets>=7,<8\n",
    "!pip install jsonlines\n",
    "!pip install datasets==2.15.0\n",
    "!pip install pandas==2.1.3\n",
    "!pip install matplotlib==3.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel for packages to take effect\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3 \n",
    "import time\n",
    "import pprint\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "region = \"us-east-1\" \n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region}-{account_id}\"\n",
    "bucket_name = f\"bedrock-customization-{s3_suffix}\"\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=region) \n",
    "iam = boto3.client('iam', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"AmazonBedrockCustomizationRole1\"\n",
    "s3_bedrock_finetuning_access_policy=\"AmazonBedrockCustomizationPolicy1\"\n",
    "customization_role = f\"arn:aws:iam::{account_id}:role/{role_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing boto3 connection\n",
    "\n",
    "We will list the foundation models to test the boto3 connection and make sure bedrock client has been successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in bedrock.list_foundation_models(\n",
    "    byCustomizationType=\"FINE_TUNING\")[\"modelSummaries\"]:\n",
    "    for key, value in model.items():\n",
    "        print(key, \":\", value)\n",
    "    print(\"-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create S3 bucket\n",
    "\n",
    "In this step we will create an S3 bucket, which will be used to store data for fine-tuning with Nova Micro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket for knowledge base data source\n",
    "s3bucket = s3_client.create_bucket(\n",
    "    Bucket=bucket_name,\n",
    "    ## Uncomment the following if you run into errors\n",
    "    # CreateBucketConfiguration={\n",
    "    #     'LocationConstraint':region,\n",
    "    # },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating role and policies required to run customization jobs with Amazon Bedrock\n",
    "This JSON object defines the trust relationship that allows the bedrock service to assume a role that will give it the ability to talk to other required AWS services. The conditions set restrict the assumption of the role to a specific account ID and a specific component of the bedrock service (model_customization_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {{\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            }},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {{\n",
    "                \"StringEquals\": {{\n",
    "                    \"aws:SourceAccount\": \"{account_id}\"\n",
    "                }},\n",
    "                \"ArnEquals\": {{\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{region}:{account_id}:model-customization-job/*\"\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This JSON object defines the permissions of the role we want bedrock to assume to allow access to the S3 bucket that we created that will hold our fine-tuning datasets and allow certain bucket and object manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_POLICY_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetBucketAcl\",\n",
    "                \"s3:GetBucketNotification\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutBucketNotification\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{bucket_name}\",\n",
    "                \"arn:aws:s3:::{bucket_name}/*\"\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\"  \n",
    "\n",
    "response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=ROLE_DOC,\n",
    "    Description=\"Role for Bedrock to access S3 for finetuning\",\n",
    ")\n",
    "\n",
    "pprint.pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = response[\"Role\"][\"Arn\"]\n",
    "pprint.pp(role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = iam.create_policy(\n",
    "    PolicyName=s3_bedrock_finetuning_access_policy,\n",
    "    PolicyDocument=ACCESS_POLICY_DOC,\n",
    ")\n",
    "\n",
    "pprint.pp(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_arn = response[\"Policy\"][\"Arn\"]\n",
    "pprint.pp(policy_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for running other notebooks on fine-tuning with Nova Micro is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare CNN news article dataset for fine-tuning job and evaluation\n",
    "\n",
    "The dataset that will be used is a collection of news articles from CNN and the associated highlights from that article. More information can be found at huggingface: https://huggingface.co/datasets/cnn_dailymail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load cnn dataset from huggingface\n",
    "dataset = load_dataset(\"cnn_dailymail\",'3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the structure of the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Fine-tuning Dataset for Nova Micro\n",
    "\n",
    "For Nova Micro, we need to use the `bedrock-conversation-2024` schema format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "  \"system\": [\n",
    "    {\n",
    "      \"text\": \"System instruction here\"\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"User message here\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"text\": \"Assistant response here\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "We'll convert our CNN dataset to this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system instruction for summarization\n",
    "system_instruction = \"You are a helpful assistant that summarizes news articles accurately and concisely.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dataset to Nova Micro format\n",
    "def convert_to_nova_micro_format(data_point):\n",
    "    return {\n",
    "        \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "        \"system\": [\n",
    "            {\n",
    "                \"text\": system_instruction\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": f\"Summarize the following news article:\\n\\n{data_point['article']}\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": data_point['highlights']\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the datasets\n",
    "datapoints_train = [convert_to_nova_micro_format(dp) for dp in dataset['train']]\n",
    "datapoints_valid = [convert_to_nova_micro_format(dp) for dp in dataset['validation']]\n",
    "datapoints_test = [convert_to_nova_micro_format(dp) for dp in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example of the processed datapoint\n",
    "import json\n",
    "print(json.dumps(datapoints_train[4], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and filter the dataset\n",
    "\n",
    "We'll filter the dataset based on length and limit the number of samples. For Nova Micro, we'll cap the dataset at 20,000 samples as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_transform(data_points, num_dps, max_dp_length):\n",
    "    lines = []\n",
    "    for dp in data_points:\n",
    "        # Calculate total length of text in the datapoint\n",
    "        total_length = len(dp['system'][0]['text']) + \\\n",
    "                       len(dp['messages'][0]['content'][0]['text']) + \\\n",
    "                       len(dp['messages'][1]['content'][0]['text'])\n",
    "        \n",
    "        if total_length <= max_dp_length:\n",
    "            lines.append(dp)\n",
    "    \n",
    "    random.shuffle(lines)\n",
    "    lines = lines[:min(num_dps, 20000)]  # Cap at 20,000 samples as specified\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_converter(dataset, file_name):\n",
    "    print(file_name)\n",
    "    with jsonlines.open(file_name, 'w') as writer:\n",
    "        for line in dataset:\n",
    "            writer.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data partitions with a character limit of 3,000\n",
    "train = dp_transform(datapoints_train, 1000, 3000)  \n",
    "validation = dp_transform(datapoints_valid, 100, 3000)  \n",
    "test = dp_transform(datapoints_test, 10, 3000)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local directory for datasets\n",
    "\n",
    "Please note that your training dataset for fine-tuning cannot be greater than 20K records for Nova Micro, and validation dataset has a maximum limit of 1K records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"fine-tuning-datasets\"\n",
    "train_file_name = \"train-cnn-nova-micro.jsonl\"\n",
    "validation_file_name = \"validation-cnn-nova-micro.jsonl\"\n",
    "test_file_name = \"test-cnn-nova-micro.jsonl\"\n",
    "\n",
    "!mkdir -p fine-tuning-datasets\n",
    "abs_path = os.path.abspath(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSONL format datasets for Nova Micro fine-tuning\n",
    "jsonl_converter(train, f'{abs_path}/{train_file_name}')\n",
    "jsonl_converter(validation, f'{abs_path}/{validation_file_name}')\n",
    "jsonl_converter(test, f'{abs_path}/{test_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to S3 bucket\n",
    "\n",
    "Uploading both training and test dataset. \n",
    "\n",
    "We will use the training and validation datasets for fine-tuning the model. The test dataset will be used for evaluating the performance of the model on unseen input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(f'{abs_path}/{train_file_name}', bucket_name, f'fine-tuning-datasets/train/{train_file_name}')\n",
    "s3_client.upload_file(f'{abs_path}/{validation_file_name}', bucket_name, f'fine-tuning-datasets/validation/{validation_file_name}')\n",
    "s3_client.upload_file(f'{abs_path}/{test_file_name}', bucket_name, f'fine-tuning-datasets/test/{test_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_uri = f's3://{bucket_name}/fine-tuning-datasets/train/{train_file_name}'\n",
    "s3_validation_uri = f's3://{bucket_name}/fine-tuning-datasets/validation/{validation_file_name}'\n",
    "s3_test_uri = f's3://{bucket_name}/fine-tuning-datasets/test/{test_file_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing variables to be used in other notebooks\n",
    "\n",
    "> Please make sure to use the same kernel as used for 01_setup_nova_micro.ipynb for other notebooks on fine-tuning with Nova Micro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store role_arn\n",
    "%store bucket_name\n",
    "%store role_name\n",
    "%store policy_arn\n",
    "%store s3_train_uri\n",
    "%store s3_validation_uri\n",
    "%store s3_test_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready to create a fine-tuning job with Nova Micro on Amazon Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Created AWS infrastructure: S3 bucket for data storage and IAM role/policy for Bedrock customization jobs\n",
    "- Downloaded and processed CNN/DailyMail dataset, converting it to Nova Micro's \"bedrock-conversation-2024\" schema format for summarization tasks\n",
    "- Filtered and limit dataset to 5,000 training samples, 999 validation samples, and 10 test samples with 3,000 character limit per data point\n",
    "- Uploaded processed JSONL datasets to S3 bucket and generates S3 URIs for training pipeline\n",
    "- Stored key variables (role ARN, bucket name, S3 URIs) for use in subsequent fine-tuning notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
