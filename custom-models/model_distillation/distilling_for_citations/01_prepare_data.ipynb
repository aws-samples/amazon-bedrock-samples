{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "# %pip install --upgrade pip --quiet\n",
    "# %pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "Here we're using hugging face datasets library to import the data. Alternatively you can download manually and cleanse accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5ab6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from utils import read_jsonl_to_dataframe\n",
    "\n",
    "splits = {'train': 'squad_v2/train-00000-of-00001.parquet', 'validation': 'squad_v2/validation-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"train\"])\n",
    "df_eval = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81cde384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = df_train[df_train['question'] == 'Lenin acknowledged the dependence of which countries?']\n",
    "# test_df\n",
    "# Lenin acknowledged the dependence of which countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee8fe9",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "Here we'll be leveraging Nova Premier to generate both an answer and the sources of that answer.\n",
    "This style of prompting is most optimal for Nova models and will yield the accurate responses.\n",
    "\n",
    "You can see here we're leveraging XML output which Nova is optimized for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "807c3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nova prompt for citations\n",
    "system_prompt = \"\"\"\n",
    "You are a question answering assistant. I will provide you with document context. The user will provide you with a question. Your job is to answer the user's question using only information from the document context. If the document context does not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the document context to validate a user's assertion.\n",
    "\n",
    "However, you should include <sources> tags at the end of each <answer_part> to specify which source(s) the information came from.\n",
    "Note that <sources> may contain multiple <source> if you include information from multiple results in your answer.\n",
    "\n",
    "Do NOT directly quote the <context> in your answer. Your job is to answer the user's question as concisely as possible.\n",
    "\n",
    "You must output your answer in the following format. Pay attention and follow the formatting and spacing exactly:\n",
    "<answer>\n",
    "<answer_part>\n",
    "<text>\n",
    "first answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "<answer_part>\n",
    "<text>\n",
    "second answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6c78d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "277ad88d",
   "metadata": {},
   "source": [
    "## Set Annotation Style\n",
    "Here we'll declare the formatting of our annotation that we can use when preparing the distillation training set. This will be the format of the citations whenever the model responds with an answer.\n",
    "\n",
    "For the Squad data set, the only information available to use in the citation is the sentence or sentences the answer was derived from. You may want to include other identifying information in your data set such as page number, line number, paragraph number, etc.\n",
    "\n",
    "Here we'll use a simple enumerated list of citations with the sentence that corroborates the answer provided. For example, an answer to this question would look like the following:\n",
    "\n",
    "`question: Who ruled the duchy of Normandy`\n",
    "\n",
    "`answer: Richard I`\n",
    "\n",
    "`sources: [1] The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Prepare training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4943339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_structure(answers_dict):\n",
    "    \"\"\"\n",
    "    Parse different formats of answer dictionaries and extract text and start positions.\n",
    "    Returns lists of texts and start positions.\n",
    "    \"\"\"\n",
    "    # Case 1: NumPy arrays with direct keys\n",
    "    if 'text' in answers_dict and isinstance(answers_dict['text'], np.ndarray):\n",
    "        texts = answers_dict['text'].tolist()\n",
    "        starts = answers_dict['answer_start'].tolist()\n",
    "        \n",
    "    # Case 2: Lists or single values with direct keys\n",
    "    elif 'text' in answers_dict:\n",
    "        texts = answers_dict['text'] if isinstance(answers_dict['text'], list) else [answers_dict['text']]\n",
    "        starts = answers_dict['answer_start'] if isinstance(answers_dict['answer_start'], list) else [answers_dict['answer_start']]\n",
    "        \n",
    "    # Case 4: String JSON that needs parsing (handled in calling function)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown answer format: {answers_dict}\")\n",
    "        \n",
    "    return texts, starts\n",
    "\n",
    "def create_xml_answer(row, no_answer_text='I could not find an exact answer to the question.'):\n",
    "    try:\n",
    "        # Handle answers as string (JSON) if needed\n",
    "        answers_dict = row['answers']\n",
    "        if isinstance(answers_dict, str):\n",
    "            import json\n",
    "            answers_dict = json.loads(answers_dict)\n",
    "            \n",
    "        # Parse answer structure using our helper function\n",
    "        texts, starts = parse_answer_structure(answers_dict)\n",
    "        context = row['context']\n",
    "        \n",
    "        # Split context into sentences more accurately\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)\n",
    "        \n",
    "        # Build XML structure\n",
    "        xml_parts = ['<answer>']\n",
    "        \n",
    "        if len(texts) > 0:\n",
    "            for i, (text, start) in enumerate(zip(texts, starts)):\n",
    "                xml_parts.append('<answer_part>')\n",
    "                xml_parts.append('<text>')\n",
    "                xml_parts.append(str(text))\n",
    "                xml_parts.append('</text>')\n",
    "                xml_parts.append('<sources>')\n",
    "                \n",
    "                # Find the sentence containing the answer based on the start position\n",
    "                char_count = 0\n",
    "                source_sentence = \"No relevant source found\"\n",
    "                for sentence in sentences:\n",
    "                    sentence_len = len(sentence) + 1  # +1 for the space after sentence\n",
    "                    if char_count <= int(start) < (char_count + sentence_len):\n",
    "                        source_sentence = sentence.strip()\n",
    "                        break\n",
    "                    char_count += sentence_len\n",
    "                \n",
    "                xml_parts.append(f'<source>{source_sentence}</source>')\n",
    "                xml_parts.append('</sources>')\n",
    "                xml_parts.append('</answer_part>')\n",
    "        \n",
    "            xml_parts.append('</answer>')\n",
    "        else: # use no answer text\n",
    "            xml_parts.append(f\"<answer_part>\\n<text>\\n{no_answer_text}\\n</text>\\n</answer_part></answer>\")\n",
    "        return '\\n'.join(xml_parts)\n",
    "    except Exception as e:\n",
    "        return f\"<answer>\\n<error>Error generating XML: {str(e)}</error>\\n</answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_payload(row, model_type=\"conversation\", system_prompt=None, include_answer=False, additional_params=None):\n",
    "    \"\"\"\n",
    "    Create a payload dictionary for Amazon Bedrock API requests.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the pandas DataFrame containing context, question, and optionally answers\n",
    "        model_type: The type of model payload to create (\"conversation\" or \"invoke\")\n",
    "        system_prompt: The system message to include (for conversation-based models)\n",
    "        include_answer: Whether to include the answer in the conversation (for evaluation)\n",
    "        additional_params: Dictionary of additional parameters to include in the payload\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract needed information\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Create the user prompt with context and question\n",
    "        user_prompt = f\"\"\"<context>{context}</context> <question>{question}</question>\"\"\"\n",
    "        \n",
    "        # Get the answer if needed\n",
    "        assistant_response = create_xml_answer(row) if include_answer else None\n",
    "        \n",
    "        # Create appropriate payload based on model_type\n",
    "        if model_type == \"conversation\":\n",
    "            # For conversation-based models (Claude, etc.)\n",
    "            payload = {\n",
    "                \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Add assistant response if needed (for evaluation)\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "                \n",
    "        elif model_type == \"invoke\":\n",
    "            # For basic invoke request (non-conversation models like Titan, etc.)\n",
    "            payload = {\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"inferenceConfig\":{ \n",
    "                    # \"maxTokens\": int, // greater than 0, equal or less than 5k (default: dynamic*)\n",
    "                    \"temperature\": .1, # greater then 0 and less than 1.0 (default: 0.7)\n",
    "                    \"topP\": .9, # greater than 0, equal or less than 1.0 (default: 0.9)\n",
    "                    \"topK\": 50, # 0 or greater (default: 50)\n",
    "                    \"stopSequences\": ['</answer>']\n",
    "                }\n",
    "            }\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "            \n",
    "            # Add optional parameters specific to invoke requests\n",
    "            if additional_params:\n",
    "                payload.update(additional_params)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "            \n",
    "        # Add any additional parameters passed\n",
    "        if additional_params and model_type == \"conversation\":\n",
    "            # For conversation models, additional params might need to be added at the root level\n",
    "            for key, value in additional_params.items():\n",
    "                if key not in payload:\n",
    "                    payload[key] = value\n",
    "                    \n",
    "        return payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating payload for row: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_inf_record(row, system_prompt, include_answer=False): \n",
    "    conversation = create_bedrock_payload(\n",
    "                                row=row, \n",
    "                                system_prompt=system_prompt, \n",
    "                                model_type=\"invoke\", \n",
    "                                additional_params={},\n",
    "                                include_answer=include_answer)\n",
    "    return {\n",
    "        \"recordId\": row['id'],\n",
    "        \"modelInput\": conversation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "# Filter for empty answers\n",
    "empty_answers_df = df_train[df_train['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "with_answers_df = df_train[df_train['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "df_train_revised = pd.concat([\n",
    "    empty_answers_df.sample(n=7500, random_state=42), \n",
    "    with_answers_df.sample(n=7500, random_state=42)], ignore_index=True) # max 15k for bedrock distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7460c",
   "metadata": {},
   "source": [
    "## Create distillation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9914951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_revised['answer_xml'] = df_train_revised.apply(create_xml_answer, axis=1)\n",
    "df_train_revised['conversation'] = df_train_revised.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt), axis=1)\n",
    "df_train_revised['conversation'].to_json('distillation_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b86c",
   "metadata": {},
   "source": [
    "## Create batch inference data set\n",
    "We'll use this batch inference data set to run the inferences on both our distilled model and the off-the-shelf models for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_empty_answers_df = df_eval[df_eval['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "eval_with_answers_df = df_eval[df_eval['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "batch_inf_df = pd.concat([\n",
    "    eval_empty_answers_df.sample(n=250, random_state=15), \n",
    "    eval_with_answers_df.sample(n=250, random_state=15)], ignore_index=True)\n",
    "\n",
    "\n",
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt), axis=1).to_json('batch_inf_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd681e",
   "metadata": {},
   "source": [
    "## Create Labeled data set for BYOI Bedrock Evaluation\n",
    "Here we'll include the answer so we can use in our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45d625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt=system_prompt, include_answer=True), axis=1).to_json('labeled_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d377d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import json\n",
    "# model_id = 'us.amazon.nova-premier-v1:0'\n",
    "# bedrock_runtime = boto3.client(\n",
    "#     service_name=\"bedrock-runtime\",\n",
    "#     region_name=\"us-east-1\"\n",
    "# )\n",
    "\n",
    "# # Make the invoke call to Bedrock\n",
    "# response = bedrock_runtime.invoke_model(\n",
    "#     modelId=model_id,\n",
    "#     body=json.dumps(sample_payload)\n",
    "# )\n",
    "\n",
    "# # Parse and return the response\n",
    "# response_body = json.loads(response.get('body').read())\n",
    "# print(response_body['output']['message']['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c5b6fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_body['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ef8e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dupes\n",
    "# df_train_revised[df_train_revised.duplicated(subset=['conversation'], keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
