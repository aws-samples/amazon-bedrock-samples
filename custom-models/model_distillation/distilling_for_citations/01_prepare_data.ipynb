{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "# %pip install --upgrade pip --quiet\n",
    "# %pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "# Model Distillation for Question Answering with Cited Text\n",
    "\n",
    "This notebook is part of a series demonstrating advanced model distillation techniques for creating specialized, citation-aware question-answering models. The goal is to distill the knowledge from a large language model (Amazon Nova Premier) into a smaller, more efficient model while maintaining high-quality citation capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Prepare training data for citation model distillation\n",
    "- Design structured XML output formats for consistent answer generation\n",
    "- Implement source citation tracking in model responses\n",
    "- Create evaluation datasets for measuring citation accuracy\n",
    "\n",
    "## Dataset: SQuAD v2.0\n",
    "We use the [Stanford Question Answering Dataset (SQuAD) v2.0](https://rajpurkar.github.io/SQuAD-explorer/) as our base dataset. SQuAD v2.0 is particularly suitable for citation-aware model training because:\n",
    "\n",
    "1. Contains explicit answer spans within source text\n",
    "2. Includes \"impossible\" questions to test model reliability\n",
    "3. Provides diverse question types and domains\n",
    "4. Enables source attribution tracking\n",
    "\n",
    "The dataset is loaded using the [Hugging Face Datasets library](https://huggingface.co/docs/datasets/) and stored in Parquet format for optimal performance with large-scale training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ab6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patreil/.local/share/virtualenvs/distilling_for_citations-ex_cldZ-/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from utils import read_jsonl_to_dataframe\n",
    "\n",
    "splits = {'train': 'squad_v2/train-00000-of-00001.parquet', 'validation': 'squad_v2/validation-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"train\"])\n",
    "df_eval = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cde384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = df_train[df_train['question'] == 'Lenin acknowledged the dependence of which countries?']\n",
    "# test_df\n",
    "# Lenin acknowledged the dependence of which countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee8fe9",
   "metadata": {},
   "source": [
    "## Advanced System Prompt Engineering\n",
    "\n",
    "This section implements a specialized system prompt for [Amazon Bedrock's Anthropic Claude model](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude.html). The prompt engineering focuses on:\n",
    "\n",
    "### Core Requirements\n",
    "1. **Context-Bounded Responses**: Answers must be derived solely from provided context\n",
    "2. **Source Attribution**: Mandatory citation of source text for verification\n",
    "3. **Structured Output**: XML-based response format for consistent parsing\n",
    "4. **Edge Case Handling**: Explicit handling of unanswerable questions\n",
    "\n",
    "### Technical Implementation\n",
    "The XML schema is designed for:\n",
    "- **Atomic Answer Components**: Discrete answer parts with individual citations\n",
    "- **Source Traceability**: Direct mapping between answers and source text\n",
    "- **Validation Support**: Schema-based response validation\n",
    "- **Extensibility**: Future addition of metadata and confidence scores\n",
    "\n",
    "### Performance Considerations\n",
    "- Token efficiency in prompt design\n",
    "- Optimized XML structure for parsing speed\n",
    "- Minimal overhead in response generation\n",
    "- Scalable for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807c3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nova prompt for citations\n",
    "system_prompt = \"\"\"\n",
    "You are a question answering assistant. I will provide you with document context. The user will provide you with a question. Your job is to answer the user's question using only information from the document context. If the document context does not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the document context to validate a user's assertion.\n",
    "\n",
    "However, you should include <sources> tags at the end of each <answer_part> to specify which source(s) the information came from.\n",
    "Note that <sources> may contain multiple <source> if you include information from multiple results in your answer.\n",
    "\n",
    "Do NOT directly quote the <context> in your answer. Your job is to answer the user's question as concisely as possible.\n",
    "\n",
    "You must output your answer in the following format. Pay attention and follow the formatting and spacing exactly:\n",
    "<answer>\n",
    "<answer_part>\n",
    "<text>\n",
    "first answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "<answer_part>\n",
    "<text>\n",
    "second answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6c78d",
   "metadata": {},
   "source": [
    "## Technical Implementation Overview\n",
    "\n",
    "The data preparation process involves several key technical components:\n",
    "\n",
    "1. **Data Loading**: Efficient loading of Parquet-formatted SQuAD data using Pandas\n",
    "2. **Answer Span Processing**: Extraction and validation of answer positions within source text\n",
    "3. **XML Structure Generation**: Creation of consistent, parseable output formats\n",
    "4. **Bedrock API Integration**: Preparation of payloads for model inference\n",
    "\n",
    "The implementation focuses on scalability and reproducibility, essential for production model distillation pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ad88d",
   "metadata": {},
   "source": [
    "## Citation Schema Design and Implementation\n",
    "\n",
    "### Citation Architecture\n",
    "The citation system is designed for:\n",
    "1. **Data Integrity**: Maintaining consistent relationships between answers and sources\n",
    "2. **Validation Pipeline**: Automated checking of citation accuracy\n",
    "3. **Traceability Chain**: Complete path from answer to source text\n",
    "4. **Quality Metrics**: Quantitative assessment of citation accuracy\n",
    "\n",
    "### Technical Constraints and Extensions\n",
    "Current implementation uses sentence-level citations due to SQuAD v2.0 structure. Production deployments should consider additional metadata levels:\n",
    "\n",
    "**Document-Level Metadata**\n",
    "- Page numbers for multi-page documents\n",
    "- Section identifiers for document structure\n",
    "- Document timestamps for versioning\n",
    "\n",
    "**Segment-Level Metadata**\n",
    "- Paragraph identifiers for context\n",
    "- Line numbers for precise location\n",
    "- Character offsets for exact spans\n",
    "\n",
    "**Semantic-Level Metadata**\n",
    "- Topic tags for content classification\n",
    "- Confidence scores for answer reliability\n",
    "- Relevance scores for source matching\n",
    "\n",
    "### XML Schema Implementation\n",
    "```xml\n",
    "<question>Who ruled the duchy of Normandy?</question>\n",
    "<answer>\n",
    "<answer_part>\n",
    "<text>Richard I</text>\n",
    "<sources>\n",
    "<source>The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure.</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "</answer>\n",
    "```\n",
    "\n",
    "### Schema Benefits\n",
    "- **Atomic Structure**: Each answer component is self-contained\n",
    "- **Validation Support**: XML schema enables automated checks\n",
    "- **Processing Pipeline**: Consistent format for batch operations\n",
    "- **Metric Generation**: Structured data for performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline Implementation\n",
    "\n",
    "### Core Processing Functions\n",
    "The following code implements a robust data processing pipeline:\n",
    "\n",
    "1. **Answer Structure Processing**\n",
    "   - Parse complex answer structures\n",
    "   - Validate data integrity\n",
    "   - Handle nested JSON formats\n",
    "\n",
    "2. **XML Generation Engine**\n",
    "   - Construct valid XML outputs\n",
    "   - Maintain proper escaping\n",
    "   - Ensure schema compliance\n",
    "\n",
    "3. **Edge Case Management**\n",
    "   - No-answer scenarios\n",
    "   - Multiple valid answers\n",
    "   - Conflicting source texts\n",
    "\n",
    "4. **Training Data Generation**\n",
    "   - Consistent example formatting\n",
    "   - Batch processing support\n",
    "   - Quality validation checks\n",
    "\n",
    "These components form the core data preparation infrastructure for the distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4943339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_structure(answers_dict):\n",
    "    \"\"\"\n",
    "    Parse different formats of answer dictionaries and extract text and start positions.\n",
    "    Returns lists of texts and start positions.\n",
    "    \"\"\"\n",
    "    # Case 1: NumPy arrays with direct keys\n",
    "    if 'text' in answers_dict and isinstance(answers_dict['text'], np.ndarray):\n",
    "        texts = answers_dict['text'].tolist()\n",
    "        starts = answers_dict['answer_start'].tolist()\n",
    "        \n",
    "    # Case 2: Lists or single values with direct keys\n",
    "    elif 'text' in answers_dict:\n",
    "        texts = answers_dict['text'] if isinstance(answers_dict['text'], list) else [answers_dict['text']]\n",
    "        starts = answers_dict['answer_start'] if isinstance(answers_dict['answer_start'], list) else [answers_dict['answer_start']]\n",
    "        \n",
    "    # Case 4: String JSON that needs parsing (handled in calling function)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown answer format: {answers_dict}\")\n",
    "        \n",
    "    return texts, starts\n",
    "\n",
    "def create_xml_answer(row, no_answer_text='I could not find an exact answer to the question.'):\n",
    "    try:\n",
    "        # Handle answers as string (JSON) if needed\n",
    "        answers_dict = row['answers']\n",
    "        if isinstance(answers_dict, str):\n",
    "            import json\n",
    "            answers_dict = json.loads(answers_dict)\n",
    "            \n",
    "        # Parse answer structure using our helper function\n",
    "        texts, starts = parse_answer_structure(answers_dict)\n",
    "        context = row['context']\n",
    "        \n",
    "        # Split context into sentences more accurately\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)\n",
    "        \n",
    "        # Build XML structure\n",
    "        xml_parts = ['<answer>']\n",
    "        \n",
    "        if len(texts) > 0:\n",
    "            for i, (text, start) in enumerate(zip(texts, starts)):\n",
    "                xml_parts.append('<answer_part>')\n",
    "                xml_parts.append('<text>')\n",
    "                xml_parts.append(str(text))\n",
    "                xml_parts.append('</text>')\n",
    "                xml_parts.append('<sources>')\n",
    "                \n",
    "                # Find the sentence containing the answer based on the start position\n",
    "                char_count = 0\n",
    "                source_sentence = \"No relevant source found\"\n",
    "                for sentence in sentences:\n",
    "                    sentence_len = len(sentence) + 1  # +1 for the space after sentence\n",
    "                    if char_count <= int(start) < (char_count + sentence_len):\n",
    "                        source_sentence = sentence.strip()\n",
    "                        break\n",
    "                    char_count += sentence_len\n",
    "                \n",
    "                xml_parts.append(f'<source>{source_sentence}</source>')\n",
    "                xml_parts.append('</sources>')\n",
    "                xml_parts.append('</answer_part>')\n",
    "        \n",
    "            xml_parts.append('</answer>')\n",
    "        else: # use no answer text\n",
    "            xml_parts.append(f\"<answer_part>\\n<text>\\n{no_answer_text}\\n</text>\\n</answer_part></answer>\")\n",
    "        return '\\n'.join(xml_parts)\n",
    "    except Exception as e:\n",
    "        return f\"<answer>\\n<error>Error generating XML: {str(e)}</error>\\n</answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_payload(row, model_type=\"conversation\", system_prompt=None, include_answer=False, additional_params=None):\n",
    "    \"\"\"\n",
    "    Create a payload dictionary for Amazon Bedrock API requests.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the pandas DataFrame containing context, question, and optionally answers\n",
    "        model_type: The type of model payload to create (\"conversation\" or \"invoke\")\n",
    "        system_prompt: The system message to include (for conversation-based models)\n",
    "        include_answer: Whether to include the answer in the conversation (for evaluation)\n",
    "        additional_params: Dictionary of additional parameters to include in the payload\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract needed information\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Create the user prompt with context and question\n",
    "        user_prompt = f\"\"\"<context>{context}</context> <question>{question}</question>\"\"\"\n",
    "        \n",
    "        # Get the answer if needed\n",
    "        assistant_response = create_xml_answer(row) if include_answer else None\n",
    "        \n",
    "        # Create appropriate payload based on model_type\n",
    "        if model_type == \"conversation\":\n",
    "            # For conversation-based models (Claude, etc.)\n",
    "            payload = {\n",
    "                \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Add assistant response if needed (for evaluation)\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "                \n",
    "        elif model_type == \"invoke\":\n",
    "            # For basic invoke request (non-conversation models like Titan, etc.)\n",
    "            payload = {\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"inferenceConfig\":{ \n",
    "                    # \"maxTokens\": int, // greater than 0, equal or less than 5k (default: dynamic*)\n",
    "                    \"temperature\": .1, # greater then 0 and less than 1.0 (default: 0.7)\n",
    "                    \"topP\": .9, # greater than 0, equal or less than 1.0 (default: 0.9)\n",
    "                    \"topK\": 50, # 0 or greater (default: 50)\n",
    "                    \"stopSequences\": ['</answer>']\n",
    "                }\n",
    "            }\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "            \n",
    "            # Add optional parameters specific to invoke requests\n",
    "            if additional_params:\n",
    "                payload.update(additional_params)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "            \n",
    "        # Add any additional parameters passed\n",
    "        if additional_params and model_type == \"conversation\":\n",
    "            # For conversation models, additional params might need to be added at the root level\n",
    "            for key, value in additional_params.items():\n",
    "                if key not in payload:\n",
    "                    payload[key] = value\n",
    "                    \n",
    "        return payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating payload for row: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_inf_record(row, system_prompt, include_answer=False): \n",
    "    conversation = create_bedrock_payload(\n",
    "                                row=row, \n",
    "                                system_prompt=system_prompt, \n",
    "                                model_type=\"invoke\", \n",
    "                                additional_params={},\n",
    "                                include_answer=include_answer)\n",
    "    return {\n",
    "        \"recordId\": row['id'],\n",
    "        \"modelInput\": conversation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe80bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "# Filter for empty answers\n",
    "empty_answers_df = df_train[df_train['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "with_answers_df = df_train[df_train['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "# take 7500 of each dataframe and combine to use in distillation. \n",
    "df_train_revised = pd.concat([\n",
    "    empty_answers_df.sample(n=7500, random_state=42), \n",
    "    with_answers_df.sample(n=7500, random_state=42)], ignore_index=True) # max 15k for bedrock distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7460c",
   "metadata": {},
   "source": [
    "## Generate Model Distillation Dataset\n",
    "\n",
    "### Training Data Generation Process\n",
    "1. **Data Transformation**: Convert SQuAD format to Bedrock Batch Inference JSONL structure\n",
    "   - Preserve answer spans and context relationships\n",
    "   - Maintain source attribution information\n",
    "\n",
    "2. **XML Formatting**:\n",
    "   - Implement consistent answer structure\n",
    "   - Include source citations with proper context\n",
    "   - Handle edge cases (no answers, multiple answers)\n",
    "\n",
    "3. **Conversation Format**:\n",
    "   - Generate teacher model examples\n",
    "   - Structure interactions for optimal learning\n",
    "   - Balance answerable vs. impossible questions\n",
    "\n",
    "4. **Output Generation**:\n",
    "   - Save in JSONL format for efficient processing\n",
    "   - Maintain data integrity and relationships\n",
    "   - Enable streaming for large-scale training\n",
    "\n",
    "The resulting dataset forms the foundation for training a specialized model that combines efficient inference with reliable citation capabilities.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Continue with Model Distillation Pipeline\n",
    "1. Proceed to [`02_distill.ipynb`](02_distill.ipynb) to:\n",
    "   - Configure the teacher model (Amazon Bedrock Claude)\n",
    "   - Set up the student model architecture\n",
    "   - Implement the distillation training loop\n",
    "\n",
    "### Key Aspects to Monitor\n",
    "- Citation accuracy metrics\n",
    "- Answer quality vs. original model\n",
    "- Inference latency improvements\n",
    "- Model size reduction ratio\n",
    "\n",
    "### Preparation Checklist\n",
    "- [x] Training data generated and validated\n",
    "- [x] XML schema implemented and tested\n",
    "- [x] Evaluation dataset prepared\n",
    "- [ ] Review distillation hyperparameters in next notebook\n",
    "- [ ] Configure AWS resources for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416c80",
   "metadata": {},
   "source": [
    "It is a best practice to include a ground truth answer for ~10% of the total training set. We will take a random sample of 10% and use our `create_bedrock_payload` with include_anwer set to True. The remaining 90% we leave out thr ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39bf45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(df_train_revised)\n",
    "ground_truth_included = 0.1 * row_count\n",
    "\n",
    "# here we'll take 10% of our training data set and add the answers\n",
    "training_data_with_gt_df = df_train_revised.sample(n=int(ground_truth_included), random_state=17)\n",
    "\n",
    "# next we'll drop our ground truth examples so as not to mix with our labels excluding answers.\n",
    "training_data_without_gt_df = df_train_revised.drop(training_data_with_gt_df.index)\n",
    "\n",
    "# next we'll build our training data with ground truth\n",
    "training_data_with_gt_df['conversation'] = training_data_with_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=True), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f6e344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we'll build our training data without ground truth\n",
    "training_data_without_gt_df['conversation'] = training_data_without_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=False), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll concatenate the dataframes\n",
    "final_training_dataset = pd.concat([training_data_with_gt_df, training_data_without_gt_df], axis=0, ignore_index=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f10b3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll output to .jsonl to use in distillation job\n",
    "final_training_dataset['conversation'].to_json('distillation_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b86c",
   "metadata": {},
   "source": [
    "## Evaluation Dataset Creation\n",
    "\n",
    "### Batch Inference Dataset Design\n",
    "The evaluation dataset is carefully constructed to assess model performance across key dimensions:\n",
    "\n",
    "1. **Data Distribution**:\n",
    "   - Balanced mix of answerable (250) and impossible (250) questions\n",
    "   - Stratified sampling to maintain domain coverage\n",
    "   - Controlled complexity distribution\n",
    "\n",
    "2. **Evaluation Metrics Focus**:\n",
    "   - Answer accuracy and relevance\n",
    "   - Citation precision and recall\n",
    "   - Source attribution reliability\n",
    "   - Handling of impossible questions\n",
    "\n",
    "3. **Comparative Analysis Support**:\n",
    "   - Teacher model (Bedrock Claude) baseline\n",
    "   - Distilled model performance\n",
    "   - Inference efficiency metrics\n",
    "\n",
    "The evaluation dataset enables comprehensive performance assessment in notebook `04_evaluate.ipynb`.\n",
    "\n",
    "## Next Steps\n",
    "1. Proceed to `02_distill.ipynb` to train the distilled model using the prepared datasets\n",
    "2. The distillation process will focus on preserving citation capabilities while reducing model size\n",
    "3. Key metrics to monitor during training:\n",
    "   - Answer accuracy\n",
    "   - Citation precision\n",
    "   - Inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_empty_answers_df = df_eval[df_eval['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "eval_with_answers_df = df_eval[df_eval['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "batch_inf_df = pd.concat([\n",
    "    eval_empty_answers_df.sample(n=250, random_state=15), \n",
    "    eval_with_answers_df.sample(n=250, random_state=15)], ignore_index=True)\n",
    "\n",
    "\n",
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt), axis=1).to_json('batch_inf_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd681e",
   "metadata": {},
   "source": [
    "## Labeled Dataset for BYOI Bedrock Evaluation\n",
    "This section creates a labeled dataset specifically for [Amazon Bedrock's Bring Your Own Model (BYOM)](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html) evaluation process. Key features:\n",
    "\n",
    "1. Includes ground truth answers for accuracy assessment\n",
    "2. Maintains XML formatting for consistency\n",
    "3. Enables automated evaluation metrics\n",
    "4. Supports both quantitative and qualitative analysis\n",
    "\n",
    "The labeled dataset will be used to:\n",
    "- Calculate model performance metrics\n",
    "- Compare against baseline models\n",
    "- Validate citation accuracy\n",
    "- Assess answer relevance and completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt=system_prompt, include_answer=True), axis=1).to_json('labeled_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d377d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import json\n",
    "# model_id = 'us.amazon.nova-premier-v1:0'\n",
    "# bedrock_runtime = boto3.client(\n",
    "#     service_name=\"bedrock-runtime\",\n",
    "#     region_name=\"us-east-1\"\n",
    "# )\n",
    "\n",
    "# # Make the invoke call to Bedrock\n",
    "# response = bedrock_runtime.invoke_model(\n",
    "#     modelId=model_id,\n",
    "#     body=json.dumps(sample_payload)\n",
    "# )\n",
    "\n",
    "# # Parse and return the response\n",
    "# response_body = json.loads(response.get('body').read())\n",
    "# print(response_body['output']['message']['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b6fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_body['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef8e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dupes\n",
    "# df_train_revised[df_train_revised.duplicated(subset=['conversation'], keep=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
