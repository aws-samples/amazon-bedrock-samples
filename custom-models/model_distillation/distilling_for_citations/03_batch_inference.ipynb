{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Batch Inference for Model Distillation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock's batch inference capabilities to process multiple inputs at once using your distilled models. Batch inference is useful when you need to process a large number of inputs efficiently without the overhead of making individual API calls.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Upload batch inference data to an S3 bucket\n",
    "2. Submit batch inference jobs for multiple models, including our provisioned throughput endpoint\n",
    "3. Monitor the status of these batch inference jobs\n",
    "4. Prepare for evaluation of accuracy improvements in our distilled model\n",
    "\n",
    "The batch inference results will be used to evaluate the accuracy improvements achieved through model distillation. By running batch inference on both our distilled model (via the provisioned throughput endpoint created in the previous notebook) and other models for comparison, we can quantitatively assess the performance of our distilled model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PT model id from previous notebook\n",
    "%store -r provisioned_model_id\n",
    "%store -r custom_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import create_s3_bucket\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\", region_name='us-east-1')\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session(region_name='us-east-1')\n",
    "region = 'us-east-1'\n",
    "sts_client = session.client(service_name='sts', region_name='us-east-1')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# Define bucket and prefixes (using the same bucket as in distillation)\n",
    "bucket_name = 'sample-data-us-east-1-228707323172-1' # '905418197933-distillation'  # Same bucket used in distillation notebook\n",
    "data_prefix = 'citations_distillation'  # Same prefix used in distillation notebook\n",
    "batch_inference_prefix = f\"{data_prefix}/batch_inference\"  # New prefix for batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Batch Inference Data to S3\n",
    "\n",
    "First, we'll upload our batch inference data file to the S3 bucket with the batch inference prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sample-data-us-east-1-228707323172-1 exists.\n",
      "Uploaded batch_inf_data.jsonl to s3://sample-data-us-east-1-228707323172-1/citations_distillation/batch_inference/batch_inf_data.jsonl\n",
      "Batch inference data uploaded to: s3://sample-data-us-east-1-228707323172-1/citations_distillation/batch_inference/batch_inf_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Define the local path to the batch inference data file\n",
    "batch_inference_file = 'batch_inf_data.jsonl'\n",
    "\n",
    "# Upload the batch inference data to S3\n",
    "def upload_batch_inference_data(bucket_name, file_name, prefix):\n",
    "    \"\"\"\n",
    "    Upload batch inference data to S3 bucket\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Check if bucket exists, if not create it\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} exists.\")\n",
    "    except ClientError:\n",
    "        print(f\"Creating bucket {bucket_name}...\")\n",
    "        create_s3_bucket(bucket_name=bucket_name)\n",
    "    \n",
    "    # Upload file to S3\n",
    "    s3_key = f\"{prefix}/{file_name}\"\n",
    "    s3_client.upload_file(file_name, bucket_name, s3_key)\n",
    "    print(f\"Uploaded {file_name} to s3://{bucket_name}/{s3_key}\")\n",
    "    \n",
    "    return f\"s3://{bucket_name}/{s3_key}\"\n",
    "\n",
    "# Upload batch inference data to S3\n",
    "batch_inference_s3_uri = upload_batch_inference_data(bucket_name, batch_inference_file, batch_inference_prefix)\n",
    "print(f\"Batch inference data uploaded to: {batch_inference_s3_uri}\")\n",
    "\n",
    "# Define the output location for batch inference results\n",
    "batch_inference_output_prefix = f\"{batch_inference_prefix}/outputs\"\n",
    "batch_inference_output_uri = f\"s3://{bucket_name}/{batch_inference_output_prefix}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit Batch Inference Jobs\n",
    "First we'll kick off our batch inference simulator to run batch inference on our Provisioned Throughput endpoint holding our distilled model.\n",
    "\n",
    "Then, we'll define a list of models and submit a batch inference job for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !python3 batch_inference_simulator.py --input distilling_for_citations/batch_inf_data.jsonl --output distilling_for_citations/batch_inference_results/distilled_results.jsonl  --model \"<provisioned throughput endpoint ARN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch inference job for model amazon.nova-lite-v1:0:300k\n",
      "Job ARN: arn:aws:bedrock:us-east-1:228707323172:model-invocation-job/99sm7iehijui\n"
     ]
    }
   ],
   "source": [
    "# Define the list of models to use for batch inference\n",
    "# We'll include the teacher model, student model, and our distilled model (provisioned throughput)\n",
    "models = [\n",
    "    # \"us.amazon.nova-premier-v1:0\",  # Teacher model (Nova Premier)\n",
    "    \"amazon.nova-lite-v1:0:300k\",   # Student model (Nova Lite)\n",
    "]\n",
    "\n",
    "# Function to submit a batch inference job\n",
    "def submit_batch_inference_job(model_id, input_s3_uri, output_s3_uri):\n",
    "    \"\"\"\n",
    "    Submit a batch inference job for the specified model\n",
    "    \"\"\"\n",
    "    # Generate a unique job name\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    model_short_name = model_id.split('/')[-1].split(':')[0]\n",
    "    job_name = f\"batch-inference-{model_short_name}-{timestamp}\"\n",
    "    \n",
    "    # Create the batch inference job\n",
    "    response = bedrock_client.create_model_invocation_job(\n",
    "        jobName=job_name,\n",
    "        modelId=model_id,\n",
    "        inputDataConfig={\n",
    "            \"s3InputDataConfig\": {\n",
    "                \"s3Uri\": input_s3_uri,\n",
    "                \"s3InputFormat\": \"JSONL\"\n",
    "            }\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f\"{output_s3_uri}{model_short_name}/\"\n",
    "            }\n",
    "        },\n",
    "        roleArn=f\"arn:aws:iam::{account_id}:role/service-role/batch-inf-service-role\" # TODO build iam role for this\n",
    "    )\n",
    "    \n",
    "    job_id = response['jobArn']\n",
    "    print(f\"Submitted batch inference job for model {model_id}\")\n",
    "    print(f\"Job ARN: {job_id}\")\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "# Submit batch inference jobs for each model\n",
    "job_ids = []\n",
    "for model in models:\n",
    "    job_id = submit_batch_inference_job(model, batch_inference_s3_uri, batch_inference_output_uri)\n",
    "    job_ids.append(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor Batch Inference Jobs\n",
    "\n",
    "Finally, we'll monitor the status of the batch inference jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-lite-v1:0\n",
      "Status: Submitted\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Function to check the status of a batch inference job\n",
    "def check_job_status(job_id):\n",
    "    \"\"\"\n",
    "    Check the status of a batch inference job\n",
    "    \"\"\"\n",
    "    response = bedrock_client.get_model_invocation_job(jobIdentifier=job_id)\n",
    "    status = response['status']\n",
    "    model_id = response['modelId']\n",
    "    \n",
    "    print(f\"Model: {model_id}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status == 'COMPLETED':\n",
    "        print(f\"Output location: {response['outputDataConfig']['s3OutputDataConfig']['s3Uri']}\")\n",
    "    elif status == 'FAILED':\n",
    "        print(f\"Failure reason: {response.get('failureMessage', 'Unknown')}\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "# Check the status of all batch inference jobs\n",
    "for job_id in job_ids:\n",
    "    status = check_job_status(job_id)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wait for Jobs to Complete\n",
    "\n",
    "If you want to wait for all jobs to complete before proceeding, you can use the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all jobs to complete\n",
    "def wait_for_jobs_completion(job_ids, check_interval=60, max_wait_time=3600):\n",
    "    \"\"\"\n",
    "    Wait for all batch inference jobs to complete\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    all_completed = False\n",
    "    \n",
    "    while not all_completed and (time.time() - start_time) < max_wait_time:\n",
    "        print(f\"Checking job status at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        statuses = []\n",
    "        for job_id in job_ids:\n",
    "            response = bedrock_runtime.get_model_invocation_job(jobIdentifier=job_id)\n",
    "            status = response['status']\n",
    "            model_id = response['modelId']\n",
    "            \n",
    "            print(f\"Model: {model_id} - Status: {status}\")\n",
    "            statuses.append(status)\n",
    "        \n",
    "        # Check if all jobs are completed or failed\n",
    "        all_completed = all(status in ['COMPLETED', 'FAILED', 'STOPPED'] for status in statuses)\n",
    "        \n",
    "        if not all_completed:\n",
    "            print(f\"Waiting {check_interval} seconds for next check...\")\n",
    "            time.sleep(check_interval)\n",
    "    \n",
    "    if all_completed:\n",
    "        print(\"All batch inference jobs have completed.\")\n",
    "    else:\n",
    "        print(f\"Maximum wait time of {max_wait_time} seconds exceeded.\")\n",
    "\n",
    "# Uncomment the following line to wait for all jobs to complete\n",
    "# wait_for_jobs_completion(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve and Prepare Results for Evaluation\n",
    "\n",
    "Once the batch inference jobs are complete, you can retrieve the results and prepare them for evaluation in the next notebook (04_evaluate.ipynb).\n",
    "\n",
    "The batch inference results will be crucial for evaluating the performance of our distilled model compared to the teacher and student models. We'll analyze metrics such as:\n",
    "\n",
    "1. **Accuracy**: How well does the distilled model match the expected outputs?\n",
    "2. **Consistency**: Does the distilled model produce consistent results across similar inputs?\n",
    "3. **Efficiency**: How does the performance compare to the computational resources required?\n",
    "4. **Specific Task Performance**: For citation generation, we'll evaluate the quality and accuracy of citations produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list batch inference output files\n",
    "def list_batch_inference_outputs(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    List the batch inference output files in the S3 bucket\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            print(f\"s3://{bucket_name}/{obj['Key']}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket_name}/{prefix}\")\n",
    "\n",
    "# List the batch inference output files\n",
    "# Uncomment the following line to list the output files\n",
    "# list_batch_inference_outputs(bucket_name, batch_inference_output_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use Amazon Bedrock's batch inference capabilities to process multiple inputs at once using different models, including our provisioned throughput endpoint for the distilled model. This approach allows us to efficiently compare the performance of different models on the same dataset.\n",
    "\n",
    "Key steps covered:\n",
    "1. Uploading batch inference data to an S3 bucket\n",
    "2. Submitting batch inference jobs for multiple models, including our distilled model\n",
    "3. Monitoring the status of batch inference jobs\n",
    "4. Preparing results for evaluation\n",
    "\n",
    "### Benefits of Using Provisioned Throughput for Batch Inference\n",
    "\n",
    "Using a provisioned throughput endpoint for batch inference offers several advantages:\n",
    "\n",
    "1. **Consistent Performance**: Dedicated capacity ensures consistent performance without competing for resources\n",
    "2. **Cost Efficiency**: For large batch jobs, provisioned throughput can be more cost-effective than on-demand pricing\n",
    "3. **Higher Throughput**: Ability to process more requests in parallel, reducing overall processing time\n",
    "4. **Predictable Latency**: More stable and predictable response times\n",
    "\n",
    "The results from this batch inference process will be used in the next notebook (04_evaluate.ipynb) to quantitatively assess the improvements achieved through model distillation. This evaluation will help determine if the distilled model meets the performance requirements while providing the efficiency benefits of a smaller model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
