{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf2edd1",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model Distillation Guide - JSONL training data available in Amazon S3 bucket\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Model distillation is a knowledge transfer technique where a smaller 'student' model learns to mimic the behavior of a larger 'teacher' model. In Amazon Bedrock, this process allows you to create more efficient models while maintaining performance by:\n",
    "\n",
    "1. Having the teacher model (e.g., Nova Premier) generate high-quality responses\n",
    "2. Training a smaller student model (e.g., Nova Lite) to replicate these responses\n",
    "3. Optimizing the student model's parameters through supervised learning\n",
    "\n",
    "This guide demonstrates how to implement model distillation using **JSONL training data in Amazon S3**. The process involves:\n",
    "\n",
    "- Set up and configure distillation jobs\n",
    "- Prepare and format training data for distillation\n",
    "- Upload and use training data from S3\n",
    "- Manage model provisioning and deployment\n",
    "- Run inference with distilled models\n",
    "\n",
    "The guide covers essential API operations including:\n",
    "- Creating and configuring distillation jobs\n",
    "- Managing training data sources in S3\n",
    "- Handling model deployments\n",
    "- Implementing production best practices using boto3 and the Bedrock SDK\n",
    "\n",
    "While model distillation offers benefits like improved efficiency and reduced costs, this guide focuses on the practical implementation details and API usage patterns needed to successfully execute distillation workflows in Amazon Bedrock.\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "When using model distillation:\n",
    "1. Ensure your training data is diverse and representative of your use case\n",
    "2. Monitor distillation metrics in the S3 output location\n",
    "3. Evaluate the distilled model's performance against your requirements\n",
    "4. Consider cost-performance tradeoffs when selecting model units for deployment\n",
    "\n",
    "The distilled model should provide faster responses and lower costs while maintaining acceptable performance for your specific use case.\n",
    "\n",
    "### Setup and Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following:\n",
    "\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing training data and output\n",
    "- Training data in JSONL format\n",
    "- Sufficient service quota to use Provisioned Throughput in Bedrock\n",
    "- An IAM role with the following permissions:\n",
    "\n",
    "IAM Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET/*\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:ListModelCustomizationJobs\",\n",
    "                \"bedrock:StopModelCustomizationJob\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Trust Relationship:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"YOUR_ACCOUNT_ID\"\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Dataset:\n",
    "As an example, in this notebook we will be using the `Uber10K dataset`.\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9296e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "063248b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b031e3",
   "metadata": {},
   "source": [
    "# Model Selection and Configuration\n",
    "\n",
    "Choosing the right teacher and student models is crucial for successful distillation. Consider these key factors:\n",
    "\n",
    "1. Performance targets\n",
    "   - Accuracy requirements for your specific use case\n",
    "   - Acceptable trade-offs between performance and efficiency\n",
    "   - Quality metrics specific to your task (e.g., citation accuracy)\n",
    "\n",
    "2. Latency requirements\n",
    "   - Maximum acceptable inference time\n",
    "   - Throughput needs (requests per second)\n",
    "   - Response time consistency requirements\n",
    "\n",
    "3. Total Cost of Ownership (TCO)\n",
    "   - Model hosting costs\n",
    "   - Inference costs per request\n",
    "   - Training and maintenance costs\n",
    "\n",
    "In this example, we're using:\n",
    "- Teacher: Amazon Nova Premier (high accuracy, larger model)\n",
    "- Student: Amazon Nova Lite (faster inference, smaller footprint)\n",
    "\n",
    "For production use cases, evaluate multiple model combinations and run thorough benchmarks. See the [Amazon Bedrock Model Selection Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-selection.html) for detailed guidance.\n",
    "\n",
    "**Note**: Run this code sample in a [supporting region](https://docs.aws.amazon.com/bedrock/latest/userguide/regions.html) for your chosen models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc786809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import create_s3_bucket, upload_training_data_to_s3, delete_s3_bucket_and_contents, \\\n",
    "create_model_distillation_role_and_permissions, delete_role_and_attached_policies, delete_distillation_buckets\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\",region_name='us-east-1')\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-east-1')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session(region_name='us-east-1')\n",
    "region =  'us-east-1' # session.region_name\n",
    "sts_client = session.client(service_name='sts',region_name='us-east-1')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# define bucket you want to create and upload the dataset to:\n",
    "bucket_name='905418197933-distillation' # Replace by your bucket name\n",
    "data_prefix = 'citations_distillation' # Replace by your defined prefix\n",
    "\n",
    "# configure teacher nd student model\n",
    "teacher_model = \"us.amazon.nova-premier-v1:0\"\n",
    "student_model = \"amazon.nova-lite-v1:0:300k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e9a9c",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Model Distillation\n",
    "\n",
    "Before we start the distillation process, we need to prepare our dataset. We'll create a function to convert our input data into the format required by Amazon Bedrock.\n",
    "\n",
    "#### Model Distillation Input Format\n",
    "\n",
    "The training data must follow the Bedrock conversation schema in JSONL format. Each line should be a valid JSON object with this structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "    \"system\": [\n",
    "        {\n",
    "            \"text\": <Your-System-Prompt>\n",
    "        }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": <Your-Prompt-And-OR-Context>\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": <Your-Ground-Truth-Response>\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Key formatting requirements:\n",
    "- Each line must be a complete JSON object\n",
    "- The schemaVersion field must be specified as `bedrock-conversation-2024`\n",
    "- System instructions should be included in the system array\n",
    "- Messages (including any context) must include both user and assistant roles in the correct order\n",
    "- All text content must be wrapped in the appropriate content structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating IAM role...\n",
      "Creating IAM policy...\n",
      "Attaching policy to role...\n",
      "Successfully created role and policy!\n",
      "Successfully created bucket '905418197933-distillation' in region 'us-east-1'\n",
      "Bucket ARN: arn:aws:s3:::905418197933-distillation\n",
      "Uploading distillation_data.jsonl to bucket 905418197933-distillation with prefix citations_distillation...\n",
      "Successfully uploaded distillation_data.jsonl to S3 bucket!\n",
      "File S3 URI: s3://905418197933-distillation/citations_distillation/distillation_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate unique names for the job and model\n",
    "distillation_dataset = 'distillation_data.jsonl'\n",
    "current_dt = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "job_name = f\"distill-citations-{current_dt}\"\n",
    "model_name = f\"distilled-citations-{current_dt}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce95f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure models and IAM role\n",
    "role_name, role_arn = create_model_distillation_role_and_permissions(bucket_name=bucket_name, account_id=account_id)\n",
    "\n",
    "# creating training data bucket\n",
    "create_s3_bucket(bucket_name=bucket_name)\n",
    "\n",
    "# Specify S3 locations\n",
    "training_data_s3_uri = upload_training_data_to_s3(bucket_name, distillation_dataset, prefix=data_prefix)\n",
    "# training_data = \"s3://sample-data-us-east-1-228707323172-1/citations_distillation/distillation_data.jsonl\"\n",
    "output_path = f\"s3://{bucket_name}/{data_prefix}/outputs/\"\n",
    "\n",
    "# Set maximum response length\n",
    "max_response_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8eebe",
   "metadata": {},
   "source": [
    "# Starting the Distillation Job\n",
    "\n",
    "With our dataset prepared, we can now start the distillation job. We'll use the `create_model_customization_job` API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ea0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add wait to let the role be persisted here\n",
    "response = bedrock_client.create_model_customization_job(\n",
    "    jobName=job_name,\n",
    "    customModelName=model_name,\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=student_model,\n",
    "    customizationType=\"DISTILLATION\",\n",
    "    trainingDataConfig={\n",
    "        \"s3Uri\": training_data_s3_uri\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    customizationConfig={\n",
    "        \"distillationConfig\": {\n",
    "            \"teacherModelConfig\": {\n",
    "                \"teacherModelIdentifier\": teacher_model,\n",
    "                \"maxResponseLengthForInference\": max_response_length \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6d0c8",
   "metadata": {},
   "source": [
    "# Monitoring the Distillation Job\n",
    "\n",
    "After starting the distillation job, it's crucial to monitor both its progress and quality metrics. The distillation process involves several phases:\n",
    "\n",
    "1. Data Preparation\n",
    "   - Loading and validating training data\n",
    "   - Preprocessing examples for teacher model inference\n",
    "\n",
    "2. Teacher Model Inference\n",
    "   - Generating high-quality responses for training examples\n",
    "   - Validating response quality and format\n",
    "\n",
    "3. Student Model Training\n",
    "   - Fine-tuning the student model on teacher outputs\n",
    "   - Optimizing for performance and efficiency\n",
    "\n",
    "We'll use the `get_model_customization_job` API to track progress and access metrics. Key status values:\n",
    "- `InProgress`: Job is actively running\n",
    "- `Completed`: Distillation finished successfully\n",
    "- `Failed`: Job encountered errors (check error messages)\n",
    "- `Stopped`: Job was manually terminated\n",
    "\n",
    "Monitor the S3 output location for detailed logs and metrics. See [Model Customization Monitoring](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-customization.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811dd77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "# Record the distillation job arn\n",
    "job_arn = response['jobArn']\n",
    "\n",
    "# print job status\n",
    "job_status = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)[\"status\"]\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a150f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_arn = \"arn:aws:bedrock:us-east-1:905418197933:model-customization-job/amazon.nova-lite-v1:0:300k/68vwpdrxdgrm\"\n",
    "model_name = \"distilled-citations-2025-06-09-15-31-33\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da8a8e",
   "metadata": {},
   "source": [
    "# Deploying the Distilled Model\n",
    "\n",
    "Once the distillation job completes successfully, we can deploy our optimized model. Deployment involves creating a Provisioned Throughput (PT) model instance, which provides:\n",
    "\n",
    "1. Dedicated Capacity\n",
    "   - Consistent performance with dedicated resources\n",
    "   - Predictable latency for production workloads\n",
    "   - Ability to scale based on demand\n",
    "\n",
    "2. Cost Management\n",
    "   - Pay only for provisioned capacity\n",
    "   - Option for cost savings with longer commitments\n",
    "   - Ability to adjust capacity as needed\n",
    "\n",
    "3. Monitoring & Management\n",
    "   - CloudWatch metrics for performance tracking\n",
    "   - Auto-scaling capabilities (if configured)\n",
    "   - Health checks and automated recovery\n",
    "\n",
    "For production deployments, consider:\n",
    "- Setting up monitoring and alerts\n",
    "- Implementing retry logic and fallbacks\n",
    "- Regular performance evaluation\n",
    "\n",
    "See [Provisioned Throughput](https://docs.aws.amazon.com/bedrock/latest/userguide/provisioned-throughput.html) for deployment best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a050d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the distilled model\n",
    "custom_model_id = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)['outputModelArn']\n",
    "distilled_model_name = model_name\n",
    "\n",
    "provisioned_model_id = bedrock_client.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=distilled_model_name,\n",
    "    # commitmentDuration # ommitted for no-commit\n",
    "    modelId=custom_model_id \n",
    ")['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca969f2e",
   "metadata": {},
   "source": [
    " We need to store the provisioned throughput endpoint ARN for use in our invoke calls in the subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6038780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'provisioned_model_id' (str)\n",
      "Stored 'custom_model_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store provisioned_model_id\n",
    "%store custom_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d80f5",
   "metadata": {},
   "source": [
    "## Purchase a PT endpoint to set up inferencing for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c705bac",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "Let's delete the resources that were created in this notebook. `Uncomment` the code below to delete the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete bucket and dataset\n",
    "# delete_distillation_buckets(bucket_name)\n",
    "\n",
    "# delete role and its policy:\n",
    "# delete_role_and_attached_policies(role_name=role_name)\n",
    "\n",
    "# delete provisioned throughput:\n",
    "# response = bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a025",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, we've walked through the entire process of model distillation using Amazon Bedrock. We covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Preparing the dataset\n",
    "3. Configuring and starting a distillation job\n",
    "4. Monitoring the job's progress\n",
    "5. Deploying the distilled model\n",
    "6. Cleaning up resources\n",
    "\n",
    "Model distillation is a powerful technique that can help you create more efficient models tailored to your specific use case. By following this guide, you should now be able to implement model distillation in your own projects using Amazon Bedrock.\n",
    "\n",
    "Remember to always consider your specific use case requirements when selecting models and configuring the distillation process. \n",
    "\n",
    "**Happy distilling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
