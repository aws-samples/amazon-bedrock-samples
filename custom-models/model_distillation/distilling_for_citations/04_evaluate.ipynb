{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b6d3af-6b1d-48b2-8d0e-2723cc6fef4f",
   "metadata": {},
   "source": [
    "# RAG Evaluation with Bring Your Own Inference Responses (BYOI) on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock RAG Evaluation capabilities now support \"Bring Your Own Inference Responses\" (BYOI), enabling you to assess any Retrieval-Augmented Generation system regardless of where it's deployed. This notebook demonstrates how to evaluate the quality of RAG systems using specialized metrics including the newly available citation metrics - Citation Precision and Citation Coverage - providing deep insights into how effectively your system uses retrieved information.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Setting up RAG evaluation configurations with BYOI\n",
    "- The creation of retrieve-and-generate evaluation jobs\n",
    "- Analyzing citation quality with the new precision and coverage metrics\n",
    "- Monitoring evaluation progress \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock\n",
    "- RAG system outputs in the required BYOI format\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Make sure these are enabled in your account.\n",
    "\n",
    "## Dataset Format for RAG BYOI\n",
    "\n",
    "### Retrieve-and-Generate Evaluation Format\n",
    "```json\n",
    "{\n",
    "  \"conversationTurns\": [\n",
    "    {\n",
    "      \"prompt\": {\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"Your prompt here\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"referenceResponses\": [\n",
    "        {\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"text\": \"Expected ground truth answer\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"output\": {\n",
    "        \"text\": \"Generated response text\",\n",
    "        \"knowledgeBaseIdentifier\": \"third-party-RAG\",\n",
    "        \"retrievedPassages\": {\n",
    "          \"retrievalResults\": [\n",
    "            {\n",
    "              \"name\": \"Optional passage name\",\n",
    "              \"content\": {\n",
    "                \"text\": \"Retrieved passage content\"\n",
    "              },\n",
    "              \"metadata\": {\n",
    "                \"source\": \"Optional metadata\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        \"citations\": [\n",
    "          {\n",
    "            \"generatedResponsePart\": {\n",
    "              \"textResponsePart\": {\n",
    "                \"span\": {\n",
    "                  \"start\": 0,\n",
    "                  \"end\": 50\n",
    "                },\n",
    "                \"text\": \"Part of the response that uses cited material\"\n",
    "              }\n",
    "            },\n",
    "            \"retrievedReferences\": [\n",
    "              {\n",
    "                \"name\": \"Optional passage name\",\n",
    "                \"content\": {\n",
    "                  \"text\": \"Source passage for the citation\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                  \"source\": \"Optional metadata\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f87802-b6fb-4101-8e73-7344c94b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42192fc5-ae68-4ca8-91d4-2dc8fd59d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38.33\n"
     ]
    }
   ],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c537a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from utils import read_jsonl_to_dataframe, upload_training_data_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b3fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_user_message(user_msg):\n",
    "\n",
    "    # Regex pattern to extract content between tags\n",
    "    pattern = r\"<context>(.*?)</context>\\s*<question>(.*?)</question>\"\n",
    "\n",
    "    # Apply regex\n",
    "    match = re.search(pattern, user_msg, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        context = match.group(1).strip()\n",
    "        question = match.group(2).strip()\n",
    "        return context, question\n",
    "        \n",
    "    else:\n",
    "        print(\"Pattern not found in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa9e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_record(results_row, labeled_row, model_identifier=None, kb_identifier='kb_id'):\n",
    "    \"\"\"\n",
    "    Takes a batch inference result data set and builds an evaluation data set for use in bedrock evaluation\n",
    "    \n",
    "    Args:\n",
    "        row:            pandas row\n",
    "        max_records:    defaults to 1000 - max for bedrock evaluation\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock Evaluations API\n",
    "    \n",
    "    \"\"\"\n",
    "    result = results_row.to_dict()\n",
    "    label = labeled_row.to_dict()\n",
    "\n",
    "    retrieval_content, question = split_user_message(result['modelInput']['messages'][0]['content'][0]['text'])\n",
    "    return {\n",
    "        \"conversationTurns\": [\n",
    "            {\n",
    "                \"prompt\": {\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": question\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"referenceResponses\": [\n",
    "                    {\n",
    "                        \"content\": label['modelInput']['messages'][1]['content'] # labeled answer content from assistant\n",
    "                    }\n",
    "                ],\n",
    "                # \"referenceContexts\": [\n",
    "                #     {\n",
    "                #         \"content\": [\n",
    "                #             {\n",
    "                #                 \"text\": \"A ground truth for a received passage\"\n",
    "                #             }\n",
    "                #         ]\n",
    "                #     }\n",
    "                # ],\n",
    "                \"output\": {\n",
    "                    \"text\": result['modelOutput']['output']['message']['content'][0]['text'],\n",
    "                    \"modelIdentifier\": model_identifier if model_identifier else 'placeholder_model',\n",
    "                    \"knowledgeBaseIdentifier\": kb_identifier,\n",
    "                    \"retrievedPassages\": {\n",
    "                        \"retrievalResults\": [ # put context from squad example here\n",
    "                            {\n",
    "                                \"name\": f\"retrieval_{results_row['recordId']}\",\n",
    "                                \"content\": {\n",
    "                                    \"text\": retrieval_content\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5c4f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(model_identifier, dataframe, labeled_df, rag_source_id=None, max_records=1000, output_filename=None):\n",
    "    \"\"\"\n",
    "    Create an evaluation dataset from model results and write to a JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame containing the results to evaluate\n",
    "    labeled_df : pandas.DataFrame\n",
    "        DataFrame containing labeled data with matching recordIds\n",
    "    rag_source_id : str, optional\n",
    "        Identifier for the knowledge base source\n",
    "    max_records : int, optional\n",
    "        Maximum number of records to process (default: 1000)\n",
    "    output_filename : str, optional\n",
    "        Custom filename for the output JSONL file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the created JSONL file\n",
    "    list\n",
    "        The evaluation dataset as a list of dictionaries\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    eval_dataset = []\n",
    "    kb_identifier = rag_source_id or f\"{model_identifier}_kb\"\n",
    "    \n",
    "    for ix, results_row in dataframe.head(max_records).iterrows():\n",
    "        labeled_row = labeled_df[labeled_df['recordId'] == results_row['recordId']].iloc[0]\n",
    "        eval_dataset.append(create_eval_record(\n",
    "            results_row=results_row, \n",
    "            labeled_row=labeled_row,\n",
    "            model_identifier=model_identifier, \n",
    "            kb_identifier=kb_identifier\n",
    "        ))\n",
    "    \n",
    "    jsonl_file = output_filename or f\"evaluation_data_{model_identifier}.jsonl\"\n",
    "    \n",
    "    # Write all records to JSONL file\n",
    "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
    "        for record in eval_dataset:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\"Successfully wrote {len(eval_dataset)} records to {jsonl_file}\")\n",
    "    return jsonl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27615df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_evaluation_job(\n",
    "    bedrock_client,\n",
    "    model_identifier, \n",
    "    rag_source_id,\n",
    "    input_data,\n",
    "    role_arn,\n",
    "    output_path,\n",
    "    evaluator_model,\n",
    "    metrics=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a Bedrock RAG evaluation job.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    bedrock_client : boto3.client\n",
    "        The Bedrock client to use for creating the job\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    rag_source_id : str\n",
    "        Identifier for the RAG knowledge base source\n",
    "    input_data : str\n",
    "        S3 URI for the input evaluation dataset\n",
    "    role_arn : str\n",
    "        IAM role ARN with sufficient permissions to run the evaluation\n",
    "    output_path : str\n",
    "        S3 URI where evaluation results will be stored\n",
    "    evaluator_model : str\n",
    "        Identifier for the model that will perform evaluations\n",
    "    metrics : list, optional\n",
    "        List of metrics to evaluate (defaults to standard RAG metrics)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The job ARN from the create_evaluation_job API call\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Set default RAG metrics if none provided\n",
    "    if metrics is None:\n",
    "        metrics = [\n",
    "            \"Builtin.Correctness\",\n",
    "            \"Builtin.Completeness\",\n",
    "            \"Builtin.Helpfulness\",\n",
    "            \"Builtin.LogicalCoherence\",\n",
    "            \"Builtin.Faithfulness\",\n",
    "            \"Builtin.CitationPrecision\",\n",
    "            \"Builtin.CitationCoverage\"\n",
    "        ]\n",
    "    \n",
    "    # Generate job name using timestamp for uniqueness\n",
    "    retrieve_generate_job_name = f\"citations-{model_identifier.replace('_','-')}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "    # Create the evaluation job\n",
    "    retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "        jobName=retrieve_generate_job_name,\n",
    "        jobDescription=\"Evaluate retrieval and generation\",\n",
    "        roleArn=role_arn,\n",
    "        applicationType=\"RagEvaluation\",\n",
    "        inferenceConfig={\n",
    "            \"ragConfigs\": [\n",
    "                {\n",
    "                    \"precomputedRagSourceConfig\": {\n",
    "                        \"retrieveAndGenerateSourceConfig\": {\n",
    "                            \"ragSourceIdentifier\": rag_source_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3Uri\": output_path\n",
    "        },\n",
    "        evaluationConfig={\n",
    "            \"automated\": {\n",
    "                \"datasetMetricConfigs\": [{\n",
    "                    \"taskType\": \"QuestionAndAnswer\",  \n",
    "                    \"dataset\": {\n",
    "                        \"name\": f\"{model_identifier}_dataset\",\n",
    "                        \"datasetLocation\": {\n",
    "                            \"s3Uri\": input_data\n",
    "                        }\n",
    "                    },\n",
    "                    \"metricNames\": metrics\n",
    "                }],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return retrieve_generate_job['jobArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = read_jsonl_to_dataframe('labeled_data.jsonl')\n",
    "\n",
    "evaluation_models = [\n",
    "    {\n",
    "        \"model_name\": \"nova_premier\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/premier_results.jsonl\",\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"nova_lite_distilled\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/distillation_results.jsonl\",\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19376e94",
   "metadata": {},
   "source": [
    "## Create IAM Service Role\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df8d49-7e2a-4302-a91a-24d7dd2ff4e4",
   "metadata": {},
   "source": [
    "To use the Python SDK for creating an RAG evaluation job with your own inference responses, use the following steps. First, set up the required configurations, which should include your model identifier for the evaluator, IAM role with appropriate permissions, S3 paths for input data containing your inference responses, and output location for results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd66ea2",
   "metadata": {},
   "source": [
    "## Create Evaluation Datasets and Submit Evaluation Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a680fc-1899-4afa-ab42-1bb67e4fbad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading evaluation_data_nova_premier.jsonl to bucket sample-data-us-east-1-228707323172-1 with prefix citations_distillation...\n",
      "Successfully uploaded evaluation_data_nova_premier.jsonl to S3 bucket!\n",
      "File S3 URI: s3://sample-data-us-east-1-228707323172-1/citations_distillation/evaluation_data_nova_premier.jsonl\n",
      "Uploading evaluation_data_nova_lite_distilled.jsonl to bucket sample-data-us-east-1-228707323172-1 with prefix citations_distillation...\n",
      "Successfully uploaded evaluation_data_nova_lite_distilled.jsonl to S3 bucket!\n",
      "File S3 URI: s3://sample-data-us-east-1-228707323172-1/citations_distillation/evaluation_data_nova_lite_distilled.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Configure knowledge base and model settings\n",
    "evaluator_model = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0' # \"<YOUR_EVALUATOR_MODEL>\"\n",
    "role_arn = \"arn:aws:iam::228707323172:role/bedrock_eval_role\" # \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = 'sample-data-us-east-1-228707323172-1' # Replace by your bucket name \"<YOUR_S3_BUCKET_NAME>\"\n",
    "PREFIX = 'citations_distillation' # \"<YOUR_BUCKET_PREFIX>\"\n",
    "# RAG_dataset_custom_name = \"<YOUR_RAG_BYOI_DATASET_NAME>\" # without the \".jsonl file extension\n",
    "\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}/\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "eval_details = []\n",
    "for model in evaluation_models:\n",
    "    dataframe = read_jsonl_to_dataframe(model['batch_inference_results_file'])\n",
    "    eval_dataset_local_file = create_evaluation_dataset(model['model_name'],dataframe, labeled_df)\n",
    "    eval_dataset_s3 = upload_training_data_to_s3(bucket_name=BUCKET_NAME, local_file_path=eval_dataset_local_file,prefix=PREFIX)\n",
    "    eval_job_arn = create_rag_evaluation_job(\n",
    "        bedrock_client,\n",
    "        model_identifier=model['model_name'], \n",
    "        rag_source_id = f\"{model['model_name']}_kb\",\n",
    "        input_data = eval_dataset_s3,\n",
    "        role_arn=role_arn,\n",
    "        output_path=output_path,\n",
    "        evaluator_model=evaluator_model,\n",
    "        metrics=None\n",
    "    )\n",
    "    eval_details.append({**model, \n",
    "                         \"eval_dataset_local_file\": eval_dataset_local_file, \n",
    "                         \"eval_dataset_s3\": eval_dataset_s3, \n",
    "                         \"eval_job_arn\": eval_job_arn, \n",
    "                         \"evaluator_model\": evaluator_model})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a72a0-ec73-47c0-8cef-e35917d215af",
   "metadata": {},
   "source": [
    "## Configuring a Retrieve and Generate RAG Evaluation Job with BYOI\n",
    "\n",
    "The code below creates an evaluation job that analyzes both retrieval and generation quality from your RAG system. The most significant aspect is the `precomputedRagSourceConfig` parameter, which enables the Bring Your Own Inference capability. This configuration tells Bedrock to evaluate pre-generated responses rather than generating new ones.\n",
    "\n",
    "Note how we're configuring a rich set of evaluation metrics, including the new citation metrics:\n",
    "\n",
    "- **CitationPrecision**: Measures how accurately your RAG system cites sources by evaluating whether cited passages actually contain the information used in the response\n",
    "- **CitationCoverage**: Evaluates how well the response's content is supported by its citations, focusing on whether all information derived from retrieved passages has been properly cited\n",
    "\n",
    "The `ragSourceIdentifier` parameter must match the identifier in your dataset (in this example, \"third-party-RAG\"), creating the link between your evaluation configuration and the responses you've provided. The job will analyze your RAG system's performance across multiple dimensions, providing comprehensive insights into both information retrieval accuracy and generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c361ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = 'nova_premier'\n",
    "\n",
    "nova_premier_eval_arn = create_rag_evaluation_job(\n",
    "    bedrock_client,\n",
    "    model_identifier, \n",
    "    rag_source_id = f\"{model_identifier}_kb\",\n",
    "    input_data = nova_premier_dataset_s3,\n",
    "    role_arn=role_arn,\n",
    "    output_path=output_path,\n",
    "    evaluator_model=evaluator_model,\n",
    "    metrics=None\n",
    ")\n",
    "\n",
    "model_identifier = 'nova_lite_distilled'\n",
    "\n",
    "nova_distilled_eval_arn = create_rag_evaluation_job(\n",
    "    bedrock_client,\n",
    "    model_identifier, \n",
    "    rag_source_id = f\"{model_identifier}_kb\",\n",
    "    input_data = nova_lite_distilled_dataset_s3,\n",
    "    role_arn=role_arn,\n",
    "    output_path=output_path,\n",
    "    evaluator_model=evaluator_model,\n",
    "    metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1e899-5f0d-41d2-a47b-ea88b678d3db",
   "metadata": {},
   "source": [
    "## Monitoring Your RAG Evaluation Jobs\n",
    "\n",
    "After submitting your evaluation jobs, you'll want to monitor their progress. The code below demonstrates how to check the status of both job types:\n",
    "\n",
    "You can run this code periodically to track your job's progress through its lifecycle. Typical status values include \"IN_PROGRESS\", \"COMPLETED\", or \"FAILED\". Once a job reaches \"COMPLETED\" status, you can proceed to retrieve and analyze the evaluation results from the S3 output location you specified when creating the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713b284-d85b-4344-a8fe-8df94d642109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of retrieve-and-generate job\n",
    "retrieve_generate_job_arn = retrieve_generate_job['jobArn']\n",
    "retrieve_generate_status = bedrock_client.get_evaluation_job(jobIdentifier=retrieve_generate_job_arn)\n",
    "print(f\"Retrieve-and-Generate Job Status: {retrieve_generate_status['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9719d49-2d36-4398-9e52-4c448cb65b1f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this guide, we've explored how to leverage Amazon Bedrock RAG Evaluation capabilities with Bring Your Own Inference Responses to assess any RAG system's performance. Key advantages of this approach include:\n",
    "\n",
    "- **Platform independence**: Evaluate RAG systems deployed anywhere - on Amazon Bedrock, other cloud providers, or on-premises\n",
    "- **Comprehensive assessment**: Analyze both retrieve and generate quality with specialized metrics\n",
    "- **Citation quality insights**: Leverage the new citation metrics to ensure responses are properly grounded in source information\n",
    "- **Systematic benchmarking**: Compare different RAG implementations to make data-driven optimization decisions\n",
    "\n",
    "By implementing regular evaluation workflows using these capabilities, you can continuously improve your RAG systems to deliver more accurate, relevant, and well-attributed responses. Whether you're fine-tuning retrieval strategies, optimizing prompt engineering, or exploring different foundation models for generation, these evaluation tools provide the quantitative insights needed to guide your development process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
