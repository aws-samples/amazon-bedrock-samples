{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Performance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook represents the final stage in our model distillation journey, where we evaluate the performance of our distilled model against the original model. We leverage Amazon Bedrock's RAG Evaluation capabilities with Bring Your Own Inference (BYOI) support to conduct a comprehensive assessment of model quality and citation capabilities.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to structure and format evaluation datasets for BYOI evaluation\n",
    "- Advanced evaluation metrics for assessing RAG system performance\n",
    "- Techniques for analyzing citation quality and knowledge transfer effectiveness\n",
    "- Best practices for production-grade model evaluation\n",
    "\n",
    "## Evaluation Metrics Deep Dive\n",
    "\n",
    "Our evaluation framework uses several sophisticated metrics designed for RAG systems:\n",
    "\n",
    "### Citation Quality Metrics\n",
    "- **Citation Precision**: Measures the accuracy of citations by comparing cited text against source passages. High precision indicates the model correctly attributes information to relevant sources.\n",
    "- **Citation Coverage**: Evaluates how comprehensively the model utilizes available context. This metric helps identify if the model is under-utilizing or over-relying on certain passages.\n",
    "\n",
    "### Response Quality Metrics\n",
    "- **Correctness**: Assesses factual accuracy by comparing generated content against ground truth responses and source documents.\n",
    "- **Completeness**: Measures response thoroughness relative to the question's requirements and available context.\n",
    "- **Faithfulness**: Evaluates how well responses align with provided context, detecting potential hallucinations or unsupported claims.\n",
    "- **Helpfulness**: Analyzes practical utility by considering factors like clarity, relevance, and actionability.\n",
    "- **Logical Coherence**: Examines response consistency and reasoning quality, particularly important for complex queries.\n",
    "\n",
    "> **Advanced Note**: These metrics are calculated using specialized evaluator models that perform semantic analysis rather than simple string matching, enabling nuanced assessment of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have completed the previous notebooks in this sequence:\n",
    "1. `01_prepare_data.ipynb`: Data preparation and formatting\n",
    "2. `02_distill.ipynb`: Model distillation process\n",
    "3. `03_batch_inference.ipynb`: Batch inference implementation\n",
    "\n",
    "Additional requirements:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region ([Enable Bedrock models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html))\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock ([IAM setup guide](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html))\n",
    "- RAG system outputs formatted according to the BYOI specification\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Ensure these are enabled in your account and you have sufficient [quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html) for your evaluation workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYOI Evaluation Schema\n",
    "\n",
    "The BYOI format enables evaluation of any RAG system by providing a standardized schema for inputs and outputs. This section details the required format and structure for evaluation data.\n",
    "\n",
    "### Schema Components\n",
    "\n",
    "1. **Conversation Turns**: Each turn represents a complete interaction\n",
    "   - Prompt: The original question or query\n",
    "   - Reference Responses: Ground truth answers for comparison\n",
    "   - Output: The system's response including:\n",
    "     * Generated text\n",
    "     * Retrieved passages\n",
    "     * Citation information\n",
    "\n",
    "2. **Citations**: Structured references linking response segments to source passages\n",
    "   - Generated Response Part: The specific text segment using cited material\n",
    "   - Retrieved References: The source passages supporting the citation\n",
    "\n",
    "3. **Metadata**: Additional context about the evaluation\n",
    "   - Model identifier\n",
    "   - Knowledge base information\n",
    "   - Optional source attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"conversationTurns\": [\n",
    "    {\n",
    "      \"prompt\": {\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"Your prompt here\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"referenceResponses\": [\n",
    "        {\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"text\": \"Expected ground truth answer\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"output\": {\n",
    "        \"text\": \"Generated response text\",\n",
    "        \"knowledgeBaseIdentifier\": \"third-party-RAG\",\n",
    "        \"retrievedPassages\": {\n",
    "          \"retrievalResults\": [\n",
    "            {\n",
    "              \"name\": \"Optional passage name\",\n",
    "              \"content\": {\n",
    "                \"text\": \"Retrieved passage content\"\n",
    "              },\n",
    "              \"metadata\": {\n",
    "                \"source\": \"Optional metadata\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        \"citations\": [\n",
    "          {\n",
    "            \"generatedResponsePart\": {\n",
    "              \"textResponsePart\": {\n",
    "                \"span\": {\n",
    "                  \"start\": 0,\n",
    "                  \"end\": 50\n",
    "                },\n",
    "                \"text\": \"Part of the response that uses cited material\"\n",
    "              }\n",
    "            },\n",
    "            \"retrievedReferences\": [\n",
    "              {\n",
    "                \"name\": \"Optional passage name\",\n",
    "                \"content\": {\n",
    "                  \"text\": \"Source passage for the citation\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                  \"source\": \"Optional metadata\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's set up our evaluation pipeline by first importing required dependencies and configuring our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and setup environment\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "print(boto3.__version__)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from utils import read_jsonl_to_dataframe, upload_training_data_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_user_message(user_msg):\n",
    "    # Regex pattern to extract content between tags\n",
    "    pattern = r\"<context>(.*?)</context>\\s*<question>(.*?)</question>\"\n",
    "    \n",
    "    # Apply regex\n",
    "    match = re.search(pattern, user_msg, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        context = match.group(1).strip()\n",
    "        question = match.group(2).strip()\n",
    "        return context, question\n",
    "        \n",
    "    else:\n",
    "        print(\"Pattern not found in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_record(results_row, labeled_row, model_identifier=None, kb_identifier='kb_id'):\n",
    "    \"\"\"\n",
    "    Takes a batch inference result data set and builds an evaluation data set for use in bedrock evaluation\n",
    "    \n",
    "    Args:\n",
    "        row:            pandas row\n",
    "        max_records:    defaults to 1000 - max for bedrock evaluation\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock Evaluations API\n",
    "    \n",
    "    \"\"\"\n",
    "    result = results_row.to_dict()\n",
    "    label = labeled_row.to_dict()\n",
    "\n",
    "    retrieval_content, question = split_user_message(result['modelInput']['messages'][0]['content'][0]['text'])\n",
    "    return {\n",
    "        \"conversationTurns\": [\n",
    "            {\n",
    "                \"prompt\": {\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": question\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"referenceResponses\": [\n",
    "                    {\n",
    "                        \"content\": label['modelInput']['messages'][1]['content'] # labeled answer content from assistant\n",
    "                    }\n",
    "                ],\n",
    "                \"output\": {\n",
    "                    \"text\": result['modelOutput']['output']['message']['content'][0]['text'],\n",
    "                    \"modelIdentifier\": model_identifier if model_identifier else 'placeholder_model',\n",
    "                    \"knowledgeBaseIdentifier\": kb_identifier,\n",
    "                    \"retrievedPassages\": {\n",
    "                        \"retrievalResults\": [ # put context from squad example here\n",
    "                            {\n",
    "                                \"name\": f\"retrieval_{results_row['recordId']}\",\n",
    "                                \"content\": {\n",
    "                                    \"text\": retrieval_content\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(model_identifier, dataframe, labeled_df, rag_source_id=None, max_records=1000, output_filename=None):\n",
    "    \"\"\"\n",
    "    Create an evaluation dataset from model results and write to a JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame containing the results to evaluate\n",
    "    labeled_df : pandas.DataFrame\n",
    "        DataFrame containing labeled data with matching recordIds\n",
    "    rag_source_id : str, optional\n",
    "        Identifier for the knowledge base source\n",
    "    max_records : int, optional\n",
    "        Maximum number of records to process (default: 1000)\n",
    "    output_filename : str, optional\n",
    "        Custom filename for the output JSONL file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the created JSONL file\n",
    "    list\n",
    "        The evaluation dataset as a list of dictionaries\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    eval_dataset = []\n",
    "    kb_identifier = rag_source_id or f\"{model_identifier}_kb\"\n",
    "    \n",
    "    for ix, results_row in dataframe.head(max_records).iterrows():\n",
    "        labeled_row = labeled_df[labeled_df['recordId'] == results_row['recordId']].iloc[0]\n",
    "        eval_dataset.append(create_eval_record(\n",
    "            results_row=results_row, \n",
    "            labeled_row=labeled_row,\n",
    "            model_identifier=model_identifier, \n",
    "            kb_identifier=kb_identifier\n",
    "        ))\n",
    "    \n",
    "    jsonl_file = output_filename or f\"evaluation_data_{model_identifier}.jsonl\"\n",
    "    \n",
    "    # Write all records to JSONL file\n",
    "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
    "        for record in eval_dataset:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\"Successfully wrote {len(eval_dataset)} records to {jsonl_file}\")\n",
    "    return jsonl_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Role Configuration\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html\n",
    "\n",
    "To run evaluation jobs securely, we need an IAM service role with the following permissions:\n",
    "\n",
    "1. **S3 Access**: Read/write permissions for evaluation data and results\n",
    "2. **Bedrock Model Access**: Permission to invoke evaluator models\n",
    "3. **Evaluation Job Management**: Ability to create and monitor jobs\n",
    "\n",
    "### Required IAM Policies\n",
    "\n",
    "1. S3 Access Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::your-bucket/*\",\n",
    "                \"arn:aws:s3:::your-bucket\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "2. Bedrock Access Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\",\n",
    "                \"bedrock:CreateEvaluationJob\",\n",
    "                \"bedrock:GetEvaluationJob\",\n",
    "                \"bedrock:ListEvaluationJobs\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "For detailed setup instructions, visit the [Bedrock Service Roles documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running Evaluation Jobs\n",
    "\n",
    "Now let's create and submit our evaluation jobs. We'll start by making a helper function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_evaluation_job(\n",
    "    bedrock_client,\n",
    "    model_identifier, \n",
    "    rag_source_id,\n",
    "    input_data,\n",
    "    role_arn,\n",
    "    output_path,\n",
    "    evaluator_model,\n",
    "    metrics=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a Bedrock RAG evaluation job.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    bedrock_client : boto3.client\n",
    "        The Bedrock client to use for creating the job\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    rag_source_id : str\n",
    "        Identifier for the RAG knowledge base source\n",
    "    input_data : str\n",
    "        S3 URI for the input evaluation dataset\n",
    "    role_arn : str\n",
    "        IAM role ARN with sufficient permissions to run the evaluation\n",
    "    output_path : str\n",
    "        S3 URI where evaluation results will be stored\n",
    "    evaluator_model : str\n",
    "        Identifier for the model that will perform evaluations\n",
    "    metrics : list, optional\n",
    "        List of metrics to evaluate (defaults to standard RAG metrics)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The job ARN from the create_evaluation_job API call\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Set default RAG metrics if none provided\n",
    "    if metrics is None:\n",
    "        metrics = [\n",
    "            \"Builtin.Correctness\",\n",
    "            \"Builtin.Completeness\",\n",
    "            \"Builtin.Helpfulness\",\n",
    "            \"Builtin.LogicalCoherence\",\n",
    "            \"Builtin.Faithfulness\",\n",
    "            \"Builtin.CitationPrecision\",\n",
    "            \"Builtin.CitationCoverage\"\n",
    "        ]\n",
    "    \n",
    "    # Generate job name using timestamp for uniqueness\n",
    "    retrieve_generate_job_name = f\"citations-{model_identifier.replace('_','-')}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "    # Create the evaluation job\n",
    "    retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "        jobName=retrieve_generate_job_name,\n",
    "        jobDescription=\"Evaluate retrieval and generation\",\n",
    "        roleArn=role_arn,\n",
    "        applicationType=\"RagEvaluation\",\n",
    "        inferenceConfig={\n",
    "            \"ragConfigs\": [\n",
    "                {\n",
    "                    \"precomputedRagSourceConfig\": {\n",
    "                        \"retrieveAndGenerateSourceConfig\": {\n",
    "                            \"ragSourceIdentifier\": rag_source_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3Uri\": output_path\n",
    "        },\n",
    "        evaluationConfig={\n",
    "            \"automated\": {\n",
    "                \"datasetMetricConfigs\": [{\n",
    "                    \"taskType\": \"QuestionAndAnswer\",  \n",
    "                    \"dataset\": {\n",
    "                        \"name\": f\"{model_identifier}_dataset\",\n",
    "                        \"datasetLocation\": {\n",
    "                            \"s3Uri\": input_data\n",
    "                        }\n",
    "                    },\n",
    "                    \"metricNames\": metrics\n",
    "                }],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return retrieve_generate_job['jobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Evaluation Jobs\n",
    "We'll now submit each of the evaluation jobs for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = read_jsonl_to_dataframe('labeled_data.jsonl')\n",
    "\n",
    "evaluation_models = [\n",
    "    {\n",
    "        \"model_name\": \"nova_premier\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/us-amazon-nova-premier-v1-0-batch_inf_data.jsonl\",\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"nova_lite\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/-batch_inf_data.jsonl\",\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"nova_lite_distilled\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/batch_inf_data.jsonl\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "evaluator_model = 'us.anthropic.claude-3-5-haiku-20241022-v1:0' # \"<YOUR_EVALUATOR_MODEL>\"\n",
    "role_arn = \"arn:aws:iam::228707323172:role/bedrock_eval_role\" # \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = 'sample-data-us-east-1-228707323172-1' # Replace by your bucket name \"<YOUR_S3_BUCKET_NAME>\"\n",
    "PREFIX = 'citations_distillation' # \"<YOUR_BUCKET_PREFIX>\"\n",
    "\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}/\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "eval_details = []\n",
    "for model in evaluation_models:\n",
    "    dataframe = read_jsonl_to_dataframe(model['batch_inference_results_file'])\n",
    "    eval_dataset_local_file = create_evaluation_dataset(model['model_name'],dataframe, labeled_df)\n",
    "    eval_dataset_s3 = upload_training_data_to_s3(bucket_name=BUCKET_NAME, local_file_path=eval_dataset_local_file,prefix=PREFIX)\n",
    "    eval_job_arn = create_rag_evaluation_job(\n",
    "        bedrock_client,\n",
    "        model_identifier=model['model_name'], \n",
    "        rag_source_id = f\"{model['model_name']}_kb\",\n",
    "        input_data = eval_dataset_s3,\n",
    "        role_arn=role_arn,\n",
    "        output_path=output_path,\n",
    "        evaluator_model=evaluator_model,\n",
    "        metrics=None\n",
    "    )\n",
    "    eval_details.append({**model, \n",
    "                        'eval_dataset_s3': eval_dataset_s3,\n",
    "                        'eval_job_arn': eval_job_arn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. **Structure Evaluation Data**: Format RAG system outputs for comprehensive evaluation using the BYOI specification\n",
    "2. **Configure Evaluation Jobs**: Set up secure IAM roles and configure evaluation parameters\n",
    "3. **Execute Evaluations**: Run parallel evaluations of multiple models using Amazon Bedrock\n",
    "4. **Analyze Results**: Interpret evaluation metrics to assess model performance\n",
    "\n",
    "This completes our four-notebook series on model distillation for citation-aware RAG systems. Through this series, we've covered:\n",
    "- Data preparation and formatting\n",
    "- Model distillation techniques\n",
    "- Batch inference implementation\n",
    "- Comprehensive model evaluation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further enhance your RAG system:\n",
    "\n",
    "1. **Advanced Metrics**: Implement custom evaluation metrics specific to your use case\n",
    "2. **Production Deployment**: \n",
    "   - Set up automated evaluation pipelines\n",
    "   - Implement A/B testing frameworks\n",
    "   - Configure monitoring and alerting\n",
    "3. **System Optimization**:\n",
    "   - Fine-tune retrieval strategies based on evaluation results\n",
    "   - Optimize citation generation based on precision/coverage metrics\n",
    "   - Implement feedback loops for continuous improvement\n",
    "\n",
    "For more information, explore:\n",
    "- [Amazon Bedrock Evaluation Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html)\n",
    "- [RAG Best Practices Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/rag-best-practices.html)\n",
    "- [Advanced Model Evaluation Techniques](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-metrics.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
