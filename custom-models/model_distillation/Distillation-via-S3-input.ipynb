{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf2edd1",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Model Distillation Guide - JSONL training data available in Amazon S3 bucket\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Model distillation in Amazon Bedrock allows you to create smaller, more efficient models while maintaining performance by learning from larger, more capable models. This guide demonstrates how to use the Amazon Bedrock APIs to implement model distillation using:\n",
    "**JSONL training data available in Amazon S3 bucket**.\n",
    "\n",
    "Through this API usage notebook, we'll explore the complete distillation workflow, from configuring teacher and student models to deploying the final distilled model. You'll learn how to:\n",
    "\n",
    "- Set up and configure distillation jobs\n",
    "- Prepare and format training data for distillation\n",
    "- Upload and use training data from S3\n",
    "- Manage model provisioning and deployment\n",
    "- Run inference with distilled models\n",
    "\n",
    "The guide covers essential API operations including:\n",
    "- Creating and configuring distillation jobs\n",
    "- Managing training data sources in S3\n",
    "- Handling model deployments\n",
    "- Implementing production best practices using boto3 and the Bedrock SDK\n",
    "\n",
    "While model distillation offers benefits like improved efficiency and reduced costs, this guide focuses on the practical implementation details and API usage patterns needed to successfully execute distillation workflows in Amazon Bedrock.\n",
    "\n",
    "## Best Practices and Considerations\n",
    "\n",
    "When using model distillation:\n",
    "1. Ensure your training data is diverse and representative of your use case\n",
    "2. Monitor distillation metrics in the S3 output location\n",
    "3. Evaluate the distilled model's performance against your requirements\n",
    "4. Consider cost-performance tradeoffs when selecting model units for deployment\n",
    "\n",
    "The distilled model should provide faster responses and lower costs while maintaining acceptable performance for your specific use case.\n",
    "\n",
    "### Setup and Prerequisites\n",
    "\n",
    "Before we begin, make sure you have the following:\n",
    "\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing training data and output\n",
    "- Training data in JSONL format\n",
    "- Sufficient service quota to use Provisioned Throughput in Bedrock\n",
    "- An IAM role with the following permissions:\n",
    "\n",
    "IAM Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET\",\n",
    "                \"arn:aws:s3:::YOUR_DISTILLATION_OUTPUT_BUCKET/*\",\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:CreateModelCustomizationJob\",\n",
    "                \"bedrock:GetModelCustomizationJob\",\n",
    "                \"bedrock:ListModelCustomizationJobs\",\n",
    "                \"bedrock:StopModelCustomizationJob\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Trust Relationship:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": [\n",
    "                    \"bedrock.amazonaws.com\"\n",
    "                ]\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"YOUR_ACCOUNT_ID\"\n",
    "                },\n",
    "                \"ArnLike\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:YOUR_REGION:YOUR_ACCOUNT_ID:model-customization-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Dataset:\n",
    "As an example, in this notebook we will be using the `Uber10K dataset`.\n",
    "\n",
    "First, let's set up our environment and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063248b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b031e3",
   "metadata": {},
   "source": [
    "# Model Selection and Configuration\n",
    "\n",
    "When selecting models for distillation, consider the following factors:\n",
    "\n",
    "1. Performance targets\n",
    "2. Latency requirements\n",
    "3. Total Cost of Ownership (TCO)\n",
    "\n",
    "Let's set up our configuration parameters for the distillation process.\n",
    "\n",
    "(We're using Amazon Nova/Micro as the example teacher/student models in this code sample. Please change it based on your use case, and run code sample in **supporting region**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc786809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import create_s3_bucket, upload_training_data_to_s3, delete_s3_bucket_and_contents, \\\n",
    "create_model_distillation_role_and_permissions, delete_role_and_attached_policies, delete_distillation_buckets\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "sts_client = session.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# define bucket you want to create and upload the dataset to:\n",
    "bucket_name='<YOUR-DISTILLATION-BUCKET-NAME>' # Replace by your bucket name\n",
    "data_prefix = '<PREFIX>' # Replace by your defined prefix\n",
    "\n",
    "# configure teacher nd student model\n",
    "teacher_model = \"amazon.nova-pro-v1:0\"\n",
    "student_model_micro = \"amazon.nova-micro-v1:0:128k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e9a9c",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Model Distillation\n",
    "\n",
    "Before we start the distillation process, we need to prepare our dataset. We'll create a function to convert our input data into the format required by Amazon Bedrock.\n",
    "\n",
    "#### Model Distillation Input Format\n",
    "\n",
    "The training data must follow the Bedrock conversation schema in JSONL format. Each line should be a valid JSON object with this structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "    \"system\": [\n",
    "        {\n",
    "            \"text\": <Your-System-Prompt>\n",
    "        }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": <Your-Prompt-And-OR-Context>\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": <Your-Ground-Truth-Response>\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Key formatting requirements:\n",
    "- Each line must be a complete JSON object\n",
    "- The schemaVersion field must be specified as `bedrock-conversation-2024`\n",
    "- System instructions should be included in the system array\n",
    "- Messages (including any context) must include both user and assistant roles in the correct order\n",
    "- All text content must be wrapped in the appropriate content structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_dataset(input_file, output_file, system_message):\n",
    "    try:\n",
    "        # Create the base conversation template\n",
    "        conversation_template = {\n",
    "            \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "            \"system\": [{\"text\": system_message}],\n",
    "            \"messages\": []\n",
    "        }\n",
    "        \n",
    "        # Process input file and write output\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "            # Read input file line by line\n",
    "            for line in infile:\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    # Parse the input JSON line\n",
    "                    input_data = json.loads(line)\n",
    "                    \n",
    "                    # Create a new conversation for each line\n",
    "                    conversation = conversation_template.copy()\n",
    "                    \n",
    "                    # Add user message\n",
    "                    user_message = {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": input_data[\"prompt\"]}]\n",
    "                    }\n",
    "                    \n",
    "                    # Add assistant message\n",
    "                    assistant_message = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": input_data[\"completion\"]}]\n",
    "                    }\n",
    "                    \n",
    "                    # Add messages to conversation\n",
    "                    conversation[\"messages\"] = [user_message, assistant_message]\n",
    "                    \n",
    "                    # Write the conversation to output file\n",
    "                    outfile.write(json.dumps(conversation) + '\\n')\n",
    "                \n",
    "        print(f\"Successfully converted {input_file} to Bedrock format and saved to {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cce93",
   "metadata": {},
   "source": [
    "### Now that we have our data preparation function, let's use it to create our distillation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11818e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a specialized financial analyst assistant trained to analyze SEC filings, financial documents, and regulatory submissions. Your role is to:\n",
    "- Extract and interpret key information from 10-K, 10-Q, and other SEC filings\n",
    "- Provide accurate, factual responses based solely on the provided document context\n",
    "- Focus on specific financial, legal, and corporate governance details\n",
    "- Present information clearly and concisely without speculation\n",
    "- Maintain accuracy in reporting numbers, dates, and regulatory details\n",
    "When responding, only use information explicitly stated in the provided context.\"\"\"\n",
    "\n",
    "input_data_file = 'SampleData/uber10K.jsonl'\n",
    "output_data_file = 'model_distillation_dataset.jsonl'\n",
    "\n",
    "prepare_training_dataset(\n",
    "    input_file=input_data_file,\n",
    "    output_file=output_data_file,\n",
    "    system_message=system_message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique names for the job and model\n",
    "job_name = f\"distillation-job-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Configure models and IAM role\n",
    "role_name, role_arn = create_model_distillation_role_and_permissions(bucket_name=bucket_name, account_id=account_id)\n",
    "\n",
    "# creating training data bucket\n",
    "create_s3_bucket(bucket_name=bucket_name)\n",
    "\n",
    "# Specify S3 locations\n",
    "training_data = upload_training_data_to_s3(bucket_name, output_data_file, prefix=data_prefix)\n",
    "output_path = f\"s3://{bucket_name}/output/\"\n",
    "\n",
    "# Set maximum response length\n",
    "max_response_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8eebe",
   "metadata": {},
   "source": [
    "# Starting the Distillation Job\n",
    "\n",
    "With our dataset prepared, we can now start the distillation job. We'll use the `create_model_customization_job` API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.create_model_customization_job(\n",
    "    jobName=job_name,\n",
    "    customModelName=model_name,\n",
    "    roleArn=role_arn,\n",
    "    baseModelIdentifier=student_model_micro,\n",
    "    customizationType=\"DISTILLATION\",\n",
    "    trainingDataConfig={\n",
    "        \"s3Uri\": training_data\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    customizationConfig={\n",
    "        \"distillationConfig\": {\n",
    "            \"teacherModelConfig\": {\n",
    "                \"teacherModelIdentifier\": teacher_model,\n",
    "                \"maxResponseLengthForInference\": max_response_length \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6d0c8",
   "metadata": {},
   "source": [
    "# Monitoring the Distillation Job\n",
    "\n",
    "After starting the distillation job, it's important to monitor its progress. We can use the `get_model_customization_job` API to check the status of our job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the distillation job arn\n",
    "job_arn = response['jobArn']\n",
    "\n",
    "# print job status\n",
    "job_status = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)[\"status\"]\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da8a8e",
   "metadata": {},
   "source": [
    "# Deploying the Distilled Model\n",
    "\n",
    "Once the distillation job is complete, we can deploy our distilled model. This involves creating a Provisioned Throughput model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the distilled model\n",
    "custom_model_id = bedrock_client.get_model_customization_job(jobIdentifier=job_arn)['outputModelArn']\n",
    "distilled_model_name = f\"distilled-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "provisioned_model_id = bedrock_client.create_provisioned_model_throughput(\n",
    "    modelUnits=1,\n",
    "    provisionedModelName=distilled_model_name,\n",
    "    modelId=custom_model_id \n",
    ")['provisionedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c705bac",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "Let's delete the resources that were created in this notebook. `Uncomment` the code below to delete the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete bucket and dataset\n",
    "# delete_distillation_buckets(bucket_name)\n",
    "\n",
    "# delete role and its policy:\n",
    "# delete_role_and_attached_policies(role_name=role_name)\n",
    "\n",
    "# delete provisioned throughput:\n",
    "# response = bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a025",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, we've walked through the entire process of model distillation using Amazon Bedrock. We covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Preparing the dataset\n",
    "3. Configuring and starting a distillation job\n",
    "4. Monitoring the job's progress\n",
    "5. Deploying the distilled model\n",
    "6. Cleaning up resources\n",
    "\n",
    "Model distillation is a powerful technique that can help you create more efficient models tailored to your specific use case. By following this guide, you should now be able to implement model distillation in your own projects using Amazon Bedrock.\n",
    "\n",
    "Remember to always consider your specific use case requirements when selecting models and configuring the distillation process. \n",
    "\n",
    "**Happy distilling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
