{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Performance Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook represents the final stage in our model distillation journey, where we evaluate the performance of our distilled model against the original model. We leverage Amazon Bedrock's RAG Evaluation capabilities with Bring Your Own Inference (BYOI) support to conduct a comprehensive assessment of model quality and citation capabilities.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to structure and format evaluation datasets for BYOI evaluation\n",
    "- Advanced evaluation metrics for assessing RAG system performance\n",
    "- Techniques for analyzing citation quality and knowledge transfer effectiveness\n",
    "\n",
    "## Evaluation Metrics Deep Dive\n",
    "\n",
    "Our evaluation framework uses several sophisticated metrics designed for RAG systems:\n",
    "\n",
    "### Citation Quality Metrics\n",
    "- **Citation Coverage**: Evaluates how comprehensively the model utilizes available context. This metric helps identify if the model is under-utilizing or over-relying on certain passages.\n",
    "\n",
    "### Response Quality Metrics\n",
    "- **Correctness**: Assesses factual accuracy by comparing generated content against ground truth responses and source documents.\n",
    "- **Completeness**: Measures response thoroughness relative to the question's requirements and available context.\n",
    "- **Faithfulness**: Evaluates how well responses align with provided context, detecting potential hallucinations or unsupported claims.\n",
    "- **Helpfulness**: Analyzes practical utility by considering factors like clarity, relevance, and actionability.\n",
    "- **Logical Coherence**: Examines response consistency and reasoning quality, particularly important for complex queries.\n",
    "\n",
    "> **Advanced Note**: These metrics are calculated using specialized evaluator models that perform semantic analysis rather than simple string matching, enabling nuanced assessment of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have completed the previous notebooks in this sequence:\n",
    "1. `01_prepare_data.ipynb`: Data preparation and formatting\n",
    "2. `02_distill.ipynb`: Model distillation process\n",
    "3. `03_batch_inference.ipynb`: Batch inference implementation\n",
    "\n",
    "Additional requirements:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region ([Enable Bedrock models](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html))\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock ([IAM setup guide](https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html))\n",
    "- RAG system outputs formatted according to the BYOI specification\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Ensure these are enabled in your account and you have sufficient [quotas](https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html) for your evaluation workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYOI Evaluation Schema\n",
    "\n",
    "The BYOI format enables evaluation of any RAG system by providing a standardized schema for inputs and outputs. This section details the required format and structure for evaluation data.\n",
    "\n",
    "### Schema Components\n",
    "\n",
    "1. **Conversation Turns**: Each turn represents a complete interaction\n",
    "   - Prompt: The original question or query\n",
    "   - Reference Responses: Ground truth answers for comparison\n",
    "   - Output: The system's response including:\n",
    "     * Generated text\n",
    "     * Retrieved passages\n",
    "     * Citation information\n",
    "\n",
    "2. **Citations**: Structured references linking response segments to source passages\n",
    "   - Generated Response Part: The specific text segment using cited material\n",
    "   - Retrieved References: The source passages supporting the citation\n",
    "\n",
    "3. **Metadata**: Additional context about the evaluation\n",
    "   - Model identifier\n",
    "   - Knowledge base information\n",
    "   - Optional source attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Schema\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"conversationTurns\": [\n",
    "    {\n",
    "      \"prompt\": {\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"Your prompt here\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"referenceResponses\": [\n",
    "        {\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"text\": \"Expected ground truth answer\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"output\": {\n",
    "        \"text\": \"Generated response text\",\n",
    "        \"knowledgeBaseIdentifier\": \"third-party-RAG\",\n",
    "        \"retrievedPassages\": {\n",
    "          \"retrievalResults\": [\n",
    "            {\n",
    "              \"name\": \"Optional passage name\",\n",
    "              \"content\": {\n",
    "                \"text\": \"Retrieved passage content\"\n",
    "              },\n",
    "              \"metadata\": {\n",
    "                \"source\": \"Optional metadata\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        \"citations\": [\n",
    "          {\n",
    "            \"generatedResponsePart\": {\n",
    "              \"textResponsePart\": {\n",
    "                \"span\": {\n",
    "                  \"start\": 0,\n",
    "                  \"end\": 50\n",
    "                },\n",
    "                \"text\": \"Part of the response that uses cited material\"\n",
    "              }\n",
    "            },\n",
    "            \"retrievedReferences\": [\n",
    "              {\n",
    "                \"name\": \"Optional passage name\",\n",
    "                \"content\": {\n",
    "                  \"text\": \"Source passage for the citation\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                  \"source\": \"Optional metadata\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Let's set up our evaluation pipeline by first importing required dependencies and configuring our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and setup environment\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "print(boto3.__version__)\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "from utils import read_jsonl_to_dataframe, upload_training_data_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_user_message(user_msg):\n",
    "    # Regex pattern to extract content between tags\n",
    "    pattern = r\"<context>(.*?)</context>\\s*<question>(.*?)</question>\"\n",
    "    \n",
    "    # Apply regex\n",
    "    match = re.search(pattern, user_msg, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        context = match.group(1).strip()\n",
    "        question = match.group(2).strip()\n",
    "        return context, question\n",
    "        \n",
    "    else:\n",
    "        print(\"Pattern not found in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_record(results_row, labeled_row, model_identifier=None, kb_identifier='kb_id'):\n",
    "    \"\"\"\n",
    "    Takes a batch inference result data set and builds an evaluation data set for use in bedrock evaluation\n",
    "    \n",
    "    Args:\n",
    "        row:            pandas row\n",
    "        max_records:    defaults to 1000 - max for bedrock evaluation\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock Evaluations API\n",
    "    \n",
    "    \"\"\"\n",
    "    result = results_row.to_dict()\n",
    "    label = labeled_row.to_dict()\n",
    "\n",
    "    retrieval_content, question = split_user_message(result['modelInput']['messages'][0]['content'][0]['text'])\n",
    "    return {\n",
    "        \"conversationTurns\": [\n",
    "            {\n",
    "                \"prompt\": {\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": question\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"referenceResponses\": [\n",
    "                    {\n",
    "                        \"content\": label['modelInput']['messages'][1]['content'] # labeled answer content from assistant\n",
    "                    }\n",
    "                ],\n",
    "                \"output\": {\n",
    "                    \"text\": result['modelOutput']['output']['message']['content'][0]['text'],\n",
    "                    \"modelIdentifier\": model_identifier if model_identifier else 'placeholder_model',\n",
    "                    \"knowledgeBaseIdentifier\": kb_identifier,\n",
    "                    \"retrievedPassages\": {\n",
    "                        \"retrievalResults\": [ # put context from squad example here\n",
    "                            {\n",
    "                                \"name\": f\"retrieval_{results_row['recordId']}\",\n",
    "                                \"content\": {\n",
    "                                    \"text\": retrieval_content\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(model_identifier, dataframe, labeled_df, rag_source_id=None, max_records=1000, output_filename=None):\n",
    "    \"\"\"\n",
    "    Create an evaluation dataset from model results and write to a JSONL file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    dataframe : pandas.DataFrame\n",
    "        DataFrame containing the results to evaluate\n",
    "    labeled_df : pandas.DataFrame\n",
    "        DataFrame containing labeled data with matching recordIds\n",
    "    rag_source_id : str, optional\n",
    "        Identifier for the knowledge base source\n",
    "    max_records : int, optional\n",
    "        Maximum number of records to process (default: 1000)\n",
    "    output_filename : str, optional\n",
    "        Custom filename for the output JSONL file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the created JSONL file\n",
    "    list\n",
    "        The evaluation dataset as a list of dictionaries\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    eval_dataset = []\n",
    "    kb_identifier = rag_source_id or f\"{model_identifier}_kb\"\n",
    "    \n",
    "    for ix, results_row in dataframe.head(max_records).iterrows():\n",
    "        labeled_row = labeled_df[labeled_df['recordId'] == results_row['recordId']].iloc[0]\n",
    "        eval_dataset.append(create_eval_record(\n",
    "            results_row=results_row, \n",
    "            labeled_row=labeled_row,\n",
    "            model_identifier=model_identifier, \n",
    "            kb_identifier=kb_identifier\n",
    "        ))\n",
    "    \n",
    "    jsonl_file = output_filename or f\"evaluation_data_{model_identifier}.jsonl\"\n",
    "    \n",
    "    # Write all records to JSONL file\n",
    "    with open(jsonl_file, 'w', encoding='utf-8') as f:\n",
    "        for record in eval_dataset:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\"Successfully wrote {len(eval_dataset)} records to {jsonl_file}\")\n",
    "    return jsonl_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM Role Configuration\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html\n",
    "\n",
    "To run evaluation jobs securely, we need an IAM service role with the following permissions:\n",
    "\n",
    "1. **S3 Access**: Read/write permissions for evaluation data and results\n",
    "2. **Bedrock Model Access**: Permission to invoke evaluator models\n",
    "3. **Evaluation Job Management**: Ability to create and monitor jobs\n",
    "\n",
    "### Required IAM Policies\n",
    "\n",
    "1. S3 Access Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::your-bucket/*\",\n",
    "                \"arn:aws:s3:::your-bucket\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "2. Bedrock Access Policy:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:InvokeModel\",\n",
    "                \"bedrock:CreateEvaluationJob\",\n",
    "                \"bedrock:GetEvaluationJob\",\n",
    "                \"bedrock:ListEvaluationJobs\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "For detailed setup instructions, visit the [Bedrock Service Roles documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/judge-service-roles.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running Evaluation Jobs\n",
    "\n",
    "Now let's create and submit our evaluation jobs. We'll start by making a helper function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_evaluation_job(\n",
    "    bedrock_client,\n",
    "    model_identifier, \n",
    "    rag_source_id,\n",
    "    input_data,\n",
    "    role_arn,\n",
    "    output_path,\n",
    "    evaluator_model,\n",
    "    metrics=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a Bedrock RAG evaluation job.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    bedrock_client : boto3.client\n",
    "        The Bedrock client to use for creating the job\n",
    "    model_identifier : str\n",
    "        Identifier for the model being evaluated\n",
    "    rag_source_id : str\n",
    "        Identifier for the RAG knowledge base source\n",
    "    input_data : str\n",
    "        S3 URI for the input evaluation dataset\n",
    "    role_arn : str\n",
    "        IAM role ARN with sufficient permissions to run the evaluation\n",
    "    output_path : str\n",
    "        S3 URI where evaluation results will be stored\n",
    "    evaluator_model : str\n",
    "        Identifier for the model that will perform evaluations\n",
    "    metrics : list, optional\n",
    "        List of metrics to evaluate (defaults to standard RAG metrics)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The job ARN from the create_evaluation_job API call\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Set default RAG metrics if none provided\n",
    "    if metrics is None:\n",
    "        metrics = [\n",
    "            \"Builtin.Correctness\",\n",
    "            \"Builtin.Completeness\",\n",
    "            \"Builtin.Helpfulness\",\n",
    "            \"Builtin.LogicalCoherence\",\n",
    "            \"Builtin.Faithfulness\"\n",
    "        ]\n",
    "    \n",
    "    # Generate job name using timestamp for uniqueness\n",
    "    retrieve_generate_job_name = f\"citations-{model_identifier.replace('_','-')}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "    # Create the evaluation job\n",
    "    retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "        jobName=retrieve_generate_job_name,\n",
    "        jobDescription=\"Evaluate retrieval and generation\",\n",
    "        roleArn=role_arn,\n",
    "        applicationType=\"RagEvaluation\",\n",
    "        inferenceConfig={\n",
    "            \"ragConfigs\": [\n",
    "                {\n",
    "                    \"precomputedRagSourceConfig\": {\n",
    "                        \"retrieveAndGenerateSourceConfig\": {\n",
    "                            \"ragSourceIdentifier\": rag_source_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3Uri\": output_path\n",
    "        },\n",
    "        evaluationConfig={\n",
    "            \"automated\": {\n",
    "                \"datasetMetricConfigs\": [{\n",
    "                    \"taskType\": \"QuestionAndAnswer\",  \n",
    "                    \"dataset\": {\n",
    "                        \"name\": f\"{model_identifier}_dataset\",\n",
    "                        \"datasetLocation\": {\n",
    "                            \"s3Uri\": input_data\n",
    "                        }\n",
    "                    },\n",
    "                    \"metricNames\": metrics\n",
    "                }],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return retrieve_generate_job['jobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Evaluation Jobs\n",
    "We'll now submit each of the evaluation jobs for the models. Enter in each of the batch inference results file locations along with a model name that will be used to name the evaluation job.\n",
    "\n",
    "\n",
    "Bedrock Evaluation jobs, like Batch Inference Jobs, can take a number of hours to run. Once the jobs have kicked off feel free to return here to continue the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = read_jsonl_to_dataframe('labeled_data.jsonl')\n",
    "\n",
    "evaluation_models = [\n",
    "    {\n",
    "        \"model_name\": \"nova_premier\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/amazon-nova-premier-data.jsonl\",\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"nova_lite_distilled\",\n",
    "        \"batch_inference_results_file\": \"./batch_inference_results/distilled_results.jsonl\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "evaluator_model = 'us.anthropic.claude-3-5-haiku-20241022-v1:0' # \"<YOUR_EVALUATOR_MODEL>\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = '<YOUR_S3_BUCKET_NAME>' # Replace by your bucket name \"<YOUR_S3_BUCKET_NAME>\"\n",
    "PREFIX = 'citations_distillation' # \"<YOUR_BUCKET_PREFIX>\"\n",
    "\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "eval_details = []\n",
    "for model in evaluation_models:\n",
    "    dataframe = read_jsonl_to_dataframe(model['batch_inference_results_file'])\n",
    "    eval_dataset_local_file = create_evaluation_dataset(model['model_name'],dataframe, labeled_df)\n",
    "    eval_dataset_s3 = upload_training_data_to_s3(bucket_name=BUCKET_NAME, local_file_path=eval_dataset_local_file,prefix=PREFIX)\n",
    "    eval_job_arn = create_rag_evaluation_job(\n",
    "        bedrock_client,\n",
    "        model_identifier=model['model_name'], \n",
    "        rag_source_id = f\"{model['model_name']}_kb\",\n",
    "        input_data = eval_dataset_s3,\n",
    "        role_arn=role_arn,\n",
    "        output_path=output_path,\n",
    "        evaluator_model=evaluator_model,\n",
    "        metrics=None\n",
    "    )\n",
    "    eval_details.append({**model, \n",
    "                        'eval_dataset_s3': eval_dataset_s3,\n",
    "                        'eval_job_arn': eval_job_arn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Evaluation Results\n",
    "Here we've provided a helper function to download the evaluation results using only an evaluation job ARN. This will download results to a folder titled `evaluation_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def download_evaluation_results(bedrock_client, eval_job_arn):\n",
    "    # Get the evaluation job details\n",
    "    eval_results_response = bedrock_client.get_evaluation_job(\n",
    "        jobIdentifier=eval_job_arn\n",
    "    )\n",
    "    \n",
    "    # Check if the job is completed\n",
    "    job_status = eval_results_response.get('status', '')\n",
    "    if job_status not in ['Completed']:\n",
    "        print(f\"Evaluation job status is {job_status}, not ready to download results yet.\")\n",
    "        return\n",
    "    \n",
    "    # Extract the S3 output URI with the correct path construction\n",
    "    base_s3_uri = eval_results_response.get('outputDataConfig', {}).get('s3Uri')\n",
    "    job_name = eval_results_response.get('jobName', '')\n",
    "    job_id = eval_job_arn.split('/')[-1]\n",
    "    dataset_name = eval_results_response.get('evaluationConfig', {}).get('automated', {}).get('datasetMetricConfigs',{})[0].get('dataset', {}).get('name', {})\n",
    "    \n",
    "    s3_output_uri = f\"{base_s3_uri}{job_name}/{job_id}/inference_configs/0/datasets/{dataset_name}\"\n",
    "    \n",
    "    if not s3_output_uri:\n",
    "        print(\"Could not construct S3 output URI\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Looking for .jsonl files in: {s3_output_uri}\")\n",
    "    \n",
    "    # Parse the S3 URI\n",
    "    parsed_uri = urlparse(s3_output_uri)\n",
    "    bucket_name = parsed_uri.netloc\n",
    "    # Remove leading '/' from the path to get the prefix\n",
    "    prefix = parsed_uri.path.lstrip('/')\n",
    "    \n",
    "    # Create the evaluation_results directory if it doesn't exist\n",
    "    output_dir = 'evaluation_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # List objects in the S3 bucket with the given prefix\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "    \n",
    "    if 'Contents' not in response:\n",
    "        print(f\"No files found in {s3_output_uri}\")\n",
    "        return\n",
    "    \n",
    "    # Download only .jsonl files\n",
    "    jsonl_files_found = False\n",
    "    \n",
    "    for obj in response['Contents']:\n",
    "        # Get the file path relative to the prefix\n",
    "        key = obj['Key']\n",
    "        \n",
    "        # Only download .jsonl files\n",
    "        if not key.lower().endswith('.jsonl'):\n",
    "            continue\n",
    "            \n",
    "        jsonl_files_found = True\n",
    "        \n",
    "        # Determine the local file name by extracting just the filename\n",
    "        filename = os.path.basename(f\"eval_{dataset_name}.jsonl\")\n",
    "        local_file_path = os.path.join(output_dir, filename)\n",
    "        \n",
    "        print(f\"Downloading {key} to {local_file_path}\")\n",
    "        s3_client.download_file(bucket_name, key, local_file_path)\n",
    "    \n",
    "    if jsonl_files_found:\n",
    "        print(f\"Downloaded all .jsonl evaluation results to {output_dir}/\")\n",
    "    else:\n",
    "        print(f\"No .jsonl files found in {s3_output_uri}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Example usage:\n",
    "# output_path = download_evaluation_results(bedrock_client, eval['eval_job_arn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the following details about each of the evaluation jobs and build a list of dictionaries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_details_temp = [\n",
    "    {\n",
    "      \"model_name\": \"nova_lite_distilled\",\n",
    "      \"batch_inference_results_file\": \"./batch_inference_results/distilled_results.jsonl\", # local results.jsonl file from batch inferences\n",
    "      \"eval_dataset_s3\": f\"s3://{BUCKET_NAME}/{PREFIX}/evaluation_data.jsonl\",\n",
    "      \"eval_job_arn\": \"arn:aws:bedrock:us-east-1:<account_id>:evaluation-job/<job_id>\"\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "for eval in eval_details_temp:\n",
    "    print(eval['eval_job_arn'])\n",
    "    eval_results_response = bedrock_client.get_evaluation_job(\n",
    "    jobIdentifier=eval['eval_job_arn']\n",
    "    )\n",
    "    print(eval_results_response)\n",
    "    # s3_results_prefix = f\"{eval_results_response.get('outputDataConfig', {}).get('s3Uri')}/{eval_results_response['jobName']}/{eval['eval_job_arn'].split('/')[-1]}\"\n",
    "    # print(s3_results_prefix)\n",
    "    output_directory = download_evaluation_results(bedrock_client=bedrock_client, eval_job_arn=eval['eval_job_arn'])\n",
    "    eval_results.append(\n",
    "        {\n",
    "            **eval,\n",
    "            **{\n",
    "                \"output_directory\": output_directory\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "Now with our evaluations completed, we'll add one more metric using an LLM as a judge - citation coverage. \n",
    "\n",
    "**Citation coverage** is a measure of how well the response is supported by cited passages. The higher the score, the better the responses are supported by citations on average. Responses are graded on a 5-point likert scale.\n",
    "\n",
    "We will use this 5-point scale to measure the model's ability to answer a question given the passages its citing. Note that if the model has correctly stated it cannot answer the question given the passages, we will assign a 5 as that is the correct answer.\n",
    "\n",
    "We've built `eval_json_parser.py` to help with this. It uses Claude 3.5 Haiku as the judge, with a prompt from the Bedrock Evaluations documentation on evaluator prompts: https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-kb-haiku35.html. Feel free to use these to build further metrics into this process.\n",
    "\n",
    "This process uses cross-region inference and can take quite a while. Be sure to checkout the `time.sleep()` rate limiting and adjust accordingly. More robust rate limiting for production purposes would warrant some further development here.\n",
    "\n",
    "We'll iterate through the 500 records for each of our evaluation sets, add in our citation coverage metrics, then aggregate across all of our results four our final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_jsonl_parser import parse_jsonl_to_df, aggregate_metrics_by_model\n",
    "\n",
    "for e in eval_results:\n",
    "    df = parse_jsonl_to_df(e['output_dir'])\n",
    "    pkl_name = f\"{e['output_dir'].split('/')[0]}/{e['model_name']}_dataframe_pickle.pkl\"\n",
    "    print(f\"saving backup: {pkl_name}\")\n",
    "    df.to_pickle(pkl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_jsonl_parser import parse_jsonl_to_df, aggregate_metrics_by_model\n",
    "\n",
    "# eval_nova_distilled_results_df = parse_jsonl_to_df('evaluation_results/eval_nova_lite_distilled_dataset.jsonl')\n",
    "# eval_nova_premier_results_df = parse_jsonl_to_df('evaluation_results/eval_nova_premier_dataset.jsonl')\n",
    "# eval_nova_lite_results_df = parse_jsonl_to_df('evaluation_results/eval_nova_lite_dataset.jsonl')\n",
    "# eval_nova_micro_results_df = parse_jsonl_to_df('evaluation_results/eval_nova_micro_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional back up to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "# eval_nova_distilled_results_df.to_pickle('evaluation_results/eval_nova_lite_distilled_dataframe_pickle.pkl')\n",
    "# eval_nova_premier_results_df.to_pickle('evaluation_results/eval_nova_premier_dataframe_pickle.pkl')\n",
    "# eval_nova_lite_results_df.to_pickle('evaluation_results/eval_nova_lite_dataframe_pickle.pkl')\n",
    "# eval_nova_micro_results_df.to_pickle('evaluation_results/eval_nova_micro_dataframe_pickle.pkl')\n",
    "\n",
    "# Load DataFrame from pickle file\n",
    "# df = pd.read_pickle('backup_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional load from .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "aggregated_dfs = []\n",
    "\n",
    "# Convert string path to Path object if needed\n",
    "directory = Path('evaluation_results')\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for file_path in directory.iterdir():\n",
    "    # Check if the file has .pkl extension\n",
    "    if file_path.suffix.lower() == '.pkl':\n",
    "        try:\n",
    "            # Read the pickle file into a dataframe\n",
    "            df = pd.read_pickle(file_path)\n",
    "            # Aggregate the dataframe\n",
    "            aggregated_df = aggregate_metrics_by_model(df)\n",
    "            aggregated_dfs.append(aggregated_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "# Combine all aggregated dataframes into a single dataframe\n",
    "final_df = pd.concat(aggregated_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_identifier</th>\n",
       "      <th>metric_correctness</th>\n",
       "      <th>metric_completeness</th>\n",
       "      <th>metric_helpfulness</th>\n",
       "      <th>metric_logicalcoherence</th>\n",
       "      <th>metric_faithfulness</th>\n",
       "      <th>all_citations_valid</th>\n",
       "      <th>citation_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nova_micro</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.7943</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nova_premier</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.8415</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>0.9005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nova_lite_distilled</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.6005</td>\n",
       "      <td>0.5336</td>\n",
       "      <td>0.9709</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>0.542</td>\n",
       "      <td>2.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nova_lite</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.8030</td>\n",
       "      <td>0.7533</td>\n",
       "      <td>0.9006</td>\n",
       "      <td>0.8465</td>\n",
       "      <td>0.106</td>\n",
       "      <td>2.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model_identifier  metric_correctness  metric_completeness  \\\n",
       "0           nova_micro               0.838               0.8350   \n",
       "1         nova_premier               0.863               0.8415   \n",
       "2  nova_lite_distilled               0.932               0.6005   \n",
       "3            nova_lite               0.824               0.8030   \n",
       "\n",
       "   metric_helpfulness  metric_logicalcoherence  metric_faithfulness  \\\n",
       "0              0.7943                   0.9217               0.8785   \n",
       "1              0.7846                   0.9309               0.9005   \n",
       "2              0.5336                   0.9709               0.5641   \n",
       "3              0.7533                   0.9006               0.8465   \n",
       "\n",
       "   all_citations_valid  citation_coverage  \n",
       "0                0.020              2.466  \n",
       "1                0.008              2.340  \n",
       "2                0.542              2.728  \n",
       "3                0.106              2.302  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. **Structure Evaluation Data**: Format RAG system outputs for comprehensive evaluation using the BYOI specification\n",
    "2. **Configure Evaluation Jobs**: Set up secure IAM roles and configure evaluation parameters\n",
    "3. **Execute Evaluations**: Run parallel evaluations of multiple models using Amazon Bedrock\n",
    "4. **Analyze Results**: Interpret evaluation metrics to assess model performance\n",
    "\n",
    "This completes our four-notebook series on model distillation for citation-aware RAG systems. Through this series, we've covered:\n",
    "- Data preparation and formatting\n",
    "- Model distillation techniques\n",
    "- Batch inference implementation\n",
    "- Comprehensive model evaluation\n",
    "\n",
    "For more information, explore:\n",
    "- [Amazon Bedrock Evaluation Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html)\n",
    "- [RAG Best Practices Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/rag-best-practices.html)\n",
    "- [Advanced Model Evaluation Techniques](https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
