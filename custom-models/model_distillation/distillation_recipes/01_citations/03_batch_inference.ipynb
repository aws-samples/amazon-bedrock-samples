{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Batch Inference for Model Distillation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Design and implement efficient batch inference workflows for distilled models\n",
    "2. Configure and optimize batch inference jobs for maximum throughput\n",
    "3. Implement robust monitoring and error handling for batch processing\n",
    "4. Compare performance characteristics across model variants using batch inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Batch inference represents a critical deployment pattern for machine learning models, particularly in scenarios requiring high-throughput processing of large datasets. In the context of model distillation, batch inference serves two key purposes:\n",
    "\n",
    "1. **Performance Validation**: Enables systematic comparison of teacher, student, and distilled models across large test sets\n",
    "2. **Production Readiness**: Validates the distilled model's ability to handle production-scale workloads\n",
    "\n",
    "This notebook demonstrates advanced batch inference patterns using Amazon Bedrock, focusing on:\n",
    "\n",
    "- Optimizing batch sizes and concurrency for maximum throughput\n",
    "- Leveraging provisioned throughput endpoints for predictable performance\n",
    "- Implementing robust error handling and retry mechanisms\n",
    "- Gathering detailed performance metrics for model comparison\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The batch inference workflow implemented here follows a distributed processing architecture:\n",
    "\n",
    "```\n",
    "S3 Input Bucket → Bedrock Batch Processing → S3 Output Bucket\n",
    "                     ↓\n",
    "              Performance Metrics\n",
    "                     ↓\n",
    "             Evaluation Pipeline\n",
    "```\n",
    "\n",
    "This architecture enables:\n",
    "- Horizontal scaling for large datasets\n",
    "- Fault tolerance through automatic retries\n",
    "- Detailed performance monitoring\n",
    "- Cost optimization through batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "\n",
    "We'll configure our environment with the necessary dependencies and AWS client libraries. This setup assumes you have completed the previous notebooks and have a provisioned throughput endpoint available for your distilled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install boto3 --upgrade --quiet\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PT model id from previous notebook\n",
    "%store -r provisioned_model_id\n",
    "%store -r custom_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "from utils import create_s3_bucket\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\", region_name='us-east-1')\n",
    "\n",
    "# Create runtime client for inference\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "# Region and accountID\n",
    "session = boto3.session.Session(region_name='us-east-1')\n",
    "region = 'us-east-1'\n",
    "sts_client = session.client(service_name='sts', region_name='us-east-1')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# Define bucket and prefixes (using the same bucket as in distillation)\n",
    "BUCKET_NAME = '<BUCKET_NAME>' # Same bucket used in distillation notebook\n",
    "DATA_PREFIX = 'citations_distillation'  # Same prefix used in distillation notebook\n",
    "batch_inference_prefix = f\"{DATA_PREFIX}/batch_inference\"  # New prefix for batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload Batch Inference Data to S3\n",
    "\n",
    "The first step in our batch inference pipeline is preparing and uploading the test dataset. For optimal performance, consider these best practices:\n",
    "\n",
    "- **Data Format**: Use JSONL format for efficient streaming processing\n",
    "- **File Size**: Aim for files between 1-10GB for optimal throughput\n",
    "- **Compression**: Consider using GZIP compression for large datasets\n",
    "- **Data Validation**: Implement schema validation before upload\n",
    "\n",
    "The following code implements these practices while handling edge cases and errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local path to the batch inference data file\n",
    "batch_inference_file = 'batch_inf_data.jsonl'\n",
    "\n",
    "# Upload the batch inference data to S3\n",
    "def upload_batch_inference_data(bucket_name, file_name, prefix):\n",
    "    \"\"\"\n",
    "    Upload batch inference data to S3 bucket\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Check if bucket exists, if not create it\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} exists.\")\n",
    "    except ClientError:\n",
    "        print(f\"Creating bucket {bucket_name}...\")\n",
    "        create_s3_bucket(bucket_name=bucket_name)\n",
    "    \n",
    "    # Upload file to S3\n",
    "    s3_key = f\"{prefix}/{file_name}\"\n",
    "    s3_client.upload_file(file_name, bucket_name, s3_key)\n",
    "    print(f\"Uploaded {file_name} to s3://{bucket_name}/{s3_key}\")\n",
    "    \n",
    "    return f\"s3://{bucket_name}/{s3_key}\"\n",
    "\n",
    "# Upload batch inference data to S3\n",
    "batch_inference_s3_uri = upload_batch_inference_data(BUCKET_NAME, batch_inference_file, batch_inference_prefix)\n",
    "print(f\"Batch inference data uploaded to: {batch_inference_s3_uri}\")\n",
    "\n",
    "# Define the output location for batch inference results\n",
    "batch_inference_output_prefix = f\"{batch_inference_prefix}/outputs\"\n",
    "batch_inference_output_uri = f\"s3://{BUCKET_NAME}/{batch_inference_output_prefix}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit Batch Inference Jobs\n",
    "\n",
    "When submitting batch inference jobs, several key configuration parameters affect performance and reliability:\n",
    "\n",
    "1. **Concurrency Configuration**\n",
    "   - MaxConcurrentInvocations: Controls parallel processing\n",
    "   - BatchSize: Number of records per batch\n",
    "   - TimeoutInSeconds: Maximum processing time per batch\n",
    "\n",
    "2. **Resource Optimization**\n",
    "   - Memory allocation\n",
    "   - CPU/GPU utilization\n",
    "   - Network bandwidth\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Retry strategies\n",
    "   - Dead letter queues\n",
    "   - Error logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first run batch inference on our provisioned throughput endpoint using a script that simulates the results exactly as bedrock inference. `batch_inference_simulator.py` will take the same data format as input as a normal batch inference job would. It also outputs the same format. Note that this designed to work specifically for Nova models. Feel free to use this to speed up this process, or enjoy a reduce cost per inference using batch inference.\n",
    "\n",
    "\n",
    "We'll then compare results with other model variants.\n",
    "\n",
    "Once your distilled model batch inferences are complete, be sure to delete the provisioned throughput endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 batch_inference_simulator.py --input batch_inf_data.jsonl --output batch_inference_results/distilled_results.jsonl  --model \"arn:aws:bedrock:us-east-1:<account_id>:provisioned-model/pt_endpoint_id\" # lite\n",
    "# !python3 batch_inference_simulator.py --input batch_inf_data.jsonl --output batch_inference_results/nova_micro_results.jsonl  --model \"us.amazon.micro-v1:0\" # micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete PT Endpoint\n",
    "\n",
    "Proper cleanup of resources is essential for cost management. Use the following code to remove created resources when they're no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete provisioned throughput:\n",
    "response = bedrock_client.delete_provisioned_model_throughput(provisionedModelId=provisioned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll submit batch inference jobs for our out-of-the-box models.\n",
    "You'll need to create a batch inference service role before moving forward: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-iam-sr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inf_role_arn=f\"arn:aws:iam::{account_id}:role/AmazonNovaBedrockBatchServiceRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of models to use for batch inference\n",
    "# We'll include the teacher model, student model, and our distilled model (provisioned throughput)\n",
    "models = [\n",
    "    \"us.amazon.nova-premier-v1:0\",  # Teacher model (Nova Premier)\n",
    "    \"amazon.nova-lite-v1:0\",   # Student model (Nova Lite)\n",
    "    \"amazon.nova-micro-v1:0\", \n",
    "]\n",
    "\n",
    "# Function to submit a batch inference job\n",
    "def submit_batch_inference_job(model_id, input_s3_uri, output_s3_uri):\n",
    "    \"\"\"\n",
    "    Submit a batch inference job for the specified model\n",
    "    \"\"\"\n",
    "    # Generate a unique job name\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    model_short_name = model_id.split('/')[-1].split(':')[0]\n",
    "    job_name = f\"distillation-bench-{model_short_name}-{timestamp}\"\n",
    "    \n",
    "    # Create the batch inference job\n",
    "    response = bedrock_client.create_model_invocation_job(\n",
    "        jobName=job_name,\n",
    "        modelId=model_id,\n",
    "        inputDataConfig={\n",
    "            \"s3InputDataConfig\": {\n",
    "                \"s3Uri\": input_s3_uri,\n",
    "                \"s3InputFormat\": \"JSONL\"\n",
    "            }\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f\"{output_s3_uri}{model_short_name}/\"\n",
    "            }\n",
    "        },\n",
    "        roleArn=batch_inf_role_arn\n",
    "    )\n",
    "    \n",
    "    job_id = response['jobArn']\n",
    "    print(f\"Submitted batch inference job for model {model_id}\")\n",
    "    print(f\"Job ARN: {job_id}\")\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "# Submit batch inference jobs for each model\n",
    "job_ids = []\n",
    "for model in models:\n",
    "    job_id = submit_batch_inference_job(model, batch_inference_s3_uri, batch_inference_output_uri)\n",
    "    job_ids.append(job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor Batch Inference Jobs\n",
    "🕐 Its important to remember that batch inference jobs can take many hours to complete, in exchange for a reduction in inference pricing. It will likely be 12-24 hours to complete, so come back to this notebook once those batch inference jobs have completed. Alternatively, you can run the above batch simulator using Nova on-demand inferencing to speed this process up at on-demand pricing.\n",
    "\n",
    "Let's check the status of our jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the status of a batch inference job\n",
    "def check_job_status(job_id):\n",
    "    \"\"\"\n",
    "    Check the status of a batch inference job\n",
    "    \"\"\"\n",
    "    response = bedrock_client.get_model_invocation_job(jobIdentifier=job_id)\n",
    "    status = response['status']\n",
    "    model_id = response['modelId']\n",
    "    \n",
    "    print(f\"Model: {model_id}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status == 'COMPLETED':\n",
    "        print(f\"Output location: {response['outputDataConfig']['s3OutputDataConfig']['s3Uri']}\")\n",
    "    elif status == 'FAILED':\n",
    "        print(f\"Failure reason: {response.get('failureMessage', 'Unknown')}\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "# Check the status of all batch inference jobs\n",
    "for job_id in job_ids:\n",
    "    status = check_job_status(job_id)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieve and Prepare Results for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Results Downloader\n",
    "Function: download_batch_results\n",
    "\n",
    "This function retrieves output files from Amazon Bedrock batch inference jobs stored in S3.\n",
    "\n",
    "**What it does:**\n",
    "* Retrieves job details from Amazon Bedrock to locate the S3 output location\n",
    "* Lists all objects in the output directory of the specified batch job\n",
    "* Filters and downloads only result files (ending in .out, excluding manifest files)\n",
    "* Removes the .out extension from downloaded files\n",
    "* Handles errors appropriately during the download process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download outputs from jobs\n",
    "from urllib.parse import urlparse\n",
    "def download_batch_results(job_id, target_directory=None):\n",
    "    \"\"\"\n",
    "    Download batch job results from S3 to a local directory.\n",
    "    \n",
    "    Args:\n",
    "        job_id (str): The identifier for the Bedrock model invocation job\n",
    "        target_directory (str, optional): Directory to save the downloaded file.\n",
    "                                          Defaults to current directory if None.\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to the downloaded files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get job details to find the S3 output location\n",
    "        response = bedrock_client.get_model_invocation_job(jobIdentifier=job_id)\n",
    "        output_s3_prefix = response['outputDataConfig']['s3OutputDataConfig']['s3Uri']\n",
    "        job_prefix = job_id.split('/')[-1]\n",
    "        model_id = response['modelId'].split('/')[-1].replace('.','-').replace(':', '-')\n",
    "        output_location = f\"{output_s3_prefix}{job_prefix}/\"\n",
    "        \n",
    "        # Parse the S3 URI\n",
    "        parsed_uri = urlparse(output_location)\n",
    "        bucket_name = parsed_uri.netloc\n",
    "        s3_prefix = parsed_uri.path.lstrip('/')\n",
    "        \n",
    "        # Set target directory if not provided\n",
    "        if target_directory is None:\n",
    "            target_directory = os.getcwd()\n",
    "        \n",
    "        # Create target directory if it doesn't exist\n",
    "        os.makedirs(target_directory, exist_ok=True)\n",
    "        \n",
    "        # List objects in the output location\n",
    "        s3_client = boto3.client('s3')\n",
    "        downloaded_files = []\n",
    "        \n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix)\n",
    "        \n",
    "        for page in pages:\n",
    "            if 'Contents' not in page:\n",
    "                continue\n",
    "                \n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                \n",
    "                # Download only files ending with .out and not manifest.json.out\n",
    "                if key.endswith('jsonl.out') and not key.endswith('manifest.json.out'):\n",
    "                    # Create the output filename by removing .out extension\n",
    "                    filename = os.path.basename(key)\n",
    "                    output_filename = f\"{model_id}-{filename[:-4]}\" if filename.endswith('.out') else filename\n",
    "                    local_file_path = os.path.join(target_directory, output_filename)\n",
    "                    \n",
    "                    print(f\"Downloading {key} to {local_file_path}\")\n",
    "                    s3_client.download_file(bucket_name, key, local_file_path)\n",
    "                    downloaded_files.append(local_file_path)\n",
    "        \n",
    "        return downloaded_files\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"Error downloading batch results: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in job_ids:\n",
    "    download_batch_results(job_id=job, target_directory=\"batch_inference_results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've walked through how to submit batch inference jobs. The results from these jobs will be what's used to evaluate our distilled model's performance.\n",
    "You should see the batch inference results under the `evaluation_results` directory.\n",
    "\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Proceed to [04_evaluate.ipynb](04_evaluate.ipynb) to:\n",
    "1. Analyze batch inference results across multiple dimensions\n",
    "2. Compare performance metrics between model variants\n",
    "3. Evaluate the success of the distillation process\n",
    "4. Make data-driven decisions about production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilling_for_citations-ex_cldZ-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
