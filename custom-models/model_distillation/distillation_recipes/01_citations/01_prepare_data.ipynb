{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3adadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade boto3 \n",
    "# %pip install --upgrade pip --quiet\n",
    "# %pip install boto3 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdea093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d3606",
   "metadata": {},
   "source": [
    "# Model Distillation for Question Answering with Cited Text\n",
    "\n",
    "This notebook is part of a series demonstrating advanced model distillation techniques for creating specialized, citation-aware question-answering models. The goal is to distill the knowledge from a large language model (Amazon Nova Premier) into a smaller, more efficient model while maintaining high-quality citation capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Prepare training data for citation model distillation\n",
    "- Design structured XML output formats for consistent answer generation\n",
    "- Implement source citation tracking in model responses\n",
    "- Create evaluation datasets for measuring citation accuracy\n",
    "\n",
    "## Dataset: SQuAD v2.0\n",
    "We use the [Stanford Question Answering Dataset (SQuAD) v2.0](https://rajpurkar.github.io/SQuAD-explorer/) as our base dataset. SQuAD v2.0 is particularly suitable for citation-aware model training because:\n",
    "\n",
    "1. Contains explicit answer spans within source text\n",
    "2. Includes \"impossible\" questions to test model reliability\n",
    "3. Provides diverse question types and domains\n",
    "4. Enables source attribution tracking\n",
    "\n",
    "The dataset is loaded using the [Hugging Face Datasets library](https://huggingface.co/docs/datasets/) and stored in Parquet format for optimal performance with large-scale training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ab6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patreil/.local/share/virtualenvs/01_citations-soRsQGlz/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "skip_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(skip_dir)\n",
    "from utils import read_jsonl_to_dataframe\n",
    "\n",
    "splits = {'train': 'squad_v2/train-00000-of-00001.parquet', 'validation': 'squad_v2/validation-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"train\"])\n",
    "df_eval = pd.read_parquet(\"hf://datasets/rajpurkar/squad_v2/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee8fe9",
   "metadata": {},
   "source": [
    "## Advanced System Prompt Engineering\n",
    "\n",
    "This section implements a specialized system prompt for [Amazon Nova Foundation models](https://docs.aws.amazon.com/nova/latest/userguide/prompting.html). The prompt engineering focuses on:\n",
    "\n",
    "1. **Context-Bounded Responses**: Answers must be derived solely from provided context\n",
    "2. **Source Attribution**: Mandatory citation of source text for verification\n",
    "3. **Structured Output**: XML-based response format for consistent parsing\n",
    "4. **Edge Case Handling**: Explicit handling of unanswerable questions\n",
    "\n",
    "### System Prompt XML Schema\n",
    "The system prompt leverages the following formatting to support N answers with N sources. In the following cells we'll build helper functions to parse these out to measure citation accuracy. This style of prompting is specific to Amazon Nova and will provide the best accuracy for citations use cases.\n",
    "- **Atomic Answer Components**: Discrete answer parts with individual citations\n",
    "- **Source Traceability**: Direct mapping between answers and source text\n",
    "- **Validation Support**: Schema-based response validation\n",
    "- **Extensibility**: Future addition of metadata and confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277ad88d",
   "metadata": {},
   "source": [
    "```xml\n",
    "<question>Who ruled the duchy of Normandy?</question>\n",
    "<answer>\n",
    "<answer_part>\n",
    "<text>Richard I</text>\n",
    "<sources>\n",
    "<source>The Duchy of Normandy, which they formed by treaty with the French crown, was a great fief of medieval France, and under Richard I of Normandy was forged into a cohesive and formidable principality in feudal tenure.</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "</answer>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set nova prompt for citations\n",
    "system_prompt = \"\"\"\n",
    "You are a question answering assistant. I will provide you with document context. The user will provide you with a question. Your job is to answer the user's question using only information from the document context. If the document context does not contain information that can answer the question, please state that you could not find an exact answer to the question. Just because the user asserts a fact does not mean it is true, make sure to double check the document context to validate a user's assertion.\n",
    "\n",
    "However, you should include <sources> tags at the end of each <answer_part> to specify which source(s) the information came from.\n",
    "Note that <sources> may contain multiple <source> if you include information from multiple results in your answer.\n",
    "\n",
    "Do NOT directly quote the <context> in your answer. Your job is to answer the user's question as concisely as possible.\n",
    "\n",
    "You must output your answer in the following format. Pay attention and follow the formatting and spacing exactly:\n",
    "<answer>\n",
    "<answer_part>\n",
    "<text>\n",
    "first answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "<answer_part>\n",
    "<text>\n",
    "second answer text\n",
    "</text>\n",
    "<sources>\n",
    "<source>source sentence</source>\n",
    "</sources>\n",
    "</answer_part>\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3c74",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline Implementation\n",
    "\n",
    "The evaluation of distilled models using Bedrock native tools requires 3 different datasets, all using our squad dataset.\n",
    "1. **Distillation dataset.** This dataset will be used to fine-tune Nova Lite. Here we're using a 10% mix-in, so 10% of the records will include ground-truth answers and the rest will not. For more information on these best practices please visit [our documentation on this topic](https://docs.aws.amazon.com/nova/latest/userguide/custom-distill-prepare.html)\n",
    "2. **Batch Inference dataset.** This will be the hold out of records we'll use for evaluating each model's performance. We'll use this dataset in Batch Inference to get each model's responses.\n",
    "3. **Labeled dataset.** Using the same records from our batch inference dataset, we'll create a labeled dataset that includes the correct answers. We'll use this in our Evaluation job to measure each model's response to the ground-truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_structure(answers_dict):\n",
    "    \"\"\"\n",
    "    Parse different formats of answer dictionaries and extract text and start positions.\n",
    "    Returns lists of texts and start positions.\n",
    "    \"\"\"\n",
    "    # Case 1: NumPy arrays with direct keys\n",
    "    if 'text' in answers_dict and isinstance(answers_dict['text'], np.ndarray):\n",
    "        texts = answers_dict['text'].tolist()\n",
    "        starts = answers_dict['answer_start'].tolist()\n",
    "        \n",
    "    # Case 2: Lists or single values with direct keys\n",
    "    elif 'text' in answers_dict:\n",
    "        texts = answers_dict['text'] if isinstance(answers_dict['text'], list) else [answers_dict['text']]\n",
    "        starts = answers_dict['answer_start'] if isinstance(answers_dict['answer_start'], list) else [answers_dict['answer_start']]\n",
    "        \n",
    "    # Case 4: String JSON that needs parsing (handled in calling function)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown answer format: {answers_dict}\")\n",
    "        \n",
    "    return texts, starts\n",
    "\n",
    "def create_xml_answer(row, no_answer_text='I could not find an exact answer to the question.'):\n",
    "    \"\"\"\n",
    "    takes a pandas df row and parses the 'answers' column XML answers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle answers as string (JSON) if needed\n",
    "        answers_dict = row['answers']\n",
    "        if isinstance(answers_dict, str):\n",
    "            import json\n",
    "            answers_dict = json.loads(answers_dict)\n",
    "            \n",
    "        # Parse answer structure using our helper function\n",
    "        texts, starts = parse_answer_structure(answers_dict)\n",
    "        context = row['context']\n",
    "        \n",
    "        # Split context into sentences more accurately\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', context)\n",
    "        \n",
    "        # Build XML structure\n",
    "        xml_parts = ['<answer>']\n",
    "        \n",
    "        if len(texts) > 0:\n",
    "            for i, (text, start) in enumerate(zip(texts, starts)):\n",
    "                xml_parts.append('<answer_part>')\n",
    "                xml_parts.append('<text>')\n",
    "                xml_parts.append(str(text))\n",
    "                xml_parts.append('</text>')\n",
    "                xml_parts.append('<sources>')\n",
    "                \n",
    "                # Find the sentence containing the answer based on the start position\n",
    "                char_count = 0\n",
    "                source_sentence = \"No relevant source found\"\n",
    "                for sentence in sentences:\n",
    "                    sentence_len = len(sentence) + 1  # +1 for the space after sentence\n",
    "                    if char_count <= int(start) < (char_count + sentence_len):\n",
    "                        source_sentence = sentence.strip()\n",
    "                        break\n",
    "                    char_count += sentence_len\n",
    "                \n",
    "                xml_parts.append(f'<source>{source_sentence}</source>')\n",
    "                xml_parts.append('</sources>')\n",
    "                xml_parts.append('</answer_part>')\n",
    "        \n",
    "            xml_parts.append('</answer>')\n",
    "        else: # use no answer text\n",
    "            xml_parts.append(f\"<answer_part>\\n<text>\\n{no_answer_text}\\n</text>\\n</answer_part></answer>\")\n",
    "        return '\\n'.join(xml_parts)\n",
    "    except Exception as e:\n",
    "        return f\"<answer>\\n<error>Error generating XML: {str(e)}</error>\\n</answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_payload(row, model_type=\"conversation\", system_prompt=None, include_answer=False, additional_params=None):\n",
    "    \"\"\"\n",
    "    Create a payload dictionary for Amazon Bedrock batch inference API requests.\n",
    "    Batch inference uses the invoke api.\n",
    "    \n",
    "    Args:\n",
    "        row: A row from the pandas DataFrame containing context, question, and optionally answers\n",
    "        model_type: The type of model payload to create (\"conversation\" or \"invoke\")\n",
    "        system_prompt: The system message to include (for conversation-based models)\n",
    "        include_answer: Whether to include the answer in the conversation (for evaluation)\n",
    "        additional_params: Dictionary of additional parameters to include in the payload\n",
    "    \n",
    "    Returns:\n",
    "        dict: A formatted payload dictionary ready for Bedrock batch inference API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract needed information\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Create the user prompt with context and question\n",
    "        user_prompt = f\"\"\"<context>{context}</context> <question>{question}</question>\"\"\"\n",
    "        \n",
    "        # Get the answer if needed\n",
    "        assistant_response = create_xml_answer(row) if include_answer else None\n",
    "        \n",
    "        # Create appropriate payload based on model_type\n",
    "        if model_type == \"conversation\":\n",
    "            \n",
    "            payload = {\n",
    "                \"schemaVersion\": \"bedrock-conversation-2024\",\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Add assistant response if needed (for evaluation)\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "                \n",
    "        elif model_type == \"invoke\":\n",
    "            # For basic invoke request (non-conversation models like Titan, etc.)\n",
    "            payload = {\n",
    "                \"system\": [{\"text\": system_prompt}] if system_prompt else [],\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"text\": user_prompt}]\n",
    "                    }\n",
    "                ],\n",
    "                \"inferenceConfig\":{ \n",
    "                    # \"maxTokens\": int, \n",
    "                    \"temperature\": .1, \n",
    "                    \"topP\": .9, \n",
    "                    \"topK\": 50, \n",
    "                    \"stopSequences\": ['</answer>']\n",
    "                }\n",
    "            }\n",
    "            if include_answer and assistant_response:\n",
    "                payload[\"messages\"].append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [{\"text\": assistant_response}]\n",
    "                })\n",
    "            \n",
    "            # Add optional parameters specific to invoke requests\n",
    "            if additional_params:\n",
    "                payload.update(additional_params)\n",
    "                \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "            \n",
    "        # Add any additional parameters passed\n",
    "        if additional_params and model_type == \"conversation\":\n",
    "            # For conversation models, additional params might need to be added at the root level\n",
    "            for key, value in additional_params.items():\n",
    "                if key not in payload:\n",
    "                    payload[key] = value\n",
    "                    \n",
    "        return payload\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating payload for row: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_inf_record(row, system_prompt, include_answer=False):\n",
    "    \"\"\"\n",
    "    Takes a pandas df row and creates a jsonl record for batch inference\n",
    "    \"\"\"\n",
    "    conversation = create_bedrock_payload(\n",
    "                                row=row, \n",
    "                                system_prompt=system_prompt, \n",
    "                                model_type=\"invoke\", \n",
    "                                additional_params={},\n",
    "                                include_answer=include_answer)\n",
    "    return {\n",
    "        \"recordId\": row['id'],\n",
    "        \"modelInput\": conversation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64f297",
   "metadata": {},
   "source": [
    "### Including non-answers\n",
    "Any citations use case will need to support scenarios where a correct answer is not possible given the passages available.\n",
    "To this end, we'll make half of our training dataset include non-answers, and half will include examples with answers.\n",
    "Bedrock distillation jobs can have a maximum of 15,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create a new column\n",
    "# Filter for empty answers\n",
    "empty_answers_df = df_train[df_train['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "with_answers_df = df_train[df_train['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "# take 7500 of each dataframe and combine to use in distillation. \n",
    "df_train_revised = pd.concat([\n",
    "    empty_answers_df.sample(n=7500, random_state=42), \n",
    "    with_answers_df.sample(n=7500, random_state=42)], ignore_index=True) # max 15k for bedrock distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416c80",
   "metadata": {},
   "source": [
    "As stated earlier, it is a best practice to include a ground truth answer for ~10% of the total training set. We will take a random sample of 10% and use our `create_bedrock_payload` with include_anwer set to True. The remaining 90% we leave out the ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bf45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = len(df_train_revised)\n",
    "ground_truth_included = 0.1 * row_count\n",
    "\n",
    "# here we'll take 10% of our training data set and add the answers\n",
    "training_data_with_gt_df = df_train_revised.sample(n=int(ground_truth_included), random_state=17)\n",
    "\n",
    "# next we'll drop our ground truth examples so as not to mix with our labels excluding answers.\n",
    "training_data_without_gt_df = df_train_revised.drop(training_data_with_gt_df.index)\n",
    "\n",
    "# next we'll build our training data with ground truth\n",
    "training_data_with_gt_df['conversation'] = training_data_with_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=True), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we'll build our training data without ground truth\n",
    "training_data_without_gt_df['conversation'] = training_data_without_gt_df.apply(lambda row: create_bedrock_payload(row=row, model_type=\"conversation\", system_prompt=system_prompt, include_answer=False), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll concatenate the dataframes\n",
    "final_training_dataset = pd.concat([training_data_with_gt_df, training_data_without_gt_df], axis=0, ignore_index=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll output to .jsonl to use in distillation job\n",
    "final_training_dataset['conversation'].to_json('distillation_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4b86c",
   "metadata": {},
   "source": [
    "## Batch Inference Dataset Creation\n",
    "\n",
    "Now that our distillation data set is created, we'll move on to creating our batch inference dataset.\n",
    "Because we'll also be using the same dataset (with labeled answers) for our evaluation jobs, Bedrock Evaluations will only handle a maxium of 1000 records.\n",
    "We'll use 500 total rows our data set, as this is a sufficient number for evaluation and the right balance between processing time and proper evaluation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_empty_answers_df = df_eval[df_eval['answers'].apply(lambda x: \n",
    "    len(x['text']) == 0 and len(x['answer_start']) == 0)]\n",
    "\n",
    "# Filter for rows with actual answers\n",
    "eval_with_answers_df = df_eval[df_eval['answers'].apply(lambda x: len(x['text']) > 0)]\n",
    "\n",
    "batch_inf_df = pd.concat([\n",
    "    eval_empty_answers_df.sample(n=250, random_state=15), \n",
    "    eval_with_answers_df.sample(n=250, random_state=15)], ignore_index=True)\n",
    "\n",
    "\n",
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt), axis=1).to_json('batch_inf_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd681e",
   "metadata": {},
   "source": [
    "## Labeled Dataset for BYOI Bedrock Evaluation\n",
    "This section creates a labeled dataset by applying our `create_batch_inf_record` method on each row and setting `include_answer` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d625d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inf_df.apply(lambda row: create_batch_inf_record(row, system_prompt=system_prompt, include_answer=True), axis=1).to_json('labeled_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba4edf",
   "metadata": {},
   "source": [
    "### Datasets Created\n",
    "By now you should see 3 `.jsonl` files:\n",
    "1. distillation_data.jsonl. This is the dataset we'll use for distillation.\n",
    "2. batch_inf_data.jsonl. This is the dataset we'll use to run inference on all of our models, including our distilled one.\n",
    "3. labeled_data.jsonl. This is the dataset we'll use to evaluate each model's performance against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4faab9",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "Proceed to [02_distill.ipynb](02_distill.ipynb) to:\n",
    "1. Submit a distillation job using our distillation dataset\n",
    "2. Create a provisioned throughput endpoint to hose our distilled model.\n",
    "\n",
    "You can now move on to `02_distill.ipynb` where we'll kick of our distillation job!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e8fcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01_citations-soRsQGlz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
