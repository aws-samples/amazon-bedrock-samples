{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4065854f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Mistral LLM and importing into Bedrock: A Step-by-Step Instructional Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60efa3-87fa-47f7-a132-e5a4a6589149",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "In this notebook we will walk through how to fine-tune a Mistral LLM for Question Answering on Amazon SageMaker using PyTorch FSDP and Flash Attention 2 including Q-LORA and PEFT. This notebook also explains using PEFT and merging the adapters. This fine tuned model will then be imported into Amazon Bedrock Custom Model Import (CMI). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29aefcd",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via [Custom Model Import (CMI)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html). \n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef35827-d334-460d-b7de-fdae61e9c3be",
   "metadata": {},
   "source": [
    "## Use case Details\n",
    "\n",
    "We will quantize the model as bf16 model. We use [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer) (SFT) for fine tuning the model. We will use [Open-Orca/OpenOrca] (https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset for fine tuning the model. This is a reading comprehension dataset containing over 650K question-answer-evidence triples. \n",
    "\n",
    "Using [FSDP](https://pytorch.org/docs/main/fsdp.html) and [Q-Lora](https://arxiv.org/abs/2305.14314) allows us to fine tune Mistral models on 2x consumer GPU's. FSDP enables sharding model parameters, optimizer states and gradients across data parallel workers. Q- LORA helps reduce the memmory usage for finetuning LLM while preserving full 16-bit task performance. For fine tuning in this notebook we use ml.g5.12xlarge as a SageMaker Training Job. \n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker) provides a fully managed service that enables build, train and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more â€“ all in one integrated development environment (IDE). [SageMaker Model Training](https://aws.amazon.com/sagemaker/train/) reduces the time and cost to train and tune machine learning (ML) models at scale without the need to manage infrastructure.\n",
    "\n",
    "For detailed instructions please refer to [Importing a model with customer model import Bedrock Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).\n",
    "\n",
    "This notebook is inspired by Philipp Schmid Blog - https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d54b4",
   "metadata": {},
   "source": [
    "## Model License information\n",
    "\n",
    "In this notebook we use the Mistral-7B-v0.3 model from HuggingFace repository. This model is a gated model within HuggingFace repository. Mistral released this model under the Apache 2.0 license (https://mistral.ai/news/announcing-mistral-7b/). To use the model from Huggingface as this model is a gated model you have to request access to the model before it using in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff11166",
   "metadata": {},
   "source": [
    "## Pre-Requisites \n",
    "\n",
    "You will require an AWS account and access to Amazon Sagemaker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fd290",
   "metadata": {},
   "source": [
    "## Code with comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea6bf6-1868-40f8-afa6-11def2965fe8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7cace-b9c3-4e25-8ccc-948049056f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 uninstall autogluon autogluon-multimodal -y\n",
    "!pip3 install transformers \"sagemaker>=2.190.0\" \"huggingface_hub\" \"datasets[s3]==2.18.0\" --upgrade --quiet\n",
    "!pip3 install boto3 s3fs \"aiobotocore==2.11.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691eb1f7",
   "metadata": {},
   "source": [
    "Logging into the HuggingFace Hub and requesting access to the mistralai/Mistral-7B-v0.3 is required to download the model and finetune the same. Please follow the [HuggingFace User Token Documentation](https://huggingface.co/docs/hub/en/security-tokens) to request tokens to be provided in the textbox appearning below after you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcad8fe-0d3d-475f-ba3c-927bec3502b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49071e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We will initialize the SageMaker Session required to finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedc4e2-8c8d-4f8f-94cb-2bedaf3664f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bca72",
   "metadata": {},
   "source": [
    "### Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba6a16-bc0f-410a-8d50-6d307743dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.3\"\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/trivia_qa'\n",
    "use_bf16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76231598",
   "metadata": {},
   "source": [
    "### Dataset Prepare\n",
    "We will use [Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset to finetune the Mistral 7B model. Kindly refer to the [Licensing Information](https://huggingface.co/datasets/Open-Orca/OpenOrca/tree/main#licensing-information) regarding this dataset before proceeding further.\n",
    "\n",
    "We will transform the messages to OAI format and split the data into Train and Test set. The Train and Test dataset will be uploaded into S3 - SageMaker Session Bucket for use during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ad611-7cbe-42da-adbd-0c1a177af2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def create_conversation(row):\n",
    "    row[\"messages\"] = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": row[\"system_prompt\"],\n",
    "            },        \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": row[\"response\"]\n",
    "            },\n",
    "    ]\n",
    "    return row\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "flan_dataset = dataset.filter(lambda example, indice: \"flan\" in example[\"id\"], with_indices=True)\n",
    "flan_dataset = flan_dataset[\"train\"].train_test_split(test_size=0.01, train_size=0.035)\n",
    "\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "flan_dataset = flan_dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "# save datasets to s3\n",
    "flan_dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "flan_dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\", force_ascii=False)\n",
    " \n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d49b2-d4a2-4e7f-bde7-4e38589a0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Row Count - {len(flan_dataset[\"train\"])}')\n",
    "print(f'Validation/Test Row Count - {len(flan_dataset[\"test\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37c15d",
   "metadata": {},
   "source": [
    "### Training script and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda805f",
   "metadata": {},
   "source": [
    "Create the scripts directory to hold the training script and dependencies list. This directory will be provided to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d84d5-4a2e-4a1b-8312-7e1acc6de17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"scripts/trl/mistral-qna\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e37c2c",
   "metadata": {},
   "source": [
    "Create the requirements file that will be used by the SageMaker Job container to initialize the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09188dd4-0486-4f22-943d-fbc43f73bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/mistral-qna/requirements.txt\n",
    "torch==2.2.2\n",
    "transformers==4.40.2\n",
    "sagemaker>=2.190.0\n",
    "datasets==2.18.0\n",
    "accelerate==0.29.3\n",
    "evaluate==0.4.1\n",
    "bitsandbytes==0.43.1\n",
    "trl==0.8.6\n",
    "peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e55e8",
   "metadata": {},
   "source": [
    "Training Script that will use PyTorch FSDP, QLORA, PEFT and train the model using SFT Trainer. This script also includes prepping the data to Mistral 7B chat template (Anthropic/Vicuna format). This training script is being written to the scripts folder along with the requirements file that will be used by the SageMaker Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f22ff-d331-4e95-8048-edcee35173e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/mistral-qna/run_fsdp_qlora.py\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.system(\"pip install flash-attn --no-build-isolation --upgrade\")\n",
    "except:\n",
    "    print(\"flash-attn failed to install\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    BitsAndBytesConfig,\n",
    "        set_seed,\n",
    "\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "from trl import (SFTTrainer)\n",
    "\n",
    "MISTRAL_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "    use_qlora: bool = field(default=False, metadata={\"help\": \"Whether to use QLORA\"})\n",
    "    merge_adapters: bool = field(\n",
    "        metadata={\"help\": \"Wether to merge weights for LoRA.\"},\n",
    "        default=False,\n",
    "    )  \n",
    "\n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "\n",
    "    # Tokenizer        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n",
    "    \n",
    "    # template dataset\n",
    "    def template_dataset(examples):\n",
    "        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "\n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "    if script_args.use_qlora:\n",
    "        print(f\"Using QLoRA - {torch_dtype}\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch_dtype,\n",
    "                bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "            )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={'':torch.cuda.current_device()},\n",
    "        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "\n",
    "    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False,  # No need to add additional separator token\n",
    "        },\n",
    "    )\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    ##########################\n",
    "    # Train model\n",
    "    ##########################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    sagemaker_save_dir = \"/opt/ml/model\"\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "    if script_args.merge_adapters:\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        print('########## Merging Adapters  ##########')\n",
    "        trainer.model.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(sagemaker_save_dir)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # list file in output_dir\n",
    "        print(os.listdir(training_args.output_dir))\n",
    "\n",
    "        # load PEFT model in fp16\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_into_dataclasses()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    training_function(script_args, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad87b10",
   "metadata": {},
   "source": [
    "Hyperparameters, which are passed into the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a30a83-1345-404c-ae18-2b6a4a7f6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id': model_id,                              # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 1,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 1,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"epoch\",\n",
    "  'learning_rate': 0.0002,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                  # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129856c",
   "metadata": {},
   "source": [
    "Use the SageMaker HuggingFace Estimator to finetune the model passing in the hyperparameters and the scripts directory from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d023ca9-0e42-4469-8ee1-87e46c0f39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'{model_id.replace(\".\", \"-\").replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }'\n",
    " \n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/trl/mistral-qna/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08551c6f",
   "metadata": {},
   "source": [
    "### Print the Model location to be used in Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6e1c3-4a4b-411c-9358-cd37576a6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998f1f3",
   "metadata": {},
   "source": [
    "### Import the Finetuned model into Bedrock:\n",
    "\n",
    "#### Now follow the steps from the link below to continue to import this model\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447bd05",
   "metadata": {},
   "source": [
    "![Model Import Screen](./images/Mistral-ft-ImportScreenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a693fd",
   "metadata": {},
   "source": [
    "![Model Import Job Details Screen](./images/MistralImportJobDetailsScreenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cce41",
   "metadata": {},
   "source": [
    "![Model Import Job Details Screen](./images/MistralModelListScreenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55c4c5",
   "metadata": {},
   "source": [
    "![SageMaker Finetuning Log](./images/MistralModelDetailsScreenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f9eef",
   "metadata": {},
   "source": [
    "![Model Import Job Details Screen](./images/Mistral_qna_finetuned.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ea44a",
   "metadata": {},
   "source": [
    "![SageMaker Finetuning Log](./images/Mistral-ft-SageMakerTrainingLog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2cd645",
   "metadata": {},
   "source": [
    "\n",
    "### Invoke the imported model using Bedrock API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 botocore --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc64793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8802ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "model_id = \"<<replace with the imported bedrock model arn>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_invoke_model_and_print(native_request):\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = client.invoke_model(modelId=model_id, body=request)\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "        response_text = model_response[\"outputs\"][0][\"text\"]\n",
    "        print(response_text)\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6319fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
    "\n",
    "Is the sentiment of the following sentence positive or negative (see options at the end)? is a tribute not only to his craft , but to his legend * negative. * positive.\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]</s>\"\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.91\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an AI assistant. You will be given a task. You must generate a detailed and long answer. \n",
    "\n",
    "Read the text and determine if the sentence is true: Then the most important part of the day begins as Muslims go to the mosque, their special place of worship. Sentence: Muslims pray at the mosque. OPTIONS: (1). yes; (2). no; \n",
    "\n",
    "A:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]</s>\"\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.6\n",
    "}\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e619378",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
    "\n",
    "Bildirgenin kabul edildiÄŸini 12 AÄŸustos SalÄ± gÃ¼nÃ¼ aciklayan hÃ¼kÃ¼met, belgenin hukukun Ã¼stÃ¼nlÃ¼ÄŸÃ¼ ve insan haklariyla medeni haklara saygÄ±ya dayanan demokratik bir toplum oluÅŸturulmasÄ± yolunda varÄ±lmÄ±ÅŸ ulusal gorusbirligini gosterdigini belirtti. Could you please translate this to English?\n",
    "\n",
    "A: \n",
    "\"\"\"\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]</s>\"\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 64,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.6,\n",
    "}\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da225b3-6c73-4464-9464-2104a07b5d00",
   "metadata": {},
   "source": [
    "### Clean Up \n",
    "\n",
    "You can delete your Imported Model in the console as shown in the image below:\n",
    "\n",
    "![Delete](./images/delete.png \"Delete\")\n",
    "\n",
    "Ensure to shut down your instance/compute that you have run this notebook on.\n",
    "\n",
    "**END OF NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
