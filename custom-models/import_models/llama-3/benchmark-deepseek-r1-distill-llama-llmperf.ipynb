{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df07d6e0-a652-4c47-8f1b-48f08ff01e9d",
   "metadata": {},
   "source": [
    "# Benchmarking Bedrock Custom Imported Models with LLMPerf and LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaff294-1095-469e-a8b5-f572e63e3e7e",
   "metadata": {},
   "source": [
    "Amazon Bedrock [custom model import](https://aws.amazon.com/bedrock/custom-model-import/) allows users to deploy their own model weights, with Bedrock automatically optimizing deployment for performance, security, and scalability. While this feature accelerates and simplified the process, it raises a critical question: *how fast is this deployment?*\n",
    "\n",
    "This notebook provides a quick way to benchmark the performance of a Bedrock-managed deployment using [LLMPerf](https://github.com/ray-project/llmperf) (a lightweight benchmarking tool for large language models) and [LiteLLM](https://github.com/BerriAI/litellm) (a universal model gateway that abstracts API calls across different providers). This process is illustrated using [deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) as the imported model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfdaf0-dcbf-4af1-8812-d7126474e6ea",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- Use *SageMaker Distribution 1.12.10* as the image in an Amazon SageMaker Studio JupyterLab app.\n",
    "- This notebook uses [deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B). You will need more than 16 GB in instance memory and storage to download the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b25eb8-6624-4c7b-ae9a-971e549cc333",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf0eeb-539e-425f-934b-5adf86d80eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sagemaker --quiet\n",
    "!pip install boto3 huggingface huggingface_hub hf_transfer --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df2f31-b6ca-461b-b262-e7b177b2f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f673a6-b45f-498d-937c-beb4b347883f",
   "metadata": {},
   "source": [
    "### Downloading model artifacts from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77b956-39ac-4d7d-88e8-f40fd6d6178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3_prefix = \"deepseek-r1-distill-llama-8b\"\n",
    "if not os.path.exists(s3_prefix):\n",
    "    os.makedirs(s3_prefix)\n",
    "    print(f\"Directory '{s3_prefix}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{s3_prefix}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469abbe3-d73a-4a90-9437-a7e02ad269f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model_id,\n",
    "    local_dir=f\"./{s3_prefix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f54c9-5c6e-4702-bca7-2f694aad7575",
   "metadata": {},
   "source": [
    "### Uploading model artifacts to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76e043-e7d6-45e6-a399-3716137cf61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory: str, bucket_name: str, s3_prefix: str):\n",
    "    \"\"\"\n",
    "    Upload files from a local directory to a user-defined prefix in an Amazon S3 bucket\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "\n",
    "    all_files = []\n",
    "    # Walk the local directory\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "\n",
    "    # Upload to S3\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(str(local_path),bucket_name,s3_key)\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "upload_directory_to_s3(s3_prefix, sess.default_bucket(), s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589ad49-cd4e-426f-a3b6-f39d0b3926cc",
   "metadata": {},
   "source": [
    "## Importing model into Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306fda4-6a2e-483a-8a05-b037244aaddd",
   "metadata": {},
   "source": [
    "### Create IAM role for model import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5e708-da8a-4cfd-89fd-b42d2b2b1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_bedrock_execution_role(role_name, account_id, region, s3_bucket):\n",
    "    \"\"\"\n",
    "    Creates an IAM role that allows Amazon Bedrock to assume the role and that grants S3 read permissions.\n",
    "    \"\"\"\n",
    "    iam = boto3.client('iam')\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"1\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"S3ReadAccess\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{s3_bucket}\",\n",
    "                    f\"arn:aws:s3:::{s3_bucket}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"s3:ResourceAccount\": account_id\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description=\"Execution role for Amazon Bedrock model import jobs\"\n",
    "        )\n",
    "        policy_name = f\"{role_name}-S3ReadAccess\"\n",
    "        policy_response = iam.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy),\n",
    "            Description=\"Allows S3 read access for Bedrock execution role\"\n",
    "        )\n",
    "        iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=policy_response['Policy']['Arn']\n",
    "        )\n",
    "        logger.info(f\"Successfully created role: {role_name}\")\n",
    "        return response['Role']\n",
    "\n",
    "    except ClientError as error:\n",
    "        logger.error(f\"Failed to create role: {error}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "bedrock_role_name = name_from_base(\"BedrockExecutionRole\")\n",
    "aws_account = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "bedrock_job_import_role = create_bedrock_execution_role(bedrock_role_name, aws_account, region, sagemaker_session_bucket)\n",
    "if bedrock_job_import_role:\n",
    "    print(f\"Role ARN: {bedrock_job_import_role['Arn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c866f93-415d-4b7c-98bd-0ec039e35a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock',region_name=region)\n",
    "s3_uri = f's3://{sess.default_bucket()}/{s3_prefix}/'\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "job_name = name_from_base(s3_prefix+\"-job\")\n",
    "imported_model_name = name_from_base(s3_prefix+\"-model\")\n",
    "\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=bedrock_job_import_role['Arn'],\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "print(f\"Model import job created with ARN: {job_Arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fe253-4799-4ff0-afdd-b1d3a05e3e5a",
   "metadata": {},
   "source": [
    "The model import process can take ~10 min. The following cell will wait for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940dbfd-9153-4650-bfbc-4770e7777587",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f574e7e-c9b2-4664-855c-8073eb47f804",
   "metadata": {},
   "source": [
    "Once the model has been imported, we may still have to give Bedrock a few minutes to initialize a serving container to handle our requests. If the model is not available yet, we will get an error message saying that the model is **not ready**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc4473-05d6-40f6-9fb8-8db4e36c54a8",
   "metadata": {},
   "source": [
    "## Testing model inference with the InvokeModel API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb67bc1-a20f-4cd6-983c-2878d183973c",
   "metadata": {},
   "source": [
    "### Configuration to work with DeepSeek models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c10444-d857-4de7-aebe-da8dcafdfef3",
   "metadata": {},
   "source": [
    "DeepSeek models expect inputs to follow a specific format defined in the `tokenizer_config.json` file. This format ensures that the model receives prompts in the same structure that it was trained on. We are going to initialize a `tokenizer` and then use it to shape our model requests to ensure that the model sees prompts with the expected structure.\n",
    "\n",
    "We are also going to edit the configuration of the Bedrock runtime client to increase the timeout and the number of retries so we can work with long wait times and the potential unavailability of the model when the serving container is still being prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367a5f6-0967-42eb-80aa-b640aed1eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    "    config=Config(\n",
    "        connect_timeout=300,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766c7d0-ea9c-40b2-9073-c36304cae9a1",
   "metadata": {},
   "source": [
    "The `generate` function is going to take our messages and send them to the DeepSeek model using proper tokenization and a robust retry mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11dadc-4259-4b85-a330-fb9121d16288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=not continuation)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "\n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return prompt, result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4328ec-4ef8-4a01-b310-e4c6841c1297",
   "metadata": {},
   "source": [
    "### Running simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1dcf5-89c2-4bc2-908e-11e14c7aaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "prompt, response = generate(messages)\n",
    "print(\"Test prompt\")\n",
    "print(prompt)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367771d-b213-4ac3-bf80-140b07554e06",
   "metadata": {},
   "source": [
    "## Running benchmark with LLMPerf\n",
    "\n",
    "[LLMPerf](https://github.com/ray-project/llmperf) is a benchmarking library for large language models. A load test is going to spawn a number of concurrent requests to the LLM API and measure the intertoken latnecy and generation throughput per request and across concurrent requests.\n",
    "\n",
    "LLMPerf already supports LiteLLM, which we can use to send API requests to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab08f66-b659-4679-b379-6b7305c49deb",
   "metadata": {},
   "source": [
    "### Install LLMPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3025424-4dfc-4c22-9b46-45335d276d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ray-project/llmperf.git\n",
    "!cd llmperf; pip install -e . --quiet; cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009a040-01d1-4f4c-affe-2e795bcd11ae",
   "metadata": {},
   "source": [
    "### Setting up AWS credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a0db2-df1a-42fb-922b-94463d3fffa2",
   "metadata": {},
   "source": [
    "[LiteLLM](https://github.com/BerriAI/litellm) is a Python library that provides consitent input and output formats for invoking generative models from various providers. LiteLLLM can be used to invoke models on Amazon Bedrock, but requires that we configure the authorization parameters. More information available [here](https://docs.litellm.ai/docs/providers/bedrock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ddbc0-f416-4c93-a9e1-858c30669747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "AWS_ACCESS_KEY_ID = getpass.getpass(\"Enter your AWS access key ID: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ae399-8434-409e-ac4b-40f54ae425cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SECRET_ACCESS_KEY = getpass.getpass(\"Enter your AWS secret access key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8709582-e0ab-40b3-919d-7858a0f1f765",
   "metadata": {},
   "source": [
    "### Single invocation using LiteLLM\n",
    "\n",
    "DeepSeek R1 models use a custom request/response spec, that we must define in the `model` param. The `deepseek-ai/DeepSeek-R1-Distill-Llama-8B` model can be invoked with both the Llama chat template and the DeepSeek R1 chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe7fd0-a486-4e51-866b-4ce10b540dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=f\"bedrock/deepseek_r1/{model_id}\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"\"\"Given the following financial data:\n",
    "        - Company A's revenue grew from $10M to $15M in 2023\n",
    "        - Operating costs increased by 20%\n",
    "        - Initial operating costs were $7M\n",
    "        \n",
    "        Calculate the company's operating margin for 2023. Please reason step by step.\"\"\"},\n",
    "                      {\"role\": \"assistant\", \"content\": \"<think>\"}],\n",
    "            max_tokens=4096,\n",
    "        )\n",
    "        print(response['choices'][0]['message']['content'])\n",
    "        break\n",
    "    except:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bddd0-ba15-4279-b5eb-8541d299ccbc",
   "metadata": {},
   "source": [
    "### Write testing script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ccbda-f60e-46d3-b43e-d1734f1bbd73",
   "metadata": {},
   "source": [
    "We are going to define a Shell script that will run the `token_benchmark_ray.py` script that is part of the `LLMPerf` library. There will be some minor modifications to use our `LiteLLM` compatible model and to pass our AWS credentials as environment variables.\n",
    "\n",
    "We are going to run this test using the Llama chat template because, as of Feb 2025, `LLMPerf` does not support the DeepSeek R1 chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a3dfa-94ab-44c4-a85e-0263813c3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "def write_benchmarking_script(mean_input_tokens: int,\n",
    "                              stddev_input_tokens: int,\n",
    "                              mean_output_tokens: int,\n",
    "                              stddev_output_tokens: int,\n",
    "                              num_concurrent_requests: int,\n",
    "                              num_requests_per_client: int):\n",
    "    \"\"\"\n",
    "    Write a benchmarking script that uses the token_benchmark_ray script\n",
    "    \"\"\"\n",
    "    results_dir = os.path.join(\"outputs\",\n",
    "                               name_from_base(hf_model_id)\n",
    "                              )\n",
    "    \n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "export LLM_PERF_CONCURRENT={num_concurrent_requests}\n",
    "export LLM_PERF_MAX_REQUESTS=$(expr $LLM_PERF_CONCURRENT \\* {num_requests_per_client})\n",
    "export LLM_PERF_SCRIPT_DIR=$HOME/llmperf\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=\"{AWS_ACCESS_KEY_ID}\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"{AWS_SECRET_ACCESS_KEY}\"\n",
    "export AWS_REGION_NAME=\"{region}\"\n",
    "\n",
    "export LLM_PERF_OUTPUT={results_dir}\n",
    "\n",
    "mkdir -p $LLM_PERF_OUTPUT\n",
    "cp \"$0\" \"${{LLM_PERF_OUTPUT}}\"/\n",
    "\n",
    "python3 ${{LLM_PERF_SCRIPT_DIR}}/token_benchmark_ray.py \\\\\n",
    "    --model \"bedrock/llama/{model_id}\" \\\\\n",
    "    --mean-input-tokens {mean_input_tokens} \\\\\n",
    "    --stddev-input-tokens {stddev_input_tokens} \\\\\n",
    "    --mean-output-tokens {mean_output_tokens} \\\\\n",
    "    --stddev-output-tokens {stddev_output_tokens} \\\\\n",
    "    --max-num-completed-requests ${{LLM_PERF_MAX_REQUESTS}} \\\\\n",
    "    --timeout 1800 \\\\\n",
    "    --num-concurrent-requests ${{LLM_PERF_CONCURRENT}} \\\\\n",
    "    --results-dir \"${{LLM_PERF_OUTPUT}}\" \\\\\n",
    "    --llm-api litellm \\\\\n",
    "    --additional-sampling-params '{{}}'\n",
    "\"\"\"\n",
    "\n",
    "    os.makedirs(\"scripts\", exist_ok=True)\n",
    "\n",
    "    script_path = \"scripts/run_benchmark.sh\"\n",
    "    with open(script_path, \"w\") as file:\n",
    "        file.write(script_content)\n",
    "\n",
    "    current_permissions = os.stat(script_path).st_mode\n",
    "    os.chmod(script_path, current_permissions | stat.S_IXUSR)\n",
    "\n",
    "    print(f\"Script written to {script_path} and made executable.\")\n",
    "\n",
    "    return results_dir\n",
    "\n",
    "results_dir = write_benchmarking_script(\n",
    "    mean_input_tokens=500,\n",
    "    stddev_input_tokens=25,\n",
    "    mean_output_tokens=1000,\n",
    "    stddev_output_tokens=100,\n",
    "    num_concurrent_requests=2,\n",
    "    num_requests_per_client=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338a6a9-b63c-495f-a7e5-98c0645d4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ./scripts/run_benchmark.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3987421-dd75-41af-a384-3b9ddb632e89",
   "metadata": {},
   "source": [
    "###  Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962bed7-cbc5-4ad0-915e-2565941f5dd6",
   "metadata": {},
   "source": [
    "LLMPerf will write two files in the results directory: a summary file, whose results we can already see above, and the individual responses, which we can extract for plotting and detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ba243-de53-47e3-af2a-c281ba139db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_responses_json(local_dir):\n",
    "    \"\"\"\n",
    "    Load JSON file with detailed responses from the LLMPerf test\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(local_dir):\n",
    "        raise FileNotFoundError(f\"Directory '{local_dir}' does not exist.\")\n",
    "\n",
    "    for file_name in os.listdir(local_dir):\n",
    "        if file_name.endswith(\"individual_responses.json\"):\n",
    "            file_path = os.path.join(local_dir, file_name)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "    raise FileNotFoundError(\"No file ending with 'individual_responses.json' found in the directory.\")\n",
    "\n",
    "try:\n",
    "    individual_responses = load_responses_json(results_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e3ce3-4022-47e6-847f-d193a886317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ttft_values = [d[\"ttft_s\"] for d in individual_responses if \"ttft_s\" in d]\n",
    "throughput_values = [d[\"request_output_throughput_token_per_s\"] for d in individual_responses if \"request_output_throughput_token_per_s\" in d]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(ttft_values, bins=50, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Time to first token (s)\")\n",
    "axes[1].hist(throughput_values, bins=50, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Throughput (tokens/s)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"token_benchmark.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0dad0-e704-496b-934f-e7795d5be9c7",
   "metadata": {},
   "source": [
    "Feel free to edit the arguments of the `write_benchmarking_script` function to re-run the analysis for your specific scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dc1d9-5f34-4e13-8864-5939b4c9fe9a",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64052fb-cfa8-4b7e-a2f3-55df3a2b124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock', region_name=region)\n",
    "response = bedrock.delete_imported_model(\n",
    "    modelIdentifier=imported_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eeb654-818a-46c7-8961-0a716e7d48a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
