{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770fcd21",
   "metadata": {},
   "source": [
    "# Continuous Fine-Tuning of LlaMA 3 and Importing into Bedrock: A Step-by-Step Instructional Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495f494-6da2-451a-9bc3-9f04776c2c96",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "In this notebook we will walk through how to _continuously_ fine-tune a Llama-3 LLM on Amazon SageMaker using PyTorch FSDP and Flash Attention 2 including Q-LORA and PEFT. This notebook also explains using PEFT and merging the adapters. To demonstrate continous fine tuning we will take a Llama-3 Base Model fine tune using English dataset and then fine tune again using a Portuguese language dataset. Each of the fine tuned model will be imported into Bedrock. This demonstrates the ability to iteratively fine tune a model as new data originates or when there is a need to expand out further (ex: language addition in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5906342-ed58-432f-8150-d355156d787e",
   "metadata": {},
   "source": [
    "## Usecase \n",
    "\n",
    "We will quantize the model as bf16 model. We use [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer) (SFT) for fine tuning the model. We will use Anthropic/Vicuna like Chat Template with User: and Assistant: roles to fine tune the model. We will use [ngram/medchat-qa](https://huggingface.co/datasets/ngram/medchat-qa) dataset for fine tuning the model. This is a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. Using [FSDP](https://pytorch.org/docs/main/fsdp.html) and [Q-Lora](https://arxiv.org/abs/2305.14314) allows us to fine tune Llama-3 models on 2x consumer GPU's. FSDP enables sharding model parameters, optimizer states and gradients across data parallel workers. Q- LORA helps reduce the memmory usage for finetuning LLM while preserving full 16-bit task performance. For fine tuning in this notebook we use ml.g5.12xlarge as a SageMaker Training Job. \n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker) provides a fully managed service that enables build, train and deploy ML models at scale using tools like notebooks, debuggers, profilers, pipelines, MLOps, and more – all in one integrated development environment (IDE). [SageMaker Model Training](https://aws.amazon.com/sagemaker/train/) reduces the time and cost to train and tune machine learning (ML) models at scale without the need to manage infrastructure.\n",
    "\n",
    "In this notebook you will leverage the ability of SageMaker Training job to download training data to download the fine tuned Large Language Model for further fine tuning.\n",
    "\n",
    "For detailed instructions please refer to [Importing a model with customer model import Bedrock Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).\n",
    "\n",
    "This notebook is inspired by Philipp Schmid Blog - https://www.philschmid.de/fsdp-qlora-llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da662ec",
   "metadata": {},
   "source": [
    "## Model License information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541e27d",
   "metadata": {},
   "source": [
    "In this notebook we use the Meta Llama3 model from HuggingFace. This model is a gated model within HuggingFace repository. To use this model you have to agree to the license agreement (https://llama.meta.com/llama3/license) and request access to the model before it can be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069d3b7-7543-4411-9f13-06f6f8dae8f5",
   "metadata": {},
   "source": [
    "## Notebook code with comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2860a3-68b3-435d-8d2c-753c90a24da3",
   "metadata": {},
   "source": [
    "### Install the Pre-Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a83fd-92b8-47ad-9128-c820fb4a6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !rm -fR /opt/conda/lib/python3.10/site-packages # Run if you are getting conflicts with fsspec packages.\n",
    "!pip3 uninstall autogluon autogluon-multimodal --y\n",
    "!pip3 install transformers \"sagemaker>=2.190.0\" \"huggingface_hub\" \"datasets[s3]==2.18.0\" --upgrade --quiet\n",
    "!pip3 install boto3 s3fs \"aiobotocore==2.11.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf77c9",
   "metadata": {},
   "source": [
    "Logging into the HuggingFace Hub and requesting access to the meta-llama/Meta-Llama-3-8B is required to download the model and finetune the same. Please follow the [HuggingFace User Token Documentation](https://huggingface.co/docs/hub/en/security-tokens) to request tokens to be provided in the textbox appearning below after you run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ce221-9721-4089-8f5f-9c2d4afba925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f5960-12b5-4310-8711-323dde090b96",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We will initialize the SageMaker Session required to finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cc282-7e49-4db4-ac7f-782b11558395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52c6b5-984a-4ab4-b9b1-e61feb154e49",
   "metadata": {},
   "source": [
    "### Define the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4d604-1fb4-4268-9b0a-6fae07ca53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# save train_dataset to s3 using our SageMaker session\n",
    "training_input_base_path = f\"s3://{sess.default_bucket()}/datasets/ngram/medchat-qa/\"\n",
    "training_input_path = f\"{training_input_base_path}finetune_ip\"\n",
    "use_bf16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef6d88-133b-46cc-b949-439dc06aa278",
   "metadata": {},
   "source": [
    "### Dataset Prepare\n",
    "We will use [ngram/medchat-qa](https://huggingface.co/datasets/ngram/medchat-qa) dataset to finetune the Llama 3 model. Kindly refer to the [Licensing Information](https://huggingface.co/datasets/ngram/medchat-qa) regarding this dataset before proceeding further.\n",
    "\n",
    "We will transform the messages to OAI format and split the data into Train and Test set. The Train and Test dataset will be uploaded into S3 - SageMaker Session Bucket for use during finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de024deb-c757-49c9-a734-bc31bdc5985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, VerificationMode\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import aiobotocore.session\n",
    "s3_session = aiobotocore.session.AioSession()\n",
    "storage_options = {\"session\": s3_session}\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, a medical expert tasked with providing the most accurate and succinct answers to specific questions based on detailed medical data. Focus on precision and directness in your responses, ensuring that each answer is factual, concise, and to the point. Avoid unnecessary elaboration and prioritize accuracy over sounding confident.\"\"\"\n",
    " \n",
    "def create_conversation(row):\n",
    "    row[\"messages\"] = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": row[\"question\"]\n",
    "    },{\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": row[\"answer\"]\n",
    "    }]\n",
    "    return row\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"ngram/medchat-qa\", split=\"train[:100%]\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "print(f'Schema for dataset: {dataset}')\n",
    "\n",
    "dataset.save_to_disk(f\"{training_input_base_path}/en/\", storage_options=storage_options)\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_from_disk(f\"s3://{sagemaker_session_bucket}/datasets/ngram/medchat-qa/en/\"\n",
    "                         , storage_options=storage_options)\n",
    "\n",
    "print(f'Number of Rows: {dataset.num_rows}')\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size=0.3)\n",
    "print(f'Schema for dataset: {dataset}')\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "\n",
    "print(f\"Number of Rows: {dataset.num_rows}\")\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67418b39-254d-42d8-907b-4acc92593ae4",
   "metadata": {},
   "source": [
    "### Training script and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aa17e-6551-439b-b519-d5531528dec2",
   "metadata": {},
   "source": [
    "Create the scripts directory to hold the training script and dependencies list. This directory will be provided to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c48a48-9890-4404-a3dd-688e77ee7c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"scripts/trl\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4276101-a612-4a65-8a2c-e58e759f8c06",
   "metadata": {},
   "source": [
    "Create the requirements file that will be used by the SageMaker Job container to initialize the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c9c19-c920-4279-8508-9abc01ac89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/requirements.txt\n",
    "torch==2.2.2\n",
    "transformers==4.40.2\n",
    "sagemaker>=2.190.0\n",
    "datasets==2.18.0\n",
    "accelerate==0.29.3\n",
    "evaluate==0.4.1\n",
    "bitsandbytes==0.43.1\n",
    "trl==0.8.6\n",
    "peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533b519-3926-4f5f-b1f2-9de267d22195",
   "metadata": {},
   "source": [
    "Training Script that will use PyTorch FSDP, QLORA, PEFT and train the model using SFT Trainer. This script also includes prepping the data to Llama 3 chat template (Anthropic/Vicuna format). This training script is being written to the scripts folder along with the requirements file that will be used by the SageMaker Job.\n",
    "\n",
    "The training script also uses either the HuggingFace Model Id or a local path (script_args.model_id_path) to load the Large Language Model for Fine Tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c5f36-833d-4d45-9138-63711a805b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/run_fsdp_qlora.py\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "try:\n",
    "    os.system(\"pip install flash-attn --no-build-isolation --upgrade\")\n",
    "except:\n",
    "    print(\"flash-attn failed to install\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    Conv1D\n",
    ")\n",
    "from transformers import logging as transf_logging\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "\n",
    "from trl import (\n",
    "   SFTTrainer)\n",
    "\n",
    "# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n",
    "# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "\n",
    "# Anthropic/Vicuna like template without the need for special tokens\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "transf_logging.set_verbosity_error()\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Model S3 Path to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "    use_qlora: bool = field(default=False, metadata={\"help\": \"Whether to use QLORA\"})\n",
    "    merge_adapters: bool = field(\n",
    "        metadata={\"help\": \"Wether to merge weights for LoRA.\"},\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    torch.cuda.memory_summary(abbreviated=True) # Return a human-readable printout of the current memory allocator statistics for a given device.\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "\n",
    "    print(f\"##################     Using model_id_path: {script_args.model_id_path}        ################\")\n",
    "    # Tokenizer        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=script_args.model_id_path\n",
    "                                              , use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "    \n",
    "    # template dataset\n",
    "    def template_dataset(examples):\n",
    "        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    \n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "    if script_args.use_qlora:\n",
    "        print(f\"Using QLoRA - {torch_dtype}\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch_dtype,\n",
    "                bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "            )\n",
    "        # For 8 bit quantization\n",
    "        # quantization_config = BitsAndBytesConfig(load_in_8bit=True,\n",
    "        #                                          llm_int8_threshold=200.0)\n",
    "    else:\n",
    "        quantization_config = None\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=script_args.model_id_path,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={'':torch.cuda.current_device()},\n",
    "        attn_implementation=\"flash_attention_2\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "\n",
    "    print(f\"Model Layers: {list(set(get_specific_layer_names(model)))} \")\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "\n",
    "    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        #target_modules=\"all-linear\",\n",
    "        target_modules=['q_proj', 'v_proj'],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "        # modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False,  # No need to add additional separator token\n",
    "        },\n",
    "    )\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        print(\"###############     Printing Trainable Parameters     ###############\")\n",
    "        trainer.model.print_trainable_parameters()\n",
    "        print(\"###############     Printing Trainable Parameters - Completed!!!     ###############\")\n",
    "\n",
    "    ##########################\n",
    "    # Train model\n",
    "    ##########################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    sagemaker_save_dir = \"/opt/ml/model\"\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "    if script_args.merge_adapters:\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        print('########## Merging Adapters  ##########')\n",
    "        trainer.model.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(sagemaker_save_dir) \n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # list file in output_dir\n",
    "        print(f\" contents of {training_args.output_dir} : {os.listdir(training_args.output_dir)}\")\n",
    "\n",
    "        # list files in sagemaker_save_dir\n",
    "        print(f\" contents of {sagemaker_save_dir} : {os.listdir(sagemaker_save_dir)}\")\n",
    "        \n",
    "        # load PEFT model\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16, # loading in other precision types gives errors.\n",
    "            is_trainable=True, # Setting this to true only will allow further fine tuning.\n",
    "            device_map={'':torch.cuda.current_device()}\n",
    "        )\n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        print(f\"#########              Saving Merged Model to {sagemaker_save_dir}               #########\")\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "        \n",
    "    # list files in sagemaker_save_dir\n",
    "    print(f\" contents of {sagemaker_save_dir} : {os.listdir(sagemaker_save_dir)}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_into_dataclasses()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        training_function(script_args, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e54e8-be51-4d61-9c65-9a36c26e9864",
   "metadata": {},
   "source": [
    "### First Iteration of fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879676-6e95-4210-a783-8eb6a3a180d5",
   "metadata": {},
   "source": [
    "In this iteration of training you will download the Llama-3 base model from HuggingFace repository and fine tune the model using English language version of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c74d7a-9045-4e24-a0b1-6154f5ff66e1",
   "metadata": {},
   "source": [
    "Hyperparameters, which are passed into the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bbfe3-c4b2-478e-90b8-68a657109c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id_path': f\"{model_id}\",         # path where the safetensor model file is downloaded to\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 3,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 1,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"epoch\",\n",
    "  'learning_rate': 0.0002,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                  # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292a6b3-3123-451b-ad2c-cf6fda7edc54",
   "metadata": {},
   "source": [
    "Use the SageMaker HuggingFace Estimator to finetune the model passing in the hyperparameters and the scripts directory from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd877c88-812b-4c53-b6c2-2c7fb27a6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'{model_id.replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/trl/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    "    #enable_remote_debug=True\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': f\"{training_input_path}\"}\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a05e8a-0317-4f99-8181-f859a9022656",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pubmed_model_s3_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "print(f\"EN PubMed Fine Tuned Model S3 Location: {en_pubmed_model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878eb60a-f09e-4a5a-9a1c-dd6571fee55d",
   "metadata": {},
   "source": [
    "### Second Iteration of fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f29b80-79ff-4143-85e1-76101d2436ad",
   "metadata": {},
   "source": [
    "In this iteration of fine tuning we will take the English Language Fine Tuned model from above and fine tune with a Portuguese translated version of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4886766-4391-4e4f-937e-96879251e902",
   "metadata": {},
   "source": [
    "First you will translate the dataset into Portuguese using Amazon Translate. You will format the dataset into OAI format and uploaded to the SageMaker Session Bucket for Fine Tuning.\n",
    "\n",
    "Please make sure the SageMaker Role has permission to access Amazon Translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c03c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import aiobotocore.session\n",
    "\n",
    "s3_session = aiobotocore.session.AioSession()\n",
    "storage_options = {\"session\": s3_session}\n",
    "\n",
    "global trn_client \n",
    "trn_client = boto3.client('translate')\n",
    "\n",
    "def translate2pt(txt):\n",
    "    response = trn_client.translate_text(\n",
    "        Text=txt,\n",
    "        SourceLanguageCode='en',\n",
    "        TargetLanguageCode='pt',\n",
    "    )\n",
    "    return response[\"TranslatedText\"]\n",
    "\n",
    "def add_pt_content_to_pubmed(row):\n",
    "    row['question_pt'] = translate2pt(row['question'])\n",
    "    row['answer_pt'] = translate2pt(row['answer'])\n",
    "    return row\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"ngram/medchat-qa\", split=\"train[:100%]\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "print(f'Schema for dataset: {dataset}')\n",
    "dataset_pt = dataset.map(add_pt_content_to_pubmed, batched=False)\n",
    "\n",
    "dataset_pt = dataset_pt.remove_columns([\"question\", \"answer\"])\n",
    "dataset_pt = dataset_pt.rename_column(\"question_pt\", \"question\")\n",
    "dataset_pt = dataset_pt.rename_column(\"answer_pt\", \"answer\")\n",
    "\n",
    "dataset_pt.save_to_disk(f\"{training_input_base_path}/pt/\", storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61fcfb-07c9-46a5-baad-745e2013281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, VerificationMode\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import aiobotocore.session\n",
    "s3_session = aiobotocore.session.AioSession()\n",
    "storage_options = {\"session\": s3_session}\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"Você é Llama, um especialista médico encarregado de fornecer as respostas mais precisas e sucintas a perguntas específicas com base em dados médicos detalhados. Concentre-se na precisão e na franqueza de suas respostas, garantindo que cada resposta seja factual, concisa e objetiva. Evite elaborações desnecessárias e priorize a precisão em vez de parecer confiante.\"\"\"\n",
    " \n",
    "def create_conversation(row):\n",
    "    row[\"messages\"] = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": row[\"question\"]\n",
    "    },{\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": row[\"answer\"]\n",
    "    }]\n",
    "    return row\n",
    "\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_from_disk(f\"s3://{sagemaker_session_bucket}/datasets/ngram/medchat-qa/pt/\", storage_options=storage_options)\n",
    "print(f'Number of Rows: {dataset.num_rows}')\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size=0.3)\n",
    "print(f'Schema for dataset: {dataset}')\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    " \n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{training_input_path}/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(f\"{training_input_path}/test_dataset.json\", orient=\"records\", force_ascii=False)\n",
    " \n",
    "print(f\"Training data uploaded to:\")\n",
    "print(f\"{training_input_path}/train_dataset.json\")\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={training_input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f99495-bc18-44ad-a302-d7b475af0f40",
   "metadata": {},
   "source": [
    "Hyperparameters for the Fine Tuning job.\n",
    "\n",
    "Below you will notice that the location of the downloaded English model is provided as input to model_id_path variable instead of the HuggingFace model id as in the English dataset fine tuning. The training script will load the model from the Training job local disk which is automatically downloaded from S3 bucket by SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5d6bb-0af0-4993-ace5-a11931b377a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id_path': '/opt/ml/input/data/model/',      # path where the safetensor model file is downloaded to\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 1,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 1,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"epoch\",\n",
    "  'learning_rate': 0.0002,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                  # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc929824-ed67-4562-8b8d-4b719d9ece86",
   "metadata": {},
   "source": [
    "Below you will use the SageMaker Hugging Face model and estimator to Fine Tune the model using the Portguese Dataset. \n",
    "\n",
    "Kindly note that you will provide the S3 location of the English Language Fine Tuned model into the fit method. The S3 path is provided in the pt_data variable in the model attribute. SageMaker automatically downloads the files from the respective S3 bucket provided to the fit method of the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61a65a-d715-432d-b288-10c0353778f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "pt_job_name = f'{model_id.replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }'\n",
    "\n",
    "# create the Estimator\n",
    "pt_huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/trl/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.24xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = pt_job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  pt_hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    "    # enable_remote_debug=True\n",
    ")\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "pt_data = {'training': training_input_path, \n",
    "        'model': f'{en_pubmed_model_s3_path}'}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "pt_huggingface_estimator.fit(pt_data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7640771-6eaa-4bce-a756-a3d8233063bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pubmed_model_s3_path = pt_huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "print(f\"PT PubMed Fine Tuned Model S3 Location: {pt_pubmed_model_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7db4b-d338-4f1f-a4a5-f597e46c74b1",
   "metadata": {},
   "source": [
    "### Import the finetuned model into Bedrock:\n",
    "\n",
    "Below works only after Bedrock Custom Model Import feature is Generally Available (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0decf0b-49ed-4fde-8250-cdf2fe54efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf2050-ba43-45d6-89ee-15eb0364e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_client = boto3.client('bedrock', region_name='us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a790345-2cc7-4276-9cd1-91a4b3eb0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model_nm = \"Meta-Llama-3-8B-bf16-MedChatQA-PT\"\n",
    "pt_imp_jb_nm = f\"{pt_model_nm}-{datetime.datetime.now().strftime('%Y%m%d%M%H%S')}\"\n",
    "role_arn = \"<<bedrock_role_with_custom_model_import_policy>>\"\n",
    "pt_model_src = {\"s3DataSource\": {\"s3Uri\": f\"{pt_pubmed_model_s3_path}\"}}\n",
    "\n",
    "resp = br_client.create_model_import_job(jobName=pt_imp_jb_nm,\n",
    "                                  importedModelName=pt_model_nm,\n",
    "                                  roleArn=role_arn,\n",
    "                                  modelDataSource=pt_model_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d13e90-da80-4530-b775-952a35de30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model_nm = \"Meta-Llama-3-8B-bf16-MedChatQA-EN\"\n",
    "en_imp_jb_nm = f\"{en_model_nm}-{datetime.datetime.now().strftime('%Y%m%d%M%H%S')}\"\n",
    "role_arn = \"<<bedrock_role_with_custom_model_import_policy>>\"\n",
    "en_model_src = {\"s3DataSource\": {\"s3Uri\": f\"{en_pubmed_model_s3_path}\"}}\n",
    "\n",
    "resp = br_client.create_model_import_job(jobName=en_imp_jb_nm,\n",
    "                                  importedModelName=en_model_nm,\n",
    "                                  roleArn=role_arn,\n",
    "                                  modelDataSource=en_model_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f4e64",
   "metadata": {},
   "source": [
    "### Invoke the imported model using Bedrock API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 botocore --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13860684",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock-runtime\", region_name=\"<<region-name>>\")\n",
    "\n",
    "model_id = \"<<bedrock-model-arn>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_invoke_model_and_print(native_request):\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = client.invoke_model(modelId=model_id, body=request)\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "        response_text = model_response[\"outputs\"][0][\"text\"]\n",
    "        print(response_text)     \n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"[INST]You are a medical expert tasked with providing the most accurate and succinct answers to specific questions based on detailed medical data. Focus on precision and directness in your responses, ensuring that each answer is factual, concise, and to the point. Avoid unnecessary elaboration and prioritize accuracy over sounding confident. Here are some guidelines for your responses:\n",
    "\n",
    "- Provide clear, direct answers without filler or extraneous details.\n",
    "- Base your responses solely on the information available in the medical text provided.\n",
    "- Ensure that your answers are straightforward and easy to understand, yet medically accurate.\n",
    "- Avoid speculative or generalized statements that are not directly supported by the text.\n",
    "\n",
    "Use these guidelines to formulate your answers to the questions presented [/INST]\"\"\"\n",
    "\n",
    "prompt = \"\"\"What is the recommended treatment for metformin overdosage?<|end_of_text|>\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "formatted_prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.6,    \n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0663b-5830-41d2-87e3-abbd6cd39742",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"[INST]You are a medical expert tasked with providing the most accurate and succinct answers to specific questions based on detailed medical data. Focus on precision and directness in your responses, ensuring that each answer is factual, concise, and to the point. Avoid unnecessary elaboration and prioritize accuracy over sounding confident. Here are some guidelines for your responses:\n",
    "\n",
    "- Provide clear, direct answers without filler or extraneous details.\n",
    "- Base your responses solely on the information available in the medical text provided.\n",
    "- Ensure that your answers are straightforward and easy to understand, yet medically accurate.\n",
    "- Avoid speculative or generalized statements that are not directly supported by the text.\n",
    "\n",
    "Use these guidelines to formulate your answers to the questions presented [/INST]\"\"\"\n",
    "\n",
    "prompt = \"\"\"What is the recommended treatment for metformin overdosage?<|end_of_text|>\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "formatted_prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.6,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    streaming_response = client.invoke_model_with_response_stream(\n",
    "        modelId=model_id, body=request\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in streaming_response[\"body\"]:\n",
    "        chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "        if \"outputs\" in chunk:\n",
    "            print(chunk[\"outputs\"][0].get(\"text\"), end=\"\")\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}''. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8b748",
   "metadata": {},
   "source": [
    "### Clean up the Bedrock Imported Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f32261",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = br_client.delete_imported_model(modelIdentifier=model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
