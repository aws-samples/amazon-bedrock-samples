{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4044ee2-645c-40d1-9d59-d768e28c681c",
   "metadata": {},
   "source": [
    "# Fine tuning and deploying Llama 3 8B to Amazon Bedrock using Custom Model Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b1f22-bed9-4c39-bc7d-07fd9906fec0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook illustrates the process of fine tuning [Llama 3 8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and deploying the custom model in Amazon Bedrock using Custom Model Import (CMI). \n",
    "\n",
    "This notebook will use an Amazon SageMaker training job to fine tune Llama 3 8B. The training script uses [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) and QLoRA for parameter efficient fine tuning. Once trained, the adapters are **merged** back into the original model to get an updated set of weights persisted as `safetensors` files (Bedrock custom model import does not support separate LoRA adapters). The resulting files are later imported into Bedrock using the [custom model import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html) option.\n",
    "\n",
    "This notebook is inspired by [Philipp Schmid's Blog](https://www.philschmid.de/fsdp-qlora-llama3).\n",
    "\n",
    "### Model License Information\n",
    "\n",
    "In this notebook we use the Meta Llama 3 model from HuggingFace. This model is a gated model within HuggingFace repository. To use this model you have to agree to the [license agreement](https://llama.meta.com/llama3/license) and request access before the model can be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3cf2b-e854-4c07-87b1-75124dc01eda",
   "metadata": {},
   "source": [
    "## Usecase \n",
    "\n",
    "The usecase for this example will be LLM code generation, the code generation scenario will be text to SQL generation, which is sometimes needed to improve the quality of the generated queries or when using a non-standard SQL dialect. The same script can be adapted to other code generation scenarios by changing the fine tuning instructions and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c390808-421d-4659-a28b-0d3f2a45fca0",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via [Custom Model Import (CMI)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html). \n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6fc30-f6d1-4d6c-a4b5-348cf433ec4f",
   "metadata": {},
   "source": [
    "## Architecture Diagram\n",
    "\n",
    "![text-to-sql architecture](images/text-to-sql-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65222abf-9f42-4673-aa64-b57c496afbec",
   "metadata": {},
   "source": [
    "## Notebook code with comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10d748-6125-4899-b5b0-d83afe7578d0",
   "metadata": {},
   "source": [
    "### Installing pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11130253-ccc6-4b22-af9d-9c7d75b716e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall autogluon autogluon-multimodal autogluon-core autogluon-timeseries autogluon-tabular -y\n",
    "!pip install sagemaker huggingface_hub datasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3592df-a8e5-4953-84a6-5f952227c3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip list | grep -e torch -e datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657170c-c3b5-4c8c-8364-e254d2dbfbdc",
   "metadata": {},
   "source": [
    "Llama 3 8B is a gated model on the Hugging Face Hub. You will need to request access and then authenticate on this notebook by entering your Hugging Face access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c165557-88f1-4429-b2b8-50c32eb2c43a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f1478-48ae-4298-be1e-0ed7193b9cf3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Loading the information from this SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d35b36-b132-4ee7-923c-98b71de61169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "    \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d160878-db73-4fdd-9868-18bc2304be34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3 prefix for fine tuning training data\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/sql-context'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dae0c6-2590-48bc-9f32-bc08fef1707a",
   "metadata": {},
   "source": [
    "### Preparing the data set\n",
    "\n",
    "We are going to use the [sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) available on Hugging Face to train this model. The data set contains 78,577 records, and we will use 99% of them for training. The data set has three columns:\n",
    "\n",
    "- *question*: The question made by a user in natural language\n",
    "- *content*: Schema of the relevant table(s)\n",
    "- *answer*: The SQL query\n",
    "\n",
    "Please refer to the [Licensing Information](https://huggingface.co/datasets/b-mc2/sql-create-context) regarding this dataset before proceeding further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205b9f3-d40f-4659-b7c8-0165d8d6065b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "system_message = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database.\"\"\"\n",
    "\n",
    "def create_conversation(record):\n",
    "    sample = {\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_message + f\"\"\"You can use the following table schema for context: {record[\"context\"]}\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Return the SQL query that answers the following question: {record[\"question\"]}\"\"\"},\n",
    "        {\"role\" : \"assistant\", \"content\": f\"\"\"{record[\"answer\"]}\"\"\"}\n",
    "    ]}\n",
    "    return sample\n",
    "\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "dataset = dataset.map(create_conversation, batched=False).remove_columns(['answer', 'question', 'context'])\n",
    "\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.01, seed=42) # only 1% for testing\n",
    "# Training and test sets\n",
    "training_data = train_test_split[\"train\"]\n",
    "test_data = train_test_split[\"test\"]\n",
    "\n",
    "training_data.to_json(\"data/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "test_data.to_json(\"data/test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc5103-1c31-4317-9b2c-2bc000aa3759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload train and test data sets to S3\n",
    "train_s3_path = sagemaker.s3.S3Uploader.upload(\"data/train_dataset.json\", training_input_path)\n",
    "test_s3_path = sagemaker.s3.S3Uploader.upload(\"data/test_dataset.json\", training_input_path)\n",
    "print(\"Training data uploaded to \", train_s3_path)\n",
    "print(\"Test data uploaded to \", test_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bafcf9-1645-4bec-8bbe-2d765e402743",
   "metadata": {},
   "source": [
    "## Fine tuning the model\n",
    "\n",
    "In this step we are going to fine tune Llama 3 8B using PyTorch FSDP and QLora, with the help of the Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Tranformers](https://huggingface.co/docs/transformers/index), [PEFT](https://huggingface.co/docs/peft/index), and [dadtasets](https://huggingface.co/docs/datasets/index) libraries. The code will be packaged to run inside a SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1941713-1370-410c-8147-9b81812b3a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "use_bf16 = True  # use bfloat16 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3ebd3-7765-4bc7-bc00-b7f0e1d04329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf scripts && mkdir scripts && mkdir scripts/trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5580fd5-1bab-4e40-8fc1-506727786ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/requirements.txt\n",
    "torch==2.2.2\n",
    "transformers==4.40.2\n",
    "sagemaker>=2.190.0\n",
    "datasets==2.18.0\n",
    "accelerate==0.29.3\n",
    "evaluate==0.4.1\n",
    "bitsandbytes==0.43.1\n",
    "trl==0.8.6\n",
    "peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d658ddb-6d49-41d4-9c90-ddf08c794141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/trl/run_fsdp_qlora.py\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.system(\"pip install flash-attn --no-build-isolation --upgrade\")\n",
    "except:\n",
    "    print(\"flash-attn failed to install\")\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from trl.commands.cli_utils import  TrlParser\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    BitsAndBytesConfig,\n",
    "        set_seed,\n",
    "\n",
    ")\n",
    "from trl import setup_chat_format\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "from trl import (\n",
    "   SFTTrainer)\n",
    "\n",
    "# Anthropic/Vicuna like template without the need for special tokens\n",
    "# Use the same template in inference\n",
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    dataset_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to the dataset\"\n",
    "        },\n",
    "    )\n",
    "    model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n",
    "    )\n",
    "    use_qlora: bool = field(default=False, metadata={\"help\": \"Whether to use QLORA\"})\n",
    "    merge_adapters: bool = field(\n",
    "        metadata={\"help\": \"Whether to merge weights for LoRA.\"},\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def training_function(script_args, training_args):\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    \n",
    "    train_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "    test_dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Model & Tokenizer\n",
    "    ################\n",
    "\n",
    "    # Tokenizer        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "    \n",
    "    # template dataset\n",
    "    def template_dataset(examples):\n",
    "        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "    \n",
    "    # print random sample\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log a few random samples from the processed training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Model    \n",
    "    torch_dtype = torch.bfloat16 if training_args.bf16 else torch.float32\n",
    "    quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "    if script_args.use_qlora:\n",
    "        print(f\"Using QLoRA - {torch_dtype}\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch_dtype,\n",
    "                bnb_4bit_quant_storage=quant_storage_dtype,\n",
    "            )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        script_args.model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        #device_map=\"auto\",\n",
    "        device_map={'':torch.cuda.current_device()},\n",
    "        attn_implementation=\"flash_attention_2\", # use sdpa, alternatively use \"flash_attention_2\"\n",
    "        torch_dtype=quant_storage_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    ################\n",
    "    # PEFT\n",
    "    ################\n",
    "    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    ################\n",
    "    # Training\n",
    "    ################\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,  # We template with special tokens\n",
    "            \"append_concat_token\": False,  # No need to add additional separator token\n",
    "        },\n",
    "    )\n",
    "    if trainer.accelerator.is_main_process:\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    ##########################\n",
    "    # Train model\n",
    "    ##########################\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    ##########################\n",
    "    # SAVE MODEL FOR SAGEMAKER\n",
    "    ##########################\n",
    "    sagemaker_save_dir = \"/opt/ml/model\"\n",
    "\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "    if script_args.merge_adapters:\n",
    "        # persist tokenizer\n",
    "        trainer.tokenizer.save_pretrained(sagemaker_save_dir)\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        print('########## Merging Adapters  ##########')\n",
    "        trainer.model.save_pretrained(training_args.output_dir)\n",
    "        trainer.tokenizer.save_pretrained(training_args.output_dir)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # load PEFT model\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            training_args.output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        # Merge LoRA and base model and persist weights\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        trainer.model.save_pretrained(sagemaker_save_dir, safe_serialization=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = HfArgumentParser((ScriptArguments, TrainingArguments))\n",
    "    script_args, training_args = parser.parse_args_into_dataclasses()    \n",
    "    \n",
    "    # set use reentrant to False\n",
    "    if training_args.gradient_checkpointing:\n",
    "        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "    # set seed\n",
    "    set_seed(training_args.seed)\n",
    "  \n",
    "    # launch training\n",
    "    training_function(script_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a1c53-bd50-47cb-b30f-996b32aa5b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "  ### SCRIPT PARAMETERS ###\n",
    "  'dataset_path': '/opt/ml/input/data/training/',    # path where sagemaker will save training dataset\n",
    "  'model_id': model_id,                              # or `mistralai/Mistral-7B-v0.1`\n",
    "  'max_seq_len': 3072,                               # max sequence length for model and packing of the dataset\n",
    "  'use_qlora': True,                                 # use QLoRA model\n",
    "  ### TRAINING PARAMETERS ###\n",
    "  'num_train_epochs': 2,                             # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                  # batch size per device during training\n",
    "  'per_device_eval_batch_size': 1,                   # batch size for evaluation    \n",
    "  'gradient_accumulation_steps': 4,                  # number of steps before performing a backward/update pass\n",
    "  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n",
    "  'optim': \"adamw_torch\",                            # use fused adamw optimizer\n",
    "  'logging_steps': 10,                               # log every 10 steps\n",
    "  'save_strategy': \"epoch\",                          # save checkpoint every epoch\n",
    "  'evaluation_strategy': \"epoch\",\n",
    "  'learning_rate': 0.0002,                           # learning rate, based on QLoRA paper\n",
    "  'bf16': use_bf16,                                      # use bfloat16 precision\n",
    "  'tf32': True,                                      # use tf32 precision\n",
    "  'max_grad_norm': 0.3,                              # max gradient norm based on QLoRA paper\n",
    "  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n",
    "  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n",
    "  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n",
    "  'output_dir': '/tmp/tun',                          # Temporary output directory for model checkpoints\n",
    "  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n",
    "  'fsdp': '\"full_shard auto_wrap offload\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7fa70-b99b-4746-b943-d7fc140762ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder \n",
    "import time\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'{model_id.replace(\"/\", \"-\")}-{\"bf16\" if use_bf16 else \"f32\" }-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b361e37-aa6a-415e-8156-11ce7c88493b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',    # train script\n",
    "    source_dir           = 'scripts/trl/',      # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment          = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # Retrieve HuggingFace Token to be used for downloading base models from\n",
    "        \"ACCELERATE_USE_FSDP\":\"1\", \n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\":\"1\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ef98c-f4b4-45de-9dae-4d97321dda04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    " \n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41fabd-1a25-4525-bb84-8ecfb4e7975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_files_path = huggingface_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "print(\"Model artifacts stored in: \", s3_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376ab89-d2ec-45bc-b5fa-3926e11d4d65",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Go to the AWS console and, on the left-hand size, click on `Imported models` under `Foundation models`.\n",
    "\n",
    "![imported models](images/text-2-sql-imported-models-menu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61201b2f-9d89-4796-a7ba-57eb058ee82d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Click on `Import model`.\n",
    "\n",
    "![import model](images/text-2-sql-import-model-button.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa3478-077c-4d81-941b-f8262f190f91",
   "metadata": {},
   "source": [
    "Use `llama-3-8b-text-to-sql` as the `Model name`, and enter the S3 location from above. Click on `Import model`.\n",
    "\n",
    "![import model job](images/text-2-sql-import-model-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d4f44-48a6-4a8b-b132-4d7dabccea92",
   "metadata": {},
   "source": [
    "When the import job completes, click on `Models` to see your model. Copy the ARN because we will need it in the next steps.\n",
    "\n",
    "![import model job](images/text-2-sql-take-model-arn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dedae9-c1ad-4af9-9130-c164fdcc2c01",
   "metadata": {},
   "source": [
    "You can test the model using the `Bedrock Playground`. Select your model and enter a question, as shown below.\n",
    "\n",
    "![test model playground](images/text-2-sql-demo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c2fdd-fcaa-435a-a6ad-b12c888acd19",
   "metadata": {},
   "source": [
    "## Invoke the model\n",
    "\n",
    "We are going to use the `InvokeModel` API from the Bedrock runtime to call the model on our test data set. Pleace enter your custom model ARN under `model_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fc5b4-f6df-478f-8bba-f5f0414d4778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "region = sess.boto_region_name\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "model_id = \"<ENTER_YOUR_MODEL_ARN_HERE>\"\n",
    "\n",
    "assert model_id != \"<ENTER_YOUR_MODEL_ARN_HERE>\", \"ERROR: Please enter your model id\"\n",
    "\n",
    "def get_sql_query(system_prompt, user_question, max_retries=10, return_prompt=False):\n",
    "    \"\"\"\n",
    "    Generate a SQL query using Llama 3 8B\n",
    "    Remember to use the same template used in fine tuning\n",
    "    \"\"\"\n",
    "    formatted_prompt = (\n",
    "                            f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>\"\n",
    "                            f\"\\n\\nHuman: {user_question}[/INST]\"\n",
    "                            \"\\n\\nAssistant: \"\n",
    "                        )\n",
    "    if return_prompt:\n",
    "        print(formatted_prompt)\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        response = client.invoke_model(modelId=model_id, body=json.dumps(native_request))\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        if 'generation' in response_body:\n",
    "            response_text = response_body['generation'].strip()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Model does not appear to be ready. Retrying.\")\n",
    "            attempt += 1\n",
    "            time.sleep(30)\n",
    "\n",
    "    return response_text or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62075070-5431-4fbb-a706-70b17cacf2a9",
   "metadata": {},
   "source": [
    "Let us try a sample invocation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f92b5-6c64-43ca-af76-c7af60378729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You can use the following table schema for context: CREATE TABLE table_name_11 (tournament VARCHAR)\"\n",
    "user_question = \"Return the SQL query that answers the following question: Which Tournament has A in 1987?\"\n",
    "\n",
    "query = get_sql_query(system_prompt, user_question, return_prompt=True)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b048b48-964e-46be-8849-bb7438df8d59",
   "metadata": {},
   "source": [
    "Now let us go through the test data set and, using an LLM as a judge, we'll quantify how well the model approximates the correct SQL queries given in the data set. Since using an LLM as a judge takes time, we will only process 100 records for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac8149-9fa8-4859-84ce-89bf3c187f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_json(\"data/test_dataset.json\", lines=True)[\"messages\"]\n",
    "\n",
    "def extract_content(dicts, role):\n",
    "    for d in dicts:\n",
    "        if d['role'] == role:\n",
    "            return d['content']\n",
    "    return None\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for role in ['system', 'user', 'assistant']:\n",
    "    df[role] = test_df.apply(lambda x: extract_content(x, role))\n",
    "del test_df\n",
    "\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45e577-111e-48c4-872e-3c8db7c086fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['llama'] = df.apply(lambda row: get_sql_query(row['system'], row['user']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8641564-6d76-440e-905f-3547ed4f564b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7c05f-6468-4000-802d-c237c9c76ba7",
   "metadata": {},
   "source": [
    "### Evaluation using an LLM as a judge\n",
    "\n",
    "Since we have access to the \"right\" answer, we can evaluate similarity between the SQL queries returned by the fine-tuned Llama model and the right answer. Evaluation can be a bit tricky, since there is no single metric that evaluates semantic and syntactic similarity between two SQL queries. One alternative is to use a more powerful LLM, like Claude 3 Sonnet, to measure the similarity between the two SQL queries (LLM as a judge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2f4ec-f90d-4283-88c0-2efef499acca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function because Claude requires the Messages API\n",
    "\n",
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "my_config = Config(connect_timeout=60*3, read_timeout=60*3)\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)\n",
    "MAX_ATTEMPTS = 3\n",
    "\n",
    "def ask_claude(messages,system=\"\", model_version=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    prompt_json = {\n",
    "        \"system\": system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\": \"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    \n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(prompt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt += 1\n",
    "            if attempt > MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else: #retry in 2 seconds\n",
    "                time.sleep(2)\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736dd8e4-33f2-4343-ba3d-496d0acb34fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_score(system, user, assistant, llama):\n",
    "    db_schema = system[139:] # Remove generic instructions\n",
    "    question = user[58:] # Remove generic instructions\n",
    "    correct_answer = assistant\n",
    "    test_answer = llama\n",
    "    formatted_prompt = (\n",
    "        \"You are a data science teacher that is introducing students to SQL. Consider the following question and schema:\\n\"\n",
    "        f\"<question>{question}</question>\\n\"\n",
    "        f\"<schema>{db_schema}</schema>\\n\"\n",
    "        \"Here is the correct answer:\\n\"\n",
    "        f\"<correct_answer>{correct_answer}</correct_answer>\\n\"\n",
    "        \"Here is the student's answer:\\n\"\n",
    "        f\"<student_answer>{test_answer}<student_answer>\\n\\n\"\n",
    "        \"Please provide a numeric score from 0 to 100 on how well the student's answer matches the correct answer for this question.\\n\"\n",
    "        \"The score should be high if the answers say essentially the same thing.\\n\"\n",
    "        \"The score should be lower if some parts are missing, or if extra unnecessary parts have been included.\\n\"\n",
    "        \"The score should be 0 for an entirely wrong answer. Put the score in <SCORE> XML tags.\\n\"\n",
    "        \"Do not consider your own answer to the question, but instead score based only on the correct answer above.\"\n",
    "    )\n",
    "\n",
    "    _, result = ask_claude(formatted_prompt, model_version=\"sonnet\")\n",
    "    pattern = r'<SCORE>(.*?)</SCORE>'\n",
    "    match = re.search (pattern, result)\n",
    "    \n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74077b5f-e1e0-450c-8628-edbf66bf26dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for ix in range(len(df)):\n",
    "    response = float(get_score(df[\"system\"][ix], df[\"user\"][ix], df[\"assistant\"][ix], df[\"llama\"][ix]))\n",
    "    scores.append(response)\n",
    "print(\"Assigned scores: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69ebd7-42de-44ef-a581-f393d677bf7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The average score of the fine tuned model is: \", sum(scores)/float(len(scores)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639101c-f61d-4933-813d-9ce619b14314",
   "metadata": {},
   "source": [
    "The average score given to this fine-tuned Llama 3 8B model is approximately 95%, which is very good for a relatively small language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830e8ef-f306-459c-a7e4-13f0521b05bb",
   "metadata": {},
   "source": [
    "### Clean Up \n",
    "\n",
    "You can delete your Imported Model in the console as shown in the image below:\n",
    "\n",
    "![Delete](./images/delete.png \"Delete\")\n",
    "\n",
    "Ensure to shut down your instance/compute that you have run this notebook on.\n",
    "\n",
    "**END OF NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
