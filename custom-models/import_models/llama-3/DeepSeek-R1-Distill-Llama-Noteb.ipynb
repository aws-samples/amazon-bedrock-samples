{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170cd66c-9f7f-4db9-9673-31b4d7e9e1fe",
   "metadata": {},
   "source": [
    "# Import DeepSeek-R1-Distill-Llama Models to Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to import DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI) feature. We'll use the 8B parameter model as an example, <u>but the same process applies to the 70B variant</u>.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "DeepSeek has released several distilled versions of their models based on Llama architecture. These models maintain strong performance while being more efficient than their larger counterparts. The 8B model we'll use here is derived from Llama 3.1 and has been **optimized for reasoning tasks**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM roles and permissions for Bedrock and Amazon S3, following [the instruction here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)\n",
    "- A S3 bucket prepared to store the custom model\n",
    "- Sufficient local storage space (At least 17GB for 8B and 135GB for 70B models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15844386-cbc9-49c9-8dbe-fcbf2dfab1eb",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e0984-0d83-4e83-8710-8d4870444af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install boto3 --upgrade\n",
    "!pip install -U huggingface_hub\n",
    "!pip install hf_transfer huggingface huggingface_hub \"huggingface_hub[hf_transfer]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec46496-b12a-407e-8ad5-bf4e60a7bf97",
   "metadata": {},
   "source": [
    "### Step 2: Configure Parameters\n",
    "\n",
    "Update these parameters according to your AWS environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd8cb8-d3c0-4e6c-ba58-071cdee2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters (please update this part based on your setup)\n",
    "bucket_name = \"<YOUR-PREDEFINED-S3-BUCKET-TO-HOST-IMPORT-MODEL>\"\n",
    "s3_prefix = \"<S3-PREFIX>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "local_directory = \"<LOCAL-FOLDER-TO-STORE-DOWNLOADED-MODEL>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "\n",
    "job_name = '<CMI-JOB-NAME>' # E.x. Deepseek-8B-job\n",
    "imported_model_name = '<CMI-MODEL-NAME>' # E.x. Deepseek-8B-model\n",
    "role_arn = '<IAM-ROLE-ARN>' # Please make sure it has sufficient permission as listed in the pre-requisite\n",
    "\n",
    "# Region (currently only 'us-west-2' and 'us-east-1' support CMI with Deepseek-Distilled-Llama models)\n",
    "region_info = 'us-west-2' # You can modify to 'us-east-1' based on your need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffabe2-0c4b-431a-aadb-167f5cb26fa4",
   "metadata": {},
   "source": [
    "### Step 3: Download Model from Hugging Face\n",
    "\n",
    "Download the model files from Hugging Face. \n",
    "\n",
    "- Note that you can also use the 70B model by changing the model_id to \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7789f-958b-4459-a345-67891c5c86ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Downloading the 8B model files may take 2-10 minutes depending on your internet connection speed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12e1c1-2aec-4fda-9199-a32d95d2e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Enable hf_transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Download using snapshot_download with hf_transfer enabled\n",
    "snapshot_download(repo_id=hf_model_id, local_dir=f\"./{local_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557e7ac-0626-4b9a-ba62-d2207e25cd23",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to S3\n",
    "\n",
    "Upload the downloaded model files to your S3 bucket\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Uploading the 8B model files normally takes 10-20 minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed56d4-ee79-4378-8a77-105889f840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory, bucket_name, s3_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "    \n",
    "    # Get list of all files first\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "    \n",
    "    # Upload with progress bar\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(\n",
    "                str(local_path),\n",
    "                bucket_name,\n",
    "                s3_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Upload all files\n",
    "upload_directory_to_s3(local_directory, bucket_name, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042657-a7e1-4063-abb5-73ea1f8cda80",
   "metadata": {},
   "source": [
    "### Step 5: Create Custom Model Import Job\n",
    "\n",
    "Initialize the import job in Amazon Bedrock\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Creating CMI job for 8B model could take 5-20 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f898ec5-6123-4647-8852-456045efcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=region_info)\n",
    "\n",
    "s3_uri = f's3://{bucket_name}/{s3_prefix}/'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a18c49-e5a9-4706-abe5-f11e1dfa4c3a",
   "metadata": {},
   "source": [
    "### Step 6: Monitor Import Job Status\n",
    "\n",
    "Check the status of your import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4235f3-4136-43a8-b461-f7c4e96e174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d199-ffcf-4ffd-803e-b05e1c148f88",
   "metadata": {},
   "source": [
    "### Step 7: Wait for Model Initialization\n",
    "\n",
    "Allow time for the model to initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e3ed-4cb8-4e9e-9740-3e972fec8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5mins for cold start \n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319747f-4b8f-41b7-abf2-a046e0a23d51",
   "metadata": {},
   "source": [
    "### Step 8: Model Inference with Proper Tokenization\n",
    "\n",
    "#### Understanding the Tokenization Process\n",
    "When working with DeepSeek models, proper tokenization is crucial for optimal performance. The model expects inputs to follow a specific format defined in its `tokenizer_config.json`. This format ensures the model receives prompts in the same structure it was trained on.\n",
    "\n",
    "#### Key Components\n",
    "1. **Tokenizer**: Uses HuggingFace's AutoTokenizer to properly format inputs\n",
    "2. **Generation Function**: Handles the core interaction with the model\n",
    "3. **Auto-Generate Function**: Manages longer responses that might exceed token limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88862d-930e-4721-84d4-46705c2b7908",
   "metadata": {},
   "source": [
    "#### 8.1 Setting Up the Tokenizer\n",
    "First, we'll initialize the tokenizer and Bedrock runtime client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995cabd-9148-4ce8-b458-c0921130a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region_info,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a8b29-6110-4874-9c76-264fb89b188e",
   "metadata": {},
   "source": [
    "#### 8.2 Core Generation Function\n",
    "\n",
    "This function handles the basic model interaction with proper tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f07be7-7fa4-4deb-84c8-ec1de9829d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                         add_generation_prompt=not continuation)\n",
    "    \n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51f911-7368-4244-8291-11ec18fb031a",
   "metadata": {},
   "source": [
    "#### 8.3 Extended Generation Function\n",
    "\n",
    "The thinking process of the model can become quite extensive, especially when dealing with complex reasoning problems that require step-by-step analysis. This often exceeds the output context length we set for the model. To address this limitation:\n",
    "\n",
    "1. We first attempt to generate a complete response\n",
    "2. If the response is truncated (indicated by stop_reason = \"length\"), we:\n",
    "   - Concatenate the partial response to the original prompt\n",
    "   - Make another API call with `continuation=True`\n",
    "   - This sets `add_generation_prompt=False` in the tokenizer call\n",
    "3. This process continues until we get a complete response\n",
    "\n",
    "This approach ensures we capture the model's complete reasoning process while maintaining coherence throughout the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfe60b-6fb8-4ade-a40f-52aaf592e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_generate(messages, **kwargs):\n",
    "    \"\"\"\n",
    "    Handle longer responses that exceed token limit\n",
    "    \n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries\n",
    "        **kwargs: Additional parameters for generate function\n",
    "    \n",
    "    Returns:\n",
    "        dict: Enhanced response including thinking process and final answer\n",
    "    \"\"\"\n",
    "    res = generate(messages, **kwargs)\n",
    "    while res[\"stop_reason\"] == \"length\":\n",
    "        for v in messages:\n",
    "            if v.get(\"role\") == \"user\":\n",
    "               v[\"content\"] += res[\"generation\"]\n",
    "        res = generate(messages, **kwargs, continuation=True)\n",
    "\n",
    "    for v in messages:\n",
    "        if v.get(\"role\") == \"user\":\n",
    "           gen = v[\"content\"] + res[\"generation\"]\n",
    "           answer = gen.split(\"</think>\")[-1]\n",
    "           think = gen.split(\"</think>\")[0].split(\"<think>\")[-1]\n",
    "           res = {**res, \"generation\": gen, \"answer\": answer, \"think\": think}\n",
    "           return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15a690-1d6a-4b31-ac67-1975ede304ff",
   "metadata": {},
   "source": [
    "### Usage Examples\n",
    "#### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874032c-1f2f-483f-81da-62bcdab2adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = generate(messages)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634701c-1485-4038-bef3-83dbec966fc8",
   "metadata": {},
   "source": [
    "#### Advanced Usage with Complex Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586afd6-a4f0-4ee8-bb2f-82a48fb9bc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"Solve the following optimization problem:\n",
    "\n",
    "A manufacturing company produces two types of products: A and B. \n",
    "They need to determine the optimal production quantities to maximize profit.\n",
    "\n",
    "Given constraints:\n",
    "1. Manufacturing capacity: 60 hours per week\n",
    "2. Product A takes 4 hours to produce\n",
    "3. Product B takes 3 hours to produce\n",
    "4. Storage space can hold maximum 20 units total\n",
    "5. Profit per unit:\n",
    "   - Product A: $200\n",
    "   - Product B: $150\n",
    "6. Minimum required production:\n",
    "   - At least 3 units of Product A\n",
    "   - At least 2 units of Product B\n",
    "\n",
    "Please:\n",
    "1. Set up the linear programming equations\n",
    "2. Solve step by step\n",
    "3. Verify all constraints are met\n",
    "4. Calculate maximum profit\n",
    "5. Analyze sensitivity to changes in constraints\n",
    "6. Recommend optimal production plan\n",
    "\n",
    "Show all your work and reasoning at each step.\"\"\"\n",
    "\n",
    "# System prompt to encourage detailed mathematical reasoning\n",
    "system_prompt = \"\"\"You are a mathematical optimization expert. \n",
    "Please provide detailed step-by-step solutions showing:\n",
    "- All equations and their development\n",
    "- Each calculation step\n",
    "- Verification of constraints\n",
    "- Clear reasoning for each decision\n",
    "- Visual representations where helpful\"\"\"\n",
    "\n",
    "# Run the analysis with auto_generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": complex_prompt}\n",
    "]\n",
    "\n",
    "response = auto_generate(messages, temperature=0.7, max_tokens=4096, top_p=0.9)\n",
    "\n",
    "# Display the response\n",
    "print(\"\\n=== Thinking Process ===\")\n",
    "display(Markdown(response[\"think\"]))\n",
    "print(\"\\n=== Solution ===\")\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24455a7f-89eb-4538-aadd-784ab1a817d7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the end-to-end process of importing DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI). Starting from downloading the model from HuggingFace, through preparing and uploading files to S3, to creating a CMI job and performing inference, we've covered the essential steps to get your DeepSeek distilled Llama models running on Amazon Bedrock.\n",
    "\n",
    "\n",
    "While we've used the DeepSeek-R1-Distill-Llama-8B model in this example, the same process applies to other variants including the 70B model. For more information about Custom Model Import and its features, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
