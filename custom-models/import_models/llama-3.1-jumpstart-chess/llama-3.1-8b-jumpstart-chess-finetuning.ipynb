{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9356ca0f-ee2c-4796-9c1e-f9968f146783",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "In this notebook Llama 3.1 8b will be fine-tuned to play chess. The model will be pulled from the Sagemaker Jumpstart model hub and fine-tuned using a Sagemaker training job. Sagemaker Jumpstart is a model hub with 400+ LLM's that can be deployed as is or fine-tuned. An overview of Llama 3.1 on Sagemaker Jumpstart can be found [here](https://aws.amazon.com/blogs/machine-learning/meta-llama-3-1-models-are-now-available-in-amazon-sagemaker-jumpstart/). \n",
    "\n",
    "This notebook can be run on any environment of the user's choice as the training compute will be offloaded into a \"ml.g5.24xlarge\". If this notebook is run outside of an Amazon Sagemaker environment, please ensure the AWS user credentials are correctly initialized. Some environments this notebook can be run on some examples:\n",
    "\n",
    "1. Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: Link\n",
    "2. Configuring an Amazon Sagemaker environment: Link\n",
    "3. Configure your own environment, with adequate compute to run this notebook.\n",
    "\n",
    "The Chess moves dataset is pulled from [here](https://www.pgnmentor.com/) under players --> Carlsen.  This dataset outlines the board state in FEN notation, and states the next legal move based on the board state & player turn. More information about what FEN notation is and how to interpret it can be found [here](https://www.chess.com/terms/fen-chess).\n",
    "\n",
    "Once the model is imported at the end of this notebook, please open \"test_chess_model.ipynb\" to use the model to play chess. If this is the intention skip the \"Clean up\" section at the bottom of this notebook.\n",
    "\n",
    "**_NOTE:_** This notebook was tested in the us-east-1 region of AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023fdae-56b9-48f0-b641-a11b0f6ff281",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via Custom Model Import (CMI).\n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e25183-991a-4447-8c33-c4faaa60622b",
   "metadata": {},
   "source": [
    "## What will you learn from this notebook\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "* Pull a model from Sagemaker Jumpstart & finetune it \n",
    "* Use a custom dataset & process it to conform to a prompt template of choice\n",
    "* Finetune the model (Llama 3.1 8b) using Sagemaker training jobs\n",
    "* Deploy the finetuned model to Amazon Bedrock Custom Import & Conduct Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9e3ff-de16-4d9e-954c-d8b87dbfdb35",
   "metadata": {},
   "source": [
    "## Architectural Pattern\n",
    "\n",
    "![Diagram](./images/architecture.png \"Architecture\")\n",
    "\n",
    "As can be seen from the diagram above, the model (Llama 3.1 8b) gets pulled from Sagemaker Jumpstart, and The dataset is a chess dataset from pgnmentor.com mirroring Magnus Carlson's games. The model files are then stored in an S3 bucket, to then be imported into Amazon Bedrock. This architecture is modular because the Notebook can be run anywhere, that the appropriate compute is available (as explained earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710d1bf7-49d5-4fd3-b0bb-93c3dc14a556",
   "metadata": {},
   "source": [
    "## Code with comments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbd9ec-4dec-476a-b971-3ef2aac6b91b",
   "metadata": {},
   "source": [
    "### Install dependencies & restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c118a-68f5-4d97-ba10-4933336d17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U 'aiobotocore[boto3]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c4ddc-72f1-4338-b814-e5362fb6ed39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install setuptools==69.0.2\n",
    "!pip install chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30660db-e035-46cd-bc07-6fb7282c5336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5744b166-8103-45e2-a5db-12523f4866e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656af867-994c-4216-a063-b714f7f2777c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45ae70-e8b3-4c6f-bd0f-6691596ac3f0",
   "metadata": {},
   "source": [
    "### Setup Sagemaker client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925d8cc-e1b9-4c91-97ca-0f731d027051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "print(f\"Sagemaker Version - {sagemaker.__version__}\")\n",
    "print(f\"Transformers version - {transformers.__version__}\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b4984-d169-40aa-a2a3-283c3c8edd49",
   "metadata": {},
   "source": [
    "### Identify model id from Sagemaker Jumpstart & Setup data location\n",
    "Sagemaker jumpstart has unique identifiers for each model present. Below is the model id & the model version for Llama 3.1 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85416e-71a5-4c4f-81d9-0f90a97a7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-1-8b\", \"2.2.2\"\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975db8d-dc68-4ca0-bbd9-9188cada66d6",
   "metadata": {},
   "source": [
    "In the cell below the training data location is being specified to the Sagemaker default S3 bucket. In addition the local training/validation paths are being identified to store in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25e4e1-1e1f-40b0-ac9f-019855a01b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_data_location = f's3://{sess.default_bucket()}/datasets/3-1-8b'\n",
    "local_train_data_file = \"data/train.jsonl\"\n",
    "local_test_data_file = \"data/validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f63be-a997-46eb-ae73-4221bba7521a",
   "metadata": {},
   "source": [
    "## Dataset Download section \n",
    "\n",
    "the dataset is being downloaded from [pgnmentor.com](https://www.pgnmentor.com/) this website allows for the extensive viewing & analysis of previously played chess games. The data that will be used to fine tune this model will be data from some of Magnus Carlsen's games. The cells below outline the process of downloading the data to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988cd8b-a22d-450a-9362-936aff226376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Carlsen's games zip file to the target directory\n",
    "!curl -o ./data/Carlsen.zip https://www.pgnmentor.com/players/Carlsen.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e85449-9a69-4d1c-b45c-ab8ee3531917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file in the target directory\n",
    "!unzip ./data/Carlsen.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d90ee2-a1f4-4e21-9219-17b925040a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the unzipped folder to 'top'\n",
    "!mv ./data/Carlsen.pgn ./data/top.pgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0764c15-0d97-46f4-9250-4be38201efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the zip file after unzipping\n",
    "!rm ./data/Carlsen.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa65b0-93b3-45cf-a3cb-79ec4c695d7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Processing & Validation \n",
    "In the cell below the data is being processed to fit the correct format, and being saved to the local directory.\n",
    "Some points of importance in the cell below:\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e21013-505a-4b95-870b-495ae9e3512b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_chess_message(fen, next_move_color, best_move_san=None):\n",
    "    instruction = \"You are a chess engine. Analyze the given position and generate the next best valid move in SAN format.\"\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"fen\": fen,\n",
    "        \"nxt-color\": next_move_color,\n",
    "        \"move\": best_move_san if best_move_san else None\n",
    "    }\n",
    "\n",
    "def process_sample(sample):\n",
    "    return create_chess_message(\n",
    "        fen=sample['fen'],\n",
    "        next_move_color=sample['nxt_color'],\n",
    "        best_move_san=sample.get('move')\n",
    "    )\n",
    "\n",
    "# Initialize a counter for the number of records written (set to 35000000 for the full dataset)\n",
    "#set to 100 by default to finish the training job fast\n",
    "records_written = 0\n",
    "max_records = 100\n",
    "\n",
    "# Your existing code for creating data.json remains the same\n",
    "pathlist = Path(\"data/top/\").glob('**/*.pgn')\n",
    "with open(\"data/top/data.json\", 'w') as f:\n",
    "    for path in pathlist:\n",
    "        print(f'File being processed - {path}')\n",
    "        pgn = open(path)\n",
    "        while True:\n",
    "            if records_written >= max_records:\n",
    "                print(f\"Reached {max_records} records. Stopping.\")\n",
    "                break\n",
    "            try:\n",
    "                game = chess.pgn.read_game(pgn)\n",
    "                if game is None:\n",
    "                    break\n",
    "                else:\n",
    "                    result = game.headers[\"Result\"]\n",
    "                    if result == \"1-0\":\n",
    "                        winner = \"WHITE\"\n",
    "                    elif result == \"0-1\":\n",
    "                        winner = \"BLACK\"\n",
    "                    else:\n",
    "                        winner = \"DRAW\"\n",
    "\n",
    "                    board = chess.Board()  # Create a new board for each game\n",
    "                    for move in game.mainline_moves():\n",
    "                        current_color = \"WHITE\" if board.turn else \"BLACK\"\n",
    "                        if winner == \"DRAW\" or current_color == winner: #consider the moves only if it's a winner's move or neutral\n",
    "                            if move in board.legal_moves:\n",
    "                                move_json = {\n",
    "                                    \"move\": board.san(move),\n",
    "                                    \"fen\": board.fen(),\n",
    "                                    \"nxt_color\": current_color,\n",
    "                                }\n",
    "                                f.write(json.dumps(move_json) + \"\\n\")\n",
    "                                records_written += 1\n",
    "                                board.push(move)\n",
    "                            else:\n",
    "                                print(f\"Illegal move encountered: {board.san(move)} in position {board.fen()}. Skipping this move.\")\n",
    "                        else:\n",
    "                            board.push(move)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing game: {str(e)}. Skipping this game.\")\n",
    "\n",
    "        if records_written >= max_records:\n",
    "            break\n",
    "            \n",
    "print(f\"Total records written: {records_written}\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"data/top/data.json\", split=\"train[:100%]\")\n",
    "\n",
    "# Create a 70/30 train/test split\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "# Shuffle each split\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].shuffle(seed=42)\n",
    "\n",
    "# Apply the processing function to each split\n",
    "dataset = dataset.map(\n",
    "    process_sample,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    num_proc=os.cpu_count()  # Use multiple processes for faster processing\n",
    ")\n",
    "\n",
    "dataset = dataset.remove_columns(\"instruction\") # Not needed as we are adding the instruction in template\n",
    "\n",
    "print(f'New schema for dataset: {dataset}')\n",
    "print(f'\\nDataset sizes:')\n",
    "for split in dataset:\n",
    "    print(f'{split}: {len(dataset[split])} samples')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Save the processed datasets\n",
    "dataset['train'].save_to_disk(\"data/processed_chess_train\")\n",
    "dataset['test'].save_to_disk(\"data/processed_chess_test\")\n",
    "\n",
    "dataset[\"train\"].to_json(local_train_data_file, orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(local_test_data_file, orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d29c7-7ab1-4f23-9ef2-f2869b191a99",
   "metadata": {},
   "source": [
    "Using the Python Chess library, the dataset is being validated to ensure all recommended moves are valid. This will ensure the LLM has 100% true examples to learn from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d33250-d0ab-4daa-978a-0bb022b832be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validating the dataset created has all valid and legal moves only - for the safer side!\n",
    "import json\n",
    "import chess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_move(fen, move_san):\n",
    "    try:\n",
    "        board = chess.Board(fen)\n",
    "        # Parse SAN move and validate it's legal\n",
    "        move = board.parse_san(move_san)\n",
    "        return move in board.legal_moves\n",
    "    except ValueError:\n",
    "        # If the move can't be parsed in SAN format\n",
    "        return False\n",
    "\n",
    "def validate_data_json(file_path):\n",
    "    valid_count = 0\n",
    "    invalid_count = 0\n",
    "    invalid_records = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(tqdm(lines, desc=\"Validating moves\")):\n",
    "        record = json.loads(line)\n",
    "        fen = record['fen']\n",
    "        move = record['move']\n",
    "\n",
    "        try:\n",
    "            if validate_move(fen, move):\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "                invalid_records.append((i, fen, move))\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected errors\n",
    "            invalid_count += 1\n",
    "            invalid_records.append((i, fen, f\"{move} (Error: {str(e)})\"))\n",
    "\n",
    "    print(f\"Total records: {len(lines)}\")\n",
    "    print(f\"Valid moves: {valid_count}\")\n",
    "    print(f\"Invalid moves: {invalid_count}\")\n",
    "\n",
    "    if invalid_records:\n",
    "        print(\"\\nInvalid records:\")\n",
    "        for record in invalid_records[:10]:  # Print first 10 invalid records\n",
    "            print(f\"Line {record[0]}: FEN: {record[1]}, Move: {record[2]}\")\n",
    "        \n",
    "        if len(invalid_records) > 10:\n",
    "            print(f\"... and {len(invalid_records) - 10} more.\")\n",
    "\n",
    "    return valid_count == len(lines)\n",
    "\n",
    "# Run the validation\n",
    "file_path = \"data/top/data.json\"\n",
    "all_valid = validate_data_json(file_path)\n",
    "\n",
    "if all_valid:\n",
    "    print(\"All moves are valid!\")\n",
    "else:\n",
    "    print(\"Some moves are invalid. Please check the output above for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7726-0085-4153-b8fe-3e62607f98a3",
   "metadata": {},
   "source": [
    "Print random samples from the dataset to view the cleaned & prepped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f8cec-ce7d-4751-924a-eb3ae421259d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print random samples\n",
    "for split in ['train', 'test']:\n",
    "    print(f\"\\nRandom samples from {split} set:\")\n",
    "    for index in random.sample(range(len(dataset[split])), 2):\n",
    "        print(dataset[split][index][\"fen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f32cee-a1a3-4e4e-aeb2-3cb2eecbb42d",
   "metadata": {},
   "source": [
    "Create the prompt template. This template is needed by Sagemaker Jumpstart to understand the given template from the dataset. This file must be in the same directory as the training data, and named \"template.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5c469-977d-4d9e-aa7e-57563a8e32b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": (\n",
    "        \"<s>[INST] You are a chess engine. Given a chess position in FEN notation and the color to move, \"\n",
    "        \"provide the next best valid move in SAN (Standard Algebraic Notation) format to progress towards winning the game of chess. \"\n",
    "        \"Your response must be a single move wrapped in <move></move> tags.\\n\\n\"\n",
    "        \"Chess Position (FEN): {fen}\\n\"\n",
    "        \"Color to Move: {nxt-color} [/INST]\"\n",
    "    ),\n",
    "    \"completion\": \" <move>{move}</move> </s>\"\n",
    "}\n",
    "\n",
    "with open(\"data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3b7cd-8e7e-4a94-83c6-807b33451c74",
   "metadata": {},
   "source": [
    "In the final step of dataset preparation the training data, testing data, and prompt template will be uploaded to the S3 buckets previously initialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d50d9e-79a1-4a7d-aee8-d4dba26b645d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "S3Uploader.upload(local_train_data_file, train_test_data_location)\n",
    "S3Uploader.upload(local_test_data_file, train_test_data_location)\n",
    "S3Uploader.upload(\"data/template.json\", train_test_data_location)\n",
    "\n",
    "print(f\"Training data: {train_test_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e7322-fec1-4228-9874-10c83a00787b",
   "metadata": {},
   "source": [
    "### Submit the training job\n",
    "\n",
    "An estimator object is needed by Sagemaker to submit training jobs. In this estimator object there are a some items that should be taken note of:\n",
    "\n",
    "* The model id & version are being passed into the estimator.\n",
    "* The eula must be set to \"true\" - this is due to different LLM's on Sagemaker Jumpstart, being from different model providers. Each provider has its own eula.\n",
    "* instance type - this is the compute instance(s) being used to conduct the fine tuning job on.\n",
    "* The .fit method for the estimator is what actually submits the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077696ae-c7aa-4aed-9998-94478f72614d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change this to {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.24xlarge\" #please change to \"ml.g5.48xlarge\" if max records are being used. For the default of 100 the default instance type set here is fine.\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=True, epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_test_data_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09e6a2-6353-4f12-8a07-ba9d49b2e2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709d760-2285-4991-8b6b-5f8264d53912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rivchess_model_src = {\"s3DataSource\": {\"s3Uri\": f'{ estimator.model_data[\"S3DataSource\"][\"S3Uri\"] }'}}\n",
    "%store rivchess_model_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b7bb9-278f-4048-8844-800313414241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Copy the model files to the AWS account/bucket where the model should be deployed\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Create S3 clients with optimized config\n",
    "config = Config(\n",
    "    retries={'max_attempts': 3},\n",
    "    tcp_keepalive=True\n",
    ")\n",
    "\n",
    "def copy_files():\n",
    "    # Configuration\n",
    "    source_bucket = 'BUCKET-WHERE-MODEL-WAS-SAVED' \n",
    "    source_prefix = 'PREFIX-WHERE-MODEL-WAS-SAVED'\n",
    "    dest_bucket = 'BUCKET-WHERE-MODEL-WILL-GO'\n",
    "    dest_prefix = 'PREFIX-WHERE-MODEL-WILL-GO'\n",
    "    \n",
    "    # Initialize S3 clients\n",
    "    s3_source = boto3.client('s3', region_name='us-east-1', config=config)\n",
    "    s3_dest = boto3.client('s3', region_name='us-west-2', config=config)\n",
    "    \n",
    "    # Get list of all objects\n",
    "    print(\"Fetching list of files...\")\n",
    "    paginator = s3_source.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=source_bucket, Prefix=source_prefix)\n",
    "    \n",
    "    # Prepare file list with sizes\n",
    "    files_to_copy = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                source_key = obj['Key']\n",
    "                size_mb = obj['Size'] / (1024 * 1024)  # Convert to MB\n",
    "                total_size += size_mb\n",
    "                files_to_copy.append({\n",
    "                    'source_key': source_key,\n",
    "                    'size_mb': size_mb,\n",
    "                    'dest_key': dest_prefix + source_key.replace(source_prefix, '')\n",
    "                })\n",
    "    \n",
    "    print(f\"Total files to copy: {len(files_to_copy)}\")\n",
    "    print(f\"Total size: {total_size:.2f} MB\")\n",
    "    \n",
    "    # Create DataFrame for tracking\n",
    "    df = pd.DataFrame(files_to_copy)\n",
    "    df['status'] = 'pending'\n",
    "    df['attempts'] = 0\n",
    "    \n",
    "    # Copy files with progress tracking\n",
    "    copied = 0\n",
    "    failed = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Update progress display\n",
    "            clear_output(wait=True)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Progress: {copied}/{len(files_to_copy)} files copied\")\n",
    "            print(f\"Failed: {failed} files\")\n",
    "            print(f\"Elapsed time: {elapsed_time:.1f} seconds\")\n",
    "            print(f\"Currently copying: {row['source_key']}\")\n",
    "            print(f\"File size: {row['size_mb']:.2f} MB\")\n",
    "            \n",
    "            # Copy the file\n",
    "            copy_source = {\n",
    "                'Bucket': source_bucket,\n",
    "                'Key': row['source_key']\n",
    "            }\n",
    "            \n",
    "            s3_dest.copy(\n",
    "                copy_source,\n",
    "                dest_bucket,\n",
    "                row['dest_key']\n",
    "            )\n",
    "            \n",
    "            df.at[index, 'status'] = 'completed'\n",
    "            copied += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError copying {row['source_key']}: {str(e)}\")\n",
    "            df.at[index, 'status'] = 'failed'\n",
    "            df.at[index, 'error'] = str(e)\n",
    "            df.at[index, 'attempts'] += 1\n",
    "            failed += 1\n",
    "            time.sleep(1)  # Brief pause before continuing\n",
    "    \n",
    "    # Final summary\n",
    "    clear_output(wait=True)\n",
    "    print(\"\\nTransfer Complete!\")\n",
    "    print(f\"Total files copied: {copied}\")\n",
    "    print(f\"Failed files: {failed}\")\n",
    "    print(f\"Total time: {(time.time() - start_time):.1f} seconds\")\n",
    "    \n",
    "    # Show failed files if any\n",
    "    if failed > 0:\n",
    "        print(\"\\nFailed files:\")\n",
    "        failed_df = df[df['status'] == 'failed']\n",
    "        display(failed_df[['source_key', 'size_mb', 'error', 'attempts']])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the copy operation\n",
    "transfer_results = copy_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68a9e7-9d0b-4cd3-a5d4-6c8774b1f186",
   "metadata": {},
   "source": [
    "### Import the fine tuned model into Bedrock using Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The fine tuned model will now be imported into Amazon Bedrock. This model will be able to be used via the invoke model & invoke model with response stream apis. This model will run in a serverless capacity. Take note that the first inference conducted after a 5 minute window of no inferences, will face a cold startup time. Subsequent inference requests will be conducted as normal, as long as there is less than a 5 minute window between them. To import the model an Import Job must be commenced, which is outlined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd2cc9-3e6d-4e2f-a30a-480e648be36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d221f-9999-47f3-8c1f-0d76bdac0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "br_run_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "model_split = model_id.split(\"/\")\n",
    "if len(model_split) > 1:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[1]}\"\n",
    "else:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[0]}\"\n",
    "\n",
    "rivchess_imp_jb_nm = f\"{rivchess_model_nm}-job-{datetime.datetime.now().strftime('%Y%m%d%M%H%S')}\"\n",
    "role_arn = role\n",
    "\n",
    "create_model_import_job_resp = br_client.create_model_import_job(jobName=rivchess_imp_jb_nm,\n",
    "                                  importedModelName=rivchess_model_nm,\n",
    "                                  roleArn=role_arn,\n",
    "                                  modelDataSource=rivchess_model_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4544ef6-da52-4f89-8e3e-99bb884d8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model_import_jobs_response = br_client.list_model_import_jobs(\n",
    "    nameContains=rivchess_imp_jb_nm)\n",
    "\n",
    "print(f\"BR CMI Import Job - {create_model_import_job_resp['jobArn']} is - {list_model_import_jobs_response['modelImportJobSummaries'][0]['status']}\")\n",
    "while list_model_import_jobs_response['modelImportJobSummaries'][0]['status'] != 'Completed':\n",
    "    interactive_sleep(30)\n",
    "    list_model_import_jobs_response = br_client.list_model_import_jobs(nameContains=rivchess_imp_jb_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20476bbc-9b2b-4df4-8016-d0d2527287c0",
   "metadata": {},
   "source": [
    "### Invoke the imported model using Bedrock API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b66cd-a346-4f6a-948f-a9720a2dee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5778e6-d1e0-4d2d-9684-06bc410bfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_imported_model_response = br_client.get_imported_model(\n",
    "        modelIdentifier=rivchess_model_nm\n",
    "    )\n",
    "\n",
    "    br_model_id = get_imported_model_response['modelArn']\n",
    "    br_model_id\n",
    "except br_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Model not yet imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1812b-7ccd-405b-8e00-676ae4b80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_invoke_model_and_print(native_request):\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = br_run_client.invoke_model(modelId=br_model_id, body=request)\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        # print(f\"model_response: {model_response}\")\n",
    "        response_text = model_response['generation'].replace(\"\\n\", \"\").replace(\"### Response:\", \"\")\n",
    "        return response_text\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{br_model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9790b5-1d6a-49d8-ae59-03fd9d90382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a103cd-4a69-4307-9fb4-46e1529ebeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_color = \"BLACK\"\n",
    "board_fen = \"6k1/p1q1prbp/b3n1p1/2pPPp2/5P1Q/4BN2/Pr2N1PP/R1R4K b - - 0 21\"\n",
    "\n",
    "# Format the prompt using the template\n",
    "formatted_prompt = template[\"prompt\"].format(\n",
    "    fen=board_fen,\n",
    "    nxt_color=move_color\n",
    ")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cc1fd-4022-4621-a192-785aa2167613",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_color = \"WHITE\"\n",
    "board_fen = \"1r1qk2r/p2bppbp/n2P1np1/1pp5/3P4/2P1BB2/PP2QPPP/RN2K1NR w KQk - 3 11\"\n",
    "\n",
    "# Format the prompt using the template\n",
    "formatted_prompt = template[\"prompt\"].format(\n",
    "    fen=board_fen,\n",
    "    nxt_color=move_color\n",
    ")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb9e06-7481-4054-9a3f-c2b2442e6d4c",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "Once the model will no longer be used, it may be deleted with the cell below. If additional testing for the model needs to be done, the \"test_chess_model.ipynb\" notebook can be opened to test the model against a Chess engine called Stockfish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb94591-000e-4b58-a47b-a5d75d3856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_imported_model_response = br_client.delete_imported_model(\n",
    "    modelIdentifier=br_model_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
