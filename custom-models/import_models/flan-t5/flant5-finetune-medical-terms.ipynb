{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c81179-f87d-4632-a94c-b16d53230cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine tuning & deploying Flan-T5-Large to Amazon Bedrock using Custom Model Import (Using PEFT & SFTTrainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4a809-1335-49ba-a091-5eae8cb883d1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "this notebook was tested on a [\"g5.12xlarge\"](https://aws.amazon.com/ec2/instance-types/g5/) instance. \n",
    "\n",
    "This notebook will use the HuggingFace Transformers library to fine tune FLAN-T5-Large. due to the fine tuning process being \"local\" you can run this notebook anywhere as long as the proper compute is available. \n",
    "\n",
    "1. Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: [Link](https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html)\n",
    "2. Configuring an Amazon Sagemaker environment: [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html)\n",
    "3. Configure your own environment, with equivalent compute\n",
    "\n",
    "This notebook covers the step by step process of fine tuning a [FLAN-T5 large](https://huggingface.co/google/flan-t5-large) mode and deploying it using Bedrock's [Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html) feature. \n",
    "The fine tuning data we will be using is based on medical terminology this data can be found on HuggingFace [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). In many discplines, industry specific jargon is used and an LLM may not have the correct understanding of words, context or abbreviations. By fine tuning a model on medical terminology in this case, the LLM is given the ability to understand specific jargon and answer questions the user might have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bc7f8-4200-4282-b5ab-a3ad7bdcf201",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via [Custom Model Import (CMI)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html). \n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6117f5-aa36-474f-973d-4ecc69602077",
   "metadata": {},
   "source": [
    "## Use Case \n",
    "\n",
    "Often times the broad knowledge of Foundation Models allows for usage in multiple use cases. In some workflows, where domain specific jargon is required, out of the box foundation models may not be sufficient in recognizing or defining terms. In this example the model will be finetuned on Medical Terminology, to be able to recognize & define medical domain specific terms. A customized model such as this can be useful for patients who may receive a diagnosis and need it broken down in simpler, everyday terms. This type of customized model could even help medical students while they study.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b59fd-84c9-408b-b135-b7b6c1e62c68",
   "metadata": {},
   "source": [
    "## What will you learn from this notebook\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "* Pull a model from the HuggingFace Hub & quantize it\n",
    "* Pull a dataset from HuggingFace & Process it \n",
    "* Finetune the model (Flan-T5)\n",
    "* Deploy the finetuned model to Amazon Bedrock Custom Import & Conduct Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3226d-9d39-41b4-a69c-026117c9df1d",
   "metadata": {},
   "source": [
    "## Architectural pattern \n",
    "\n",
    "![Diagram](./images/diagram.png \"Diagram\")\n",
    "\n",
    "As can be seen from the diagram above, the dataset & model (Flan-T5) get pulled from the HuggingFace hub, and are finetuned on a Jupyter Notebook. The model files are then stored in an S3 bucket, to then be imported into Amazon Bedrock. This architecture is modular because the Notebook can be run anywhere, that the appropriate compute is available (as explained earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8b8d3-0a06-4484-a2af-0c8d1d22e160",
   "metadata": {},
   "source": [
    "## Flan-T5\n",
    "\n",
    "Flan-T5 is the model we will be finetuning in this notebook. Flan-T5 is an efficient smaller model that is easy to use, and easy to deploy with Bedrock Custom Model Import. It is a Sequence to Sequence (Seq2Seq) model which uses an encoder-decoder architecture. What this means is this model processes in two parts - processes inputs with the encoder, and processes outputs with the decoder. This two part process allows models like Flan-T5 to be efficient in tasks like summarization, translation, and Q & A.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ace43-f355-4421-aa21-f9c67093da9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installs\n",
    "\n",
    "we will be utilizing HuggingFace Transformers library to pull a pretrained model from the Hub and fine tune it. The dataset we will be finetuning on will also be pulled from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a76af-9965-46cf-84f3-cc1e9a317eda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers --quiet\n",
    "%pip install torch\n",
    "%pip install -U bitsandbytes accelerate transformers peft trl\n",
    "%pip install datasets --quiet\n",
    "%pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188ad86-e313-41b0-bb7f-ea6735377948",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import the libraries installed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bcfeb-0298-49c8-bc75-b50c0c89fbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, T5ForConditionalGeneration, T5Tokenizer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import accelerate\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a958ef-c7ef-4d8a-9bef-2146a8c2345c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pull a pre-trained model from HuggingFace \n",
    "\n",
    "as mentioned at the beginning of the notebook, we will be fine tuning google's FLAN-t5-large from HuggingFace. This model is free to pull - no HuggingFace account needed.\n",
    "\n",
    "The model is loaded with the \"bitsandbytes\" library. This allows the model to be quantized in 4-bit, to set it up for fine tuning with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995a1ca-453a-4316-8348-65d26597d0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "#Set Model Attributes\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8c807-c2b0-478d-a612-68ca168b3ac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pull a dataset from HuggingFace \n",
    "\n",
    "The dataset we are pulling can be looked at [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). This is a dataset containing over 6000+ medical terms, and their wiki definitions. \n",
    "\n",
    "Since the model is being trained to recognize & understand medical terminology, with the function below the dataset will be converted to a Q & A format. \"What is\" is added as a prefix to the medical terms column, and the other column stays as the definition.\n",
    "\n",
    "One important aspect here is ensuring the \"padding=True\" argument is passed. This is needed when training a FLAN-T5 model with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf5542-2754-4248-a2b0-7d608fe7c25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load data from huggingface\n",
    "ds = load_dataset(\"gamino/wiki_medical_terms\")\n",
    "\n",
    "#Using 70% of the dataset to train\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.3)\n",
    "\n",
    "#Process data to fit a Q & A format \n",
    "prefix = \"What is \"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "   # Transform page title into an answer:\n",
    "   inputs = [prefix + doc for doc in examples[\"page_title\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "  \n",
    "   # keep explanations as is:\n",
    "   labels = tokenizer(text_target=examples[\"page_text\"], \n",
    "                      max_length=512, truncation=True, padding=True)\n",
    "\n",
    "   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "   return model_inputs\n",
    "\n",
    "# Map the preprocessing function across our dataset\n",
    "ds = ds.map(preprocess_function, batched=True)\n",
    "#Take the training portion of the dataset\n",
    "ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717088e-10bb-43ee-8b4d-9eb1381ac16b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up PEFT training parameters\n",
    "\n",
    "In the first cell below, the PEFT configurations are being set up. The term PEFT has been mentioned a couple of times in this notebook already, but what is it? PEFT stands for Parameter Efficient Fine Tuning. It allows the majority of the model parameters to be frozen, and only fine tune a small number of them. This technique decreases computation & storage costs, without performance suffering. Read more about HuggingFace's PEFT library [here](https://huggingface.co/docs/peft/en/index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fa5c4-ccfd-45bf-9396-b40cd18e3392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up PEFT Configurations \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5ca06-36cf-4bf6-b53c-ce01a9fbb482",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up SFT trainer parameters\n",
    "\n",
    "In this cell all training parameters are being passed into the \"SFTTrainer\" class. This is HuggingFace's Supervised Finetuning Trainer. Since the dataset has been prepared as Q & A pairs this is the appropriate trainer class to use. More can be read about SFTTrainer [here](https://huggingface.co/docs/trl/en/sft_trainer) \n",
    "\n",
    "Note: Training is set to 1 epoch. This is for a faster training time to showcase Bedrock Custom Model Import. If you require fine tuning with higher performance, consider increasing the epochs & optimizing hyperparameters passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fccca90-a4f3-42e9-8e22-e7ae4c1f365c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pass all parameters to SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args= TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        num_train_epochs=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_accumulation_steps=2\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8049a0a-dcc5-4ad7-ad81-31eaa01e4d70",
   "metadata": {},
   "source": [
    "### Start model training\n",
    "\n",
    "In the first cell we empty the pytorch cache to free as much memory on the device possible. In the next cell, \"trainer.train()\" actually stars the training job.\n",
    "\n",
    "In the training parameters passed, the line output_dir=\"./results\" saves the model checkpoints into the \"results\" folder in the device directory (creates it if not already created). The final model is in this directory as \"checkpoint-XXX\" - XXX being the largest number. \n",
    "\n",
    "This training job will take approx. 30 mins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11331833-0fdc-4374-9dda-9031b5a5f072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Empty pytorch cache\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97aef8-eeac-4383-b446-37e8154e2561",
   "metadata": {},
   "source": [
    "### Model Inference \n",
    "\n",
    "We will now take our latest checkpoint and generate text with it to ensure it is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9177198-1a0c-4c04-84f3-34bd34957768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model Inference \n",
    "last_checkpoint = \"./results/checkpoint-600\" #Load checkpoint that you want to test \n",
    "\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "\n",
    "med_term = \"what is Dexamethasone suppression test\" \n",
    "\n",
    "query = tokenizer(med_term, return_tensors=\"pt\")\n",
    "output = finetuned_model.generate(**query, max_length=512, no_repeat_ngram_size=True)\n",
    "answer = tokenizer.decode(output[0])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221185f9-646b-44d6-9369-78312fb8d95b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Upload \n",
    "\n",
    "with out model now generating text related to medical terminology we will now upload it to S3 to ensure readiness for Bedrock Custom Model Import. Depending on the environment you have chosen to run this notebook in, your AWS credentials will have to be initialized to upload the model files to your S3 bucket of choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d7c0c-9d35-4c16-b1fe-30d1463edade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload model to S3 Bucket\n",
    "import boto3\n",
    "import os\n",
    "# Set up S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name and the prefix (folder) where you want to upload the files\n",
    "bucket_name = 'your-bucket-here'#YOU BUCKET HERE\n",
    "model_name = \"results/checkpoint-#\" #YOUR LATEST CHECKPOINT HERE (this will be in the \"results\" folder in your notebook directory replace the \"#\" with the latest checkpoint number)\n",
    "prefix = 'flan-t5-large-medical/' + model_name\n",
    "\n",
    "# Upload files to S3\n",
    "def upload_directory_to_s3(local_directory, bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "            \n",
    "            print(f'Uploading {local_path} to s3://{bucket}/{s3_path}')\n",
    "            s3.upload_file(local_path, bucket, s3_path)\n",
    "\n",
    "# Call the function to upload the downloaded model files to S3\n",
    "upload_directory_to_s3(model_name, bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c1dce-bd2c-4d86-b8b6-f67365d71fdf",
   "metadata": {},
   "source": [
    "### Importing Model to Amazon Bedrock\n",
    "\n",
    "Now that our model artifacts are uploaded into an S3 bucket, we can import it into Amazon Bedrock \n",
    "\n",
    "in the AWS console, we can go to the Amazon Bedrock page. On the left side under \"Foundation models\" we will click on \"Imported models\"\n",
    "\n",
    "![Step 1](./images/step1.png \"Step 1\")\n",
    "\n",
    "\n",
    "You can now click on \"Import model\"\n",
    "\n",
    "![Step 2](./images/step2.png \"Step 2\")\n",
    "\n",
    "In this next step you will have to configure:\n",
    "\n",
    "1. Model Name \n",
    "2. Import Job Name \n",
    "3. Model Import Settings \n",
    "    a. Select Amazon S3 bucket \n",
    "    b. Select your bucket location (uploaded in the previous section)\n",
    "4. Create a IAM role, or use an existing one (not shown in image)\n",
    "5. Click Import (not shown in image)\n",
    "\n",
    "![Step 3](./images/step3.png \"Step 3\")\n",
    "\n",
    "\n",
    "You will now be taken to the page below. Your model may take up to an hour to import. \n",
    "\n",
    "![Step 4](./images/step4.png \"Step 4\")\n",
    "\n",
    "After your model imports you will then be able to test it via the playground or API! \n",
    "\n",
    "![Playground](./images/playground.gif \"Playground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e5234-196a-4dcd-a3e6-00983ad13d5c",
   "metadata": {},
   "source": [
    "### Clean Up \n",
    "\n",
    "You can delete your Imported Model in the console as shown in the image below:\n",
    "\n",
    "![Delete](./images/delete.png \"Delete\")\n",
    "\n",
    "Ensure to shut down your instance/compute that you have run this notebook on.\n",
    "\n",
    "**END OF NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
