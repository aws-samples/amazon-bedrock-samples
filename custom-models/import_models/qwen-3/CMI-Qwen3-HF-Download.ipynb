{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792db57d-b84f-43ce-a99f-4425c98b217c",
   "metadata": {},
   "source": [
    "# Import Qwen3 models to Amazon Bedrock (via local download)\n",
    "\n",
    "This notebook demonstrates how to deploy custom (fine-tuned) Qwen 3 models to Amazon Bedrock for serverless inference, **downloading a full copy** of the model from [Hugging Face Hub](https://huggingface.co/models) to the notebook during the import.\n",
    "\n",
    "> ℹ️ **Alternatively**, check out [CMI-Qwen3-HF-StreamCopy.ipynb](CMI-Qwen3-HF-StreamCopy.ipynb) for an example that requires much less local storage.\n",
    ">\n",
    "> If your Qwen model is already on Amazon S3, you can skip the download/upload steps and either notebook should work fine.\n",
    "\n",
    "You can run this example on modest compute (for example the default 2vCPU, 4GB RAM `ml.t3.medium` instance type on SageMaker), although using instance types with higher network performance (like the c7i family) may help to speed up model copy processes. See our example run times listed with \"⏰\" in each section below, and consider specifying your Hugging Face API key for faster downloads.\n",
    "\n",
    "You'll also **need to configure notebook storage** to accommodate your target model as described in the [\"Notebook infrastructure\" section of the import_models README.md](../README.md#Notebook-infrastructure).\n",
    "- For example, [Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507/tree/main) is ~61GB, so we'd suggest to provision at least 67GB to allow for the sample repo and other working files.\n",
    "- ⚠️ Note the **limits on maximum importable model size** in the [Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html#model-customization-import-model-architecture). At the time of writing, the 470GB [Qwen3-235B-A22B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507/tree/main) model exceeds these limits so cannot be imported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9025037-594e-4d5f-8079-b40d1db0d3d8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---\n",
    "\n",
    "First, we'll walk through setting up:\n",
    "1. An Amazon S3 Bucket in the region where you want to deploy your model\n",
    "2. Sufficient access/permissions to your target AWS account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d347f65-a1f0-4d69-8130-864c87a7d460",
   "metadata": {},
   "source": [
    "### S3 Bucket\n",
    "---\n",
    "\n",
    "**For quickest setup**, you can avoid manual work here by running the notebook in a SageMaker AI environment (e.g. a [Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) or [SageMaker AI Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated.html) JupyterLab Space) and letting it create a default bucket for you - which will be named `sagemaker-{AWS_REGION}-{AWS_ACCOUNT_ID}`.\n",
    "\n",
    "...But to keep your environment organized, you might prefer to [create a bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) in your target AWS account and region - or use an existing one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f5ca2-5207-4f83-842f-f990357cdc77",
   "metadata": {},
   "source": [
    "### AWS permissions\n",
    "---\n",
    "\n",
    "Refer to the [\"AWS permissions\" section of import_models/README.md](../README.md#AWS-permissions), which contains example policies that should work for this notebook:\n",
    "- Your CMI Job role needs to be assumable by Bedrock, and provide `s3:GetObject` and `s3:ListBucket` permissions to read your custom model from your S3 Bucket.\n",
    "- Your notebook environment needs permissions to 1/ Upload your model to your S3 bucket, 2/ Manage Bedrock Custom Model Import jobs, 3/ View (and optionally for clean-up, delete) imported Custom Models, 4/ [Pass](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) your CMI Job role to Bedrock, and 5/ Invoke the imported model to check it's working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85789f7-55f6-49dd-82bf-6de4ad680a36",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "---\n",
    "\n",
    "With the prerequisites in place and this notebook up and running on appropriate infrastructure, we're ready to dive in to the code.\n",
    "\n",
    "First, you'll need some Python libraries:\n",
    "- `boto3`, the AWS SDK for Python.\n",
    "- `huggingface_hub` is **optional** if you've already saved your model to local disk or Amazon S3\n",
    "- `sagemaker` (the high-level SDK for Amazon SageMaker) is **optional** if:\n",
    "    1. You know the ARN of the IAM Role you want to use for the Bedrock CMI job, **and**\n",
    "    2. You already have an S3 bucket to store your model, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495b3f3-0a04-4833-a827-4d7ac94a8151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install boto3 \"huggingface_hub>=1.2,<2\" sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e2b34-559b-40a7-ae0e-f5d1f00297bf",
   "metadata": {},
   "source": [
    "Next, import the main libraries and set up the Python clients for the AWS services we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47a2cf-26c8-40ed-b1c1-94bca604d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# External Libraries:\n",
    "import boto3\n",
    "\n",
    "bedrock = boto3.client(\"bedrock\")  # Bedrock control plane for managing imports\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")  # Bedrock runtime for invoking models\n",
    "s3 = boto3.client(\"s3\")  # S3 for uploading model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019b6dd-39be-4d08-a05b-c30fad5cb986",
   "metadata": {},
   "source": [
    "With your Python environment set up, it's time to configure some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207fa80-9bab-4098-924b-bbd66557d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom model's location on Hugging Face Hub (ignore if you're not importing from HF):\n",
    "HF_REPO_ID = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n",
    "\n",
    "# Job name and model name for importing your model to Bedrock:\n",
    "# (Note these must both be unique within your AWS Account and Region, so you'll need to change them\n",
    "# if re-running the import)\n",
    "CMI_JOB_NAME = \"import-Qwen3\"\n",
    "IMPORTED_MODEL_NAME = \"Qwen3-imported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a431c9-3b08-4466-b77f-c3e9e1cd50d3",
   "metadata": {},
   "source": [
    "If you're running this notebook in SageMaker, happy to use the default SageMaker S3 Bucket, using a single execution Role for both the notebook and the CMI Job - then the following parameters can be configured for you automatically.\n",
    "\n",
    "> ⚠️ Otherwise, configure them manually (and you can remove the `sagemaker` dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66b5e0-8360-4204-81ac-c1759c6bcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# IAM Role ARN for the Bedrock CMI Job:\n",
    "CMI_ROLE = sagemaker.get_execution_role()  # Use current SM notebook execution as CMI Job role\n",
    "print(\"IAM role for Bedrock CMI job:\", CMI_ROLE)\n",
    "\n",
    "# S3 Bucket name and folder prefix to upload your model to for import:\n",
    "S3_BUCKET = sagemaker.Session().default_bucket()\n",
    "S3_PREFIX = \"bedrock-custom-models/qwen3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68342f3b-f89d-4f09-9c97-19f149b9c32a",
   "metadata": {},
   "source": [
    "## Download model artifacts from Hugging Face\n",
    "---\n",
    "\n",
    "> ℹ️ If your model is already available locally and doesn't need fetching from Hugging Face, you can skip this step and instead set `local_folder = \"{your-model-folder}\"`.\n",
    "\n",
    "The [Hugging Face Hub client library](https://huggingface.co/docs/huggingface_hub/v1.3.1/en/index) manages downloads via a local cache directory to avoid unnecessary repeat downloads.\n",
    "\n",
    "To pull very large models, we'll first need to ensure this cache folder is created inside the large storage volume we provisioned for the notebook (which is mounted at `/home/ec2-user/SageMaker` in SageMaker Notebook Instances or `/home/sagemaker-user` in SageMaker AI Studio).\n",
    "\n",
    "> For more information on configuration, see [\"Understand caching\"](https://huggingface.co/docs/huggingface_hub/v1.3.1/en/guides/manage-cache) in the Hugging Face docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad39b9-294e-4f36-a65f-0380aa06363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all temporary + cache files on the large storage volume:\n",
    "os.environ[\"HF_HOME\"] = os.path.abspath(\"cache/hf\")\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.abspath(\"cache/hf\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.abspath(\"cache/hf\")\n",
    "os.environ[\"TMPDIR\"] = os.path.abspath(\"cache/tmp\")\n",
    "\n",
    "# If you face failures or flakiness with CAS or Xet downloads (sometimes observed due to network\n",
    "# restrictions/firewalls, corporate proxies, older SSL stacks, etc), you could also set the below\n",
    "# variables *BEFORE* importing huggingface_hub (restart your notebook kernel if already imported):\n",
    "#    os.environ[\"HF_HUB_ENABLE_HFS_CAS\"] = \"0\"\n",
    "#    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "# Create the temp/cache folders if they don't exist already:\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "os.makedirs(os.environ[\"TMPDIR\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48650d50-983b-44f4-9770-f84dac8d267d",
   "metadata": {},
   "source": [
    "Now we're ready to download the artifacts from Hugging Face to JupyterLab's local file system.\n",
    "\n",
    "Note that we won't bother to specify a separate `local_dir` here - we'll just read the model directly from the HF cache folder later.\n",
    "\n",
    "> ⏰ This step may take significant time to complete, depending on the size of your model and the network connection speed from the Hugging Face Hub to the environment where you're running the notebook.\n",
    ">\n",
    "> For example in tests with an `ml.t3.medium` notebook in us-east-1 we saw ~8min for a 61GB model. Your results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186b628-cbc2-427f-b5a0-7c6673367f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(f\"Fetching model {HF_REPO_ID}\")\n",
    "local_folder = snapshot_download(repo_id=HF_REPO_ID)\n",
    "print(f\"Downloaded to:\\n{local_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a13598-d3bc-47ef-a3f9-2d9cf31ae793",
   "metadata": {},
   "source": [
    "## Upload model artifacts to S3\n",
    "---\n",
    "Now the model artifacts are available locally, we can upload them to Amazon S3 ready for Bedrock import.\n",
    "\n",
    "In the cell below we've implemented a Python function to perform the upload, but you could also use the AWS CLI if preferred. For example: `aws s3 sync {local_folder} s3://{S3_BUCKET}/{S3_PREFIX}`\n",
    "\n",
    "> ⏰ This step may also take a while for larger models - potentially longer than the download.\n",
    ">\n",
    "> In tests with an `ml.t3.medium` notebook in us-east-1, we saw ~11min for a 61GB model. Your results may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58896890-20fe-4509-a3a8-3d0b153b5eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def upload_directory_to_s3(local_dir, bucket, prefix):\n",
    "    \"\"\"Recursively upload a local directory to S3 under the specified prefix.\n",
    "\n",
    "    Preserves relative folder structure exactly.\n",
    "    \"\"\"\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for filename in sorted(files):\n",
    "            local_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = f\"{prefix}/{relative_path}\"\n",
    "\n",
    "            print(f\"Uploading: {local_path} → s3://{bucket}/{s3_key}\")\n",
    "            s3.upload_file(local_path, bucket, s3_key)\n",
    "\n",
    "    print(\"Upload complete.\")\n",
    "\n",
    "\n",
    "upload_directory_to_s3(local_dir=local_folder, bucket=S3_BUCKET, prefix=S3_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3edfe6-e991-4df7-94ba-20aca899e234",
   "metadata": {},
   "source": [
    "## Import to Bedrock\n",
    "---\n",
    "\n",
    "Once the model artifacts are on S3, we're ready to start a CMI job to prepare a Bedrock model ready to serve requests.\n",
    "\n",
    "> ⏰ CMI jobs may take a few minutes, depending on the size of the model artifacts. As an indication, for a Qwen3 30B model (61GB model artifacts) we saw ~15min in one test.\n",
    "\n",
    "**Troubleshooting errors:**\n",
    "\n",
    "If your import job fails, you can find more information by looking at the \"Imported Models\" -> \"Jobs\" tab of the [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock/home?#/import-models). Click on the failed job and you should see the failure reason with more information.\n",
    "\n",
    "- One possible cause is incorrect IAM permissions - in which case the failure reason should indicate which part of the IAM policy you need to fix.\n",
    "- You may also receive a quota exceeded exception if you have too many imported models in your AWS Account+region, or start too many model import jobs at the same time. Refer to the [Service Quotas Console](https://console.aws.amazon.com/servicequotas/home/services/bedrock/quotas) to check your current quotas and request increases where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c43b49-2211-4c0e-bf52-b417b47cfa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_uri = f\"s3://{S3_BUCKET}/{S3_PREFIX}/\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=CMI_JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=CMI_ROLE,\n",
    "    modelDataSource={\n",
    "        \"s3DataSource\": {\"s3Uri\": s3_uri},\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response[\"jobArn\"]\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n",
    "\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Job status: {status}\")\n",
    "\n",
    "    if status == \"FAILED\":\n",
    "        raise RuntimeError(\n",
    "            \"Model import job failed - see the Bedrock console for more details\"\n",
    "        )\n",
    "\n",
    "    if status == \"COMPLETED\":\n",
    "        # Get the model ID\n",
    "        model_arn = response[\"importedModelArn\"]\n",
    "        print(f\"\\n\\n✅ Model imported with ID:\\n{model_arn}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(30)  # Check every 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee40994-5411-4204-aa49-9fc56881bf67",
   "metadata": {},
   "source": [
    "## Model information (including pricing)\n",
    "---\n",
    "\n",
    "In addition to the [\"Imported Models\" page of the Amazon Bedrock Console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/import-models), you can use the [GetImportedModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GetImportedModel.html) to look up useful information about your imported model, including for example:\n",
    "- `customModelUnits.customModelUnitsPerModelCopy`: The number of Custom Model Units which will be consumed per running copy of your model\n",
    "    - This determines the pricing of your imported model - per the [Bedrock pricing page](https://aws.amazon.com/bedrock/pricing/)\n",
    "    - For example, if a model requires 4 CMU per copy then (using the listed rates for us-east-1 region at the time of writing), it would cost 4⨉1.95=\\\\$7.80/month for storage, plus 4⨉0.5718≈\\\\$0.23/minute per active copy running inference\n",
    "    - Note the Bedrock `ModelCopy` metric in Amazon CloudWatch tracks the number of deployed copies of your imported model over time, which auto-scales based on requests\n",
    "- `instructSupported`: Whether or not this particular imported model supports the Bedrock Converse API\n",
    "- `modelDataSource`: The (Amazon S3) location your model was imported from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fa5c3-7db5-4228-96e7-a5ef50b95339",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_resp = bedrock.get_imported_model(modelIdentifier=model_arn)\n",
    "\n",
    "# Render the JSON response nicely:\n",
    "print(json.dumps(get_model_resp, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6c80b-be08-46ac-bd4a-3da47559b472",
   "metadata": {},
   "source": [
    "## Test inference with the imported model\n",
    "---\n",
    "\n",
    "When the model import job is completed, you're ready to use your Qwen model for inference.\n",
    "\n",
    "⚠️ First though, some important points to be aware of (correct at the time of writing in January 2026):\n",
    "\n",
    "1. **Bedrock's [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) is not supported** for imported Qwen3 models - only the [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) or the streaming [InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html).\n",
    "2. **Auto-scaling and Cold start `ModelNotReadyException` errors:**\n",
    "    - Bedrock auto-scales capacity for your imported model based on usage, and you're charged for this capacity in \"Custom Model Units\" (see [CMI documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/import-model-calculate-cost.html) and the [Bedrock pricing page](https://aws.amazon.com/bedrock/pricing/) for more details).\n",
    "    - When your model is first imported, or hasn't been invoked at all for some time, this capacity will scale down to zero.\n",
    "    - The first request made to a \"cold\" model with zero capacity will return a `ModelNotReadyException` and start re-provisioning your model in the background.\n",
    "    - Once your capacity is ready (which may take a few minutes), subsequent requests will be successful.\n",
    "    - As a result, you probably want to configure solutions using imported Bedrock models to retry after some time on receiving this error. For use-cases where traffic is low but improving response time is a higher priority than saving cost, you could also consider automating regular \"keep-alive\" requests to keep at least some capacity available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547fc54-142c-4a6d-89e4-89a54d27a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_textgen_model(request: dict, model_id: str) -> str | None:\n",
    "    \"\"\"Invoke model with a JSON payload and return text output\"\"\"\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=json.dumps(request).encode(\"utf-8\"),\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # robust field selection\n",
    "    output_text = (\n",
    "        response_body.get(\"generation\")\n",
    "        or response_body.get(\"output_text\")\n",
    "        or response_body.get(\"generated_text\")\n",
    "        or response_body.get(\"text\")\n",
    "    )\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d1c17-1c2e-4e59-b6b5-bc178dff58e1",
   "metadata": {},
   "source": [
    "Remember, you'll likely receive a `ModelNotReadyException` error the first time you run the following cell, as discussed above. Try the following request a few times until your model is ready to respond:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b474f-e0c3-4be0-ac5f-e25664a8940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "\n",
    "request = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_gen_len\": 1000,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "reply = invoke_textgen_model(request, model_arn)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31a2e6-e679-40df-811e-b6882fa8a29f",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Your imported model will automatically scale down to zero *active* Custom Model Units when not in use, but there are ongoing storage and other costs you may like to avoid when you're done experimenting.\n",
    "\n",
    "> ℹ️ We've commented-out the code cells in this section to avoid you accidentally losing work if clicking \"Run all cells\" on the notebook. If you do want to run them, you can select the text and type `Ctrl` + `/` to un-comment.\n",
    "\n",
    "First, since Bedrock itself stores a copy of your imported model, you can delete your artifacts from Amazon S3 without affecting your deployed endpoint:\n",
    "\n",
    "> ⚠️ Do note though, that your model and import job will continue to list the original S3 location in their metadata.\n",
    ">\n",
    "> S3 storage costs for model artifacts are usually significantly lower than Bedrock imported model storage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6982d-ee2a-4210-b942-0b2c69e732a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri_to_delete = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "# print(f\"(Interrupt this cell to cancel...)\\nAbout to delete {uri_to_delete}\")\n",
    "# time.sleep(5)\n",
    "# !aws s3 rm --recursive {uri_to_delete}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55266a09-be83-4052-8ea6-ed7164f9b4f5",
   "metadata": {},
   "source": [
    "Second, you can delete your imported model from Bedrock to avoid the ongoing Bedrock model storage costs. If you do this, your model will no longer be available to invoke until re-imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d173b-5141-40c3-9129-86ac23553df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Deleting imported model {model_arn}\")\n",
    "# bedrock.delete_imported_model(modelIdentifier=model_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147cd98-12d8-4afc-b2b5-bdad7dd10e8e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we showed the end-to-end process of copying a custom Qwen 3 model from Hugging Face Hub to Amazon S3; importing it from there to Amazon Bedrock with Custom Model Import (CMI); and invoking the imported model for inference.\n",
    "\n",
    "The same process shown here applies to other Qwen 3 variants. For more information about Custom Model Import and its features, refer to the [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
