{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc722971-307b-466f-a2bb-5ab2a221d211",
   "metadata": {},
   "source": [
    "# Import a finetuned Qwen3 model from HuggingFace into Amazon Bedrock CMI, using SageMaker JupyterLab environment\n",
    "\n",
    "Custom Model Import (CMI) is a feature that allows you to import model artifacts of certain model architectures to Amazon Bedrock and run them in a **serverless way**. For more information on CMI check the [Bedrock documentation on CMI](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html).\n",
    "\n",
    "## The challenge\n",
    "---\n",
    "In order for CMI to work, model artifacts need to be on an S3 bucket. The size of model artifacts nowadays can be quite large (e.g. exceeding 100GB) which can create issues with downloading them locally. In this notebook we will show how to download large Qwen 3 model artifacts from HuggingFace to JupyterLab environment in SageMaker Studio, and import them to Bedrock using CMI, in order to have serverless access to different types of Qwen 3 models. The same approach can be generalized for any other model artifacts from all the supported families in CMI. \n",
    "\n",
    "We will be focusing on the following steps: \n",
    "\n",
    "- Configuring SageMaker Studio and the JupyterLab environment, in order to allow receiving large model artifacts.\n",
    "- Downloading the model artifacts from HuggingFace locally to JupyterLab.\n",
    "- Uploading the model artifacts to S3.\n",
    "- Initiating a CMI job.\n",
    "- Performing inference on the imported model.\n",
    "\n",
    "You can find more details in this AWS Blogpost: [Deploy Qwen models with Amazon Bedrock Custom Model Import](https://aws.amazon.com/blogs/machine-learning/deploy-qwen-models-with-amazon-bedrock-custom-model-import)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7a297-0996-4cf8-ae7a-d8a51675941f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---\n",
    "In order to run this notebook, you need the right IAM permissions. Specifically:\n",
    "- The notebook's execution role needs to be able to access Amazon Bedrock (and specifically all CMI-related actions) and have a trusted relationship with Amazon Bedrock (in order for it to be able to assume this role and carry out CMI operations). \n",
    "- You need appropriate Permissions to access model files in Amazon S3. \n",
    "- You need a S3 bucket to store the custom model artifacts (later to be imported to Bedrock using CMI).\n",
    "- Sufficient local storage space (this is what we will be setting up next).\n",
    "\n",
    "In order to set the correct IAM roles and permissions, follow [the instruction here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html) . \n",
    "The following are some policy examples that you can consider. Substitue the `AWS_REGION`, `ACCOUNT_ID`, `BUCKET_NAME` and `IAM_ROLE_ARN` variables with the ones relevant for your case, from the role that you will create."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82faef2d-aae7-4895-8c9e-b10772b12920",
   "metadata": {},
   "source": [
    "**Policy for allowing access to CMI-related tasks**: Your notebook role should have these following minimum permissions to allow CMI tasks.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Sid\": \"BedrockInvokeModelAndStreaming\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:bedrock:{AWS_REGION}:{ACCOUNT_ID}:imported-model/*\",\n",
    "        \"arn:aws:bedrock:{AWS_REGION}:{ACCOUNT_ID}:custom-model-deployment/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"Sid\": \"BedrockCustomModelImportJobManagement\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:CreateModelImportJob\",\n",
    "        \"bedrock:ListModelImportJobs\",\n",
    "        \"bedrock:GetModelImportJob\"\n",
    "      ],\n",
    "      \"Resource\": \"*\"\n",
    "    },\n",
    "    {\n",
    "      \"Sid\": \"BedrockImportedModelLifecycle\",\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:ListImportedModels\",\n",
    "        \"bedrock:GetImportedModel\",\n",
    "        \"bedrock:DeleteImportedModel\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:bedrock:{AWS_REGION}:{ACCOUNT_ID}:imported-model/*\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "       \"Sid\": \"AccessModelFilesInS3\",\n",
    "       \"Effect\": \"Allow\",\n",
    "       \"Action\": [\n",
    "         \"s3:GetObject\",\n",
    "         \"s3:ListBucket\",\n",
    "         \"s3:PutObject\"\n",
    "       ],\n",
    "       \"Resource\": [\n",
    "         \"arn:aws:s3:::{BUCKET_NAME}\",\n",
    "         \"arn:aws:s3:::{BUCKET_NAME}/*\"\n",
    "       ],\n",
    "       \"Condition\": {\n",
    "         \"StringEquals\": {\n",
    "         \"aws:ResourceAccount\": \"{ACCOUNT_ID}\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "       \"Sid\": \"PassRoleToBedrockForCMI\",\n",
    "       \"Effect\": \"Allow\",\n",
    "       \"Action\": \"iam:PassRole\",\n",
    "       \"Resource\": \"{IAM_ROLE_ARN}\",\n",
    "       \"Condition\": {\n",
    "         \"StringEquals\": {\n",
    "         \"iam:PassedToService\": \"bedrock.amazonaws.com\"\n",
    "         }\n",
    "       }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Trust relationship**: The following policy allows Amazon Bedrock to assume this role and carry out model import operations. Include the following policy to your IAM Role's Trusted Relationships. \n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\":\"2012-10-17\",\t\t \t \t \n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"1\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"aws:SourceAccount\": \"{ACCOUNT_ID}\"\n",
    "                },\n",
    "                \"ArnEquals\": {\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{AWS_REGION}:{ACCOUNT_ID}:model-import-job/*\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85789f7-55f6-49dd-82bf-6de4ad680a36",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626801a-64c4-4c54-9a69-cf07363a710b",
   "metadata": {},
   "source": [
    "We need to have a large local space in order to save the model artifacts that will be downloaded from HuggingFace. For example, for Qwen 3 30B the model artifacts are around 122GB. By default, JupyterLab and SageMaker Studio Domains, don't allow for such large available spaces. You need to explicitly configure them to make this happen. In this section we will be performing the following steps: \n",
    "\n",
    "- Enabling a large EBS size in your SageMaker Studio Domain.\n",
    "- Creating a JupyterLab space with adequate EBS storage.\n",
    "- Making this storage available to store the HF model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c34fe4-3619-4b25-84de-c57b451143c1",
   "metadata": {},
   "source": [
    "Configure your SageMaker Studio Domain to allow large EBS storage spaces, by setting up a large storage space under \"Edit storage settings\". Here you need to select an adequate number to cover the size of the model artifacts. In the following image we have selected 500GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4174ba8-f6a2-4577-abd7-68d240002792",
   "metadata": {},
   "source": [
    "![Alt text](images/sm-studio-domain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad024f3b-41f4-4ea4-93cf-28c9f48d3771",
   "metadata": {},
   "source": [
    "You need also to create a new JupyterLab space (from within SageMaker Studio) and allow adequate storage for JupyterLab. The maximum storage you can select in this step, is bounded by the maximum number you have set in your Studio Domain configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019b6dd-39be-4d08-a05b-c30fad5cb986",
   "metadata": {},
   "source": [
    "![Alt text](images/sm-studio-jupyterlab-space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1e0c5-5cfc-4ef5-8395-dc937ed67f6a",
   "metadata": {},
   "source": [
    "Within the notebook, inside the JupyterLab environment, you also need to set the required environmental variables that will allow you to download the model artifacts from HuggingFace and safe them within the local file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad755b0-8a05-443b-be78-4b384de56e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Put all temporary + cache files on the large 500GB volume\n",
    "os.environ[\"HF_HOME\"] = \"/home/sagemaker-user/hf_cache\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/home/sagemaker-user/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/sagemaker-user/hf_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/home/sagemaker-user/tmp\"\n",
    "\n",
    "# Disable the Xet CAS download mechanism\n",
    "os.environ[\"HF_HUB_ENABLE_HFS_CAS\"] = \"0\"\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "os.makedirs(\"/home/sagemaker-user/hf_cache\", exist_ok=True)\n",
    "os.makedirs(\"/home/sagemaker-user/tmp\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfbb2f-1298-44f6-ab05-02ad5e7ea0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "import sagemaker\n",
    "\n",
    "HF_REPO_ID = \"<huggingface-repo-id>\"       # change this with your relevant repo id location in HuggingFace, e.g. prakhag/qwen3-30b-ft\n",
    "MODEL_ARTIFACTS = \"llm_artifacts\"          # this is the local directory that will store the model artifacts from HuggingFace\n",
    "S3_BUCKET_ARTIFACTS = \"<your-s3-bucket>\"   # change this to the name of your S3 bucket that will store the model artifacts\n",
    "S3_PREFIX = \"qwen3-artifacts\"              # change this to a prefix name in your S3 bucket, where all the model artifacts will be stored\n",
    "CMI_JOB_NAME = \"import-Qwen3\"              # Job names need to be unique in your account and region, so if you rerun, you need to change to a new job name\n",
    "IMPORTED_MODEL_NAME = \"Qwen3-imported\"     # Imported model names need to be unique in your account and region, so if you rerun, you need to change to a new model name\n",
    "CMI_ROLE = sagemaker.get_execution_role()  # the current role of the notebook or any other IAM role you want\n",
    "print(\"IAM role to be used for the CMI job:\", CMI_ROLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48650d50-983b-44f4-9770-f84dac8d267d",
   "metadata": {},
   "source": [
    "## Download the model artifacts from HuggingFace\n",
    "---\n",
    "Now we are ready to download the model artifacts in the JupyterLab's local file system. You will need to point to a specific HuggingFace repo for a particular model. In the following cell subtitute the `repo_id` with your relevant repo in HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668013cc-3f8f-45ce-9e81-0ac0c0901b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# upgrading the huggingface_hub package\n",
    "!pip install --upgrade --no-cache-dir huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186b628-cbc2-427f-b5a0-7c6673367f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model artifacts from HF locally to JupyterLab \n",
    "# this may take some time depending on the size of the model artifacts and your network connection (e.g. 15-20min for ~130GB)\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    local_dir=f\"/home/sagemaker-user/{MODEL_ARTIFACTS}\",\n",
    "    local_dir_use_symlinks=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a13598-d3bc-47ef-a3f9-2d9cf31ae793",
   "metadata": {},
   "source": [
    "## Upload model artifacts to S3 and initiate a CMI job\n",
    "---\n",
    "We will now upload the model artifacts from the local folder to S3. This operation make take some time, depending on the size of the model artifacts and your network connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58896890-20fe-4509-a3a8-3d0b153b5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def upload_directory_to_s3(local_dir, bucket, prefix):\n",
    "    \"\"\"\n",
    "    Recursively uploads a local directory to S3 under the specified prefix.\n",
    "    Preserves relative folder structure exactly.\n",
    "    \"\"\"\n",
    "    local_dir = os.path.abspath(local_dir)\n",
    "\n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            # compute relative path within the directory\n",
    "            relative_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = f\"{prefix}/{relative_path}\"\n",
    "\n",
    "            print(f\"Uploading: {local_path} â†’ s3://{bucket}/{s3_key}\")\n",
    "\n",
    "            s3.upload_file(local_path, bucket, s3_key)\n",
    "\n",
    "    print(\"Upload complete.\")\n",
    "\n",
    "\n",
    "upload_directory_to_s3(\n",
    "    local_dir=MODEL_ARTIFACTS, \n",
    "    bucket=S3_BUCKET_ARTIFACTS, \n",
    "    prefix=S3_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3edfe6-e991-4df7-94ba-20aca899e234",
   "metadata": {},
   "source": [
    "We are ready now to initiate a CMI job, that will ingest the model artifacts from S3 and will prepare a model ready to serve requests. CMI jobs may take a few minutes, depending on the size of the model artifacts. As an indication, for a Qwen3 30B parameters (130GB model artifacts) it takes around 15min. \n",
    "\n",
    "If you haven't set the IAM permissions correctly, as indicated in the Prerequsites section, the CMI will fail. You can find the exact reason of why the job failed by looking at the Amazon Bedrock Console -> \"Imported Models\" -> \"Jobs\" tab. Clich on the (failed) job and you will see the reason of failing, which will indicate which part of the IAM policy you need to fix. \n",
    "\n",
    "Also, there is a limited number of imported models that you can have in a region on an AWS account. If you receive a quote exceed exception, you will either need to delete some of your other imported models, or increase your quotas for the number of imported models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c43b49-2211-4c0e-bf52-b417b47cfa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock')\n",
    "\n",
    "s3_uri = f\"s3://{S3_BUCKET_ARTIFACTS}/{S3_PREFIX}/\"\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=CMI_JOB_NAME,\n",
    "    importedModelName=IMPORTED_MODEL_NAME,\n",
    "    roleArn=CMI_ROLE,  # or any other IAM role you want\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n",
    "\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status of import job: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)  # Check every 30 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6c80b-be08-46ac-bd4a-3da47559b472",
   "metadata": {},
   "source": [
    "## Testing the imported model\n",
    "---\n",
    "**IMPORTANT**: (as of today: January 2026) for Qwen3 models, `Converse` API is **not supported**. Only the `Invoke` API is supported! For more information please take a look at the [CMI documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547fc54-142c-4a6d-89e4-89a54d27a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "def call_invoke_model_and_print(request):\n",
    "    body = json.dumps(request).encode(\"utf-8\")\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # robust field selection\n",
    "    response_text = (\n",
    "        model_response.get(\"generation\")\n",
    "        or model_response.get(\"output_text\")\n",
    "        or model_response.get(\"generated_text\")\n",
    "        or model_response.get(\"text\")\n",
    "    )\n",
    "\n",
    "    # print(response_text)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d1c17-1c2e-4e59-b6b5-bc178dff58e1",
   "metadata": {},
   "source": [
    "**IMPORTANT**: there will be a cold start period, counting from the time of the first model invocation. During that time, you will receive a `ModelNotReadyException` error. This may take a few minutes, until the model is \"hot\" and ready to serve requests. Try the following request a few times until the model is ready to respond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b474f-e0c3-4be0-ac5f-e25664a8940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the meaning of life?\"\n",
    "\n",
    "request = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_gen_len\": 1000,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147cd98-12d8-4afc-b2b5-bdad7dd10e8e",
   "metadata": {},
   "source": [
    "If the model is not invoked for 5 minutes, then it will become \"cold\". After that, any subsequent request will trigger the cold start period where the model will become ready again to surve requests. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
