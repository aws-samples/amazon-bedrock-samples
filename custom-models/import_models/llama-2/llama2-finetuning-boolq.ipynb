{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d235b0a7",
   "metadata": {},
   "source": [
    "# **Fine-Tuning LLaMA 2: A Step-by-Step Instructional Guide and then import into Bedrock**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c5b0b2-0435-4e1f-ada1-4a7cb6e9c206",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will fine-tune LlaMA 2 so it can summarize customer support tickets. This will be done by using SFTTrainer from HuggingFace, and the BoolQ dataset pulled from the HuggingFace hub. \n",
    "\n",
    "This notebook will use the HuggingFace Transformers library to fine tune LLaMA 2. due to the fine tuning process being \"local\" you can run this notebook anywhere as long as the proper compute is available. \n",
    "\n",
    "1. Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: [Link](https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html)\n",
    "2. Configuring an Amazon Sagemaker environment: [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html)\n",
    "3. Configure your own environment, with equivalent compute\n",
    "\n",
    "For this tutorial, 4 A10 High-RAM GPUs were used, which provided robust performance for running the the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be25e45c-aa69-4da3-bfea-0778bb5780c6",
   "metadata": {},
   "source": [
    "## Usecase \n",
    "\n",
    "BoolQ is a question answering dataset for yes/no questions. This will allow the model to be optimized for answering user questions. this philosophy can be extrapolated to all Q & A datasets. If the LLM answers need to be standardized Supervised fine tuning is an excellent choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209c01c-5aa1-480d-94b2-0aea730f0255",
   "metadata": {},
   "source": [
    "## Amazon Bedrock Custom Model Import (CMI)\n",
    "\n",
    "The resulting model files are imported into Amazon Bedrock via [Custom Model Import (CMI)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html). \n",
    "\n",
    "Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fd543-8742-4d1b-9b30-2a3fec26f694",
   "metadata": {},
   "source": [
    "## Pre-requisites \n",
    "\n",
    "Below are the commands to install the essential libraries:\n",
    "\n",
    "*   The keys steps are :\n",
    "1. Run the training job \n",
    "2. Save the LORA peft weights as safetensors\n",
    "3. Load and Merge with the base adapter and save it to a folder as safetensors\n",
    "4. Upload the merged weights to S3\n",
    "5. Run the Import jobs in Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ab75f-9504-49d1-b8d7-a250330f3aad",
   "metadata": {},
   "source": [
    "## Notebook code with comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c558311",
   "metadata": {},
   "source": [
    "#### Install the essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate peft bitsandbytes transformers==4.38.2 trl==0.7.10 torch==2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a44fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f1bd6",
   "metadata": {},
   "source": [
    "### We will use the BoolQ data set and the SFTTrainer from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac4dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from safetensors.torch import save_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62691b",
   "metadata": {},
   "source": [
    "#### Change the cache location to match your local set up -- below is an example location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8c6f65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "hf_cache_dir='/home/ubuntu/sm/cache/huggingface/hub'\n",
    "!mkdir -p {hf_cache_dir}\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = hf_cache_dir\n",
    "os.environ['HF_HOME'] = hf_cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = hf_cache_dir\n",
    "os.environ['TORCH_HOME'] = hf_cache_dir\n",
    "!export TRANSFORMERS_CACHE=/home/ubuntu/sm/cache/huggingface/hub\n",
    "!export HF_HOME=/home/ubuntu/sm/cache/huggingface/hub\n",
    "!export HF_DATASETS_CACHE=/home/ubuntu/sm/cache/huggingface/hub\n",
    "!export TORCH_HOME=/home/ubuntu/sm/cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efacac67",
   "metadata": {},
   "source": [
    "### Optional Download the model -- \n",
    "\n",
    "**From HuggingFace TheBloke/Llama-2-7b-fp16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1923037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"/home/ubuntu/sm/llama_train/llama-7b\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"TheBloke/Llama-2-7b-fp16\"\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\", \"*.chk\", \"*.pth\", \"*.py\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "# model_download_path = snapshot_download(\n",
    "#     repo_id=model_name, cache_dir=local_model_path, allow_patterns=allow_patterns\n",
    "# )\n",
    "# print(model_download_path)\n",
    "#\n",
    "#- Identify the snapshot id\n",
    "# model_download_path='/home/ubuntu/SageMaker/models/llama/llama-7b/models--TheBloke--Llama-2-7b-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239'\n",
    "# print(model_download_path)\n",
    "\n",
    "# model_loc = \"/home/ubuntu/sm/llama_train/llama-7b/actual_model/\"\n",
    "# !cp -L {model_download_path}/* {model_loc} --recursive\n",
    "# !ls -alrt {model_loc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827cb23",
   "metadata": {},
   "source": [
    "## Continue the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa208401",
   "metadata": {},
   "source": [
    "### You can alternately set the model_loc to be \n",
    "\n",
    "**TheBloke/Llama-2-7b-fp16 or use the actual path to the downloaded weights** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cfbfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- base model loc -\n",
    "model_loc = \"/home/ubuntu/sm/llama_train/llama-7b/actual_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bc014",
   "metadata": {},
   "source": [
    "#### Configurations\n",
    "\n",
    "The lora peft values. Please note these are just for sample and not for production use case. We will save the trained model under llama-2-7b-boolq and then merge the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa425c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_and_tokenizer(model_loc, bnb_config):\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_loc,\n",
    "        cache_dir=hf_cache_dir,\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 #torch.float32, # - float16 \n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "185b78fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "#model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"google/boolq\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-boolq\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "! rm -rf ./results\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# OPTIONAL depending on Instance Type Load the entire model on the GPU 0\n",
    "#device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e217858",
   "metadata": {},
   "source": [
    "#### Load the data set\n",
    "\n",
    "we are using Boolq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5340d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process dataset\n",
    "boolq_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "def process_dataset(dataset):\n",
    "    processed_data = []\n",
    "\n",
    "    for row in dataset:\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        passage = row['passage']\n",
    "\n",
    "        system_prompt = f\"Context: {passage}\"\n",
    "        user_message = f\"Question: {question}\"\n",
    "        model_answer = answer\n",
    "\n",
    "        formatted_example = f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>{user_message} [/INST] {model_answer} </s>\"\n",
    "        processed_data.append({\"text\": formatted_example})\n",
    "\n",
    "    return processed_data\n",
    "#Create list of dictionaries from the dataset. Dataset class expects list of dictionaries\n",
    "dataset_list = process_dataset(boolq_dataset)\n",
    "\n",
    "# Create a Dataset Object\n",
    "dataset = Dataset.from_list(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "#dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# # Load LLaMA tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "model, tokenizer = create_model_and_tokenizer(model_loc,bnb_config)\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model as .bin\n",
    "#trainer.model.save_pretrained(new_model)\n",
    "\n",
    "#save model as safetensors\n",
    "#save_model(trainer.model, new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828a8a9",
   "metadata": {},
   "source": [
    "### Print the location of the new saved Peft Weights and save it in safetensors format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2162ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama-2-7b-boolq'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3d9bc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sm/virtualenv/trainenv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/ubuntu/sm/llama_train/llama-7b/actual_model/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model, safe_serialization=True) # saves the peft weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b1e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 67884\n",
      "-rw-rw-r--  1 ubuntu ubuntu     1061 May 12 00:34 tokenizer_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu   499723 May 12 00:34 tokenizer.model\n",
      "-rw-rw-r--  1 ubuntu ubuntu      434 May 12 00:34 special_tokens_map.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu       21 May 12 00:34 added_tokens.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu  1842945 May 12 00:34 tokenizer.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu      712 May 12 00:34 config.json\n",
      "drwxrwxr-x  2 ubuntu ubuntu     4096 May 12 00:34 .\n",
      "drwxrwxr-x 27 ubuntu ubuntu     4096 May 12 01:01 ..\n",
      "-rw-rw-r--  1 ubuntu ubuntu     5126 May 12 01:40 README.md\n",
      "-rw-rw-r--  1 ubuntu ubuntu 67126104 May 12 01:40 adapter_model.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu      625 May 12 01:40 adapter_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls -alrt {new_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cc7a8",
   "metadata": {},
   "source": [
    "#### Merge the weights and save to local location\n",
    "\n",
    "The steps:\n",
    "1. Load from the saved weights. this loads peft and the base weights\n",
    "2. run merge_and_unload()\n",
    "3. Save these in a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a6f77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5388ee3a34db6bedb18ce21ede89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./llama2_boolq_peft_merged_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26324356\n",
      "drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..\n",
      "-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model\n",
      "-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json\n",
      "drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "\n",
    "# merge base + LoRa models and save the model\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "    \n",
    "shutil.rmtree(\"./llama2_boolq_peft_merged_model\", ignore_errors=True)\n",
    "os.makedirs(\"./llama2_boolq_peft_merged_model\", exist_ok=True)\n",
    "\n",
    "print(\"./llama2_boolq_peft_merged_model\")\n",
    "\n",
    "merged_model = trained_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./llama2_boolq_peft_merged_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"./llama2_boolq_peft_merged_model\")\n",
    "\n",
    "!ls -alrt ./llama2_boolq_peft_merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd64e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26324376\n",
      "drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..\n",
      "-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model\n",
      "-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json\n",
      "drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .\n"
     ]
    }
   ],
   "source": [
    "!ls -alrt ./llama2_boolq_peft_merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38ec4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free memory for merging weights\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b1f1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2609d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"is the bicuspid valve the same as the mitral valve?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2355f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] is the bicuspid valve the same as the mitral valve? [/INST] False. The mitral valve is located between the left atrium and the left ventricle, whereas the bicuspid valve is located between the right atrium and the right ventricle.<</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</</\n"
     ]
    }
   ],
   "source": [
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdfe9b",
   "metadata": {},
   "source": [
    "#### Alternate way of creating the merged model. Here \n",
    "\n",
    "1. we first load the base model\n",
    "2. Load the peft model and merge\n",
    "3. set the config to false to avoid any transformers library issue \n",
    "4. Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52248eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_loc,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7a8417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2008cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_loc, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained('./llama2_boolq_peft_merged_model')\n",
    "merged_model.generation_config.do_sample = True\n",
    "merged_model.save_pretrained(\"./llama2_boolq_peft_merged_model\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08033596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26324376\n",
      "drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..\n",
      "-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors\n",
      "-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model\n",
      "-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json\n",
      "-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json\n",
      "drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .\n"
     ]
    }
   ],
   "source": [
    "!ls -alrt ./llama2_boolq_peft_merged_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849d475",
   "metadata": {},
   "source": [
    "### Copy to S3 location\n",
    "\n",
    "**Important** this file assumes you are running the training job on a EC2. Hence we assume a role and then upload to S3. This steps is completely optional and if you run via SageMaker studio you will not need to assume the role and instead just run the s3_upload cells directly\n",
    "\n",
    "the merged weights are under -- ./llama2_boolq_peft_merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "boto3_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0fc7d",
   "metadata": {},
   "source": [
    "#### This is optional \n",
    "**If you are runing the notebook from outise of SageMaker then use the below to so sts:assume role successfully**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63725663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"your key\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your access\"\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "\n",
    "\n",
    "assumed_role = None # \"set to your assume role if needed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25114b",
   "metadata": {},
   "source": [
    "**Assume this role using sts to import** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3003e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assumed_role)\n",
    "session = boto3.Session()\n",
    "if assumed_role:\n",
    "    sts = session.client(\"sts\")\n",
    "    response = sts.assume_role(\n",
    "        RoleArn=str(assumed_role),\n",
    "        RoleSessionName=\"langchain-llm-1\"\n",
    "    )\n",
    "    print(response)\n",
    "    boto3_kwargs = dict(\n",
    "        aws_access_key_id=response['Credentials']['AccessKeyId'],\n",
    "        aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n",
    "        aws_session_token=response['Credentials']['SessionToken']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbca1d1",
   "metadata": {},
   "source": [
    "#### Continue the run -- to save the weights into S3 location\n",
    "\n",
    "**Important set your bucket and key values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "bucket = \"your bucket\"\n",
    "key = 'your_key/' # add the rest of the path later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1d7cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.SageMakerRuntime object at 0x7f7717c0e950>\n"
     ]
    }
   ],
   "source": [
    "boto3_sm_client = boto3.client(\n",
    "    \"sagemaker-runtime\",\n",
    "    **boto3_kwargs\n",
    ")\n",
    "print(boto3_sm_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5839c790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x7f76dce18ac0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    **boto3_kwargs\n",
    ")\n",
    "s3_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58989e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00006-of-00006.safetensors\n",
      "special_tokens_map.json\n",
      "model-00001-of-00006.safetensors\n",
      "model.safetensors.index.json\n",
      "tokenizer.json\n",
      "model-00002-of-00006.safetensors\n",
      "model-00004-of-00006.safetensors\n",
      "generation_config.json\n",
      "tokenizer.model\n",
      "model-00005-of-00006.safetensors\n",
      "model-00003-of-00006.safetensors\n",
      "config.json\n",
      "tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('./llama2_boolq_peft_merged_model/'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10d53068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00006-of-00006.safetensors\n",
      "None\n",
      "special_tokens_map.json\n",
      "None\n",
      "model-00001-of-00006.safetensors\n",
      "None\n",
      "model.safetensors.index.json\n",
      "None\n",
      "tokenizer.json\n",
      "None\n",
      "model-00002-of-00006.safetensors\n",
      "None\n",
      "model-00004-of-00006.safetensors\n",
      "None\n",
      "generation_config.json\n",
      "None\n",
      "tokenizer.model\n",
      "None\n",
      "model-00005-of-00006.safetensors\n",
      "None\n",
      "model-00003-of-00006.safetensors\n",
      "None\n",
      "config.json\n",
      "None\n",
      "tokenizer_config.json\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#s3.upoad_file(file_name, bucket, key)\n",
    "\n",
    "key = f'{key}code_merged/llama7b_boolq'\n",
    "\n",
    "file_name = './merged_model/*.safetensors'\n",
    "\n",
    "for one_file in os.listdir('./llama2_boolq_peft_merged_model/'):\n",
    "    print(one_file)\n",
    "    uploaded = s3_client.upload_file(f\"./llama2_boolq_peft_merged_model/{one_file}\", bucket, f\"{key}/{one_file}\")\n",
    "    print(uploaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f7709",
   "metadata": {},
   "source": [
    "## Now follow the steps from the link below to continue to import this model\n",
    "\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c6615",
   "metadata": {},
   "source": [
    "### The steps to import the Model into Bedrock can be \n",
    "\n",
    "1. Login to the Bedrock console on your account\n",
    "2. Click on the import Model screen, it will bring you the screen as shown below\n",
    "\n",
    "![Amazon Bedrock - Import Models](./images/import_jobs.png)\n",
    "\n",
    "3. Fill in the details including the S3 location where the weights have been uploaded as shown below\n",
    "\n",
    "![Amazon Bedrock - Import Jobs Configure](./images/import_jobs_perms.png)\n",
    "\n",
    "4. Specify the role or create a new one and then click import to run your jobs. You will see the imported jobs in the import jobs list as shown in the 1st image above\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/iam_role_import_jobs.png)\n",
    "\n",
    "Open your model in the Play ground to test it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c77e4",
   "metadata": {},
   "source": [
    "### Optional - Upload to HuggingFace Hub if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78863748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9866248",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df61173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693afb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bba3f-c4ad-4f26-8e21-7ea60a6978a6",
   "metadata": {},
   "source": [
    "### Clean Up \n",
    "\n",
    "You can delete your Imported Model in the console as shown in the image below:\n",
    "\n",
    "![Delete](./images/delete.png \"Delete\")\n",
    "\n",
    "Ensure to shut down your instance/compute that you have run this notebook on.\n",
    "\n",
    "**END OF NOTEBOOK**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
