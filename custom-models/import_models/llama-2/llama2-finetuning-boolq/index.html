<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Amazon Bedrock cookbook website"><meta name=author content=Bedrock-GTM><link href=https://github.amazon-bedrock-samples.com/custom-models/import_models/llama-2/llama2-finetuning-boolq/ rel=canonical><link rel=icon href=../../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.49"><title>Llama2 finetuning boolq - Amazon Bedrock Recipes</title><link rel=stylesheet href=../../../../assets/stylesheets/main.6f8fc17f.min.css><link rel=stylesheet href=../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Amazon Bedrock Recipes" class="md-header__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../../logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Amazon Bedrock Recipes </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Llama2 finetuning boolq </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Amazon Bedrock Recipes" class="md-nav__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../../logo.png alt=logo> </a> Amazon Bedrock Recipes </label> <div class=md-nav__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Features </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> Intro to Amazon Bedrock </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Intro to Amazon Bedrock </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> API Usage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> API Usage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../introduction-to-bedrock/bedrock_apis/01_invoke_api/ class=md-nav__link> <span class=md-ellipsis> Invoke Model API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../../introduction-to-bedrock/bedrock_apis/04_agents_api/ class=md-nav__link> <span class=md-ellipsis> Agents API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../../introduction-to-bedrock/bedrock_apis/03_knowledgebases_api/ class=md-nav__link> <span class=md-ellipsis> Knowledge Bases API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../../introduction-to-bedrock/bedrock_apis/02_guardrails_api/ class=md-nav__link> <span class=md-ellipsis> Guardrail API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../../introduction-to-bedrock/converse_api/01_converse_api/ class=md-nav__link> <span class=md-ellipsis> Converse API Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1> <label class=md-nav__link for=__nav_2_2_1 id=__nav_2_2_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/introduction-to-agents/how_to_create_custom_agents/ class=md-nav__link> <span class=md-ellipsis> How to create an Agent </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_2> <label class=md-nav__link for=__nav_2_2_1_2 id=__nav_2_2_1_2_label tabindex=0> <span class=md-ellipsis> Bedrock Agent Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_2> <span class="md-nav__icon md-icon"></span> Bedrock Agent Features </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/01-create-agent-with-function-definition/01-create-agent-with-function-definition/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Function Definition </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/02-create-agent-with-api-schema/02-create-agent-with-api-schema/ class=md-nav__link> <span class=md-ellipsis> Create Agent with API Schema </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/03-create-agent-with-return-of-control/03-create-agent-with-return-of-control/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/04-create-agent-with-single-knowledge-base/04-create-agent-with-single-knowledge-base/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Single Knowledge Base </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/05-create-agent-with-knowledge-base-and-action-group/05-create-agent-with-knowledge-base-and-action-group/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Knowledge Base and Action Group </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/06-prompt-and-session-attributes/06-prompt-and-session-attributes/ class=md-nav__link> <span class=md-ellipsis> Prompt and Session Attributes </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/07-advanced-prompts-and-custom-parsers/07-custom-prompt-and-lambda-parsers/ class=md-nav__link> <span class=md-ellipsis> Custom Prompt and Lambda Parsers </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/08-create-agent-with-guardrails/08-create-agent-with-guardrails/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Guardrails </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/09-create-agent-with-memory/09-create-agent-with-memory/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Memory </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/10-create-agent-with-code-interpreter/10-create-agent-with-code-interpreter/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Code Interpreter </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/14-create-agent-with-custom-orchestration/custom_orchestration_example/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Custom Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents/inline-agent-api-usage/ class=md-nav__link> <span class=md-ellipsis> Create Dynamic Tooling Inline Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_3> <label class=md-nav__link for=__nav_2_2_1_3 id=__nav_2_2_1_3_label tabindex=0> <span class=md-ellipsis> Bedrock Flows </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_3> <span class="md-nav__icon md-icon"></span> Bedrock Flows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/bedrock-flows/Getting_started_with_Prompt_Management_Flows/ class=md-nav__link> <span class=md-ellipsis> Getting Started with Prompt Management Flows </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_4> <label class=md-nav__link for=__nav_2_2_1_4 id=__nav_2_2_1_4_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_4> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/use-case-examples/text-2-sql-agent/create_and_invoke_sql_agent/ class=md-nav__link> <span class=md-ellipsis> Text to SQL Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/use-case-examples/agentsforbedrock-retailagent/workshop/test_retailagent_agentsforbedrock/ class=md-nav__link> <span class=md-ellipsis> Retail Agent Workshop </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/bedrock-agents/use-case-examples/product-review-agent/main/ class=md-nav__link> <span class=md-ellipsis> Product Review Agent </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_2> <label class=md-nav__link for=__nav_2_2_2 id=__nav_2_2_2_label tabindex=0> <span class=md-ellipsis> Function Calling </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_2> <span class="md-nav__icon md-icon"></span> Function Calling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/function-calling/function_calling_with_converse/function_calling_with_converse/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/function-calling/function_calling_with_invoke/function_calling_model_with_invoke/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Invoke </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/function-calling/return_of_control/return_of_control/ class=md-nav__link> <span class=md-ellipsis> Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/function-calling/tool_binding/tool_bindings/ class=md-nav__link> <span class=md-ellipsis> Tool Binding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3> <label class=md-nav__link for=__nav_2_2_3 id=__nav_2_2_3_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_1> <label class=md-nav__link for=__nav_2_2_3_1 id=__nav_2_2_3_1_label tabindex=0> <span class=md-ellipsis> CrewAI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_1> <span class="md-nav__icon md-icon"></span> CrewAI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/crew.ai/Find%20dream%20destination%20with%20CrewAI/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_2> <label class=md-nav__link for=__nav_2_2_3_2 id=__nav_2_2_3_2_label tabindex=0> <span class=md-ellipsis> LangGraph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_2> <span class="md-nav__icon md-icon"></span> LangGraph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-single-agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-agents-multimodal/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi-Modal Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/02_medibot_V3_agents/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent For Medical Chatbot </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-fact-checker-feedback-loop/ class=md-nav__link> <span class=md-ellipsis> LangGraph Fact Checker with Multi Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/03_langgraph_agents_of_agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent with tools </span> </a> </li> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/open-source-agents/langgraph/Travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Managing Memory for Multi Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_3> <label class=md-nav__link for=__nav_2_2_3_3 id=__nav_2_2_3_3_label tabindex=0> <span class=md-ellipsis> Multi Agent </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_3> <span class="md-nav__icon md-icon"></span> Multi Agent </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../agents-and-function-calling/introduction-to-agents/how_to_create_multi_agents_from_custom_agents/ class=md-nav__link> <span class=md-ellipsis> Multi Agent Orchestration </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> RAG </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1> <label class=md-nav__link for=__nav_2_3_1 id=__nav_2_3_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Knowledge Bases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Knowledge Bases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_1> <label class=md-nav__link for=__nav_2_3_1_1 id=__nav_2_3_1_1_label tabindex=0> <span class=md-ellipsis> Zero Setup </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_1> <span class="md-nav__icon md-icon"></span> Zero Setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/00-zero-setup-chat-with-your-document/chat_with_document_kb/ class=md-nav__link> <span class=md-ellipsis> Chat with Your Document </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_2> <label class=md-nav__link for=__nav_2_3_1_2 id=__nav_2_3_1_2_label tabindex=0> <span class=md-ellipsis> RAG Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_2> <span class="md-nav__icon md-icon"></span> RAG Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/01-rag-concepts/01_create_ingest_documents_test_kb_multi_ds/ class=md-nav__link> <span class=md-ellipsis> Create and Ingest Documents with Multi-Data Sources </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/01-rag-concepts/02_managed_rag_custom_prompting_and_no_of_results/ class=md-nav__link> <span class=md-ellipsis> Managed RAG with Custom Prompting </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/01-rag-concepts/03_customized-rag-retreive-api-hybrid-search-claude-3-sonnet-langchain/ class=md-nav__link> <span class=md-ellipsis> Customized RAG with Claude 3 and Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/01-rag-concepts/04_customized-rag-retreive-api-langchain-claude-evaluation-ragas/ class=md-nav__link> <span class=md-ellipsis> RAG Evaluation with Langchain and RAGAS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_3> <label class=md-nav__link for=__nav_2_3_1_3 id=__nav_2_3_1_3_label tabindex=0> <span class=md-ellipsis> Optimizing Retrieval Results </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_3> <span class="md-nav__icon md-icon"></span> Optimizing Retrieval Results </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/advanced_chunking_options/ class=md-nav__link> <span class=md-ellipsis> Advanced Chunking Options </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/csv_metadata_customization/ class=md-nav__link> <span class=md-ellipsis> CSV Metadata Customization </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/query_reformulation/ class=md-nav__link> <span class=md-ellipsis> Query Reformulation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4> <label class=md-nav__link for=__nav_2_3_1_4 id=__nav_2_3_1_4_label tabindex=0> <span class=md-ellipsis> Advanced Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4> <span class="md-nav__icon md-icon"></span> Advanced Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/03-advanced-concepts/dynamic-metadata-filtering/dynamic-metadata-filtering-KB/ class=md-nav__link> <span class=md-ellipsis> Dynamic Metadata Filtering </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4_2> <label class=md-nav__link for=__nav_2_3_1_4_2 id=__nav_2_3_1_4_2_label tabindex=0> <span class=md-ellipsis> Reranking </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4_2> <span class="md-nav__icon md-icon"></span> Reranking </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/01_deploy-reranking-model-sm/ class=md-nav__link> <span class=md-ellipsis> Deploy Reranking Model </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/02_kb-reranker/ class=md-nav__link> <span class=md-ellipsis> Knowledge Base Reranker </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/qa-generator/ class=md-nav__link> <span class=md-ellipsis> QA Generator </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_5> <label class=md-nav__link for=__nav_2_3_1_5 id=__nav_2_3_1_5_label tabindex=0> <span class=md-ellipsis> Responsible AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_5> <span class="md-nav__icon md-icon"></span> Responsible AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/features-examples/05-responsible-ai/contextual-grounding/ class=md-nav__link> <span class=md-ellipsis> Contextual Grounding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6> <label class=md-nav__link for=__nav_2_3_1_6 id=__nav_2_3_1_6_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_1> <label class=md-nav__link for=__nav_2_3_1_6_1 id=__nav_2_3_1_6_1_label tabindex=0> <span class=md-ellipsis> Metadata Filter Access Control </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_1> <span class="md-nav__icon md-icon"></span> Metadata Filter Access Control </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/use-case-examples/metadata-filter-access-control/kb-end-to-end-acl/ class=md-nav__link> <span class=md-ellipsis> End-to-End ACL with Knowledge Base </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_2> <label class=md-nav__link for=__nav_2_3_1_6_2 id=__nav_2_3_1_6_2_label tabindex=0> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_2> <span class="md-nav__icon md-icon"></span> RAG with Structured and Unstructured Data </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/0-create-dummy-structured-data/ class=md-nav__link> <span class=md-ellipsis> Create Dummy Structured Data </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/1_create_sql_dataset_optional/ class=md-nav__link> <span class=md-ellipsis> Create SQL Dataset (Optional) </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/2_rag_with_structured_unstructured_data/ class=md-nav__link> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2> <label class=md-nav__link for=__nav_2_3_2 id=__nav_2_3_2_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/open-source/chatbots/qa_chatbot_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chatbot using Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../../rag/open-source/chunking/rag_chunking_strategies_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chunking strategies for RAG applications </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2_3> <label class=md-nav__link for=__nav_2_3_2_3 id=__nav_2_3_2_3_label tabindex=0> <span class=md-ellipsis> Vector Stores </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2_3> <span class="md-nav__icon md-icon"></span> Vector Stores </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../rag/open-source/vector_stores/rag_langchain_bedrock_opensearch/ class=md-nav__link> <span class=md-ellipsis> Langchain Chatbot with Opensearch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Model Customization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Model Customization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../model-distillation/Historical_invocation_distillation/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with Invocation Logs </span> </a> </li> <li class=md-nav__item> <a href=../../../model-distillation/Distillation-via-S3-input/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with S3 Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Gen AI Usecases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Gen AI Usecases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Text Generation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Text Generation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../genai-use-cases/text-generation/how_to_work_with_text_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Streaming Response with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../../genai-use-cases/text-generation/how_to_work_with_code_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Generate Python Code with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../../genai-use-cases/text-generation/how_to_work_with_text_translation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Text Translation with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../../genai-use-cases/text-generation/how_to_work_with_text-summarization-titan%2Bclaude/ class=md-nav__link> <span class=md-ellipsis> Text summarization with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../../genai-use-cases/text-generation/how_to_work_with_batch_example_for_multi_threaded_invocation/ class=md-nav__link> <span class=md-ellipsis> Generate Bulk Emails with Batch Inference </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Workshops </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Workshops </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex=0> <span class=md-ellipsis> Open-source L400 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Open-source L400 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/01_usecase_introduction/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/02_Lab_Find%20a%20Dream%20Destination_RAG%20query/ class=md-nav__link> <span class=md-ellipsis> Advanced RAG for Agents </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/02_travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Conversational Memory in Agents </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/03_travel_agent_with_tools/ class=md-nav__link> <span class=md-ellipsis> Multi-Modal and Types of Agents </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/04_travel_booking_multi_agent/ class=md-nav__link> <span class=md-ellipsis> Multi-Agent Collaboration with Human-in-loop </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/05_dream_destination_with_crewai/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/06_agent_evaluation_with_ragas/ class=md-nav__link> <span class=md-ellipsis> RAGAs Agents Evaluation </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l400/07_dynamic_tooling_agents/ class=md-nav__link> <span class=md-ellipsis> Dynamic Tool invocation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> Open-source L200 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Open-source L200 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../workshop/open-source-l200/02_contextual_text_generation/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l200/03_retrieval_based_text_application/ class=md-nav__link> <span class=md-ellipsis> Retrieval Based Text Generation </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l200/04_retrieval_based_chat_application/ class=md-nav__link> <span class=md-ellipsis> Retrieval Based Chat Application </span> </a> </li> <li class=md-nav__item> <a href=../../../../workshop/open-source-l200/05_agent_based_text_generation/ class=md-nav__link> <span class=md-ellipsis> Agent Based Text Generation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../general/tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../../general/license/ class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1> Fine-Tuning LLaMA 2: A Step-by-Step Instructional Guide and then import into Bedrock </h1> <h2> Overview </h2> <p>In this tutorial, we will fine-tune LlaMA 2 so it can summarize customer support tickets. This will be done by using SFTTrainer from HuggingFace, and the BoolQ dataset pulled from the HuggingFace hub. </p> <p>This notebook will use the HuggingFace Transformers library to fine tune LLaMA 2. due to the fine tuning process being "local" you can run this notebook anywhere as long as the proper compute is available. </p> <ol> <li>Configuring an AWS EC2 instance with a Deep Learning AMI, and setting up a Jupyter Server: <a href=https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html>Link</a></li> <li>Configuring an Amazon Sagemaker environment: <a href=https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html>Link</a></li> <li>Configure your own environment, with equivalent compute</li> </ol> <p>For this tutorial, 4 A10 High-RAM GPUs were used, which provided robust performance for running the the model. </p> <h2> Usecase </h2> <p>BoolQ is a question answering dataset for yes/no questions. This will allow the model to be optimized for answering user questions. this philosophy can be extrapolated to all Q &amp; A datasets. If the LLM answers need to be standardized Supervised fine tuning is an excellent choice. </p> <h2> Amazon Bedrock Custom Model Import (CMI) </h2> <p>The resulting model files are imported into Amazon Bedrock via <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html>Custom Model Import (CMI)</a>. </p> <p>Bedrock Custom Model Import allows for importing foundation models that have been customized in other environments outside of Amazon Bedrock, such as Amazon Sagemaker, EC2, etc. </p> <h2> Pre-requisites </h2> <p>Below are the commands to install the essential libraries:</p> <ul> <li>The keys steps are :</li> <li>Run the training job </li> <li>Save the LORA peft weights as safetensors</li> <li>Load and Merge with the base adapter and save it to a folder as safetensors</li> <li>Upload the merged weights to S3</li> <li>Run the Import jobs in Bedrock</li> </ul> <h2> Notebook code with comments: </h2> <h3> Install the essential libraries </h3> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=o>-</span><span class=n>q</span> <span class=n>accelerate</span> <span class=n>peft</span> <span class=n>bitsandbytes</span> <span class=n>transformers</span><span class=o>==</span><span class=mf>4.38.2</span> <span class=n>trl</span><span class=o>==</span><span class=mf>0.7.10</span> <span class=n>torch</span><span class=o>==</span><span class=mf>2.1.1</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>tensorboard</span> <span class=n>safetensors</span>
</code></pre></div> <h3> We will use the BoolQ data set and the SFTTrainer from HuggingFace </h3> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>from</span> <span class=nn>datasets</span> <span class=kn>import</span> <span class=n>load_dataset</span><span class=p>,</span> <span class=n>Dataset</span>
<span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=p>(</span>
    <span class=n>AutoModelForCausalLM</span><span class=p>,</span>
    <span class=n>AutoTokenizer</span><span class=p>,</span>
    <span class=n>BitsAndBytesConfig</span><span class=p>,</span>
    <span class=n>HfArgumentParser</span><span class=p>,</span>
    <span class=n>TrainingArguments</span><span class=p>,</span>
    <span class=n>pipeline</span><span class=p>,</span>
    <span class=n>logging</span><span class=p>,</span>
<span class=p>)</span>
<span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>PeftModel</span>
<span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>SFTTrainer</span>
<span class=kn>from</span> <span class=nn>safetensors.torch</span> <span class=kn>import</span> <span class=n>save_model</span>
</code></pre></div> <h3> Change the cache location to match your local set up -- below is an example location </h3> <div class=highlight><pre><span></span><code><span class=n>hf_cache_dir</span><span class=o>=</span><span class=s1>&#39;/home/ubuntu/sm/cache/huggingface/hub&#39;</span>
<span class=err>!</span><span class=n>mkdir</span> <span class=o>-</span><span class=n>p</span> <span class=p>{</span><span class=n>hf_cache_dir</span><span class=p>}</span>
<span class=kn>import</span> <span class=nn>os</span>
<span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;TRANSFORMERS_CACHE&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>hf_cache_dir</span>
<span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;HF_HOME&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>hf_cache_dir</span>
<span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;HF_DATASETS_CACHE&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>hf_cache_dir</span>
<span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;TORCH_HOME&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>hf_cache_dir</span>
<span class=err>!</span><span class=n>export</span> <span class=n>TRANSFORMERS_CACHE</span><span class=o>=/</span><span class=n>home</span><span class=o>/</span><span class=n>ubuntu</span><span class=o>/</span><span class=n>sm</span><span class=o>/</span><span class=n>cache</span><span class=o>/</span><span class=n>huggingface</span><span class=o>/</span><span class=n>hub</span>
<span class=err>!</span><span class=n>export</span> <span class=n>HF_HOME</span><span class=o>=/</span><span class=n>home</span><span class=o>/</span><span class=n>ubuntu</span><span class=o>/</span><span class=n>sm</span><span class=o>/</span><span class=n>cache</span><span class=o>/</span><span class=n>huggingface</span><span class=o>/</span><span class=n>hub</span>
<span class=err>!</span><span class=n>export</span> <span class=n>HF_DATASETS_CACHE</span><span class=o>=/</span><span class=n>home</span><span class=o>/</span><span class=n>ubuntu</span><span class=o>/</span><span class=n>sm</span><span class=o>/</span><span class=n>cache</span><span class=o>/</span><span class=n>huggingface</span><span class=o>/</span><span class=n>hub</span>
<span class=err>!</span><span class=n>export</span> <span class=n>TORCH_HOME</span><span class=o>=/</span><span class=n>home</span><span class=o>/</span><span class=n>ubuntu</span><span class=o>/</span><span class=n>sm</span><span class=o>/</span><span class=n>cache</span><span class=o>/</span><span class=n>huggingface</span><span class=o>/</span><span class=n>hub</span>
</code></pre></div> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</code></pre> <h3> Optional Download the model -- </h3> <p><strong>From HuggingFace TheBloke/Llama-2-7b-fp16</strong></p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>huggingface_hub</span> <span class=kn>import</span> <span class=n>snapshot_download</span>
<span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
<span class=kn>import</span> <span class=nn>os</span>

<span class=c1># - This will download the model into the current directory where ever the jupyter notebook is running</span>
<span class=n>local_model_path</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&quot;/home/ubuntu/sm/llama_train/llama-7b&quot;</span><span class=p>)</span>
<span class=n>local_model_path</span><span class=o>.</span><span class=n>mkdir</span><span class=p>(</span><span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>model_name</span> <span class=o>=</span> <span class=s2>&quot;TheBloke/Llama-2-7b-fp16&quot;</span>
<span class=c1># Only download pytorch checkpoint files</span>
<span class=n>allow_patterns</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&quot;*.json&quot;</span><span class=p>,</span> <span class=s2>&quot;*.txt&quot;</span><span class=p>,</span> <span class=s2>&quot;*.model&quot;</span><span class=p>,</span> <span class=s2>&quot;*.safetensors&quot;</span><span class=p>,</span> <span class=s2>&quot;*.bin&quot;</span><span class=p>,</span> <span class=s2>&quot;*.chk&quot;</span><span class=p>,</span> <span class=s2>&quot;*.pth&quot;</span><span class=p>,</span> <span class=s2>&quot;*.py&quot;</span><span class=p>]</span>

<span class=c1># - Leverage the snapshot library to donload the model since the model is stored in repository using LFS</span>
<span class=c1># model_download_path = snapshot_download(</span>
<span class=c1>#     repo_id=model_name, cache_dir=local_model_path, allow_patterns=allow_patterns</span>
<span class=c1># )</span>
<span class=c1># print(model_download_path)</span>
<span class=c1>#</span>
<span class=c1>#- Identify the snapshot id</span>
<span class=c1># model_download_path=&#39;/home/ubuntu/SageMaker/models/llama/llama-7b/models--TheBloke--Llama-2-7b-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239&#39;</span>
<span class=c1># print(model_download_path)</span>

<span class=c1># model_loc = &quot;/home/ubuntu/sm/llama_train/llama-7b/actual_model/&quot;</span>
<span class=c1># !cp -L {model_download_path}/* {model_loc} --recursive</span>
<span class=c1># !ls -alrt {model_loc}</span>
</code></pre></div> <h3> Continue the training </h3> <h3> You can alternately set the model_loc to be </h3> <p><strong>TheBloke/Llama-2-7b-fp16 or use the actual path to the downloaded weights</strong> </p> <div class=highlight><pre><span></span><code><span class=c1>#- base model loc -</span>
<span class=n>model_loc</span> <span class=o>=</span> <span class=s2>&quot;/home/ubuntu/sm/llama_train/llama-7b/actual_model/&quot;</span>
</code></pre></div> <h3> Configurations </h3> <p>The lora peft values. Please note these are just for sample and not for production use case. We will save the trained model under llama-2-7b-boolq and then merge the weights</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>create_model_and_tokenizer</span><span class=p>(</span><span class=n>model_loc</span><span class=p>,</span> <span class=n>bnb_config</span><span class=p>):</span>


    <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
        <span class=n>model_loc</span><span class=p>,</span>
        <span class=n>cache_dir</span><span class=o>=</span><span class=n>hf_cache_dir</span><span class=p>,</span>
        <span class=n>use_safetensors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>quantization_config</span><span class=o>=</span><span class=n>bnb_config</span><span class=p>,</span>
        <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
        <span class=n>device_map</span><span class=o>=</span><span class=s2>&quot;auto&quot;</span><span class=p>,</span>
        <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span> <span class=c1>#torch.float32, # - float16 </span>
    <span class=p>)</span>

    <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_cache</span> <span class=o>=</span> <span class=kc>False</span>
    <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>pretraining_tp</span> <span class=o>=</span> <span class=mi>1</span>

    <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_loc</span><span class=p>)</span>
    <span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
    <span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&quot;right&quot;</span>


    <span class=k>return</span> <span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># The model that you want to train from the Hugging Face hub</span>
<span class=c1>#model_name = &quot;NousResearch/llama-2-7b-chat-hf&quot;</span>

<span class=c1># The instruction dataset to use</span>
<span class=n>dataset_name</span> <span class=o>=</span> <span class=s2>&quot;google/boolq&quot;</span>

<span class=c1># Fine-tuned model name</span>
<span class=n>new_model</span> <span class=o>=</span> <span class=s2>&quot;llama-2-7b-boolq&quot;</span>

<span class=c1>################################################################################</span>
<span class=c1># QLoRA parameters</span>
<span class=c1>################################################################################</span>

<span class=c1># LoRA attention dimension</span>
<span class=n>lora_r</span> <span class=o>=</span> <span class=mi>64</span>

<span class=c1># Alpha parameter for LoRA scaling</span>
<span class=n>lora_alpha</span> <span class=o>=</span> <span class=mi>16</span>

<span class=c1># Dropout probability for LoRA layers</span>
<span class=n>lora_dropout</span> <span class=o>=</span> <span class=mf>0.1</span>

<span class=c1>################################################################################</span>
<span class=c1># bitsandbytes parameters</span>
<span class=c1>################################################################################</span>

<span class=c1># Activate 4-bit precision base model loading</span>
<span class=n>use_4bit</span> <span class=o>=</span> <span class=kc>True</span>

<span class=c1># Compute dtype for 4-bit base models</span>
<span class=n>bnb_4bit_compute_dtype</span> <span class=o>=</span> <span class=s2>&quot;float16&quot;</span>

<span class=c1># Quantization type (fp4 or nf4)</span>
<span class=n>bnb_4bit_quant_type</span> <span class=o>=</span> <span class=s2>&quot;nf4&quot;</span>

<span class=c1># Activate nested quantization for 4-bit base models (double quantization)</span>
<span class=n>use_nested_quant</span> <span class=o>=</span> <span class=kc>False</span>

<span class=c1>################################################################################</span>
<span class=c1># TrainingArguments parameters</span>
<span class=c1>################################################################################</span>

<span class=c1># Output directory where the model predictions and checkpoints will be stored</span>
<span class=n>output_dir</span> <span class=o>=</span> <span class=s2>&quot;./results&quot;</span>

<span class=err>!</span> <span class=n>rm</span> <span class=o>-</span><span class=n>rf</span> <span class=o>./</span><span class=n>results</span>

<span class=c1># Number of training epochs</span>
<span class=n>num_train_epochs</span> <span class=o>=</span> <span class=mi>1</span>

<span class=c1># Enable fp16/bf16 training (set bf16 to True with an A100)</span>
<span class=n>fp16</span> <span class=o>=</span> <span class=kc>False</span>
<span class=n>bf16</span> <span class=o>=</span> <span class=kc>False</span>

<span class=c1># Batch size per GPU for training</span>
<span class=n>per_device_train_batch_size</span> <span class=o>=</span> <span class=mi>4</span>

<span class=c1># Batch size per GPU for evaluation</span>
<span class=n>per_device_eval_batch_size</span> <span class=o>=</span> <span class=mi>4</span>

<span class=c1># Number of update steps to accumulate the gradients for</span>
<span class=n>gradient_accumulation_steps</span> <span class=o>=</span> <span class=mi>1</span>

<span class=c1># Enable gradient checkpointing</span>
<span class=n>gradient_checkpointing</span> <span class=o>=</span> <span class=kc>True</span>

<span class=c1># Maximum gradient normal (gradient clipping)</span>
<span class=n>max_grad_norm</span> <span class=o>=</span> <span class=mf>0.3</span>

<span class=c1># Initial learning rate (AdamW optimizer)</span>
<span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>2e-4</span>

<span class=c1># Weight decay to apply to all layers except bias/LayerNorm weights</span>
<span class=n>weight_decay</span> <span class=o>=</span> <span class=mf>0.001</span>

<span class=c1># Optimizer to use</span>
<span class=n>optim</span> <span class=o>=</span> <span class=s2>&quot;paged_adamw_32bit&quot;</span>

<span class=c1># Learning rate schedule (constant a bit better than cosine)</span>
<span class=n>lr_scheduler_type</span> <span class=o>=</span> <span class=s2>&quot;constant&quot;</span>

<span class=c1># Number of training steps (overrides num_train_epochs)</span>
<span class=n>max_steps</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>

<span class=c1># Ratio of steps for a linear warmup (from 0 to learning rate)</span>
<span class=n>warmup_ratio</span> <span class=o>=</span> <span class=mf>0.03</span>

<span class=c1># Group sequences into batches with same length</span>
<span class=c1># Saves memory and speeds up training considerably</span>
<span class=n>group_by_length</span> <span class=o>=</span> <span class=kc>True</span>

<span class=c1># Save checkpoint every X updates steps</span>
<span class=n>save_steps</span> <span class=o>=</span> <span class=mi>25</span>

<span class=c1># Log every X updates steps</span>
<span class=n>logging_steps</span> <span class=o>=</span> <span class=mi>25</span>

<span class=c1>################################################################################</span>
<span class=c1># SFT parameters</span>
<span class=c1>################################################################################</span>

<span class=c1># Maximum sequence length to use</span>
<span class=n>max_seq_length</span> <span class=o>=</span> <span class=kc>None</span>

<span class=c1># Pack multiple short examples in the same input sequence to increase efficiency</span>
<span class=n>packing</span> <span class=o>=</span> <span class=kc>False</span>

<span class=c1># OPTIONAL depending on Instance Type Load the entire model on the GPU 0</span>
<span class=c1>#device_map = {&quot;&quot;: 0}</span>
</code></pre></div> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</code></pre> <h3> Load the data set </h3> <p>we are using Boolq</p> <div class=highlight><pre><span></span><code><span class=c1>#Process dataset</span>
<span class=n>boolq_dataset</span> <span class=o>=</span> <span class=n>load_dataset</span><span class=p>(</span><span class=n>dataset_name</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&quot;train&quot;</span><span class=p>)</span>
<span class=k>def</span> <span class=nf>process_dataset</span><span class=p>(</span><span class=n>dataset</span><span class=p>):</span>
    <span class=n>processed_data</span> <span class=o>=</span> <span class=p>[]</span>

    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>dataset</span><span class=p>:</span>
        <span class=n>question</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;question&#39;</span><span class=p>]</span>
        <span class=n>answer</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;answer&#39;</span><span class=p>]</span>
        <span class=n>passage</span> <span class=o>=</span> <span class=n>row</span><span class=p>[</span><span class=s1>&#39;passage&#39;</span><span class=p>]</span>

        <span class=n>system_prompt</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;Context: </span><span class=si>{</span><span class=n>passage</span><span class=si>}</span><span class=s2>&quot;</span>
        <span class=n>user_message</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;Question: </span><span class=si>{</span><span class=n>question</span><span class=si>}</span><span class=s2>&quot;</span>
        <span class=n>model_answer</span> <span class=o>=</span> <span class=n>answer</span>

        <span class=n>formatted_example</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;</span><span class=si>{</span><span class=n>system_prompt</span><span class=si>}</span><span class=s2>&lt;&lt;/SYS&gt;&gt;</span><span class=si>{</span><span class=n>user_message</span><span class=si>}</span><span class=s2> [/INST] </span><span class=si>{</span><span class=n>model_answer</span><span class=si>}</span><span class=s2> &lt;/s&gt;&quot;</span>
        <span class=n>processed_data</span><span class=o>.</span><span class=n>append</span><span class=p>({</span><span class=s2>&quot;text&quot;</span><span class=p>:</span> <span class=n>formatted_example</span><span class=p>})</span>

    <span class=k>return</span> <span class=n>processed_data</span>
<span class=c1>#Create list of dictionaries from the dataset. Dataset class expects list of dictionaries</span>
<span class=n>dataset_list</span> <span class=o>=</span> <span class=n>process_dataset</span><span class=p>(</span><span class=n>boolq_dataset</span><span class=p>)</span>

<span class=c1># Create a Dataset Object</span>
<span class=n>dataset</span> <span class=o>=</span> <span class=n>Dataset</span><span class=o>.</span><span class=n>from_list</span><span class=p>(</span><span class=n>dataset_list</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Load dataset (you can process it here)</span>
<span class=c1>#dataset = load_dataset(dataset_name, split=&quot;train&quot;)</span>

<span class=c1># Load tokenizer and model with QLoRA configuration</span>
<span class=n>compute_dtype</span> <span class=o>=</span> <span class=nb>getattr</span><span class=p>(</span><span class=n>torch</span><span class=p>,</span> <span class=n>bnb_4bit_compute_dtype</span><span class=p>)</span>

<span class=n>bnb_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
    <span class=n>load_in_4bit</span><span class=o>=</span><span class=n>use_4bit</span><span class=p>,</span>
    <span class=n>bnb_4bit_quant_type</span><span class=o>=</span><span class=n>bnb_4bit_quant_type</span><span class=p>,</span>
    <span class=n>bnb_4bit_compute_dtype</span><span class=o>=</span><span class=n>compute_dtype</span><span class=p>,</span>
    <span class=n>bnb_4bit_use_double_quant</span><span class=o>=</span><span class=n>use_nested_quant</span><span class=p>,</span>
<span class=p>)</span>

<span class=c1># Check GPU compatibility with bfloat16</span>
<span class=k>if</span> <span class=n>compute_dtype</span> <span class=o>==</span> <span class=n>torch</span><span class=o>.</span><span class=n>float16</span> <span class=ow>and</span> <span class=n>use_4bit</span><span class=p>:</span>
    <span class=n>major</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>get_device_capability</span><span class=p>()</span>
    <span class=k>if</span> <span class=n>major</span> <span class=o>&gt;=</span> <span class=mi>8</span><span class=p>:</span>
        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;Your GPU supports bfloat16: accelerate training with bf16=True&quot;</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=s2>&quot;=&quot;</span> <span class=o>*</span> <span class=mi>80</span><span class=p>)</span>

<span class=c1># Load base model</span>
<span class=c1># model = AutoModelForCausalLM.from_pretrained(</span>
<span class=c1>#     model_name,</span>
<span class=c1>#     quantization_config=bnb_config,</span>
<span class=c1>#     device_map=&quot;auto&quot;</span>
<span class=c1># )</span>
<span class=c1># model.config.use_cache = False</span>
<span class=c1># model.config.pretraining_tp = 1</span>

<span class=c1># # Load LLaMA tokenizer</span>
<span class=c1># tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)</span>
<span class=c1># tokenizer.pad_token = tokenizer.eos_token</span>
<span class=c1># tokenizer.padding_side = &quot;right&quot; # Fix weird overflow issue with fp16 training</span>

<span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>create_model_and_tokenizer</span><span class=p>(</span><span class=n>model_loc</span><span class=p>,</span><span class=n>bnb_config</span><span class=p>)</span>

<span class=c1># Load LoRA configuration</span>
<span class=n>peft_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
    <span class=n>lora_alpha</span><span class=o>=</span><span class=n>lora_alpha</span><span class=p>,</span>
    <span class=n>lora_dropout</span><span class=o>=</span><span class=n>lora_dropout</span><span class=p>,</span>
    <span class=n>r</span><span class=o>=</span><span class=n>lora_r</span><span class=p>,</span>
    <span class=n>bias</span><span class=o>=</span><span class=s2>&quot;none&quot;</span><span class=p>,</span>
    <span class=n>task_type</span><span class=o>=</span><span class=s2>&quot;CAUSAL_LM&quot;</span><span class=p>,</span>
<span class=p>)</span>

<span class=c1># Set training parameters</span>
<span class=n>training_arguments</span> <span class=o>=</span> <span class=n>TrainingArguments</span><span class=p>(</span>
    <span class=n>output_dir</span><span class=o>=</span><span class=n>output_dir</span><span class=p>,</span>
    <span class=n>num_train_epochs</span><span class=o>=</span><span class=n>num_train_epochs</span><span class=p>,</span>
    <span class=n>per_device_train_batch_size</span><span class=o>=</span><span class=n>per_device_train_batch_size</span><span class=p>,</span>
    <span class=n>gradient_accumulation_steps</span><span class=o>=</span><span class=n>gradient_accumulation_steps</span><span class=p>,</span>
    <span class=n>optim</span><span class=o>=</span><span class=n>optim</span><span class=p>,</span>
    <span class=n>save_steps</span><span class=o>=</span><span class=n>save_steps</span><span class=p>,</span>
    <span class=n>logging_steps</span><span class=o>=</span><span class=n>logging_steps</span><span class=p>,</span>
    <span class=n>learning_rate</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>,</span>
    <span class=n>weight_decay</span><span class=o>=</span><span class=n>weight_decay</span><span class=p>,</span>
    <span class=n>fp16</span><span class=o>=</span><span class=n>fp16</span><span class=p>,</span>
    <span class=n>bf16</span><span class=o>=</span><span class=n>bf16</span><span class=p>,</span>
    <span class=n>max_grad_norm</span><span class=o>=</span><span class=n>max_grad_norm</span><span class=p>,</span>
    <span class=n>max_steps</span><span class=o>=</span><span class=n>max_steps</span><span class=p>,</span>
    <span class=n>warmup_ratio</span><span class=o>=</span><span class=n>warmup_ratio</span><span class=p>,</span>
    <span class=n>group_by_length</span><span class=o>=</span><span class=n>group_by_length</span><span class=p>,</span>
    <span class=n>lr_scheduler_type</span><span class=o>=</span><span class=n>lr_scheduler_type</span><span class=p>,</span>
    <span class=n>report_to</span><span class=o>=</span><span class=s2>&quot;tensorboard&quot;</span>
<span class=p>)</span>

<span class=c1># Set supervised fine-tuning parameters</span>
<span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
    <span class=n>train_dataset</span><span class=o>=</span><span class=n>dataset</span><span class=p>,</span>
    <span class=n>peft_config</span><span class=o>=</span><span class=n>peft_config</span><span class=p>,</span>
    <span class=n>dataset_text_field</span><span class=o>=</span><span class=s2>&quot;text&quot;</span><span class=p>,</span>
    <span class=n>max_seq_length</span><span class=o>=</span><span class=n>max_seq_length</span><span class=p>,</span>
    <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span>
    <span class=n>args</span><span class=o>=</span><span class=n>training_arguments</span><span class=p>,</span>
    <span class=n>packing</span><span class=o>=</span><span class=n>packing</span><span class=p>,</span>
<span class=p>)</span>

<span class=c1># Train model</span>
<span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>

<span class=c1># Save trained model as .bin</span>
<span class=c1>#trainer.model.save_pretrained(new_model)</span>

<span class=c1>#save model as safetensors</span>
<span class=c1>#save_model(trainer.model, new_model)</span>
</code></pre></div> <h3> Print the location of the new saved Peft Weights and save it in safetensors format </h3> <div class=highlight><pre><span></span><code><span class=n>new_model</span>
</code></pre></div> <pre><code>'llama-2-7b-boolq'
</code></pre> <div class=highlight><pre><span></span><code><span class=n>trainer</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=n>new_model</span><span class=p>,</span> <span class=n>safe_serialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=c1># saves the peft weights</span>
</code></pre></div> <pre><code>/home/ubuntu/sm/virtualenv/trainenv/lib/python3.10/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /home/ubuntu/sm/llama_train/llama-7b/actual_model/ - will assume that the vocabulary was not modified.
  warnings.warn(
</code></pre> <div class=highlight><pre><span></span><code><span class=err>!</span> <span class=n>ls</span> <span class=o>-</span><span class=n>alrt</span> <span class=p>{</span><span class=n>new_model</span><span class=p>}</span>
</code></pre></div> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


total 67884
-rw-rw-r--  1 ubuntu ubuntu     1061 May 12 00:34 tokenizer_config.json
-rw-rw-r--  1 ubuntu ubuntu   499723 May 12 00:34 tokenizer.model
-rw-rw-r--  1 ubuntu ubuntu      434 May 12 00:34 special_tokens_map.json
-rw-rw-r--  1 ubuntu ubuntu       21 May 12 00:34 added_tokens.json
-rw-rw-r--  1 ubuntu ubuntu  1842945 May 12 00:34 tokenizer.json
-rw-rw-r--  1 ubuntu ubuntu      712 May 12 00:34 config.json
drwxrwxr-x  2 ubuntu ubuntu     4096 May 12 00:34 .
drwxrwxr-x 27 ubuntu ubuntu     4096 May 12 01:01 ..
-rw-rw-r--  1 ubuntu ubuntu     5126 May 12 01:40 README.md
-rw-rw-r--  1 ubuntu ubuntu 67126104 May 12 01:40 adapter_model.safetensors
-rw-rw-r--  1 ubuntu ubuntu      625 May 12 01:40 adapter_config.json
</code></pre> <h3> Merge the weights and save to local location </h3> <p>The steps: 1. Load from the saved weights. this loads peft and the base weights 2. run merge_and_unload() 3. Save these in a new folder</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>AutoPeftModelForCausalLM</span>
<span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
<span class=kn>import</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>shutil</span>


<span class=c1># merge base + LoRa models and save the model</span>
<span class=n>trained_model</span> <span class=o>=</span> <span class=n>AutoPeftModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
    <span class=n>new_model</span><span class=p>,</span>
    <span class=c1># low_cpu_mem_usage=True,</span>
    <span class=c1># torch_dtype=torch.bfloat16</span>
<span class=p>)</span>


<span class=n>shutil</span><span class=o>.</span><span class=n>rmtree</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>,</span> <span class=n>ignore_errors</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>,</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>)</span>

<span class=n>merged_model</span> <span class=o>=</span> <span class=n>trained_model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span>
<span class=n>merged_model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>,</span> <span class=n>safe_serialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>)</span>

<span class=err>!</span><span class=n>ls</span> <span class=o>-</span><span class=n>alrt</span> <span class=o>./</span><span class=n>llama2_boolq_peft_merged_model</span>
</code></pre></div> <pre><code>Loading checkpoint shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]


./llama2_boolq_peft_merged_model


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


total 26324356
drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..
-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json
-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json
-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json
-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model
-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json
-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json
-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json
drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .
</code></pre> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>ls</span> <span class=o>-</span><span class=n>alrt</span> <span class=o>./</span><span class=n>llama2_boolq_peft_merged_model</span>
</code></pre></div> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


total 26324376
drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..
-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json
-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json
-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json
-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model
-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json
-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json
-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json
drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .
</code></pre> <div class=highlight><pre><span></span><code><span class=c1>#Free memory for merging weights</span>
<span class=k>del</span> <span class=n>trainer</span>
<span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>empty_cache</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Ignore warnings</span>
<span class=n>logging</span><span class=o>.</span><span class=n>set_verbosity</span><span class=p>(</span><span class=n>logging</span><span class=o>.</span><span class=n>CRITICAL</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=c1># Run text generation pipeline with our next model</span>
<span class=n>prompt</span> <span class=o>=</span> <span class=s2>&quot;is the bicuspid valve the same as the mitral valve?&quot;</span>
<span class=n>pipe</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=n>task</span><span class=o>=</span><span class=s2>&quot;text-generation&quot;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>merged_model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=o>=</span><span class=n>tokenizer</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>200</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>result</span> <span class=o>=</span> <span class=n>pipe</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;&lt;s&gt;[INST] </span><span class=si>{</span><span class=n>prompt</span><span class=si>}</span><span class=s2> [/INST]&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>result</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;generated_text&#39;</span><span class=p>])</span>
</code></pre></div> <pre><code>&lt;s&gt;[INST] is the bicuspid valve the same as the mitral valve? [/INST] False. The mitral valve is located between the left atrium and the left ventricle, whereas the bicuspid valve is located between the right atrium and the right ventricle.&lt;&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/&lt;/
</code></pre> <p><h3> Alternate way of creating the merged model. Here </h3></p> <ol> <li>we first load the base model</li> <li>Load the peft model and merge</li> <li>set the config to false to avoid any transformers library issue </li> <li>Save the merged model</li> </ol> <div class=highlight><pre><span></span><code><span class=c1># Reload model in FP16 and merge it with LoRA weights</span>
<span class=n>base_model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
    <span class=n>model_loc</span><span class=p>,</span>
    <span class=n>low_cpu_mem_usage</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>return_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float16</span><span class=p>,</span>
    <span class=n>device_map</span><span class=o>=</span><span class=s2>&quot;auto&quot;</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>model</span> <span class=o>=</span> <span class=n>PeftModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>base_model</span><span class=p>,</span> <span class=n>new_model</span><span class=p>)</span>
<span class=n>merged_model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>merge_and_unload</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_loc</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>tokenizer</span><span class=o>.</span><span class=n>pad_token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token</span>
<span class=n>tokenizer</span><span class=o>.</span><span class=n>padding_side</span> <span class=o>=</span> <span class=s2>&quot;right&quot;</span>
<span class=n>tokenizer</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s1>&#39;./llama2_boolq_peft_merged_model&#39;</span><span class=p>)</span>
<span class=n>merged_model</span><span class=o>.</span><span class=n>generation_config</span><span class=o>.</span><span class=n>do_sample</span> <span class=o>=</span> <span class=kc>True</span>
<span class=n>merged_model</span><span class=o>.</span><span class=n>save_pretrained</span><span class=p>(</span><span class=s2>&quot;./llama2_boolq_peft_merged_model&quot;</span><span class=p>,</span> <span class=n>safe_serialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>ls</span> <span class=o>-</span><span class=n>alrt</span> <span class=o>./</span><span class=n>llama2_boolq_peft_merged_model</span>
</code></pre></div> <pre><code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


total 26324376
drwxrwxr-x 27 ubuntu ubuntu       4096 May 12 01:40 ..
-rw-rw-r--  1 ubuntu ubuntu        132 May 12 01:40 generation_config.json
-rw-rw-r--  1 ubuntu ubuntu        731 May 12 01:40 config.json
-rw-rw-r--  1 ubuntu ubuntu 4840412800 May 12 01:40 model-00001-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206856 May 12 01:40 model-00002-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00003-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:40 model-00004-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 4857206904 May 12 01:41 model-00005-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu 2684488496 May 12 01:41 model-00006-of-00006.safetensors
-rw-rw-r--  1 ubuntu ubuntu        871 May 12 01:41 tokenizer_config.json
-rw-rw-r--  1 ubuntu ubuntu     499723 May 12 01:41 tokenizer.model
-rw-rw-r--  1 ubuntu ubuntu        434 May 12 01:41 special_tokens_map.json
-rw-rw-r--  1 ubuntu ubuntu      23950 May 12 01:41 model.safetensors.index.json
-rw-rw-r--  1 ubuntu ubuntu    1842863 May 12 01:41 tokenizer.json
drwxrwxr-x  2 ubuntu ubuntu       4096 May 12 01:41 .
</code></pre> <h3> Copy to S3 location </h3> <p><strong>Important</strong> this file assumes you are running the training job on a EC2. Hence we assume a role and then upload to S3. This steps is completely optional and if you run via SageMaker studio you will not need to assume the role and instead just run the s3_upload cells directly</p> <p>the merged weights are under -- ./llama2_boolq_peft_merged_model</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>boto3</span>


<span class=n>boto3_kwargs</span> <span class=o>=</span> <span class=p>{}</span>
</code></pre></div> <h3> This is optional </h3> <p><strong>If you are runing the notebook from outise of SageMaker then use the below to so sts:assume role successfully</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>boto3</span>


<span class=c1># os.environ[&quot;AWS_ACCESS_KEY_ID&quot;] = &quot;your key&quot;</span>
<span class=c1># os.environ[&quot;AWS_SECRET_ACCESS_KEY&quot;] = &quot;your access&quot;</span>
<span class=c1># os.environ[&quot;AWS_DEFAULT_REGION&quot;] = &quot;us-east-1&quot;</span>


<span class=n>assumed_role</span> <span class=o>=</span> <span class=kc>None</span> <span class=c1># &quot;set to your assume role if needed&quot;</span>
</code></pre></div> <p><strong>Assume this role using sts to import</strong> </p> <div class=highlight><pre><span></span><code><span class=nb>print</span><span class=p>(</span><span class=n>assumed_role</span><span class=p>)</span>
<span class=n>session</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
<span class=k>if</span> <span class=n>assumed_role</span><span class=p>:</span>
    <span class=n>sts</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>client</span><span class=p>(</span><span class=s2>&quot;sts&quot;</span><span class=p>)</span>
    <span class=n>response</span> <span class=o>=</span> <span class=n>sts</span><span class=o>.</span><span class=n>assume_role</span><span class=p>(</span>
        <span class=n>RoleArn</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>assumed_role</span><span class=p>),</span>
        <span class=n>RoleSessionName</span><span class=o>=</span><span class=s2>&quot;langchain-llm-1&quot;</span>
    <span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
    <span class=n>boto3_kwargs</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span>
        <span class=n>aws_access_key_id</span><span class=o>=</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;Credentials&#39;</span><span class=p>][</span><span class=s1>&#39;AccessKeyId&#39;</span><span class=p>],</span>
        <span class=n>aws_secret_access_key</span><span class=o>=</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;Credentials&#39;</span><span class=p>][</span><span class=s1>&#39;SecretAccessKey&#39;</span><span class=p>],</span>
        <span class=n>aws_session_token</span><span class=o>=</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;Credentials&#39;</span><span class=p>][</span><span class=s1>&#39;SessionToken&#39;</span><span class=p>]</span>
    <span class=p>)</span>
</code></pre></div> <h3> Continue the run -- to save the weights into S3 location </h3> <p><strong>Important set your bucket and key values</strong></p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>os</span>
<span class=kn>import</span> <span class=nn>boto3</span>

<span class=n>bucket</span> <span class=o>=</span> <span class=s2>&quot;your bucket&quot;</span>
<span class=n>key</span> <span class=o>=</span> <span class=s1>&#39;your_key/&#39;</span> <span class=c1># add the rest of the path later on </span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>boto3_sm_client</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=s2>&quot;sagemaker-runtime&quot;</span><span class=p>,</span>
    <span class=o>**</span><span class=n>boto3_kwargs</span>
<span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>boto3_sm_client</span><span class=p>)</span>
</code></pre></div> <pre><code>&lt;botocore.client.SageMakerRuntime object at 0x7f7717c0e950&gt;
</code></pre> <div class=highlight><pre><span></span><code><span class=n>s3_client</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=s2>&quot;s3&quot;</span><span class=p>,</span>
    <span class=o>**</span><span class=n>boto3_kwargs</span>
<span class=p>)</span>
<span class=n>s3_client</span>
</code></pre></div> <pre><code>&lt;botocore.client.S3 at 0x7f76dce18ac0&gt;
</code></pre> <div class=highlight><pre><span></span><code><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>os</span><span class=o>.</span><span class=n>listdir</span><span class=p>(</span><span class=s1>&#39;./llama2_boolq_peft_merged_model/&#39;</span><span class=p>):</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>)</span>
</code></pre></div> <pre><code>model-00006-of-00006.safetensors
special_tokens_map.json
model-00001-of-00006.safetensors
model.safetensors.index.json
tokenizer.json
model-00002-of-00006.safetensors
model-00004-of-00006.safetensors
generation_config.json
tokenizer.model
model-00005-of-00006.safetensors
model-00003-of-00006.safetensors
config.json
tokenizer_config.json
</code></pre> <div class=highlight><pre><span></span><code><span class=c1>#s3.upoad_file(file_name, bucket, key)</span>

<span class=n>key</span> <span class=o>=</span> <span class=sa>f</span><span class=s1>&#39;</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s1>code_merged/llama7b_boolq&#39;</span>

<span class=n>file_name</span> <span class=o>=</span> <span class=s1>&#39;./merged_model/*.safetensors&#39;</span>

<span class=k>for</span> <span class=n>one_file</span> <span class=ow>in</span> <span class=n>os</span><span class=o>.</span><span class=n>listdir</span><span class=p>(</span><span class=s1>&#39;./llama2_boolq_peft_merged_model/&#39;</span><span class=p>):</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>one_file</span><span class=p>)</span>
    <span class=n>uploaded</span> <span class=o>=</span> <span class=n>s3_client</span><span class=o>.</span><span class=n>upload_file</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;./llama2_boolq_peft_merged_model/</span><span class=si>{</span><span class=n>one_file</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>,</span> <span class=n>bucket</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>/</span><span class=si>{</span><span class=n>one_file</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>uploaded</span><span class=p>)</span>
</code></pre></div> <pre><code>model-00006-of-00006.safetensors
None
special_tokens_map.json
None
model-00001-of-00006.safetensors
None
model.safetensors.index.json
None
tokenizer.json
None
model-00002-of-00006.safetensors
None
model-00004-of-00006.safetensors
None
generation_config.json
None
tokenizer.model
None
model-00005-of-00006.safetensors
None
model-00003-of-00006.safetensors
None
config.json
None
tokenizer_config.json
None
</code></pre> <h3> Now follow the steps from the link below to continue to import this model </h3> <p>https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html</p> <h3> The steps to import the Model into Bedrock can be </h3> <ol> <li>Login to the Bedrock console on your account</li> <li>Click on the import Model screen, it will bring you the screen as shown below</li> </ol> <p><img alt="Amazon Bedrock - Import Models" src=../images/import_jobs.png></p> <ol> <li>Fill in the details including the S3 location where the weights have been uploaded as shown below</li> </ol> <p><img alt="Amazon Bedrock - Import Jobs Configure" src=../images/import_jobs_perms.png></p> <ol> <li>Specify the role or create a new one and then click import to run your jobs. You will see the imported jobs in the import jobs list as shown in the 1st image above</li> </ol> <p><img alt="Amazon Bedrock - Conversational Interface" src=../images/iam_role_import_jobs.png></p> <p>Open your model in the Play ground to test it</p> <h3> Optional - Upload to HuggingFace Hub if needed </h3> <div class=highlight><pre><span></span><code><span class=err>!</span><span class=n>pip</span> <span class=n>install</span> <span class=n>huggingface_hub</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>huggingface_hub</span> <span class=kn>import</span> <span class=n>notebook_login</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>notebook_login</span><span class=p>()</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>model</span><span class=o>.</span><span class=n>push_to_hub</span><span class=p>(</span><span class=n>new_model</span><span class=p>,</span> <span class=n>use_temp_dir</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>safe_serialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>tokenizer</span><span class=o>.</span><span class=n>push_to_hub</span><span class=p>(</span><span class=n>new_model</span><span class=p>,</span> <span class=n>use_temp_dir</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div> <h3> Clean Up </h3> <p>You can delete your Imported Model in the console as shown in the image below:</p> <p><img alt=Delete src=../images/delete.png title=Delete></p> <p>Ensure to shut down your instance/compute that you have run this notebook on.</p> <p><strong>END OF NOTEBOOK</strong></p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9zm4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21zm0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12zM15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3zm0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by <a href="https://github.com/aws-samples/amazon-bedrock-samples/issues/new?title=[Online Feedback]: Short-Summary-of-Issue&body=Page URL: /custom-models/import_models/llama-2/llama2-finetuning-boolq/" target=_blank rel=noopener>creating an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </div> </div> <div class=md-social> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": ["tags", "toc.integrate", "content.code.copy", "content.code.select", "content.code.annotate", "navigation.footer", "search.highlight", "search.suggest"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "tags": {"Compatibility": "compat"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.88dd0f4e.min.js></script> <script src=../../../../javascript/feedback.js></script> </body> </html>