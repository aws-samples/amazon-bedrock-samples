{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon S3 , AWS Glue, and Amazon Redshift Serverless Data Ingestion\n",
    "\n",
    "This Notebook provide step by step guide to ingest open dataset to Amazon S3, Configure AWS Glue Tables and setup Redshift serverless table. This setup can be used to test Amazon Bedrock Knowledge Bases structured data retrieval capability by configuring Amazon S3 + AWS Glue as data store and RedShift Serverless as Query Engine. \n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Download dataset and load it to S3 bucket\n",
    "2. Create AWS Glue Catalog Tables using AWS Glue Crawler\n",
    "2. Create Redshift Serverless Cluster\n",
    "3. Validate data ingestion setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment and Install Required Libraries\n",
    "\n",
    "This step will install necessary libraries and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install --upgrade pip -q --no-color\n",
    "!pip install tabulate pandas -q --no-color\n",
    "!pip install boto3 -q --no-color\n",
    "!pip install awswrangler -q --no-color\n",
    "!pip install retrying -q --no-color\n",
    "!pip install awscli -q --no-color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is part of the setup and used to :\n",
    "- Add the parent directory to the python system path\n",
    "- Imports a custom module (BedrockStructuredKnowledgeBase) from utils necessary for later executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to import utils module\n",
    "current_path = Path().resolve()\n",
    "current_path = str(current_path.parent.parent)  + \"/features-examples\"\n",
    "if str(current_path) not in sys.path:\n",
    "    sys.path.append(str(current_path))\n",
    "# Print sys.path to verify\n",
    "print(sys.path)\n",
    "from utils.structured_knowledge_base import create_glue_crawler,create_redshift_workgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download E-commerce Dataset from Kaggle\n",
    "\n",
    "This step downloads a public free dataset from Kaggle into local folder. We load that data into pandas dataframe for preview\n",
    "\n",
    "Loads the dataset and provides initial analysis:\n",
    "- Reads CSV file into pandas DataFrame\n",
    "- Displays first few rows of data\n",
    "- Shows dataset information including dtypes and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories and download dataset\n",
    "\n",
    "# Download the dataset using curl\n",
    "!curl -L -o e-commerce-dataset.zip \\\n",
    "  \"https://www.kaggle.com/api/v1/datasets/download/steve1215rogg/e-commerce-dataset\" \n",
    "\n",
    "# Unzip the downloaded file\n",
    "!unzip e-commerce-dataset.zip -d e-commerce-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preview the dataset\n",
    "df = pd.read_csv('e-commerce-data/ecommerce_dataset_updated.csv')\n",
    "print(\"Dataset Preview:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure AWS , IAM and Redshift\n",
    "\n",
    "Sets up AWS connectivity:\n",
    "- Configures AWS session with region\n",
    "- Creates S3 client\n",
    "- Generates unique timestamp for resource naming\n",
    "- Sets up database and workgroup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS session\n",
    "region_name='us-east-1'\n",
    "session = boto3.Session(region_name=region_name)\n",
    "s3 = session.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create S3 Bucket\n",
    "\n",
    "This step handles S3 storage setup:\n",
    "1. Creates new S3 bucket and Uploads the DataFrame to S3 as CSV\n",
    "2. Uses AWS Wrangler for data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 bucket\n",
    "bucket_name = f\"my-ecommerce-data-bucket-{suffix}\"\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# Upload data using AWS Wrangler\n",
    "wr.s3.to_csv(\n",
    "    df=df,\n",
    "    path=f's3://{bucket_name}/ecommerce/data.csv',\n",
    "    index=False\n",
    ")\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket_name = \"my-ecommerce-data-bucket-8214019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create AWS Glue Crawler and Tables\n",
    "\n",
    "This step creates AWS Glue Crawler based on S3 dataset. \n",
    "1. We use aws cli to create an IAM Role for AWS Glue Crawler with necessary permissions on S3 source bucket. \n",
    "2. Execute the AWS Glue Crawler to create necessary AWS Glue catalog database `ecommerce` and table `ecommerce`\n",
    "3. Once the crawler is successfully completed, we list the table to validate table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your variables\n",
    "crawler_name = \"ecommerce_crawler\"\n",
    "database_name = \"ecommerce\"\n",
    "s3_path = f\"s3://{bucket_name}/ecommerce/\"\n",
    "role_name = f\"GlueCrawlerRole-{suffix}\"\n",
    "glue_policy_name = f\"GlueS3Access-{suffix}\"\n",
    "print(s3_path)\n",
    "print(role_name)\n",
    "print(glue_policy_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_glue_crawler(\n",
    "    crawler_name=crawler_name,\n",
    "    database_name=database_name,\n",
    "    bucket_name=bucket_name,\n",
    "    s3_path=s3_path,\n",
    "    role_name=role_name,\n",
    "    glue_policy_name=glue_policy_name,\n",
    "    account_id=account_id\n",
    ")\n",
    "\n",
    "print(\"Crawler creation and start process completed!\")\n",
    "print(f\"Created resources: {json.dumps(result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor crawler status\n",
    "def check_crawler_status(crawler_name):\n",
    "    while True:\n",
    "        status = !aws glue get-crawler --name {crawler_name} --query \"Crawler.State\" --output text\n",
    "        print(f\"Crawler status: {status[0]}\")\n",
    "        \n",
    "        if status[0] == \"READY\":\n",
    "            print(\"Crawler finished!\")\n",
    "            break\n",
    "        elif status[0] == \"FAILED\":\n",
    "            print(\"Crawler failed!\")\n",
    "            break\n",
    "            \n",
    "        time.sleep(30)\n",
    "\n",
    "# Check status\n",
    "check_crawler_status(crawler_name)\n",
    "\n",
    "# Optionally, show tables created\n",
    "print(\"\\nTables created in database:\")\n",
    "!aws glue get-tables --database-name {database_name} --query \"TableList[].Name\" --output text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Create Redshift Serverless Namespace and Workgroup\n",
    "\n",
    "[You can skip this step, if you are using your existing Redshift Cluster]\n",
    "\n",
    "The step creates a Redshift Serverless Namespace and Workgroup \n",
    "1. Selects the default VPC, Subnets, and Security Groups to create Serverless WorkGroup\n",
    "2. Creates a new Namespace if it do not exist\n",
    "3. Creates a new Workgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace_name = \"my-namespace\"\n",
    "workgroup_name = \"my-workgroup\"\n",
    "\n",
    "# Run the function\n",
    "create_redshift_workgroup(namespace_name, workgroup_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Validate data ingestion\n",
    "\n",
    "Once Amazon Redshift Serverless workgroup is successfully created, following queries can be executed in Amazon Redshift Serverless query explorer to validate that the ecommerce data from AWS Glue catalog can be successfully queried from Amazon Redshift Serverless query engine\n",
    "\n",
    "--Validate if the data catalog mount is available in this region, this should return \"on\"  \n",
    "SHOW data_catalog_auto_mount;  \n",
    "--View the databases available within aws glue catalog  \n",
    "SHOW SCHEMAS FROM DATABASE awsdatacatalog;  \n",
    "--View tables accessible within AnyComp database  \n",
    "SHOW TABLES FROM SCHEMA awsdatacatalog.ecommerce;  \n",
    "--Able to query data from TABLE  \n",
    "SELECT * FROM \"awsdatacatalog\".ecommerce.ecommerce limit 10;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Resources\n",
    "\n",
    "Clean up for Redshift Serverless Namespace and Workgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "namespace_name = \"my-namespace\"\n",
    "workgroup_name = \"my-workgroup\"\n",
    "\n",
    "def cleanup_redshift_serverless(namespace_name, workgroup_name):\n",
    "    \"\"\"Delete Redshift Serverless workgroup and namespace\"\"\"\n",
    "    \n",
    "    # Initialize boto3 client\n",
    "    redshift_client = boto3.client('redshift-serverless')\n",
    "    \n",
    "    try:\n",
    "        # Check workgroup status\n",
    "        print(\"Checking workgroup status...\")\n",
    "        workgroup_status = get_workgroup_status(redshift_client, workgroup_name)\n",
    "        print(f\"Workgroup status: {workgroup_status}\")\n",
    "\n",
    "        # Delete workgroup if it exists\n",
    "        if workgroup_status != \"NONEXISTENT\":\n",
    "            print(f\"Deleting workgroup '{workgroup_name}'...\")\n",
    "            try:\n",
    "                redshift_client.delete_workgroup(workgroupName=workgroup_name)\n",
    "                print(\"Workgroup deletion initiated\")\n",
    "                \n",
    "                # Wait and verify deletion\n",
    "                print(\"Waiting for workgroup deletion...\")\n",
    "                while True:\n",
    "                    time.sleep(30)\n",
    "                    status = get_workgroup_status(redshift_client, workgroup_name)\n",
    "                    if status == \"NONEXISTENT\":\n",
    "                        print(\"Workgroup deleted successfully\")\n",
    "                        break\n",
    "                    print(f\"Waiting for workgroup deletion... Current status: {status}\")\n",
    "                    \n",
    "            except ClientError as e:\n",
    "                print(f\"Error deleting workgroup: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Workgroup does not exist, skipping deletion\")\n",
    "\n",
    "        # Check namespace status  \n",
    "        print(\"Checking namespace status...\")\n",
    "        namespace_status = get_namespace_status(redshift_client, namespace_name)\n",
    "        print(f\"Namespace status: {namespace_status}\")\n",
    "\n",
    "        # Delete namespace if it exists\n",
    "        if namespace_status != \"NONEXISTENT\":\n",
    "            print(f\"Deleting namespace '{namespace_name}'...\")\n",
    "            try:\n",
    "                redshift_client.delete_namespace(namespaceName=namespace_name)\n",
    "                print(\"Namespace deletion initiated\")\n",
    "                \n",
    "                # Wait and verify deletion\n",
    "                print(\"Waiting for namespace deletion...\")\n",
    "                while True:\n",
    "                    time.sleep(30)\n",
    "                    status = get_namespace_status(redshift_client, namespace_name)\n",
    "                    if status == \"NONEXISTENT\":\n",
    "                        print(\"Namespace deleted successfully\")\n",
    "                        break\n",
    "                    print(f\"Waiting for namespace deletion... Current status: {status}\")\n",
    "                    \n",
    "            except ClientError as e:\n",
    "                print(f\"Error deleting namespace: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Namespace does not exist, skipping deletion\")\n",
    "\n",
    "        print(\"Cleanup completed!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during cleanup: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_workgroup_status(client, workgroup_name):\n",
    "    \"\"\"Get workgroup status, return 'NONEXISTENT' if not found\"\"\"\n",
    "    try:\n",
    "        response = client.get_workgroup(workgroupName=workgroup_name)\n",
    "        return response['workgroup']['status']\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            return \"NONEXISTENT\"\n",
    "        else:\n",
    "            print(f\"Error checking workgroup status: {e}\")\n",
    "            return \"ERROR\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error checking workgroup: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "def get_namespace_status(client, namespace_name):\n",
    "    \"\"\"Get namespace status, return 'NONEXISTENT' if not found\"\"\"\n",
    "    try:\n",
    "        response = client.get_namespace(namespaceName=namespace_name)\n",
    "        return response['namespace']['status']\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceNotFoundException':\n",
    "            return \"NONEXISTENT\"\n",
    "        else:\n",
    "            print(f\"Error checking namespace status: {e}\")\n",
    "            return \"ERROR\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error checking namespace: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "# Execute the cleanup\n",
    "if __name__ == \"__main__\":\n",
    "    cleanup_redshift_serverless(namespace_name, workgroup_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up for AWS Glue Crawler, Database, and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "# 1. Delete crawler\n",
    "print(\"Deleting crawler...\")\n",
    "try:\n",
    "    glue_client.delete_crawler(Name=crawler_name)\n",
    "    print(f\"Crawler {crawler_name} deleted successfully\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "        print(\"Crawler does not exist\")\n",
    "    else:\n",
    "        print(f\"Error deleting crawler: {e}\")\n",
    "\n",
    "# 2. Delete Glue database\n",
    "print(\"Deleting Glue database...\")\n",
    "try:\n",
    "    glue_client.delete_database(Name=database_name)\n",
    "    print(f\"Database {database_name} deleted successfully\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'EntityNotFoundException':\n",
    "        print(\"Database does not exist\")\n",
    "    else:\n",
    "        print(f\"Error deleting database: {e}\")\n",
    "\n",
    "print(\"Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_command = f\"\"\"\n",
    "aws s3 rb s3://{bucket_name} --force\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Cleaning up bucket {bucket_name}...\")\n",
    "!{cleanup_command}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
