{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf62959",
   "metadata": {},
   "source": [
    "# RAG with structured and unstructed data\n",
    "This notebook demonstrates how to leverage multiple data sources, including structured data from a database and unstructed data (like pdf, txt, etc.), to answer questions using Retrieval Augmented Generation (RAG). Specifically, we'll show how to integrate a knowledge base and a database to retrieve relevant information and generate comprehensive natural language responses.\n",
    "\n",
    "We'll set up a `MultiRetrievalQAChain` that can answer queries by retrieving information from an Amazon Bedrock knowledge base and a database (using Text-to-SQL as a retriever), and then generating responses using the Claude 3.0 Sonnet language model. The `MultiRetrievalQAChain` can intelligently determine the appropriate data source for a given question, fetch relevant information, maintain conversation context, and synthesize the retrieved data into a coherent natural language answer. Here's a diagram illustrating the workflow:\n",
    "\n",
    "![Multiple Retrievers in a QA Chain Image](./image/Text2SQL-RAG.png)\n",
    "\n",
    "#### Background\n",
    "The `MultiRetrievalQAChain` is an advanced implementation of the Retrieval Augmented Generation (RAG) approach, which combines the strengths of retrieval-based and generation-based language models. By integrating multiple retrievers, each specialized in a different data source, the chain can leverage diverse information sources to generate comprehensive and accurate responses. In this notebook, we'll demonstrate how to use structured data from a database (retrieved using Text-to-SQL as a retriever) as well as a text-based knowledge base to power a RAG application.\n",
    "\n",
    "**Note:** This notebook uses a custom module designed specifically for Amazon Athena, but you can easily adapt it for other databases like Amazon Redshift and Amazon RDS by using their respective data APIs.\n",
    "\n",
    "#### Prerequisites\n",
    "**Note:** This notebook assumes that you have\n",
    "1. created a knowledge base for Amazon Bedrock using unstructred data\n",
    "2. have data available for querying via SQL in Amazon Athena.\n",
    "\n",
    "If you haven't met the prerequisite, please follow these steps:\n",
    "\n",
    "1. Create a knowledge base and ingest your documents by following this [01_create_ingest_documents_test_kb_multi_ds.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/knowledge-bases/features-examples/01-rag-concepts/01_create_ingest_documents_test_kb_multi_ds.ipynb).\n",
    "2. Note down the knowledge base ID, as you'll need it later in this notebook.\n",
    "3. If you need to use synthetic data for testing, refer to this [link](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/knowledge-bases/features-examples/synthetic_dataset) to get synthetic text data that you can use to create your knowledge base for Amazon bedrock.\n",
    "4. To create synthetic structured data, you can run [0-create-dummy-structured-data.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/0-create-dummy-structured-data.ipynb) notebook and then use [1_create_sql_dataset_optional.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/1_create_sql_dataset_optional.ipynb) notebook to create a database and table in Amazon Athena.\n",
    "\n",
    "**Note**: The `custom_database_retriever.py` file currently uses table schema for a retail order website generated by using [0-create-dummy-structured-data.ipynb](https://github.com/aws-samples/amazon-bedrock-samples/tree/main/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/0-create-dummy-structured-data.ipynb) notebook. If you choose to use a different dataset, please update the schema for tables and table information inside `custom_database_retriever.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c115cbf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04689e",
   "metadata": {},
   "source": [
    "### Setp up the custom retriever for `Text-to-SQL`\n",
    "\n",
    "The code for the custom module can be found in `CustomDatabaseRetriever.py`.\n",
    "\n",
    "The provided code defines a retriever class called `AmazonAthenaRetriever` that retrieves relevant data from an Amazon Athena database using SQL queries generated by Amazon Bedrock. The retriever interacts with the Athena database through the AWS boto3 SDK, which allows running SQL queries on data stored in Amazon S3. It generates SQL queries based on natural language input, executes the queries on Athena, and returns the results as a list of documents formatted for a LangChain RetrievalQA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff9c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from custom_database_retriever import AmazonAthenaRetriever # import AmazonAthenaRetriever for Text-2-SQL\n",
    "\n",
    "athena_client = boto3.client(\"athena\")\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bcb7e",
   "metadata": {},
   "source": [
    "Retrieve stored glue database information from `0_create_sql_dataset_optional.ipynb`. You should comment this if you did not run `0_create_sql_dataset_optional.ipynb` notebook to setup Amazon Athena database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r glue_database_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26772d",
   "metadata": {},
   "source": [
    "#### Configure variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec69917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model of your choice: defaults to Claude 3.0 Sonnet\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# define Athena output location:\n",
    "RESULT_OUTPUT_LOCATION = \"s3://<Your-Bucket-Name>/\"\n",
    "\n",
    "# define the database you are using in Amazon Athena:\n",
    "# database='<Your-database-name>'\n",
    "database = 'default' if not glue_database_name else glue_database_name\n",
    "\n",
    "# define the knowledge base id that you have already prepared in Amazon Bedrock\n",
    "kb_id = '<Your-Knowledgebase-Id>'\n",
    "\n",
    "# configure how many chunks do you want for model response generation. these chunks are retrieved from Knowledge base\n",
    "numberOfResults = 3 # can be configured (its the number chunks in knowledge base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6d7ad-3218-46a7-8271-a977e530db33",
   "metadata": {},
   "source": [
    "Setup the custom retreiver `AmazonAthenaRetriever` that when invoked takes user input to write and execute a SQL query, and finally provide the data back as Langchain documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SQL Retriever:\n",
    "sql_retriever = AmazonAthenaRetriever(\n",
    "    athena_client=athena_client,\n",
    "    bedrock_client=bedrock_client,\n",
    "    database=database,\n",
    "    RESULT_OUTPUT_LOCATION=RESULT_OUTPUT_LOCATION,\n",
    "    model_id=model_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864477c8",
   "metadata": {},
   "source": [
    "#### Test the sql retriever\n",
    "\n",
    "**Note:** The executed SQL is at the end of the Document inside the metadata. The last object's metadata is configured to always have the Execution ID (from Athena) and SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"Top 5 customers that spend most amount?\"\n",
    "response = sql_retriever.get_relevant_documents(query)\n",
    "\n",
    "# check response of SQL Retreiver\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae3699",
   "metadata": {},
   "source": [
    "### Configure Knowledgebase Retriever\n",
    "\n",
    "Now, lets use publicly available `AmazonKnowledgeBasesRetriever` from Langchain to use Knowledgebases for Amazon Bedrock. This implementation makes it easy to use your knowledge base when you are using langchain. The source for this module uses `Retrieve` API for Knowledgebase on Amazon bedrock using boto3 SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Knowledge Base Retriever:\n",
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "kb_retriever = AmazonKnowledgeBasesRetriever(\n",
    "    client=bedrock_agent_client,\n",
    "    knowledge_base_id=kb_id,\n",
    "    retrieval_config={\"vectorSearchConfiguration\": \n",
    "                      {\"numberOfResults\": numberOfResults}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccd4b4",
   "metadata": {},
   "source": [
    "#### Test Knowledge base retriever \n",
    "\n",
    "**Note:** You do have to use Langchain in order to use Knowledge base for Amazon Bedrock. This is one of the way you can use Knowledge Base when you are using langchain for your project/application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab165c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"By what percentage did AWS revenue grow year-over-year in 2022?\"\n",
    "kb_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce62e0f",
   "metadata": {},
   "source": [
    "### Configure Multi QA Retriever Chain\n",
    "\n",
    "The following code sets up a `MultiRetrievalQAChain` using the LangChain library. This chain can retrieve information from multiple data sources and employ a large language model (LLM) hosted on Amazon Bedrock to respond to user queries in a natural, contextual manner.\n",
    "\n",
    "The MultiRetrievalQAChain defines two retrievers: `one for a knowledge base` and `another for a database` containing  name and description to help the MultiRetrievalQAChain determine which retriever is most appropriate for answering a given query. \n",
    "\n",
    "A `default conversation chain` manages the back-and-forth dialogue between the user and the system. It utilizes the LLM, a custom prompt, and a memory buffer to track the conversation's context.\n",
    "\n",
    "The central component is the MultiRetrievalQAChain itself, which combines the knowledge base retriever, database retriever, and default conversation chain. Based on the user's query, this chain determines the appropriate retriever, retrieves relevant information from the corresponding data source, and then employs the LLM to generate a contextual response informed by the conversation history.\n",
    "\n",
    "This sophisticated system can handle a diverse range of queries, from general questions to complex analytical database queries, providing natural language responses by synthesizing information from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.chains.router.multi_retrieval_qa import MultiRetrievalQAChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')\n",
    "inference_params = {\"max_tokens\":4096, \n",
    "                    \"temperature\":0.01,\n",
    "                    \"top_k\":250,\n",
    "                    \"top_p\":0.01,\n",
    "                    \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                   }\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb207f7a-73ae-4750-99cb-3618f76e0429",
   "metadata": {},
   "source": [
    "Let's define the two retrievers that will be used by the MultiRetrievalQAChain later on in this notebook. The `retriever_infos` component is crucial for correct intent classification as it contains the description of the retrievers. If for some reason you see an incorrect classification for your use case, this is probably the first place to begin your troubleshooting. You want the retriever description to be simple, concise, and clear. \n",
    "\n",
    "Note: In this workshop, we are using the MultiRetrievalQAChain with only two retrievers but this chain can use more than two retrievers if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3920bc-4018-4847-9852-c805f4e0dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever names and descriptions\n",
    "retriever_names = [\"kb_retriever\", \"sql_retriever\"]\n",
    "\n",
    "# Create a list of retrievers\n",
    "retrievers = [kb_retriever, sql_retriever]\n",
    "\n",
    "# Retriever Information used by Router to determine which retriever to use for the question:\n",
    "retriever_infos = [\n",
    "{\n",
    "\"name\": \"kb_retriever\",\n",
    "\"description\": 'Suitable for answering questions related to Amazon business, services, and latest launches based on shaeholder letter by CEO.',\n",
    "\"retriever\": kb_retriever\n",
    "},\n",
    "{\n",
    "\"name\": \"sql_retriever\",\n",
    "\"description\": 'Designed for handling analytical queries and generating SQL code to retrieve and analyze data from databases about products purchased, payments made, refunds, customer reviews etc. This retriever is ideal for answering questions that require data retrieval, aggregation, filtering, or sorting based on specific criteria such as device/client status, usage statistics, counts, extremes (highest/lowest), and much more. It can return numerical or short string results or sets of relevant documents to show and answer users questions.',\n",
    "\"retriever\": sql_retriever\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031c6c7-98cb-4487-bcde-d25b587fcc86",
   "metadata": {},
   "source": [
    "Now, we will create a default chain that will be used by the MultiRetrievalQAChain later on in this notebook. Take a note at how you can customize this by using your own prompt. This is another area that you can optimize for if you are seeing any unexpected behavior specifically by the default chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1085c-3ebb-476f-bb41-2496c33bc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Bedrock Chat Model: Claude 3.0 is only supported in BedrockChat:\n",
    "llm = BedrockChat(model_id = model_id,\n",
    "                  model_kwargs=inference_params, \n",
    "                  streaming=True,\n",
    "                  callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                  client = boto3_bedrock\n",
    "                 )\n",
    "\n",
    "# Custom Prompt for the default chain:\n",
    "default_chain_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"history\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "    {history}\n",
    "    Question: {query}\"\"\"\n",
    ")\n",
    "\n",
    "# Default Chain:\n",
    "default_chain = ConversationChain(llm=llm, \n",
    "                                  verbose=verbose, \n",
    "                                  prompt=default_chain_prompt, \n",
    "                                  input_key='query',\n",
    "                                  output_key='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864f660-1d2b-4ff5-8708-3235a36f779d",
   "metadata": {},
   "source": [
    "Now, we will configure a memory that will be used by the MultiRetrievalQAChain later on in this notebook. This is used to provide historical context to MultiRetrievalQAChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fe13b-61de-414a-bec2-a344e39f596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add memory buffer:\n",
    "memory = ConversationBufferMemory(memory_key=\"MultiRetrievers\", \n",
    "                                  return_messages=False, \n",
    "                                  input_key=\"input\",\n",
    "                                  output_key=\"result\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b46140-ecf1-4609-8148-5fe88086bdcf",
   "metadata": {},
   "source": [
    "Finally, we will use all the resources created above to configure a MultiRetrievalQAChain. This will contains the retrievers and their information using `retriever_infos`, the default chain using `default_chain`, and memory buffer using `memory`. You can optionally choose to have a default retriever and a default prompt to further optimize the behavior of MultiRetrievalQAChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910ddb0-af96-4f21-9aa7-43b063250ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multi-retriever chain\n",
    "multi_retrieval_qa_chain = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm=llm,\n",
    "    retriever_infos=retriever_infos,\n",
    "    default_chain=default_chain,\n",
    "    memory=memory,\n",
    "    verbose=verbose\n",
    "#     default_retriever: optional\n",
    "#     default_prompt: optional # check below cell for more information on this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda07b50",
   "metadata": {},
   "source": [
    "Before we move forward, it is important to understand how does Multiretriever QA chain decides which retriever to use to answer user's question?\n",
    "\n",
    "It uses the prompt below along with the description of the retrievers i.e. `retriever_infos` list in above cell. So, if you are facing issues with incorrect routing, you may want to optimize and simplify the description you have for the retrievers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68682522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prompt used to route the question to relevant retriver:\n",
    "from langchain.chains.router.multi_retrieval_prompt import (\n",
    "    MULTI_RETRIEVAL_ROUTER_TEMPLATE,\n",
    ")\n",
    "\n",
    "print(MULTI_RETRIEVAL_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c905c",
   "metadata": {},
   "source": [
    "### Test MultiRetrievalQAChain Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac300f3e",
   "metadata": {},
   "source": [
    "#### Using Knowledge Bases for Amazon Bedrock\n",
    "Let's ask a question that we know can be answered by the associated knowledge base and see if the multiretrieval chain can route the question to the correct retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b310823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test the chain\n",
    "query = \"By what percentage did AWS revenue grow year-over-year in 2022?\"\n",
    "result = multi_retrieval_qa_chain({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af359814",
   "metadata": {},
   "source": [
    "#### Using Custom SQL Retriever\n",
    "You have explored Knowledgebase for Amazon bedrock quite a bit in this workshop, lets focus more on the concept of RAG here. We will ask questions that we know can be answered by using SQL but lets test if the multiretrieval QA chain can route the question to correct chain.\n",
    "\n",
    "Notice how the total duration to complete the request is so quick and probably better than most solutions out there for `Text-to-SQL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fe936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"What is the total spending by all customers?\"\n",
    "result = multi_retrieval_qa_chain({\"input\": query})\n",
    "# sql_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57724d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"What is the most ordered item?\"\n",
    "result = multi_retrieval_qa_chain({\"input\": query})\n",
    "# sql_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"How many customers wrote a review about product?\"\n",
    "result = multi_retrieval_qa_chain({\"input\": query})\n",
    "# sql_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815b6a0",
   "metadata": {},
   "source": [
    "### Test default chain\n",
    "\n",
    "Multiretrieval QA chains also have a default chain that uses LLM model's knowledge to answer question when the question cannot be determined to be answered by various retriever associated with the chain. You can also define your customized prompt to adjust the responses of the default chain. \n",
    "\n",
    "Now, lets ask a question that will trigger default chain in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This question is completely unrelated to any of the information provided in the retriever's configuration.\n",
    "query = \"What is going on in the world?\"\n",
    "result = multi_retrieval_qa_chain({\"input\": query})\n",
    "# sql_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca14ca2",
   "metadata": {},
   "source": [
    "### Finally, let's look at the current memory buffer\n",
    "\n",
    "Here is complete context that the application currently has should a new question is asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current memory buffer\n",
    "multi_retrieval_qa_chain.memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981bf32",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index.\n",
    "\n",
    "To delete Knowledge base resources, check clean up steps [here](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/knowledge-bases/features-examples/01-rag-concepts/01_create_ingest_documents_test_kb_multi_ds.ipynb)\n",
    "\n",
    "To delete the Glue database and table, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "glue_client.delete_database(Name=database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf2344",
   "metadata": {},
   "source": [
    "## Whats Next?\n",
    "\n",
    "Now that you have a good understanding of custom retrievers, you may want to optimize the SQL prompts, Retriver Information and description, optimize default prompt to customize the model response to your business or project needes.\n",
    "\n",
    "If you need even faster response, you can pre-prepare your data such that its more easily accessible and does not require join or complex conditions. You could also optimize the time delay to check and fetch the results after a query has completed successfully."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
