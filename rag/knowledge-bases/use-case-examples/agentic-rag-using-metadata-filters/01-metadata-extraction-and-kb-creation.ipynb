{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1037cb-6f78-4643-990a-03ca3bfcd37e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metadata extraction and Amazon Bedrock Knowledge Bases creation\n",
    "\n",
    "This notebook demonstrates the first part of implementing an agentic RAG system using Amazon Bedrock. You will:\n",
    "\n",
    "- Extract metadata from PDF documents to enhance retrieval capabilities\n",
    "- Create two strategic Amazon Bedrock Knowledge Bases:\n",
    "  1. A summary KB containing document overviews and metadata for initial document filtering\n",
    "  2. A detailed KB with document chunks and associated metadata for precise content retrieval\n",
    "- Set up the foundation for an agentic RAG system that makes intelligent decisions about document relevance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780c3b9-a218-4a34-9c81-09cf2dc611b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Let's do this step by step in the following sections:\n",
    "\n",
    "- **Prerequisites**: Prerequisties to execute the 2 notebooks of this repo successfully\n",
    "- **Base Infrastructure Deployment**: In this section you will deploy an Amazon Cloudformation Template which will create and configure some of the services used for the solution. \n",
    "- **Metadata association:** You will use the doctor identifiers generated by Cognito to create metadata files associated to each transcript file.\n",
    "- **Upload the dataset to Amazon S3:** You will create an Amazon S3 bucket and upload the dataset and metadata files. \n",
    "- **Create a Amazon Bedrock Knowledge Bases**: You will create and sync the Knowledge Base with the transcripts and associated metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d44db-d751-482e-a92b-f1d28aaf042b",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Access to Amazon Bedrock models. [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API.\n",
    "- Use an **IAM role** with access to the following services: **Amazon S3, AWS STS,  AWS CloudFormation, Amazon Bedrock and Amazon Opensearch Serverless**.\n",
    "- PDF documents to process\n",
    "- Required Python packages installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a2d15-3282-4dd7-aaf2-049b85375885",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Amazon Bedrock users need to request access to models before they are available for use. If you want to add additional models for text, chat, and image generation, you need to request access to models in Amazon Bedrock. To request access to additional models, select the Model access link in the left side navigation panel in the Amazon Bedrock console. For more information see: <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e768e00-e71f-4df4-b8a2-dfdeab744b5e",
   "metadata": {},
   "source": [
    "For this, you will need to request access to:\n",
    "\n",
    "- Embeddings model: **Amazon Titan Embeddings V2**\n",
    "- Text generation model: **Sonnet 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4bd3e-278b-4f71-8975-feee1fd820d1",
   "metadata": {},
   "source": [
    "## Base Infrastructure Deployment \n",
    "We have created two Amazon CloudFormation templates which will automatically set up some of the services needed for this notebook.\n",
    "\n",
    "The first CloudFormation template will automatically create the Amazon S3 bucket and Amazon OpenSearch Serverless collection. Both are necessary to create the Amazon Bedrock Knowledge Bases\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46d439-14bc-4e50-a868-4b295f0554da",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "In this step, we'll import the necessary libraries and configure our AWS credentials. This setup is crucial for interacting with Amazon Bedrock and other AWS services.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6691e2-8111-4cec-b9f9-64ab8073a91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install opensearch-py boto3 botocore PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50d670-cfab-4e6d-9f6d-4d6022b9601d",
   "metadata": {},
   "source": [
    "Let's import necessary Python modules and libraries, and initialize AWS service clients required for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08b03145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# First, set up the session with the correct profile\n",
    "session = boto3.Session()\n",
    "\n",
    "# Now, create all clients using this session\n",
    "s3_client = session.client('s3')\n",
    "sts_client = session.client('sts')\n",
    "cloudformation = session.client('cloudformation')\n",
    "bedrock_agent_client = session.client('bedrock-agent')\n",
    "bedrock = session.client(\"bedrock\")\n",
    "bedrock_agent_runtime_client = session.client('bedrock-agent-runtime')\n",
    "\n",
    "# Get the region from the session\n",
    "region = session.region_name\n",
    "\n",
    "# Get the account ID using the sts client created from the session\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "# Get the identity ARN\n",
    "identity_arn = sts_client.get_caller_identity()['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728187a9-4d27-46d8-8cee-bea122a6ea05",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will define a solution id that will be used as a prefix to create the names of the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e1b7e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def short_uuid():\n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    return uuid_str[:8]\n",
    "\n",
    "solution_id = 'KBS{}'.format(short_uuid()).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890916e-563b-42b6-a73a-cd3e593d4b43",
   "metadata": {},
   "source": [
    "Next, we define the wrapper function to create the stack in CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19d95b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "opensearch_client = session.client('opensearchserverless')\n",
    "\n",
    "def create_base_infrastructure(solution_id):\n",
    "    # Read the YAML template file\n",
    "    with open('templates/1-base-infra.yaml', 'r') as f:\n",
    "        template_body = f.read()\n",
    "\n",
    "    # Define the stack parameters\n",
    "    stack_parameters = [\n",
    "        {\n",
    "            'ParameterKey': 'SolutionId',\n",
    "            'ParameterValue': solution_id\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create the CloudFormation stack\n",
    "    stack_name = \"KB-E2E-Base-{}\".format(solution_id)\n",
    "    response = cloudformation.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=template_body,\n",
    "        Parameters=stack_parameters,\n",
    "        Capabilities=['CAPABILITY_NAMED_IAM']  # Required if your template creates IAM resources\n",
    "    )\n",
    "\n",
    "    stack_id = response['StackId']\n",
    "    print(f'Creating stack {stack_name} ({stack_id})')\n",
    "\n",
    "    # Wait for the stack to be created\n",
    "    waiter = cloudformation.get_waiter('stack_create_complete')\n",
    "    waiter.wait(StackName=stack_id)\n",
    "\n",
    "    # Get the stack outputs\n",
    "    stack_outputs = cloudformation.describe_stacks(StackName=stack_id)['Stacks'][0]['Outputs']\n",
    "\n",
    "    # Extract the output values into variables\n",
    "    s3_bucket = next((output['OutputValue'] for output in stack_outputs if output['OutputKey'] == 's3bucket'), None)\n",
    "    collection_id = next((output['OutputValue'] for output in stack_outputs if output['OutputKey'] == 'OpenSearchCollectionId'), None)\n",
    "\n",
    "    print('Stack outputs:')\n",
    "    print(f'S3 Bucket: {s3_bucket}')\n",
    "    print(f'OpenSearchCollectionId: {collection_id}')\n",
    "\n",
    "    return s3_bucket, collection_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56746366-961f-4953-afdf-f73fad3775fc",
   "metadata": {},
   "source": [
    "Ok, now we are ready to launch the CloudFormation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ca0902c-4861-4ca2-895d-6eb8617882bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stack KB-E2E-Base-kbs948efd22 (arn:aws:cloudformation:us-east-1:776299153297:stack/KB-E2E-Base-kbs948efd22/237fcd60-9189-11ef-82ab-0affcbf29b41)\n",
      "Stack outputs:\n",
      "S3 Bucket: kbs948efd22-bucket\n",
      "OpenSearchCollectionId: xlhcvyck6rthnso7caq6\n"
     ]
    }
   ],
   "source": [
    "s3_bucket, collection_id = create_base_infrastructure(solution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4601853-bce6-4cfd-9481-aae85126112b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The deployment of the Amazon Cloudformation template should take around <b>1-2 minutes</b>.\n",
    "    \n",
    "You can also follow the deployment status in the Amazon Cloudformation console. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526c15e-6b2b-46b8-846c-fff8cd35c686",
   "metadata": {},
   "source": [
    "## Metadata extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0a2da-bebb-4976-9fe5-92a32b046f29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Make sure you have enabled Anthropic Claude Sonnet 3 access in the Amazon Bedrock Console (model access). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542085f1-1d24-4002-9ef0-ad612cf832d9",
   "metadata": {},
   "source": [
    "The following code implements a PDF processing pipeline that extracts text from each PDF file and generates a summary and metadata using Anthropic Claude on Amazon Bedrock. \n",
    "\n",
    "The code first defines utility functions for cleaning text, and a function to extract text from PDFs using PyPDF2. The main function **query_bedrock** sends the extracted text to Amazon Bedrock, requesting a JSON response containing the document's filename, title, and a comprehensive summary. \n",
    "\n",
    "The script then processes all PDF files in a specified folder, generating two JSON output files for each PDF: one containing basic metadata (filename and title) stored in the original folder, and another with the full metadata including the summary stored in a separate 'Summary-PDFs' subdirectory. \n",
    "\n",
    "To use metadata filtering, we need to create a separate metadata JSON file for each file. The metadata file should share the same name as the corresponding PDF file (including the extension). For instance, if the file is named *file_001.pdf*, the metadata file should be named *file_001.pdf.metadata.json*. This nomenclature is crucial for the Knowledge Base to identify the metadata for specific files during the ingestion process. \n",
    "\n",
    "The metadata JSON file will contain key-value pairs representing the relevant metadata fields associated with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94cca673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path of the folder where the PDF files are located\n",
    "folder_path = './documents/PDFs/'\n",
    "json_folder_path = './documents/Summary-PDFs/'\n",
    "\n",
    "# Create the 'Summary-PDFs' directory if it doesn't exist\n",
    "os.makedirs(json_folder_path, exist_ok=True)\n",
    "# Create the 'PDFs' directory if it doesn't exist\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e619f571-943a-436c-aafd-766747ff4a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'msg_bdrk_01PQ6KwJrdLUV6G77RozxneN', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"metagpt.pdf\",\\n        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\\n        \"summary\": \"MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs). It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow. MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality. On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches. The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 21680, 'output_tokens': 229}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"summary\": \"MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs). It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow. MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality. On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches. The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"summary\": \"MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs). It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow. MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality. On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches. The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\"\n",
      "    }\n",
      "}\n",
      "Files metagpt.pdf.metadata.json and metagpt.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01UuzgygugHv9fvvWTkR6aQy', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"knowledge_card.pdf\",\\n        \"title\": \"KNOWLEDGE CARD: FILLING LLMS\\' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS\",\\n        \"summary\": \"This paper proposes KNOWLEDGE CARD, a framework to plug in new factual and relevant knowledge into general-purpose large language models (LLMs) through specialized language models called \\'knowledge cards\\' trained on specific domains and sources. Knowledge cards generate background knowledge which is then selected and integrated into the LLM through relevance, brevity, and factuality selectors. Two approaches (bottom-up and top-down) are proposed for integrating the selected knowledge into the LLM. Experiments show KNOWLEDGE CARD outperforms baselines on knowledge-intensive tasks like question answering, misinformation detection, and updating LLM knowledge with new information.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 30285, 'output_tokens': 208}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"knowledge_card.pdf\",\n",
      "        \"title\": \"KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes KNOWLEDGE CARD, a framework to plug in new factual and relevant knowledge into general-purpose large language models (LLMs) through specialized language models called 'knowledge cards' trained on specific domains and sources. Knowledge cards generate background knowledge which is then selected and integrated into the LLM through relevance, brevity, and factuality selectors. Two approaches (bottom-up and top-down) are proposed for integrating the selected knowledge into the LLM. Experiments show KNOWLEDGE CARD outperforms baselines on knowledge-intensive tasks like question answering, misinformation detection, and updating LLM knowledge with new information.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"knowledge_card.pdf\",\n",
      "        \"title\": \"KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes KNOWLEDGE CARD, a framework to plug in new factual and relevant knowledge into general-purpose large language models (LLMs) through specialized language models called 'knowledge cards' trained on specific domains and sources. Knowledge cards generate background knowledge which is then selected and integrated into the LLM through relevance, brevity, and factuality selectors. Two approaches (bottom-up and top-down) are proposed for integrating the selected knowledge into the LLM. Experiments show KNOWLEDGE CARD outperforms baselines on knowledge-intensive tasks like question answering, misinformation detection, and updating LLM knowledge with new information.\"\n",
      "    }\n",
      "}\n",
      "Files knowledge_card.pdf.metadata.json and knowledge_card.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_017esFbqzC8VLPp26jBss7d8', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"zipformer.pdf\",\\n        \"title\": \"ZIPFORMER: A FASTER AND BETTER ENCODER FOR AUTOMATIC SPEECH RECOGNITION\",\\n        \"summary\": \"This paper proposes Zipformer, a faster, more memory-efficient, and better-performing Transformer encoder for automatic speech recognition (ASR). Key innovations include a U-Net-like encoder structure operating at different frame rates, a redesigned block structure with more modules and attention weight reuse, BiasNorm for normalization, new activation functions SwooshR and SwooshL, and a parameter-scale-invariant optimizer called ScaledAdam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate Zipformer\\'s state-of-the-art performance while requiring less computation and memory compared to previous models like Conformer. Zipformer enables over 50% speedup during inference and faster convergence during training.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 19645, 'output_tokens': 235}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"zipformer.pdf\",\n",
      "        \"title\": \"ZIPFORMER: A FASTER AND BETTER ENCODER FOR AUTOMATIC SPEECH RECOGNITION\",\n",
      "        \"summary\": \"This paper proposes Zipformer, a faster, more memory-efficient, and better-performing Transformer encoder for automatic speech recognition (ASR). Key innovations include a U-Net-like encoder structure operating at different frame rates, a redesigned block structure with more modules and attention weight reuse, BiasNorm for normalization, new activation functions SwooshR and SwooshL, and a parameter-scale-invariant optimizer called ScaledAdam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate Zipformer's state-of-the-art performance while requiring less computation and memory compared to previous models like Conformer. Zipformer enables over 50% speedup during inference and faster convergence during training.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"zipformer.pdf\",\n",
      "        \"title\": \"ZIPFORMER: A FASTER AND BETTER ENCODER FOR AUTOMATIC SPEECH RECOGNITION\",\n",
      "        \"summary\": \"This paper proposes Zipformer, a faster, more memory-efficient, and better-performing Transformer encoder for automatic speech recognition (ASR). Key innovations include a U-Net-like encoder structure operating at different frame rates, a redesigned block structure with more modules and attention weight reuse, BiasNorm for normalization, new activation functions SwooshR and SwooshL, and a parameter-scale-invariant optimizer called ScaledAdam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate Zipformer's state-of-the-art performance while requiring less computation and memory compared to previous models like Conformer. Zipformer enables over 50% speedup during inference and faster convergence during training.\"\n",
      "    }\n",
      "}\n",
      "Files zipformer.pdf.metadata.json and zipformer.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01CKGzabeURY62b658tLDooe', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n  \"metadataAttributes\": {\\n    \"filename\": \"selfrag.pdf\",\\n    \"title\": \"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\",\\n    \"summary\": \"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM\\'s behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\"\\n  }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 30606, 'output_tokens': 237}}\n",
      "Raw JSON string: \n",
      "  \"metadataAttributes\": {\n",
      "    \"filename\": \"selfrag.pdf\",\n",
      "    \"title\": \"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\",\n",
      "    \"summary\": \"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM's behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\"\n",
      "  }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "    \"filename\": \"selfrag.pdf\",\n",
      "    \"title\": \"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\",\n",
      "    \"summary\": \"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM's behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\"\n",
      "  }\n",
      "}\n",
      "Files selfrag.pdf.metadata.json and selfrag.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01TvTQM72NQWRKFXZALucE5C', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"longlora.pdf\",\\n        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\\n        \"summary\": \"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 23237, 'output_tokens': 292}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\"\n",
      "    }\n",
      "}\n",
      "Files longlora.pdf.metadata.json and longlora.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01K6YrkVcfKtvjTJYihZtAr9', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"loftq.pdf\",\\n        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\\n        \"summary\": \"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 17705, 'output_tokens': 266}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"summary\": \"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\"\n",
      "    }\n",
      "}\n",
      "Files loftq.pdf.metadata.json and loftq.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01W8gpF4M6UYbLUaXSFkJ6KB', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"values.pdf\",\\n        \"title\": \"VALUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation\",\\n        \"summary\": \"This paper presents a framework called ValUES for systematic validation of uncertainty estimation methods in semantic segmentation tasks. The framework aims to bridge the gap between theoretical advancements and practical applications by providing 1) a controlled environment to study data ambiguities and distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for evaluating uncertainty methods on five downstream tasks: out-of-distribution detection, active learning, failure detection, calibration, and ambiguity modeling. Through empirical studies on simulated and real-world datasets, the authors demonstrate how ValUES can resolve contradictions in the literature, identify the importance of components like aggregation strategies, and provide recommendations for practitioners to select the best uncertainty method for their specific task. Key insights include that separating aleatoric and epistemic uncertainty works on simulated data but does not necessarily translate to real-world data, and that ensembles generally perform most robustly across different downstream tasks while test-time augmentation can be a lightweight alternative.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 38070, 'output_tokens': 275}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"values.pdf\",\n",
      "        \"title\": \"VALUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation\",\n",
      "        \"summary\": \"This paper presents a framework called ValUES for systematic validation of uncertainty estimation methods in semantic segmentation tasks. The framework aims to bridge the gap between theoretical advancements and practical applications by providing 1) a controlled environment to study data ambiguities and distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for evaluating uncertainty methods on five downstream tasks: out-of-distribution detection, active learning, failure detection, calibration, and ambiguity modeling. Through empirical studies on simulated and real-world datasets, the authors demonstrate how ValUES can resolve contradictions in the literature, identify the importance of components like aggregation strategies, and provide recommendations for practitioners to select the best uncertainty method for their specific task. Key insights include that separating aleatoric and epistemic uncertainty works on simulated data but does not necessarily translate to real-world data, and that ensembles generally perform most robustly across different downstream tasks while test-time augmentation can be a lightweight alternative.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"values.pdf\",\n",
      "        \"title\": \"VALUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation\",\n",
      "        \"summary\": \"This paper presents a framework called ValUES for systematic validation of uncertainty estimation methods in semantic segmentation tasks. The framework aims to bridge the gap between theoretical advancements and practical applications by providing 1) a controlled environment to study data ambiguities and distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for evaluating uncertainty methods on five downstream tasks: out-of-distribution detection, active learning, failure detection, calibration, and ambiguity modeling. Through empirical studies on simulated and real-world datasets, the authors demonstrate how ValUES can resolve contradictions in the literature, identify the importance of components like aggregation strategies, and provide recommendations for practitioners to select the best uncertainty method for their specific task. Key insights include that separating aleatoric and epistemic uncertainty works on simulated data but does not necessarily translate to real-world data, and that ensembles generally perform most robustly across different downstream tasks while test-time augmentation can be a lightweight alternative.\"\n",
      "    }\n",
      "}\n",
      "Files values.pdf.metadata.json and values.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01LTESGUSKY51D2esabuPybB', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n  \"metadataAttributes\": {\\n    \"filename\": \"metra.pdf\",\\n    \"title\": \"METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION\",\\n    \"summary\": \"This paper proposes METRA, a novel unsupervised reinforcement learning (RL) objective that aims to discover a diverse set of useful behaviors that can approximately cover the state space of complex environments. The key ideas are: 1) Instead of directly covering the entire high-dimensional state space, METRA learns to cover a compact latent space that is metrically connected to the state space by temporal distances. 2) METRA uses the temporal distance between states as the metric, which is invariant to state representations and thus scalable to pixel-based environments. By learning skills that move in different directions in this latent space, METRA obtains behaviors that approximately span the state space. Through experiments on locomotion and manipulation benchmarks, METRA demonstrates state-of-the-art performance, being the first method to discover diverse locomotion skills in pixel-based quadruped and humanoid environments without any supervision.\"\\n  }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 31434, 'output_tokens': 260}}\n",
      "Raw JSON string: \n",
      "  \"metadataAttributes\": {\n",
      "    \"filename\": \"metra.pdf\",\n",
      "    \"title\": \"METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION\",\n",
      "    \"summary\": \"This paper proposes METRA, a novel unsupervised reinforcement learning (RL) objective that aims to discover a diverse set of useful behaviors that can approximately cover the state space of complex environments. The key ideas are: 1) Instead of directly covering the entire high-dimensional state space, METRA learns to cover a compact latent space that is metrically connected to the state space by temporal distances. 2) METRA uses the temporal distance between states as the metric, which is invariant to state representations and thus scalable to pixel-based environments. By learning skills that move in different directions in this latent space, METRA obtains behaviors that approximately span the state space. Through experiments on locomotion and manipulation benchmarks, METRA demonstrates state-of-the-art performance, being the first method to discover diverse locomotion skills in pixel-based quadruped and humanoid environments without any supervision.\"\n",
      "  }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "    \"filename\": \"metra.pdf\",\n",
      "    \"title\": \"METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION\",\n",
      "    \"summary\": \"This paper proposes METRA, a novel unsupervised reinforcement learning (RL) objective that aims to discover a diverse set of useful behaviors that can approximately cover the state space of complex environments. The key ideas are: 1) Instead of directly covering the entire high-dimensional state space, METRA learns to cover a compact latent space that is metrically connected to the state space by temporal distances. 2) METRA uses the temporal distance between states as the metric, which is invariant to state representations and thus scalable to pixel-based environments. By learning skills that move in different directions in this latent space, METRA obtains behaviors that approximately span the state space. Through experiments on locomotion and manipulation benchmarks, METRA demonstrates state-of-the-art performance, being the first method to discover diverse locomotion skills in pixel-based quadruped and humanoid environments without any supervision.\"\n",
      "  }\n",
      "}\n",
      "Files metra.pdf.metadata.json and metra.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_015Eq79V4zYHp3xxsttw43d3', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n  \"metadataAttributes\": {\\n    \"filename\": \"vr_mcl.pdf\",\\n    \"title\": \"META CONTINUAL LEARNING REVISITED : IMPLIC - ITLY ENHANCING ONLINE HESSIAN APPROXIMATION VIAVARIANCE REDUCTION\",\\n    \"summary\": \"This paper revisits meta-continual learning (Meta-CL) methods and establishes a connection between Meta-CL and regularization-based continual learning methods from the perspective of Hessian matrix approximation. The key findings are: 1) Meta-CL implicitly approximates the Hessian matrix in an online manner through the hypergradient computation, enjoying timely adaptation but suffering from high variance due to random memory buffer sampling. 2) To address the high variance issue, a variance-reduced Meta-CL (VR-MCL) method is proposed by incorporating a momentum-based variance reduction technique into Meta-CL. 3) Theoretically, VR-MCL is shown to impose an effective regularization on the implicitly estimated Hessian, preventing excessive updates along wrongly estimated low-curvature directions and achieving a more accurate iterative update rule. 4) Comprehensive experiments validate the effectiveness of VR-MCL, consistently outperforming other state-of-the-art methods across various datasets and settings.\"\\n  }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 42027, 'output_tokens': 309}}\n",
      "Raw JSON string: \n",
      "  \"metadataAttributes\": {\n",
      "    \"filename\": \"vr_mcl.pdf\",\n",
      "    \"title\": \"META CONTINUAL LEARNING REVISITED : IMPLIC - ITLY ENHANCING ONLINE HESSIAN APPROXIMATION VIAVARIANCE REDUCTION\",\n",
      "    \"summary\": \"This paper revisits meta-continual learning (Meta-CL) methods and establishes a connection between Meta-CL and regularization-based continual learning methods from the perspective of Hessian matrix approximation. The key findings are: 1) Meta-CL implicitly approximates the Hessian matrix in an online manner through the hypergradient computation, enjoying timely adaptation but suffering from high variance due to random memory buffer sampling. 2) To address the high variance issue, a variance-reduced Meta-CL (VR-MCL) method is proposed by incorporating a momentum-based variance reduction technique into Meta-CL. 3) Theoretically, VR-MCL is shown to impose an effective regularization on the implicitly estimated Hessian, preventing excessive updates along wrongly estimated low-curvature directions and achieving a more accurate iterative update rule. 4) Comprehensive experiments validate the effectiveness of VR-MCL, consistently outperforming other state-of-the-art methods across various datasets and settings.\"\n",
      "  }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "    \"filename\": \"vr_mcl.pdf\",\n",
      "    \"title\": \"META CONTINUAL LEARNING REVISITED : IMPLIC - ITLY ENHANCING ONLINE HESSIAN APPROXIMATION VIAVARIANCE REDUCTION\",\n",
      "    \"summary\": \"This paper revisits meta-continual learning (Meta-CL) methods and establishes a connection between Meta-CL and regularization-based continual learning methods from the perspective of Hessian matrix approximation. The key findings are: 1) Meta-CL implicitly approximates the Hessian matrix in an online manner through the hypergradient computation, enjoying timely adaptation but suffering from high variance due to random memory buffer sampling. 2) To address the high variance issue, a variance-reduced Meta-CL (VR-MCL) method is proposed by incorporating a momentum-based variance reduction technique into Meta-CL. 3) Theoretically, VR-MCL is shown to impose an effective regularization on the implicitly estimated Hessian, preventing excessive updates along wrongly estimated low-curvature directions and achieving a more accurate iterative update rule. 4) Comprehensive experiments validate the effectiveness of VR-MCL, consistently outperforming other state-of-the-art methods across various datasets and settings.\"\n",
      "  }\n",
      "}\n",
      "Files vr_mcl.pdf.metadata.json and vr_mcl.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_0186FoNzNXutHenJR97R9PPW', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n    \"metadataAttributes\": {\\n        \"filename\": \"swebench.pdf\",\\n        \"title\": \"SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?\",\\n        \"summary\": \"SWE-bench is a benchmark that evaluates language models on the task of resolving real-world software engineering issues from popular GitHub repositories. Given a codebase and an issue description, a language model is tasked with generating a patch to edit the codebase and resolve the issue. The benchmark consists of 2,294 task instances drawn from 12 Python repositories, where resolving an issue often requires understanding long contexts, coordinating changes across multiple files, and processing complex reasoning. State-of-the-art models like Claude 2 can only resolve a mere 1.96% of the issues when using a retrieval system. The benchmark highlights the significant gap between current language models and the capabilities needed for practical software engineering tasks. SWE-bench also introduces SWE-Llama, a fine-tuned 13B parameter model that can process contexts exceeding 100,000 tokens and is competitive with Claude 2 on this benchmark. The paper discusses the unique properties of SWE-bench, the training procedure for SWE-Llama, an extensive evaluation of multiple models, qualitative analysis of generations, and future directions for language models in software engineering.\"\\n    }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 45759, 'output_tokens': 320}}\n",
      "Raw JSON string: \n",
      "    \"metadataAttributes\": {\n",
      "        \"filename\": \"swebench.pdf\",\n",
      "        \"title\": \"SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?\",\n",
      "        \"summary\": \"SWE-bench is a benchmark that evaluates language models on the task of resolving real-world software engineering issues from popular GitHub repositories. Given a codebase and an issue description, a language model is tasked with generating a patch to edit the codebase and resolve the issue. The benchmark consists of 2,294 task instances drawn from 12 Python repositories, where resolving an issue often requires understanding long contexts, coordinating changes across multiple files, and processing complex reasoning. State-of-the-art models like Claude 2 can only resolve a mere 1.96% of the issues when using a retrieval system. The benchmark highlights the significant gap between current language models and the capabilities needed for practical software engineering tasks. SWE-bench also introduces SWE-Llama, a fine-tuned 13B parameter model that can process contexts exceeding 100,000 tokens and is competitive with Claude 2 on this benchmark. The paper discusses the unique properties of SWE-bench, the training procedure for SWE-Llama, an extensive evaluation of multiple models, qualitative analysis of generations, and future directions for language models in software engineering.\"\n",
      "    }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "        \"filename\": \"swebench.pdf\",\n",
      "        \"title\": \"SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?\",\n",
      "        \"summary\": \"SWE-bench is a benchmark that evaluates language models on the task of resolving real-world software engineering issues from popular GitHub repositories. Given a codebase and an issue description, a language model is tasked with generating a patch to edit the codebase and resolve the issue. The benchmark consists of 2,294 task instances drawn from 12 Python repositories, where resolving an issue often requires understanding long contexts, coordinating changes across multiple files, and processing complex reasoning. State-of-the-art models like Claude 2 can only resolve a mere 1.96% of the issues when using a retrieval system. The benchmark highlights the significant gap between current language models and the capabilities needed for practical software engineering tasks. SWE-bench also introduces SWE-Llama, a fine-tuned 13B parameter model that can process contexts exceeding 100,000 tokens and is competitive with Claude 2 on this benchmark. The paper discusses the unique properties of SWE-bench, the training procedure for SWE-Llama, an extensive evaluation of multiple models, qualitative analysis of generations, and future directions for language models in software engineering.\"\n",
      "    }\n",
      "}\n",
      "Files swebench.pdf.metadata.json and swebench.json successfully generated and saved.\n",
      "{'id': 'msg_bdrk_01NuA4Za46RYVU1cuJRSUGFh', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': '\\n  \"metadataAttributes\": {\\n    \"filename\": \"finetune_fair_diffusion.pdf\",\\n    \"title\": \"FINETUNING TEXT-TO-IMAGE DIFFUSION MODELS FOR FAIRNESS\",\\n    \"summary\": \"This paper proposes a method to finetune text-to-image diffusion models like Stable Diffusion to reduce biases related to gender, race, age etc. in the generated images. The key contributions are: 1) A distributional alignment loss to steer the generated images towards a user-defined target distribution for specific attributes like gender, race etc. 2) An adjusted direct finetuning technique to effectively finetune the diffusion model\\'s sampling process by addressing issues with exploding gradient norms and variances. Experiments show the method can significantly reduce gender, racial and intersectional biases for occupational prompts while preserving image quality and semantics. It also allows controlling non-uniform distributions like 75% young and 25% old for age. The method scales to debiasing multiple concepts simultaneously.\"\\n  }\\n}'}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 30122, 'output_tokens': 242}}\n",
      "Raw JSON string: \n",
      "  \"metadataAttributes\": {\n",
      "    \"filename\": \"finetune_fair_diffusion.pdf\",\n",
      "    \"title\": \"FINETUNING TEXT-TO-IMAGE DIFFUSION MODELS FOR FAIRNESS\",\n",
      "    \"summary\": \"This paper proposes a method to finetune text-to-image diffusion models like Stable Diffusion to reduce biases related to gender, race, age etc. in the generated images. The key contributions are: 1) A distributional alignment loss to steer the generated images towards a user-defined target distribution for specific attributes like gender, race etc. 2) An adjusted direct finetuning technique to effectively finetune the diffusion model's sampling process by addressing issues with exploding gradient norms and variances. Experiments show the method can significantly reduce gender, racial and intersectional biases for occupational prompts while preserving image quality and semantics. It also allows controlling non-uniform distributions like 75% young and 25% old for age. The method scales to debiasing multiple concepts simultaneously.\"\n",
      "  }\n",
      "}\n",
      "Cleaned JSON string: {\"metadataAttributes\": {\n",
      "    \"filename\": \"finetune_fair_diffusion.pdf\",\n",
      "    \"title\": \"FINETUNING TEXT-TO-IMAGE DIFFUSION MODELS FOR FAIRNESS\",\n",
      "    \"summary\": \"This paper proposes a method to finetune text-to-image diffusion models like Stable Diffusion to reduce biases related to gender, race, age etc. in the generated images. The key contributions are: 1) A distributional alignment loss to steer the generated images towards a user-defined target distribution for specific attributes like gender, race etc. 2) An adjusted direct finetuning technique to effectively finetune the diffusion model's sampling process by addressing issues with exploding gradient norms and variances. Experiments show the method can significantly reduce gender, racial and intersectional biases for occupational prompts while preserving image quality and semantics. It also allows controlling non-uniform distributions like 75% young and 25% old for age. The method scales to debiasing multiple concepts simultaneously.\"\n",
      "  }\n",
      "}\n",
      "Files finetune_fair_diffusion.pdf.metadata.json and finetune_fair_diffusion.json successfully generated and saved.\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import PyPDF2\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "# Setting up the Bedrock runtime client\n",
    "bedrock_runtime = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-printable characters and control characters\n",
    "    text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "\n",
    "    # Replace any remaining problematic Unicode characters with a placeholder\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + ' '\n",
    "        return clean_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    # Remove any leading/trailing whitespace\n",
    "    json_string = json_string.strip()\n",
    "\n",
    "    # Ensure the JSON string starts and ends with curly braces\n",
    "    if not json_string.startswith('{'):\n",
    "        json_string = '{' + json_string\n",
    "    if not json_string.endswith('}'):\n",
    "        json_string = json_string + '}'\n",
    "\n",
    "    # Remove any control characters\n",
    "    json_string = ''.join(char for char in json_string if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "    \n",
    "    return json_string\n",
    "\n",
    "def query_bedrock(text, filename):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Based on the following document content and filename, please extract the title and generate a comprehensive summary of the document. \n",
    "            Return the information in JSON format as shown below. Ensure the JSON is complete and valid, starting with an opening curly brace and ending with a closing curly brace:\n",
    "            {{\n",
    "                \"metadataAttributes\": {{ \n",
    "                    \"filename\": string,\n",
    "                    \"title\": string,\n",
    "                    \"summary\": string\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            Filename: {filename}\n",
    "            Document content:\n",
    "            {text}\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\", \"content\": \"{\"\n",
    "        }  # Prefill here\n",
    "    ]\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=body,\n",
    "            modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        print(response_body)\n",
    "\n",
    "        json_string = response_body['content'][0]['text']\n",
    "        print(\"Raw JSON string:\", json_string)\n",
    "\n",
    "        cleaned_json_string = clean_json_string(json_string)\n",
    "        print(\"Cleaned JSON string:\", cleaned_json_string)\n",
    "        \n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error for file {filename}: {str(e)}\")\n",
    "        print(\"Problematic JSON string:\", cleaned_json_string)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename} in query_bedrock: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            document_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "            if document_text is None:\n",
    "                print(f\"Skipping file {filename} due to text extraction error\")\n",
    "                continue\n",
    "\n",
    "            # Query Bedrock for metadata\n",
    "            json_response = query_bedrock(document_text, filename)\n",
    "            \n",
    "            if json_response:\n",
    "                # Create JSON output filenames\n",
    "                output_filename_meta = os.path.splitext(filename)[0] + '.pdf.metadata.json'\n",
    "                output_filename_meta_w_summary = os.path.splitext(filename)[0] + '.json'\n",
    "\n",
    "                # Prepare metadata without summary\n",
    "                metadata_without_summary = {\n",
    "                    \"metadataAttributes\": {\n",
    "                        \"filename\": json_response[\"metadataAttributes\"][\"filename\"],\n",
    "                        \"title\": json_response[\"metadataAttributes\"][\"title\"]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Save the metadata without summary in the original folder\n",
    "                with open(os.path.join(folder_path, output_filename_meta), 'w', encoding='utf-8') as jsonfile:\n",
    "                    json.dump(metadata_without_summary, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "                # Save the metadata including summary in the 'Summary-PDFs' subdirectory\n",
    "                with open(os.path.join(json_folder_path, output_filename_meta_w_summary), 'w', encoding='utf-8') as jsonfile:\n",
    "                    json.dump(json_response, jsonfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "                print(f'Files {output_filename_meta} and {output_filename_meta_w_summary} successfully generated and saved.')\n",
    "            else:\n",
    "                print(f'Failed to generate metadata for {filename}')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d05ec-889e-4154-9977-368fb07520da",
   "metadata": {},
   "source": [
    "To better understand what we have generated, review the output files in the **documents** folder  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833962b8-c0c3-4fc3-af4a-9eee572c1f22",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3\n",
    "\n",
    "Amazon Bedrock Knowledge Bases currently require data to reside in an Amazon S3 bucket. We will upload all the generated files in an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7cbdf0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'kbs948efd22-bucket' created successfully.\n",
      "Uploaded ./documents/PDFs/vr_mcl.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/vr_mcl.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/finetune_fair_diffusion.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/finetune_fair_diffusion.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/longlora.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/longlora.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/metagpt.pdf to s3://kbs948efd22-bucket/PDFs/metagpt.pdf\n",
      "Uploaded ./documents/PDFs/selfrag.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/selfrag.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/zipformer.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/zipformer.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/knowledge_card.pdf to s3://kbs948efd22-bucket/PDFs/knowledge_card.pdf\n",
      "Uploaded ./documents/PDFs/zipformer.pdf to s3://kbs948efd22-bucket/PDFs/zipformer.pdf\n",
      "Uploaded ./documents/PDFs/selfrag.pdf to s3://kbs948efd22-bucket/PDFs/selfrag.pdf\n",
      "Uploaded ./documents/PDFs/longlora.pdf to s3://kbs948efd22-bucket/PDFs/longlora.pdf\n",
      "Uploaded ./documents/PDFs/loftq.pdf to s3://kbs948efd22-bucket/PDFs/loftq.pdf\n",
      "Uploaded ./documents/PDFs/swebench.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/swebench.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/knowledge_card.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/knowledge_card.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/metagpt.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/metagpt.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/values.pdf to s3://kbs948efd22-bucket/PDFs/values.pdf\n",
      "Uploaded ./documents/PDFs/values.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/values.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/loftq.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/loftq.pdf.metadata.json\n",
      "Uploaded ./documents/PDFs/metra.pdf to s3://kbs948efd22-bucket/PDFs/metra.pdf\n",
      "Uploaded ./documents/PDFs/vr_mcl.pdf to s3://kbs948efd22-bucket/PDFs/vr_mcl.pdf\n",
      "Uploaded ./documents/PDFs/swebench.pdf to s3://kbs948efd22-bucket/PDFs/swebench.pdf\n",
      "Uploaded ./documents/PDFs/finetune_fair_diffusion.pdf to s3://kbs948efd22-bucket/PDFs/finetune_fair_diffusion.pdf\n",
      "Uploaded ./documents/PDFs/metra.pdf.metadata.json to s3://kbs948efd22-bucket/PDFs/metra.pdf.metadata.json\n",
      "Uploaded ./documents/Summary-PDFs/swebench.json to s3://kbs948efd22-bucket/Summary-PDFs/swebench.json\n",
      "Uploaded ./documents/Summary-PDFs/finetune_fair_diffusion.json to s3://kbs948efd22-bucket/Summary-PDFs/finetune_fair_diffusion.json\n",
      "Uploaded ./documents/Summary-PDFs/selfrag.json to s3://kbs948efd22-bucket/Summary-PDFs/selfrag.json\n",
      "Uploaded ./documents/Summary-PDFs/longlora.json to s3://kbs948efd22-bucket/Summary-PDFs/longlora.json\n",
      "Uploaded ./documents/Summary-PDFs/values.json to s3://kbs948efd22-bucket/Summary-PDFs/values.json\n",
      "Uploaded ./documents/Summary-PDFs/knowledge_card.json to s3://kbs948efd22-bucket/Summary-PDFs/knowledge_card.json\n",
      "Uploaded ./documents/Summary-PDFs/loftq.json to s3://kbs948efd22-bucket/Summary-PDFs/loftq.json\n",
      "Uploaded ./documents/Summary-PDFs/metra.json to s3://kbs948efd22-bucket/Summary-PDFs/metra.json\n",
      "Uploaded ./documents/Summary-PDFs/vr_mcl.json to s3://kbs948efd22-bucket/Summary-PDFs/vr_mcl.json\n",
      "Uploaded ./documents/Summary-PDFs/metagpt.json to s3://kbs948efd22-bucket/Summary-PDFs/metagpt.json\n",
      "Uploaded ./documents/Summary-PDFs/zipformer.json to s3://kbs948efd22-bucket/Summary-PDFs/zipformer.json\n",
      "Uploaded ./documents/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json to s3://kbs948efd22-bucket/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json\n",
      "S3 upload completed.\n"
     ]
    }
   ],
   "source": [
    "# S3 upload functionality\n",
    "def upload_directory_to_s3(local_directory, s3_bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "            s3_client.upload_file(local_path, s3_bucket, s3_path)\n",
    "            print(f\"Uploaded {local_path} to s3://{s3_bucket}/{s3_path}\")\n",
    "\n",
    "# Create the S3 bucket if it doesn't exist\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Bucket '{s3_bucket}' created successfully.\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'BucketAlreadyOwnedByYou':\n",
    "        print(f\"Bucket '{s3_bucket}' already exists and is owned by you. Proceeding with file upload.\")\n",
    "    elif error_code == 'BucketAlreadyExists':\n",
    "        print(f\"Bucket '{s3_bucket}' already exists but is owned by another account. Please choose a different bucket name.\")\n",
    "        raise\n",
    "    else:\n",
    "        print(f\"An error occurred while creating the bucket: {e}\")\n",
    "        raise\n",
    "\n",
    "# Upload PDFs directory\n",
    "upload_directory_to_s3(folder_path, s3_bucket, \"PDFs\")\n",
    "\n",
    "# Upload Summary-PDFs directory\n",
    "upload_directory_to_s3(json_folder_path, s3_bucket, \"Summary-PDFs\")\n",
    "\n",
    "print(\"S3 upload completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2acfa-f920-4036-86d8-e6099e81931c",
   "metadata": {},
   "source": [
    "Now that the files have been uploaded to S3, let's create the necessary Knowledge Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bdeafa-15b5-4a12-88e7-d234fb80eb6f",
   "metadata": {},
   "source": [
    "## Create a Amazon Bedrock Knowledge Bases\n",
    "\n",
    "In this section we will go through all the steps to create and test a Knowledge Base. \n",
    "\n",
    "We will first prepare the Amazon Opensearch Serverless collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1519ab7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def updateDataAccessPolicy(solution_id):\n",
    "    data_access_policy_name = \"{}-kbcollection-access\".format(solution_id)\n",
    "    current_role_arn = sts_client.get_caller_identity()['Arn']\n",
    "    response = opensearch_client.get_access_policy(\n",
    "        name=data_access_policy_name,\n",
    "        type='data'\n",
    "    )\n",
    "    policy_version = response[\"accessPolicyDetail\"][\"policyVersion\"]\n",
    "    existing_policy = response['accessPolicyDetail']['policy']\n",
    "    updated_policy = existing_policy.copy()\n",
    "    updated_policy[0]['Principal'].append(current_role_arn)\n",
    "    updated_policy = str(updated_policy).replace(\"'\", '\"')\n",
    "\n",
    "    response = opensearch_client.update_access_policy(\n",
    "        description='dataAccessPolicy',\n",
    "        name=data_access_policy_name,\n",
    "        policy=updated_policy,\n",
    "        policyVersion=policy_version,\n",
    "        type='data'\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "def createAOSSIndex(indexName, region, collection_id):\n",
    "    # Set up AWS authentication\n",
    "    service = 'aoss'\n",
    "    credentials = session.get_credentials()\n",
    "    awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "    # Define index settings and mappings\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"index.knn\": \"true\"\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 1024,\n",
    "                     \"method\": {\n",
    "                         \"name\": \"hnsw\",\n",
    "                         \"engine\": \"faiss\",\n",
    "                         \"space_type\": \"innerproduct\",\n",
    "                         \"parameters\": {\n",
    "                             \"ef_construction\": 512,\n",
    "                             \"m\": 16\n",
    "                         },\n",
    "                     },\n",
    "                 },\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"text-metadata\": {\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Build the OpenSearch client\n",
    "    host = f\"{collection_id}.{region}.aoss.amazonaws.com\"\n",
    "    oss_client = OpenSearch(\n",
    "        hosts=[{'host': host, 'port': 443}],\n",
    "        http_auth=awsauth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=300\n",
    "    )\n",
    "\n",
    "    # Create index\n",
    "    response = oss_client.indices.create(index=indexName, body=json.dumps(index_settings))\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13e6b2-6ccc-4039-bc20-7b38ef1fd136",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's now update the Opensearch Serverless Access Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59083c04-1efa-430b-ad86-6137dc092c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accessPolicyDetail': {'createdDate': 1729720373787, 'description': 'dataAccessPolicy', 'lastModifiedDate': 1729720652206, 'name': 'kbs948efd22-kbcollection-access', 'policy': [{'Rules': [{'Resource': ['collection/kbs948efd22-kbcollection'], 'Permission': ['aoss:CreateCollectionItems', 'aoss:UpdateCollectionItems', 'aoss:DescribeCollectionItems'], 'ResourceType': 'collection'}, {'Resource': ['index/kbs948efd22-kbcollection/*'], 'Permission': ['aoss:CreateIndex', 'aoss:DescribeIndex', 'aoss:ReadDocument', 'aoss:WriteDocument', 'aoss:UpdateIndex', 'aoss:DeleteIndex'], 'ResourceType': 'index'}], 'Principal': ['arn:aws:iam::776299153297:role/kbs948efd22-kbrole', 'arn:aws:sts::776299153297:assumed-role/AmazonSageMaker-ExecutionRole-20240702T000932/SageMaker']}], 'policyVersion': 'MTcyOTcyMDY1MjIwNl8y', 'type': 'data'}, 'ResponseMetadata': {'RequestId': 'b21af790-2ffa-4c35-bf34-5b21a0d1c6c9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b21af790-2ffa-4c35-bf34-5b21a0d1c6c9', 'date': 'Wed, 23 Oct 2024 21:57:32 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '790', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "updateDataAccessPolicy(solution_id) # Adding the current role to the collection's data access policy\n",
    "time.sleep(60) # Changes to the data access policy might take a bit to update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3dad65-e5bc-445f-bdf5-7557a3a0196a",
   "metadata": {},
   "source": [
    "As the final step to prepare our Openserach Serverless collection we will create two different indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2697404a-685c-4ecc-9554-ef68b4ada7e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index name: kb-index-kbs948efd22\n",
      "Index name for summaries: kb-index-summaries-kbs948efd22\n"
     ]
    }
   ],
   "source": [
    "indexName = \"kb-index-\" + solution_id\n",
    "print(\"Index name:\",indexName)\n",
    "indexNameSummaries = \"kb-index-summaries-\" + solution_id\n",
    "print(\"Index name for summaries:\",indexNameSummaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a05ce59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'kb-index-kbs948efd22'}\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'kb-index-summaries-kbs948efd22'}\n"
     ]
    }
   ],
   "source": [
    "createAOSSIndex(indexName, region, collection_id) # Create the AOSS index\n",
    "createAOSSIndex(indexNameSummaries, region, collection_id) # Create the AOSS index for summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83730d6-d98f-4a7d-a493-dfb5202053bd",
   "metadata": {},
   "source": [
    "#### Create the Knowledge Base\n",
    "In this section you will create the Knowledge Base. Before creating a new KB we need to define which embeddings model we want it to use. In this case we will be using Amazon Titan Embeddings V2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a1ea0-4b2c-4803-82f7-d985d2e93b3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Make sure you have enabled Amazon Titan Embeddings V2 access in the Amazon Bedrock Console (model access). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98325514-bad0-477b-8003-0972a4608dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddingModelArn = \"arn:aws:bedrock:{}::foundation-model/amazon.titan-embed-text-v2:0\".format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982e0b9-76bb-4569-a677-89ecee25d88d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can create our Amazon Bedrock Knowledge Bases. We have created an Amazon CloudFormation template which takes care of the configuration needed.\n",
    "\n",
    "**Note that each KB will use the same Opensearch collection but different indexes: *indexName* and *indexNameSummaries***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9104e285-e051-45fb-b04c-6281f10b4bb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The deployment of the Amazon Cloudformation template should take around <b>1-2 minutes</b>.\n",
    "    \n",
    "You can also follow the deployment status in the Amazon Cloudformation console. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eebb0177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "\n",
    "def create_or_update_kb_infrastructure(solution_id, s3_bucket, embeddingModelArn, indexName, SummaryIndexName, region, account_id, collection_id):\n",
    "    # Define the template parameters\n",
    "    template_parameters = [\n",
    "        {'ParameterKey': 'SolutionId', 'ParameterValue': solution_id},\n",
    "        {'ParameterKey': 'InputBucketName', 'ParameterValue': s3_bucket},\n",
    "        {'ParameterKey': 'EmbeddingModel', 'ParameterValue': embeddingModelArn},\n",
    "        {'ParameterKey': 'IndexName', 'ParameterValue': indexName},\n",
    "        {'ParameterKey': 'SummaryIndexName', 'ParameterValue': SummaryIndexName},\n",
    "        {'ParameterKey': 'VectorFieldName', 'ParameterValue': 'vector'},\n",
    "        {'ParameterKey': 'MetaDataFieldName', 'ParameterValue': 'text-metadata'},\n",
    "        {'ParameterKey': 'TextFieldName', 'ParameterValue': 'text'},\n",
    "        {'ParameterKey': 'CollectionArn', 'ParameterValue': f\"arn:aws:aoss:{region}:{account_id}:collection/{collection_id}\"},\n",
    "    ]\n",
    "\n",
    "    # Read the CloudFormation template from a file\n",
    "    with open('templates/2-knowledgebase-infra.yaml', 'r') as template_file:\n",
    "        template_body = template_file.read()\n",
    "\n",
    "    stack_name = f\"KB-E2E-KB-{solution_id}\"\n",
    "\n",
    "    try:\n",
    "        # Check if the stack exists\n",
    "        cloudformation.describe_stacks(StackName=stack_name)\n",
    "        stack_exists = True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if \"does not exist\" in str(e):\n",
    "            stack_exists = False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if stack_exists:\n",
    "        # Update the existing stack\n",
    "        try:\n",
    "            response = cloudformation.update_stack(\n",
    "                StackName=stack_name,\n",
    "                TemplateBody=template_body,\n",
    "                Parameters=template_parameters,\n",
    "                Capabilities=['CAPABILITY_IAM', 'CAPABILITY_AUTO_EXPAND', 'CAPABILITY_NAMED_IAM']\n",
    "            )\n",
    "            print(f'Stack update initiated: {response[\"StackId\"]}')\n",
    "            waiter = cloudformation.get_waiter('stack_update_complete')\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if \"No updates are to be performed\" in str(e):\n",
    "                print(\"No updates are necessary. Stack is already up to date.\")\n",
    "                stack_id = cloudformation.describe_stacks(StackName=stack_name)['Stacks'][0]['StackId']\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            stack_id = response['StackId']\n",
    "            waiter.wait(StackName=stack_id)\n",
    "    else:\n",
    "        # Create a new stack\n",
    "        response = cloudformation.create_stack(\n",
    "            StackName=stack_name,\n",
    "            TemplateBody=template_body,\n",
    "            Parameters=template_parameters,\n",
    "            Capabilities=['CAPABILITY_IAM', 'CAPABILITY_AUTO_EXPAND', 'CAPABILITY_NAMED_IAM']\n",
    "        )\n",
    "        stack_id = response['StackId']\n",
    "        print(f'Stack creation initiated: {stack_id}')\n",
    "        waiter = cloudformation.get_waiter('stack_create_complete')\n",
    "        waiter.wait(StackName=stack_id)\n",
    "\n",
    "    # Retrieve the stack outputs\n",
    "    stack_description = cloudformation.describe_stacks(StackName=stack_id)['Stacks'][0]\n",
    "    outputs = stack_description['Outputs']\n",
    "    kb_id = next((output['OutputValue'] for output in outputs if output['OutputKey'] == 'KBID'), None)\n",
    "    datasource_id = next((output['OutputValue'].split('|')[1] for output in outputs if output['OutputKey'] == 'DS'), None)\n",
    "    summaries_kb_id = next((output['OutputValue'] for output in outputs if output['OutputKey'] == 'SummaryKBID'), None)\n",
    "    summaries_datasource_id = next((output['OutputValue'].split('|')[1] for output in outputs if output['OutputKey'] == 'SummaryDS'), None)\n",
    "\n",
    "    # Print the output values\n",
    "    for output in outputs:\n",
    "        print(f\"{output['OutputKey']}: {output['OutputValue']}\")\n",
    "\n",
    "    return kb_id, datasource_id, summaries_kb_id, summaries_datasource_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d80516aa-84c7-45d1-a3a0-5f86d6c76aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack creation initiated: arn:aws:cloudformation:us-east-1:776299153297:stack/KB-E2E-KB-kbs948efd22/f25e2d20-9189-11ef-9c68-12dd996c8721\n",
      "KBID: U168IJOXJX\n",
      "SummaryKBID: CIDQQ7USCL\n",
      "SummaryDS: CIDQQ7USCL|0ICTQBG7SD\n",
      "DS: U168IJOXJX|VO37NBGNVE\n"
     ]
    }
   ],
   "source": [
    "kb_id, datasource_id, summaries_kb_id, summaries_datasource_id = create_or_update_kb_infrastructure(solution_id, s3_bucket, embeddingModelArn, indexName, indexNameSummaries, region, account_id, collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405b118-c4f1-4abc-9faf-043b5615ec39",
   "metadata": {},
   "source": [
    "#### Sync the Knowledge Base\n",
    "As we have created and associated the data sources to the two Knowledge Bases, we can proceed to Sync the data. \n",
    "\n",
    "\n",
    "Each time you add, modify, or remove files from the S3 bucket for a data source, you must sync the data source so that it is re-indexed to the knowledge base. Syncing is incremental, so Amazon Bedrock only processes the objects in your S3 bucket that have been added, modified, or deleted since the last sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b95bcf2e-241e-48d3-b914-70c73cdb7be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingestion_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=kb_id,\n",
    "    dataSourceId=datasource_id,\n",
    "    description='Initial Ingestion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21c68a8a-f696-476a-baf8-2e5fe124cab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING\n",
      "STARTING\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "IN_PROGRESS\n",
      "COMPLETE\n",
      "Waiting for changes to take place in the vector database\n"
     ]
    }
   ],
   "source": [
    "status = bedrock_agent_client.get_ingestion_job(\n",
    "    knowledgeBaseId=ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "    dataSourceId=ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "    ingestionJobId=ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    ")[\"ingestionJob\"][\"status\"]\n",
    "print(status)\n",
    "while status not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "    status = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId=ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "        dataSourceId=ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "        ingestionJobId=ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    "    )[\"ingestionJob\"][\"status\"]\n",
    "    print(status)\n",
    "    time.sleep(30)\n",
    "print(\"Waiting for changes to take place in the vector database\")\n",
    "time.sleep(30) # Wait for all changes to take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ecb2bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_ingestion_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=summaries_kb_id,\n",
    "    dataSourceId=summaries_datasource_id,\n",
    "    description='Initial Ingestion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efe642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING\n",
      "STARTING\n",
      "COMPLETE\n"
     ]
    }
   ],
   "source": [
    "status = bedrock_agent_client.get_ingestion_job(\n",
    "    knowledgeBaseId=summaries_ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "    dataSourceId=summaries_ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "    ingestionJobId=summaries_ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    ")[\"ingestionJob\"][\"status\"]\n",
    "print(status)\n",
    "while status not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "    status = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId=summaries_ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "        dataSourceId=summaries_ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "        ingestionJobId=summaries_ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    "    )[\"ingestionJob\"][\"status\"]\n",
    "    print(status)\n",
    "    time.sleep(30)\n",
    "print(\"Waiting for changes to take place in the vector database\")\n",
    "time.sleep(30) # Wait for all changes to take place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed01aff-93ba-49cf-99cf-260c3a6d1117",
   "metadata": {},
   "source": [
    "#### Test the Knowledge Base\n",
    "\n",
    "Now the Knowledge Bases are available we can test them out using the **retrieve** API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005a770-86f9-4d20-946d-cd93c29d3e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to test both Knowledge Bases we have create 3 helper functions:\n",
    "\n",
    "- **get_filename:**\n",
    "    - Performs a vector search query against the summaries Knowledge Base containing document summaries\n",
    "    - Takes a text query as input\n",
    "    - Retrieves up to 5 most relevant results based on semantic similarity\n",
    "- **construct_metadata_filter:**\n",
    "    - Creates a metadata filter structure for filename-based filtering\n",
    "    - Generates an \"equals\" filter condition when a valid filename is present\n",
    "- **process_query:**\n",
    "    - Executes a filtered search against the documents Knowledge Base\n",
    "    - Takes both query text and a filename as parameters\n",
    "    - Creates the metadata filter using the *construct_metadata_filter*\n",
    "    - Applies metadata filtering to limit results to specific files using the *filename* metadata\n",
    "    - Returns up to 5 most relevant results that match both the semantic query and filename filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f7e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filename(text):\n",
    "    response = bedrock_agent_runtime_client.retrieve(\n",
    "        knowledgeBaseId=summaries_kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def process_query(text, filename):\n",
    "    metadata_filter = construct_metadata_filter(filename)\n",
    "    print('Here is the prepared metadata filters:')\n",
    "    print(metadata_filter)\n",
    "\n",
    "    response = bedrock_agent_runtime_client.retrieve(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"filter\": metadata_filter,\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def construct_metadata_filter(filename):\n",
    "    if not filename:\n",
    "        return None\n",
    "    metadata_filter = {\"equals\": []}\n",
    "\n",
    "    if filename and filename != 'unknown':\n",
    "        metadata_filter = {\n",
    "            \"equals\": {\n",
    "                \"key\": \"filename\",\n",
    "                \"value\": filename\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return metadata_filter if metadata_filter[\"equals\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fef9e9-1fec-4f2b-871f-350296241b80",
   "metadata": {},
   "source": [
    "Let's now test the summaries Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d650f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Which are the Retrieval-based Evaluation results for LongLora\"\n",
    "get_filename(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909da7e-7c68-4a4c-93e1-7c664a3a6a24",
   "metadata": {
    "tags": []
   },
   "source": [
    "And finally test the documents Knowledge Base using longlora.pdf filename as a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680b874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_query(\"Which are the Retrieval-based Evaluation results for LongLora\", \"longlora.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe1e82-1ab6-471e-9d27-30f311905d1a",
   "metadata": {},
   "source": [
    "As you can see, we are performing semantic search only on the *longlora.pdf* file chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c52a0",
   "metadata": {},
   "source": [
    "### Store variables to use in Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea3acb-37de-41a9-8644-b7d4d6dd7776",
   "metadata": {},
   "source": [
    "As a final step, let's store all the necessary variables that we need on the second Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae68bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store kb_id\n",
    "%store summaries_kb_id\n",
    "%store s3_bucket\n",
    "%store solution_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e68ef-4a21-44ee-8805-cde388930192",
   "metadata": {},
   "source": [
    "Great, we are all set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d309869-457a-40e8-86be-e891c94a3665",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "After completing this notebook, proceed to [02-agentic-rag-converse-api.ipynb](./02-agentic-rag-converse-api.ipynb) to implement the agentic RAG system that will leverage these Knowledge Bases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
