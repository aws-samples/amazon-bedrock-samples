{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an agentic RAG with Amazon Bedrock Converse API\n",
    "\n",
    "> ⚠️ **Important**: Complete [01-metadata-extraction-and-kb-creation.ipynb](./01-metadata-extraction-and-kb-creation.ipynb) before starting this notebook.\n",
    "\n",
    "This notebook guides you through building an agentic RAG system that makes intelligent decisions about information retrieval. The system:\n",
    "- Uses a two-step retrieval process where an agent:\n",
    "  1. First analyzes document summaries to determine which documents are most relevant\n",
    "  2. Then specifically queries chunks from selected documents using metadata filters\n",
    "- Creates a sophisticated question-answering system that understands document context and relevance\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Notebook 1\n",
    "- Two Knowledge Bases populated with summaries and document chunks\n",
    "- Amazon Bedrock access configured. [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Amazon Bedrock users need to request access to models before they are available for use. If you want to add additional models for text, chat, and image generation, you need to request access to models in Amazon Bedrock. To request access to additional models, select the Model access link in the left side navigation panel in the Amazon Bedrock console. For more information see: <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using different models of **Anthropic Claude on Amazon Bedrock**. The biggest model, Sonnet 3.5, will be in charge of planning the execution, while the smaller and faster, Haiku 3, will execute the plan. For this, you will need to request access to:\n",
    "\n",
    "- Planning model: **Sonnet 3.5 v2**\n",
    "- Execution model: **Haiku 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "First, we'll install and import the necessary libraries. We need boto3 version > 1.34.123 for Amazon Bedrock Converse API support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.35.47)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.47 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.35.47)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.47->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.47->boto3) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.47->boto3) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the boto3 version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35.46\n"
     ]
    }
   ],
   "source": [
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon Bedrock clients\n",
    "\n",
    "Set up the necessary AWS clients for interacting with Bedrock services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region1=\"us-west-2\"\n",
    "region2=\"us-east-1\"\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region1)\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=region2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Amazon Bedrock Converse API\n",
    "\n",
    "Let's test the API with a simple query to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '071939ad-7ec8-441a-8e70-94dd6d474df3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 23 Oct 2024 22:23:02 GMT', 'content-type': 'application/json', 'content-length': '278', 'connection': 'keep-alive', 'x-amzn-requestid': '071939ad-7ec8-441a-8e70-94dd6d474df3'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"I'm Claude, an AI assistant created by Anthropic. I aim to be direct and honest about what I am.\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 12, 'outputTokens': 28, 'totalTokens': 40}, 'metrics': {'latencyMs': 1963}}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": \"What is your name?\"}]}]\n",
    "\n",
    "MODEL_NAME_1 = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "MODEL_NAME_2 = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_arn = \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "response = client.converse(\n",
    "    modelId=MODEL_NAME_1,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the client-side tools\n",
    "\n",
    "Next, we'll define the client-side tools that our agent will use to assist customers. We'll create seven tools: **xxx, yyy,**\n",
    "\n",
    "These tools represent the capabilities that the agent will have access to when interacting with customers. Each tool is defined with a specific purpose and input schema, allowing the agent to use them appropriately based on the customer's needs.\n",
    "\n",
    "The tools defined include two key functions: \"get_filename\" and \"process_query\". \n",
    "\n",
    "- **get_filename** takes a user query and returns the most relevant document's filename, title, and summary. \n",
    "- **process_query** then uses this filename along with the original query to extract specific, relevant information from the identified document, returning a set of pertinent text chunks. \n",
    "\n",
    "Together, these tools enable a two-step information retrieval process, first identifying the most appropriate document and then extracting the most relevant information from it, thereby providing targeted responses to user queries.\n",
    "\n",
    "It's important to note that this code snippet only defines the specifications for these tools. The actual implementation of these functions will be created later in this notebook. These specifications serve as a blueprint for what each tool can do and what input it requires.\n",
    "\n",
    "For more information on building AI agents with Amazon Bedrock using tools, you might want to refer to the Amazon Bedrock Converse API tool use documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html \n",
    "\n",
    "This two-step process enhances the RAG's ability to handle complex queries, navigate large document repositories, and deliver tailored information, ultimately improving the overall quality and relevance of its responses.\n",
    "\n",
    "**Note:** Tool use with models is also known as **Function calling**.\n",
    "\n",
    "We'll create two tools for our agent: one to retrieve filenames and another to process queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_filename\",\n",
    "            \"description\": \"Useful to retrieve the filename of the document associated to the user's query. Returns the filename, title and summary\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The decomposed query\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"process_query\",\n",
    "            \"description\": \"Retrieves specific information related to the user's query using a filter containing the name of the filename in order to get specific details from the relevant document. Returns a set of relevant chunks\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"filename\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the filename corresponding to the relevant document\"\n",
    "                        },\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The decomposed query\"\n",
    "                        }                    },\n",
    "                    \"required\": [\"filename\", \"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement tool functions\n",
    "\n",
    "Now we'll implement the functions that our tools will use to retrieve information.\n",
    "\n",
    "Here's how the process works:\n",
    "\n",
    "1. The LLM analyzes the user's request and formulates a plan using the available tools.\n",
    "2. When the LLM determines that a tool should be used, it doesn't execute the function itself. Instead, it indicates which tool should be used and with what parameters.\n",
    "3. The agent framework (which is separate from the LLM) then executes the corresponding function and returns the result.\n",
    "4. The LLM receives the result of the tool execution and uses this information to continue its plan or formulate a response to the user.\n",
    "5. This process of planning, tool use, and result interpretation continues until the LLM can generate a final answer without needing additional tool use.\n",
    "\n",
    "This combination allows for a realistic simulation of a production environment while still providing controlled data for certain operations.\n",
    "Remember, the LLM's role is to understand the user's request, plan the necessary steps using these tools, and interpret the results to provide a coherent response to the user. The actual execution of these functions is handled by the agent framework, creating a powerful and flexible agentic RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load variables saved in prior Notebook\n",
    " \n",
    " At the end of Notebook 1 we saved several variables that are needed to continue. The following cell will load those variables into this lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filename(text):\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        knowledgeBaseId=summaries_kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def process_query(text, filename):\n",
    "    metadata_filter = construct_metadata_filter(filename)\n",
    "    print('Here is the prepared metadata filters:')\n",
    "    print(metadata_filter)\n",
    "\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"filter\": metadata_filter,\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def construct_metadata_filter(filename):\n",
    "    if not filename:\n",
    "        return None\n",
    "    metadata_filter = {\"equals\": []}\n",
    "\n",
    "    if filename and filename != 'unknown':\n",
    "        metadata_filter = {\n",
    "            \"equals\": {\n",
    "                \"key\": \"filename\",\n",
    "                \"value\": filename\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return metadata_filter if metadata_filter[\"equals\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the tools\n",
    "\n",
    "Let's test our implemented tools to ensure they're working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8f877e4b-1fdf-47c7-9d15-3cbc307daca4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 23 Oct 2024 22:10:30 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '7812',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '8f877e4b-1fdf-47c7-9d15-3cbc307daca4'},\n",
       "  'RetryAttempts': 0},\n",
       " 'retrievalResults': [{'content': {'text': '{     \"metadataAttributes\": {         \"filename\": \"longlora.pdf\",         \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",         \"summary\": \"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\"     } }'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/Summary-PDFs/longlora.json'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/Summary-PDFs/longlora.json',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3Aaz5nu5IBIm3AXrs4X2_x',\n",
       "    'x-amz-bedrock-kb-data-source-id': '0ICTQBG7SD'},\n",
       "   'score': 1.397601},\n",
       "  {'content': {'text': '{     \"metadataAttributes\": {         \"filename\": \"loftq.pdf\",         \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",         \"summary\": \"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\"     } }'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/Summary-PDFs/loftq.json'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/Summary-PDFs/loftq.json',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3Azs9nu5IBE8iR1J-hZvWb',\n",
       "    'x-amz-bedrock-kb-data-source-id': '0ICTQBG7SD'},\n",
       "   'score': 1.2940648},\n",
       "  {'content': {'text': '{     \"metadataAttributes\": {         \"filename\": \"selfrag.pdf\",         \"title\": \"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\",         \"summary\": \"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM\\'s behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\"     } }'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/Summary-PDFs/selfrag.json'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/Summary-PDFs/selfrag.json',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3Aaj5nu5IBIm3AXrs4XW_y',\n",
       "    'x-amz-bedrock-kb-data-source-id': '0ICTQBG7SD'},\n",
       "   'score': 1.2713988},\n",
       "  {'content': {'text': '{     \"metadataAttributes\": {         \"filename\": \"swebench.pdf\",         \"title\": \"SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?\",         \"summary\": \"SWE-bench is a benchmark that evaluates language models on the task of resolving real-world software engineering issues from popular GitHub repositories. Given a codebase and an issue description, a language model is tasked with generating a patch to edit the codebase and resolve the issue. The benchmark consists of 2,294 task instances drawn from 12 Python repositories, where resolving an issue often requires understanding long contexts, coordinating changes across multiple files, and processing complex reasoning. State-of-the-art models like Claude 2 can only resolve a mere 1.96% of the issues when using a retrieval system. The benchmark highlights the significant gap between current language models and the capabilities needed for practical software engineering tasks. SWE-bench also introduces SWE-Llama, a fine-tuned 13B parameter model that can process contexts exceeding 100,000 tokens and is competitive with Claude 2 on this benchmark. The paper discusses the unique properties of SWE-bench, the training procedure for SWE-Llama, an extensive evaluation of multiple models, qualitative analysis of generations, and future directions for language models in software engineering.\"     } }'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/Summary-PDFs/swebench.json'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/Summary-PDFs/swebench.json',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3AzM9nu5IBE8iR1J-hXPUq',\n",
       "    'x-amz-bedrock-kb-data-source-id': '0ICTQBG7SD'},\n",
       "   'score': 1.2709653},\n",
       "  {'content': {'text': '{     \"metadataAttributes\": {         \"filename\": \"values.pdf\",         \"title\": \"VALUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation\",         \"summary\": \"This paper presents a framework called ValUES for systematic validation of uncertainty estimation methods in semantic segmentation tasks. The framework aims to bridge the gap between theoretical advancements and practical applications by providing 1) a controlled environment to study data ambiguities and distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for evaluating uncertainty methods on five downstream tasks: out-of-distribution detection, active learning, failure detection, calibration, and ambiguity modeling. Through empirical studies on simulated and real-world datasets, the authors demonstrate how ValUES can resolve contradictions in the literature, identify the importance of components like aggregation strategies, and provide recommendations for practitioners to select the best uncertainty method for their specific task. Key insights include that separating aleatoric and epistemic uncertainty works on simulated data but does not necessarily translate to real-world data, and that ensembles generally perform most robustly across different downstream tasks while test-time augmentation can be a lightweight alternative.\"     } }'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/Summary-PDFs/values.json'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/Summary-PDFs/values.json',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3Ay89nu5IBE8iR1J-hWvWX',\n",
       "    'x-amz-bedrock-kb-data-source-id': '0ICTQBG7SD'},\n",
       "   'score': 1.206473}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Which are the Retrieval-based Evaluation results for LongLora\"\n",
    "get_filename(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the prepared metadata filters:\n",
      "{'equals': {'key': 'filename', 'value': 'longlora.pdf'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f9f5aad4-dbc4-4a15-8a25-6060c29a9e6f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 23 Oct 2024 22:10:31 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '7478',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'f9f5aad4-dbc4-4a15-8a25-6060c29a9e6f'},\n",
       "  'RetryAttempts': 0},\n",
       " 'retrievalResults': [{'content': {'text': 'This indicates the effectiveness of our efficient fine-tuning method. In Table 3, for the same training and evaluation context length cases, the perplexity decreases as the context size increases. By increasing the context window size from 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72 to 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.     In Table 4, we further examine the maximum context length that we can fine-tune on a single 8× A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length respectively. LongLoRA achieves promising results on these extremely large settings. In addition, we find some perplexity degradation on small context sizes for the extended models. This is a known limitation of Position Interpolation (Chen et al., 2023).     Retrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we compare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al., 2023).'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf',\n",
       "    'x-amz-bedrock-kb-document-page-number': 7.0,\n",
       "    'filename': 'longlora.pdf',\n",
       "    'title': 'LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3ADz5mu5IBIm3AXrs4AW5t',\n",
       "    'x-amz-bedrock-kb-data-source-id': 'VO37NBGNVE'},\n",
       "   'score': 1.548012},\n",
       "  {'content': {'text': 'Third, we split tokens into groups and reshape them into batch dimensions. Attention only computes in each group in ours while the information flows between groups via shifting. Potential information leakage might be introduced by shifting, while this is easy to prevent via a small modification on the attention mask. We ablate this in the variant 2 in Section B.3 in the appendix.     proportion of parameters in the entire LLM. For example, embedding has (< 2%) parameters, and normalization has (≤ 0.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.     In experiments, we show that LongLoRA is effective and efficient. We present experimental results of extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings of Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings. The trained models achieve comparable performance to the full-attention and fully fine-tuned results, while the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B up to 100k context, or a 70B model up to 32k, on a single 8× A100 machine.'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf',\n",
       "    'x-amz-bedrock-kb-document-page-number': 3.0,\n",
       "    'filename': 'longlora.pdf',\n",
       "    'title': 'LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3A-T5mu5IBIm3AXrs4AW1s',\n",
       "    'x-amz-bedrock-kb-data-source-id': 'VO37NBGNVE'},\n",
       "   'score': 1.4976454},\n",
       "  {'content': {'text': 'All our code, models, dataset, and demo are available at github.com/dvlab-research/LongLoRA.     2.66 2.52 2.48     2.78 2.94 2.98     3.73     2.4     2.9     3.4     3.9     8192 16364 32768 65536     Perplexity     46.3 57.4     68.8     25.6 34.6     46.3     69.8     20     40     60     80     8192 16364 32768 65536     GPU memory     Full FT LoRA LongLoRA     7.4 16.3     39.8     5.2 11.3 24.6     52.4     0     30     60     90     8192 16364 32768 65536     Training hours     Context     hoursGB     ContextContext     OOM OOM 92.5     Figure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning, while still maintaining up to 1.8× lower memory cost than full fine-tuning.'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf',\n",
       "    'x-amz-bedrock-kb-document-page-number': 1.0,\n",
       "    'filename': 'longlora.pdf',\n",
       "    'title': 'LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3A8T5mu5IBIm3AXrs4AW1s',\n",
       "    'x-amz-bedrock-kb-data-source-id': 'VO37NBGNVE'},\n",
       "   'score': 1.4901869},\n",
       "  {'content': {'text': 'We study Llama2 7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768 context length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k (as the sentence length can not be precisely controlled). For each document length, we test the model 10 times with different random passkey values. Our model achieves reasonable passkey retrieval accuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to 48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this model can handle longer documents by simply extending the position interpolation. As the dashed orange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval ability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended, Llama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.     4.3 ABLATION STUDY     In this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf',\n",
       "    'x-amz-bedrock-kb-document-page-number': 8.0,\n",
       "    'filename': 'longlora.pdf',\n",
       "    'title': 'LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3AFT5mu5IBIm3AXrs4AW5t',\n",
       "    'x-amz-bedrock-kb-data-source-id': 'VO37NBGNVE'},\n",
       "   'score': 1.4786354},\n",
       "  {'content': {'text': 'Full fine-tuning converges faster than LoRA+ at the beginning, but the final performance gap is small.     Our model achieves comparable performance to LongChat-13B (Li et al., 2023), the state-of-the-art model in this task. Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected long context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via next-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.     In Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten- tion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al., 2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a long document. We show the document format is in Section A.2 in the appendix. We study Llama2 7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768 context length.'},\n",
       "   'location': {'s3Location': {'uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf'},\n",
       "    'type': 'S3'},\n",
       "   'metadata': {'x-amz-bedrock-kb-source-uri': 's3://kbs948efd22-bucket/PDFs/longlora.pdf',\n",
       "    'x-amz-bedrock-kb-document-page-number': 8.0,\n",
       "    'filename': 'longlora.pdf',\n",
       "    'title': 'LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS',\n",
       "    'x-amz-bedrock-kb-chunk-id': '1%3A0%3AFD5mu5IBIm3AXrs4AW5t',\n",
       "    'x-amz-bedrock-kb-data-source-id': 'VO37NBGNVE'},\n",
       "   'score': 1.4736403}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(\"Which are the Retrieval-based Evaluation results for LongLora\", \"longlora.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process tool calls and return results\n",
    "\n",
    "We'll create a function to process the tool calls made by Claude and return the appropriate results. The `process_tool_call` function is crucial for bridging the gap between the language model's decisions and the actual execution of tools in our system. Here's why it's important and how it works:\n",
    "\n",
    "### What is a tool call?\n",
    "A tool call is a request made by the language model (Claude 3) to use a specific tool with certain input parameters. It's the model's way of indicating that it needs to use a particular function to gather information or perform an action in response to a user's query.\n",
    "\n",
    "### Why do we need to process tool calls?\n",
    "We need to process tool calls for several reasons:\n",
    "1. The language model doesn't directly execute code or access data sources.\n",
    "2. We need to translate the model's high-level requests into actual function calls in our system.\n",
    "3. It allows us to control and monitor what actions are being taken on behalf of the model.\n",
    "4. We can add error handling, logging, or additional logic as needed.\n",
    "\n",
    "### How the model selects tools\n",
    "The model selects tools based on its understanding of:\n",
    "- The user's query\n",
    "- The available tools and their descriptions\n",
    "- The current context of the conversation\n",
    "- What information it needs to answer the query or perform the requested task\n",
    "\n",
    "The model uses its reasoning to determine which tool is most appropriate for gathering the necessary information or performing the required action.\n",
    "\n",
    "### The process_tool_call function\n",
    "This function acts as a bridge between the model's high-level tool requests and the actual function calls in our system. It:\n",
    "- Takes the tool name and input parameters as arguments\n",
    "- Maps these to the corresponding Python functions we defined earlier\n",
    "- Calls the appropriate function with the given inputs\n",
    "- Returns the result back to the model\n",
    "\n",
    "This setup allows the language model to make decisions about what information or actions are needed, while keeping the actual execution of these actions under the control of our system. It's a way of giving the model access to external data and capabilities without giving it direct control over the system's resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_tool_call(tool_name, tool_input):\n",
    "    if tool_name == \"get_filename\":\n",
    "        return get_filename(tool_input[\"query\"])\n",
    "    elif tool_name == \"process_query\":\n",
    "        return process_query(tool_input[\"query\"], tool_input[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with the chatbot\n",
    "\n",
    "Now, we'll create a function to manage the interaction between the user and our agentic RAG. This function will encapsulate the core logic of our agent, handling the flow of information between the user, the language model (Claude 3), and our defined tools.\n",
    "\n",
    "### Agent's Logic Flow\n",
    "\n",
    "1. **Initialization**: \n",
    "   - Set up the initial context and system prompt for the agent.\n",
    "   - This prompt will guide the agent's behavior.\n",
    "\n",
    "2. **User Input**: \n",
    "   - Receive the user's message or query.\n",
    "\n",
    "3. **LLM Processing**:\n",
    "   - Send the user's input, along with the current context and system prompt, to Claude.\n",
    "   - Claude analyzes the input and determines the next action (either responding directly or using a tool or a set of tools).\n",
    "\n",
    "4. **Tool Execution (if needed)**:\n",
    "   - If Claude decides to use a tool, our function will:\n",
    "     a. Extract the tool name and parameters from Claude's response.\n",
    "     b. Call the `process_tool_call` function to execute the appropriate tool.\n",
    "     c. Capture the tool's output.\n",
    "\n",
    "5. **Result Interpretation**:\n",
    "   - Send the tool's output back to Claude for interpretation.\n",
    "   - Claude may decide to use another tool or formulate a final response.\n",
    "\n",
    "6. **Response Generation**:\n",
    "   - Once Claude has gathered all necessary information, it generates a final response to the user.\n",
    "\n",
    "7. **Conversation Update**:\n",
    "   - Update the conversation history with the user's input and the agent's response.\n",
    "\n",
    "8. **Repeat**:\n",
    "   - The process repeats for each user input, maintaining context throughout the conversation.\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The system prompt is crucial as it sets the tone, capabilities, and limitations of our agent. Our system prompt includes:\n",
    "\n",
    "- The agent's role as an AI assistant to provide meaningful responses\n",
    "- A step-by-step guide for the agent to follow when assisting customers\n",
    "- Rules for interaction and information gathering\n",
    "- Guidelines for tool usage and response formulation\n",
    "\n",
    "By encapsulating this logic in a single function, we create a modular and maintainable structure for our chatbot. This allows for easy updates to the agent's behavior and capabilities as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create the system message variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an advanced AI assistant, designed to process user queries efficiently using a ReAct (Reasoning and Acting) approach. Your task is to break down complex queries, reason about each step, and utilize appropriate tools to provide accurate and comprehensive responses.\n",
    "\n",
    "Core Process\n",
    "\n",
    "    For each user query, follow these steps:\n",
    "\n",
    "    - Query Analysis and Decomposition\n",
    "    - Metadata Search\n",
    "    - Filtered Information Retrieval\n",
    "    - Response Formulation\n",
    "\n",
    "    After each step, engage in explicit reasoning to justify your actions and plan your next move.\n",
    "\n",
    "Detailed Instructions\n",
    "\n",
    "1. Query Analysis and Decomposition\n",
    "\n",
    "    - Carefully analyze the user's query.\n",
    "    - Break it down into smaller, more specific sub-queries.\n",
    "    - Reformulate each sub-query to optimize for semantic search.\n",
    "\n",
    "    Reasoning: Explain why you decomposed the query as you did and how it will help in the information retrieval process.\n",
    "\n",
    "2. Metadata Search\n",
    "\n",
    "    - Use the metadata Knowledge Base to identify the most relevant filename(s) for the query.\n",
    "    - If multiple filenames are relevant, prioritize them based on their likely relevance.\n",
    "\n",
    "    Reasoning: Justify your choice of filename(s) and explain how they relate to the user's query.\n",
    "\n",
    "3. Filtered Information Retrieval\n",
    "\n",
    "    - Use the process_query tool to retrieve information.\n",
    "    - Apply the identified filename(s) as a filter parameter.\n",
    "    - If multiple sub-queries exist, perform separate retrievals for each.\n",
    "\n",
    "    Reasoning: Explain why the retrieved information is relevant and how it addresses the sub-queries.\n",
    "\n",
    "4. Response Formulation\n",
    "\n",
    "    - Synthesize the retrieved information into a coherent response.\n",
    "    - Ensure your answer directly addresses the user's original query.\n",
    "    - If any aspects of the query remain unanswered, acknowledge this and explain why.\n",
    "\n",
    "    Reasoning: Justify how your response addresses the user's query and identify any potential gaps or areas for further exploration.\n",
    "\n",
    "5. Comprehensive Answer Compilation\n",
    "\n",
    "    - Before finalizing your response, review all subqueries and their corresponding answers.\n",
    "    - Ensure that each subquery has been addressed in your final response.\n",
    "    - If any subquery remains unanswered, explicitly state this and explain why (e.g., lack of information, ambiguity in the query).\n",
    "    - Organize your response to clearly address each part of the original query, using subheadings if necessary for clarity.\n",
    "\n",
    "    Reasoning: Explain how your final response comprehensively addresses all aspects of the user's original query, referencing each subquery explicitly.\n",
    "\n",
    "Key Principles\n",
    "\n",
    "- Always start with query decomposition, even for seemingly simple queries.\n",
    "- Continuously refine and reformulate sub-queries to enhance semantic search effectiveness.\n",
    "- Use explicit reasoning after each step to justify your actions and plan subsequent steps.\n",
    "- Prioritize relevance and accuracy in your information retrieval and response formulation.\n",
    "- Be transparent about the process you're following and any limitations encountered.\n",
    "- After executing the get_filename tool, you should extract the filename ONLY from the \"filename\" in the \"metadataAttributes\" dictionary.\n",
    "\n",
    "By adhering to this ReAct approach, you will provide users with well-reasoned, accurate, and comprehensive responses while demonstrating your thought process throughout the interaction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's create the main function chatbot interaction. This function will accept as arguments the user message and the chat history if available. Including the chat history is very important in order for the model to have the entire context before deciding the next step.\n",
    "\n",
    "## Chatbot Interaction Function\n",
    "\n",
    "This function encapsulates the core logic of our chatbot interaction:\n",
    "\n",
    "1. **Initialization**: It sets up the conversation with the user's message and any existing chat history by extending the messages list.\n",
    "\n",
    "2. **Initial Planning**: It makes a call to a larger language model (`MODEL_NAME_1`) for comprehensive response planning.\n",
    "\n",
    "3. **Tool Use Loop**: If tools are required, it enters a loop where it:\n",
    "   a. Extracts tool use information\n",
    "   b. Processes the tool call\n",
    "   c. Prepares the result for the next model call\n",
    "   d. Calls a smaller, quicker model (`MODEL_NAME_2`) for subsequent interactions\n",
    "\n",
    "4. **Iteration**: This loop continues until no more tool use is required.\n",
    "\n",
    "5. **Response Generation**: Finally, it extracts the final response text and returns it along with the updated message history.\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "The use of two different models (a larger one for initial planning and a smaller one for subsequent interactions) is an optimization technique. This approach aims to balance between:\n",
    "\n",
    "- Comprehensive planning\n",
    "- Quick responses\n",
    "- Cost reduction\n",
    "- Improved response times\n",
    "\n",
    "By using a larger model for initial planning and a smaller, faster model for follow-up actions, we can potentially achieve better performance and efficiency in our chatbot interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def chatbot_interaction(user_message, chat_history=None):\n",
    "    print(f\"\\n{'='*50}\\nUser Message: {user_message}\\n{'='*50}\")\n",
    "\n",
    "    messages = []\n",
    "    if chat_history:\n",
    "        messages.extend(chat_history)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=MODEL_NAME_1,\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 4096,\n",
    "            'temperature': 0,\n",
    "        },\n",
    "        messages=messages,\n",
    "        system=[\n",
    "            {\n",
    "                 'text': system_message\n",
    "            },\n",
    "        ],\n",
    "        toolConfig={\"tools\": tools}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInitial Response:\")\n",
    "    print(f\"Stop Reason: {response['stopReason']}\")\n",
    "    print(f\"Content: {response['output']['message']['content']}\")\n",
    "\n",
    "    while response['stopReason'] == \"tool_use\":\n",
    "        tool_use = next(block for block in response['output']['message']['content'] if isinstance(block, dict) and 'toolUse' in block)\n",
    "        tool_name = tool_use['toolUse']['name']\n",
    "        tool_input = tool_use['toolUse']['input']\n",
    "        tool_use_id = tool_use['toolUse']['toolUseId']\n",
    "\n",
    "        print(f\"\\nTool Used: {tool_name}\")\n",
    "        print(f\"Tool Input:\")\n",
    "        print(json.dumps(tool_input, indent=2))\n",
    "\n",
    "        tool_result = process_tool_call(tool_name, tool_input)\n",
    "\n",
    "        print(f\"\\nTool Result:\")\n",
    "        print(json.dumps(tool_result, indent=2))\n",
    "\n",
    "        messages_temp = [\n",
    "            {\"role\": \"assistant\", \"content\": response['output']['message']['content']},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": tool_use_id, \n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"json\": {\n",
    "                                        \"result\": tool_result\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        messages.extend(messages_temp)\n",
    "\n",
    "        response = client.converse(\n",
    "            modelId=MODEL_NAME_2,\n",
    "            inferenceConfig={\n",
    "                'maxTokens': 4096,\n",
    "                'temperature': 0,\n",
    "            },\n",
    "            messages=messages,\n",
    "            system=[\n",
    "                {\n",
    "                    'text': system_message\n",
    "                },\n",
    "            ],\n",
    "            toolConfig={\"tools\": tools}\n",
    "        )\n",
    "\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(f\"Stop Reason: {response['stopReason']}\")\n",
    "        print(f\"Content: {response['output']['message']['content']}\")\n",
    "\n",
    "    final_response = next(\n",
    "        (block['text'] for block in response['output']['message']['content'] if 'text' in block),\n",
    "        None,\n",
    "    )\n",
    "\n",
    "    if not final_response:\n",
    "        final_response = None\n",
    "\n",
    "    print(f\"\\nFinal Response: {final_response}\")\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": [{\"text\": final_response}]})\n",
    "\n",
    "    return final_response, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, see the flowchart for function chatbot_interaction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/function-flowchart.png\" alt=\"Flowchart of the function\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the chatbot\n",
    "\n",
    "Let's test our agent with a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initialize chat_history for subsequent iterations\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "User Message: Hello!\n",
      "==================================================\n",
      "\n",
      "Initial Response:\n",
      "Stop Reason: end_turn\n",
      "Content: [{'text': \"Hello! I'm an AI assistant designed to help you with various queries and information retrieval tasks. How can I assist you today? Feel free to ask any question, and I'll do my best to provide you with a comprehensive and accurate answer using the tools and knowledge available to me.\"}]\n",
      "\n",
      "Final Response: Hello! I'm an AI assistant designed to help you with various queries and information retrieval tasks. How can I assist you today? Feel free to ask any question, and I'll do my best to provide you with a comprehensive and accurate answer using the tools and knowledge available to me.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Hello!\"\n",
    "final_response, chat_history = chatbot_interaction(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "User Message: Compare and contrast the LoRA papers (LongLoRA, LoftQ) and the metagpt paper. Analyze the approach in each paper first.\n",
      "==================================================\n",
      "\n",
      "Initial Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"I'll help analyze and compare these papers. Let me break this down systematically.\\n\\n1. First, let me retrieve information about each paper separately.\\n\\nLet me start with LongLoRA:\"}, {'toolUse': {'toolUseId': 'tooluse_2OiQBP8lSaiV0iV-ai8_Sg', 'name': 'get_filename', 'input': {'query': 'LongLoRA paper approach and methodology'}}}]\n",
      "\n",
      "Tool Used: get_filename\n",
      "Tool Input:\n",
      "{\n",
      "  \"query\": \"LongLoRA paper approach and methodology\"\n",
      "}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"2d873b8e-2bfe-478b-a7b4-b0d81ab957b2\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:26:32 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"7383\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"2d873b8e-2bfe-478b-a7b4-b0d81ab957b2\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"longlora.pdf\\\",         \\\"title\\\": \\\"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/longlora.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/longlora.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aaz5nu5IBIm3AXrs4X2_x\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.5267465\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"loftq.pdf\\\",         \\\"title\\\": \\\"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/loftq.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/loftq.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Azs9nu5IBE8iR1J-hZvWb\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.3834054\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"selfrag.pdf\\\",         \\\"title\\\": \\\"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\\\",         \\\"summary\\\": \\\"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM's behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/selfrag.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/selfrag.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aaj5nu5IBIm3AXrs4XW_y\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1866078\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"knowledge_card.pdf\\\",         \\\"title\\\": \\\"KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"The paper proposes KNOWLEDGE CARD, a framework to empower large language models (LLMs) with modular and collaborative knowledge from specialized language models called 'knowledge cards'. Knowledge cards are trained on diverse domains and sources to serve as knowledge repositories. Three knowledge selectors control for relevance, brevity and factuality of the generated knowledge. Two approaches, bottom-up and top-down, are proposed to integrate the specialized knowledge into general-purpose LLMs. Bottom-up activates all knowledge cards, selects relevant knowledge through the selectors, and provides it to the LLM. Top-down prompts the LLM to identify if external knowledge is needed and selectively activates relevant knowledge cards. Experiments show KNOWLEDGE CARD outperforms baselines on knowledge-intensive tasks like question answering, misinformation detection and updating LLM knowledge with new information.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AaT5nu5IBIm3AXrs4WG8e\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1702793\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"metra.pdf\\\",         \\\"title\\\": \\\"METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION\\\",         \\\"summary\\\": \\\"This paper proposes METRA, a novel unsupervised reinforcement learning (RL) objective that aims to discover a diverse set of useful behaviors that can approximately cover the state space of complex environments. The key ideas are: 1) Instead of directly covering the entire high-dimensional state space, METRA learns to cover a compact latent space that is metrically connected to the state space by temporal distances. 2) METRA uses the temporal distance between states as the metric, which is invariant to state representations and thus scalable to pixel-based environments. By learning skills that move in different directions in this latent space, METRA obtains behaviors that approximately span the state space. Through experiments on locomotion and manipulation benchmarks, METRA demonstrates state-of-the-art performance, being the first method to discover diverse locomotion skills in pixel-based quadruped and humanoid environments without any supervision.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metra.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metra.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Azc9nu5IBE8iR1J-hY_VK\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1681386\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"The key points about the LongLoRA paper are:\\n\\n- It proposes an efficient approach called LongLoRA to extend the context window of pre-trained large language models like LLaMA2 to much longer lengths, while maintaining computational efficiency.\\n- The key ideas are:\\n  1. Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, enabling significant computation savings.\\n  2. Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation.\\n- With LongLoRA, models like LLaMA2 7B can be fine-tuned up to 100k context length, and LLaMA2 70B up to 32k context, on a single 8xA100 GPU machine.\\n- The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient.\\n- The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\\n\\nNow let's look at the LoftQ paper:\"}, {'toolUse': {'toolUseId': 'tooluse_gFKqp8fvRV6aaO7iQTbIVQ', 'name': 'get_filename', 'input': {'query': 'LoftQ paper approach and methodology'}}}]\n",
      "\n",
      "Tool Used: get_filename\n",
      "Tool Input:\n",
      "{\n",
      "  \"query\": \"LoftQ paper approach and methodology\"\n",
      "}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"ffa24804-8959-4a7f-b7a2-ea67adaed67a\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:26:37 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"7625\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"ffa24804-8959-4a7f-b7a2-ea67adaed67a\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"loftq.pdf\\\",         \\\"title\\\": \\\"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"This paper proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning. LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning. This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks. Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization. The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/loftq.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/loftq.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Azs9nu5IBE8iR1J-hZvWb\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.4675918\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"longlora.pdf\\\",         \\\"title\\\": \\\"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"This paper proposes LongLoRA, an efficient approach to extend the context window of pre-trained large language models like Llama2 to much longer lengths, while maintaining computational efficiency. The key ideas are: 1) Using shifted sparse attention (S2-Attn) to approximate full self-attention during fine-tuning, which enables significant computation savings. 2) Making the embedding and normalization layers trainable in addition to the low-rank adaptation in LoRA, which is crucial for effective long context adaptation. With LongLoRA, models like Llama2 7B can be fine-tuned up to 100k context length, and Llama2 70B up to 32k context, on a single 8xA100 GPU machine. The fine-tuned models achieve comparable performance to full fine-tuning on language modeling benchmarks, while being much more efficient. The paper also introduces a long instruction-following dataset LongAlpaca and supervised fine-tuning of the LongLoRA models.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/longlora.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/longlora.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aaz5nu5IBIm3AXrs4X2_x\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.2043417\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"knowledge_card.pdf\\\",         \\\"title\\\": \\\"KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS\\\",         \\\"summary\\\": \\\"The paper proposes KNOWLEDGE CARD, a framework to empower large language models (LLMs) with modular and collaborative knowledge from specialized language models called 'knowledge cards'. Knowledge cards are trained on diverse domains and sources to serve as knowledge repositories. Three knowledge selectors control for relevance, brevity and factuality of the generated knowledge. Two approaches, bottom-up and top-down, are proposed to integrate the specialized knowledge into general-purpose LLMs. Bottom-up activates all knowledge cards, selects relevant knowledge through the selectors, and provides it to the LLM. Top-down prompts the LLM to identify if external knowledge is needed and selectively activates relevant knowledge cards. Experiments show KNOWLEDGE CARD outperforms baselines on knowledge-intensive tasks like question answering, misinformation detection and updating LLM knowledge with new information.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/.ipynb_checkpoints/knowledge_card-checkpoint.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AaT5nu5IBIm3AXrs4WG8e\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1648091\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"selfrag.pdf\\\",         \\\"title\\\": \\\"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\\\",         \\\"summary\\\": \\\"This paper introduces SELF-RAG, a new framework that enhances the quality and factuality of large language models (LLMs) through retrieval on demand and self-reflection. SELF-RAG trains an LLM to learn to retrieve relevant passages, generate text, and critique its own generation by predicting special tokens called reflection tokens. These tokens indicate the need for retrieval, relevance of retrieved passages, whether the output is supported by the passages, and the overall utility of the response. The framework enables controlling the LLM's behavior at inference time by leveraging the reflection tokens. Experiments on diverse tasks show that SELF-RAG significantly outperforms pre-trained LLMs, retrieval-augmented models, and concurrent approaches in terms of overall performance, factuality, and citation accuracy.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/selfrag.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/selfrag.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aaj5nu5IBIm3AXrs4XW_y\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1646111\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"swebench.pdf\\\",         \\\"title\\\": \\\"SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?\\\",         \\\"summary\\\": \\\"SWE-bench is a benchmark that evaluates language models on the task of resolving real-world software engineering issues from popular GitHub repositories. Given a codebase and an issue description, a language model is tasked with generating a patch to edit the codebase and resolve the issue. The benchmark consists of 2,294 task instances drawn from 12 Python repositories, where resolving an issue often requires understanding long contexts, coordinating changes across multiple files, and processing complex reasoning. State-of-the-art models like Claude 2 can only resolve a mere 1.96% of the issues when using a retrieval system. The benchmark highlights the significant gap between current language models and the capabilities needed for practical software engineering tasks. SWE-bench also introduces SWE-Llama, a fine-tuned 13B parameter model that can process contexts exceeding 100,000 tokens and is competitive with Claude 2 on this benchmark. The paper discusses the unique properties of SWE-bench, the training procedure for SWE-Llama, an extensive evaluation of multiple models, qualitative analysis of generations, and future directions for language models in software engineering.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/swebench.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/swebench.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AzM9nu5IBE8iR1J-hXPUq\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1645265\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"The key points about the LoftQ paper are:\\n\\n- It proposes a novel quantization framework called LoftQ (LoRA-Fine-Tuning-aware Quantization) for large language models that require quantization and LoRA fine-tuning.\\n- LoftQ alternatively applies quantization and low-rank approximation to the original pre-trained weights to obtain an initialization for the subsequent LoRA fine-tuning.\\n- This approach mitigates the discrepancy between quantized weights and pre-trained weights, providing a better initialization for fine-tuning and improving performance on downstream tasks.\\n- Extensive experiments on natural language understanding, question answering, summarization, and natural language generation show that LoftQ remarkably outperforms existing quantization methods like QLoRA, especially in low-bit regimes like 2-bit quantization.\\n- The method is effective across different model architectures (encoder-only, encoder-decoder, decoder-only) and quantization techniques.\\n\\nNow let's look at the metagpt paper:\"}, {'toolUse': {'toolUseId': 'tooluse_3dwlgUoLTpavrwODxKoYzg', 'name': 'get_filename', 'input': {'query': 'metagpt paper approach and methodology'}}}]\n",
      "\n",
      "Tool Used: get_filename\n",
      "Tool Input:\n",
      "{\n",
      "  \"query\": \"metagpt paper approach and methodology\"\n",
      "}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"be7c809e-b906-4f50-8c5a-418b863d14a7\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:26:41 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"7601\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"be7c809e-b906-4f50-8c5a-418b863d14a7\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"metagpt.pdf\\\",         \\\"title\\\": \\\"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\\\",         \\\"summary\\\": \\\"MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs). It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow. MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality. On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches. The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metagpt.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metagpt.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Ays9nu5IBE8iR1J-hVfWp\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.5120264\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"vr_mcl.pdf\\\",         \\\"title\\\": \\\"META CONTINUAL LEARNING REVISITED : IMPLIC - ITLY ENHANCING ONLINE HESSIAN APPROXIMATION VIAVARIANCE REDUCTION\\\",         \\\"summary\\\": \\\"This paper revisits meta-continual learning (Meta-CL) methods and establishes a connection between Meta-CL and regularization-based continual learning methods from the perspective of Hessian matrix approximation. The key findings are: 1) Meta-CL implicitly approximates the Hessian matrix in an online manner through the hypergradient computation, enjoying timely adaptation but suffering from high variance due to random memory buffer sampling. 2) To address the high variance issue, a variance-reduced Meta-CL (VR-MCL) method is proposed by incorporating a momentum-based variance reduction technique into Meta-CL. 3) Theoretically, VR-MCL is shown to impose an effective regularization on the implicitly estimated Hessian, preventing excessive updates along wrongly estimated low-curvature directions and achieving a more accurate iterative update rule. 4) Comprehensive experiments validate the effectiveness of VR-MCL, consistently outperforming other state-of-the-art methods across various datasets and settings.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/vr_mcl.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/vr_mcl.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AbT5nu5IBIm3AXrs4ZG_f\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1773314\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"metra.pdf\\\",         \\\"title\\\": \\\"METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION\\\",         \\\"summary\\\": \\\"This paper proposes METRA, a novel unsupervised reinforcement learning (RL) objective that aims to discover a diverse set of useful behaviors that can approximately cover the state space of complex environments. The key ideas are: 1) Instead of directly covering the entire high-dimensional state space, METRA learns to cover a compact latent space that is metrically connected to the state space by temporal distances. 2) METRA uses the temporal distance between states as the metric, which is invariant to state representations and thus scalable to pixel-based environments. By learning skills that move in different directions in this latent space, METRA obtains behaviors that approximately span the state space. Through experiments on locomotion and manipulation benchmarks, METRA demonstrates state-of-the-art performance, being the first method to discover diverse locomotion skills in pixel-based quadruped and humanoid environments without any supervision.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metra.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/metra.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Azc9nu5IBE8iR1J-hY_VK\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1644099\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"finetune_fair_diffusion.pdf\\\",         \\\"title\\\": \\\"FINETUNING TEXT-TO-IMAGE DIFFUSION MODELS FOR FAIRNESS\\\",         \\\"summary\\\": \\\"This paper proposes a method to finetune text-to-image diffusion models like Stable Diffusion to reduce biases related to gender, race, age etc. in the generated images. The key contributions are: 1) A distributional alignment loss to steer the generated images towards a user-defined target distribution for specific attributes like gender, race etc. 2) An adjusted direct finetuning technique to effectively finetune the diffusion model's sampling process by addressing issues with exploding gradient norms and variances. Experiments show the method can significantly reduce gender, racial and intersectional biases for occupational prompts while preserving image quality and semantics. It also allows controlling non-uniform distributions like 75% young and 25% old for age. The method scales to debiasing multiple concepts simultaneously.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/finetune_fair_diffusion.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/finetune_fair_diffusion.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AbD5nu5IBIm3AXrs4YW-1\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1484872\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"{     \\\"metadataAttributes\\\": {         \\\"filename\\\": \\\"values.pdf\\\",         \\\"title\\\": \\\"VALUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation\\\",         \\\"summary\\\": \\\"This paper presents a framework called ValUES for systematic validation of uncertainty estimation methods in semantic segmentation tasks. The framework aims to bridge the gap between theoretical advancements and practical applications by providing 1) a controlled environment to study data ambiguities and distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for evaluating uncertainty methods on five downstream tasks: out-of-distribution detection, active learning, failure detection, calibration, and ambiguity modeling. Through empirical studies on simulated and real-world datasets, the authors demonstrate how ValUES can resolve contradictions in the literature, identify the importance of components like aggregation strategies, and provide recommendations for practitioners to select the best uncertainty method for their specific task. Key insights include that separating aleatoric and epistemic uncertainty works on simulated data but does not necessarily translate to real-world data, and that ensembles generally perform most robustly across different downstream tasks while test-time augmentation can be a lightweight alternative.\\\"     } }\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/values.json\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/Summary-PDFs/values.json\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Ay89nu5IBE8iR1J-hWvWX\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"0ICTQBG7SD\"\n",
      "      },\n",
      "      \"score\": 1.1360042\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: end_turn\n",
      "Content: [{'text': 'The key points about the metagpt paper are:\\n\\n- MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs).\\n- It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow.\\n- MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality.\\n- On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches.\\n- The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\\n\\nComparison and Contrast:\\n\\n1. Approach:\\n   - LongLoRA focuses on extending the context window of pre-trained LLMs like LLaMA2 through efficient fine-tuning techniques, while LoftQ addresses the issue of quantization and LoRA fine-tuning for LLMs.\\n   - MetaGPT takes a different approach, proposing a meta-programming framework that leverages multi-agent collaboration and human-inspired Standardized Operating Procedures to enhance the problem-solving capabilities of LLMs.\\n\\n2. Methodology:\\n   - LongLoRA uses shifted sparse attention and trainable embedding/normalization layers to enable efficient long-context fine-tuning.\\n   - LoftQ alternates between quantization and low-rank approximation to obtain a better initialization for LoRA fine-tuning, improving performance in low-bit regimes.\\n   - MetaGPT models a group of specialized agents following a streamlined workflow, using structured communication, publish-subscribe mechanisms, and executable feedback to improve code generation quality.\\n\\n3. Evaluation and Benchmarks:\\n   - LongLoRA and LoftQ evaluate their methods on language modeling and various NLP tasks, demonstrating significant improvements in efficiency and performance.\\n   - MetaGPT focuses on code generation tasks, achieving state-of-the-art results on benchmarks like HumanEval and MBPP.\\n\\n4. Broader Implications:\\n   - LongLoRA and LoftQ contribute to the ongoing efforts to make large language models more efficient and practical for real-world applications.\\n   - MetaGPT explores the integration of human-inspired techniques, like Standardized Operating Procedures, into multi-agent AI systems, opening up new research directions for enhancing the problem-solving capabilities of LLMs.\\n\\nIn summary, the LongLoRA and LoftQ papers focus on improving the efficiency and performance of LLMs through fine-tuning and quantization techniques, while the MetaGPT paper takes a more holistic approach, leveraging multi-agent collaboration and human-inspired methods to enhance the problem-solving capabilities of LLMs.'}]\n",
      "\n",
      "Final Response: The key points about the metagpt paper are:\n",
      "\n",
      "- MetaGPT is a novel meta-programming framework that leverages Standardized Operating Procedures (SOPs) to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs).\n",
      "- It models a group of agents as a simulated software company, with specialized roles like Product Manager, Architect, Engineer, and QA Engineer following a streamlined workflow.\n",
      "- MetaGPT uses structured communication interfaces, a publish-subscribe mechanism, and an executable feedback mechanism to improve code generation quality.\n",
      "- On benchmarks like HumanEval and MBPP, MetaGPT achieves state-of-the-art performance, outperforming existing approaches.\n",
      "- The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems.\n",
      "\n",
      "Comparison and Contrast:\n",
      "\n",
      "1. Approach:\n",
      "   - LongLoRA focuses on extending the context window of pre-trained LLMs like LLaMA2 through efficient fine-tuning techniques, while LoftQ addresses the issue of quantization and LoRA fine-tuning for LLMs.\n",
      "   - MetaGPT takes a different approach, proposing a meta-programming framework that leverages multi-agent collaboration and human-inspired Standardized Operating Procedures to enhance the problem-solving capabilities of LLMs.\n",
      "\n",
      "2. Methodology:\n",
      "   - LongLoRA uses shifted sparse attention and trainable embedding/normalization layers to enable efficient long-context fine-tuning.\n",
      "   - LoftQ alternates between quantization and low-rank approximation to obtain a better initialization for LoRA fine-tuning, improving performance in low-bit regimes.\n",
      "   - MetaGPT models a group of specialized agents following a streamlined workflow, using structured communication, publish-subscribe mechanisms, and executable feedback to improve code generation quality.\n",
      "\n",
      "3. Evaluation and Benchmarks:\n",
      "   - LongLoRA and LoftQ evaluate their methods on language modeling and various NLP tasks, demonstrating significant improvements in efficiency and performance.\n",
      "   - MetaGPT focuses on code generation tasks, achieving state-of-the-art results on benchmarks like HumanEval and MBPP.\n",
      "\n",
      "4. Broader Implications:\n",
      "   - LongLoRA and LoftQ contribute to the ongoing efforts to make large language models more efficient and practical for real-world applications.\n",
      "   - MetaGPT explores the integration of human-inspired techniques, like Standardized Operating Procedures, into multi-agent AI systems, opening up new research directions for enhancing the problem-solving capabilities of LLMs.\n",
      "\n",
      "In summary, the LongLoRA and LoftQ papers focus on improving the efficiency and performance of LLMs through fine-tuning and quantization techniques, while the MetaGPT paper takes a more holistic approach, leveraging multi-agent collaboration and human-inspired methods to enhance the problem-solving capabilities of LLMs.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Compare and contrast the LoRA papers (LongLoRA, LoftQ) and the metagpt paper. Analyze the approach in each paper first.\"\n",
    "final_response, chat_history = chatbot_interaction(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "User Message: What are the evaluation metrics used in the three papers?\n",
      "==================================================\n",
      "\n",
      "Initial Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"I'll analyze the evaluation metrics used in each paper.\\n\\nLet me check LongLoRA's evaluation metrics first:\"}, {'toolUse': {'toolUseId': 'tooluse_IWfsKx1dRXmFaAWYQK89AQ', 'name': 'process_query', 'input': {'filename': 'longlora.pdf', 'query': 'evaluation metrics and results used in the paper'}}}]\n",
      "\n",
      "Tool Used: process_query\n",
      "Tool Input:\n",
      "{\n",
      "  \"filename\": \"longlora.pdf\",\n",
      "  \"query\": \"evaluation metrics and results used in the paper\"\n",
      "}\n",
      "Here is the prepared metadata filters:\n",
      "{'equals': {'key': 'filename', 'value': 'longlora.pdf'}}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"4898bc8f-ab60-4d8f-b541-f81e40caeeda\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:28:28 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"6309\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"4898bc8f-ab60-4d8f-b541-f81e40caeeda\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"Model Avg Single- Doc QA     Multi- Doc QA Summarization Few-shot     Learning Code Synthetic     GPT-3.5-Turbo 44.0 39.8 38.7 26.5 67.1 54.1 37.8 Llama2-7B-chat 31.0 24.9 22.6 24.7 60.0 48.1 5.9 LongChat-v1.5-7B 34.3 28.7 20.6 26.7 60.0 54.1 15.8 Vicuna-v1.5-7B 31.9 28.0 18.6 26.0 66.2 47.3 5.5 Ours-7B 36.8 28.7 28.1 27.8 63.7 56.0 16.7     Table 10: Evaluation on LEval (An et al., 2023) open-ended benchmark. We compare various models to GPT-3.5-Turbo and judge win rates via GPT-4.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 14.0,\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AMj5mu5IBIm3AXrs4AW5t\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.3068695\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"It achieves comparable performance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.     Evaluation Context 3k 6k 10k 13k 16k ChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02 MPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - - MPT-7B-storywriter (Team, 2023b) 0.46 0.46 0.28 0.34 0.36 LongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9 Ours-13B 1.0 0.98 0.98 0.98 0.94     zero. We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that the global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.     Datasets We use the Redpajama (Computer, 2023) dataset for training.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3ADT5mu5IBIm3AXrs4AW5t\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2972572\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.     Size Training Context Length     Evaluation Context Length 2048 4096 8192 16384 32768 65536 100,000     7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52 13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 - 70B 32768 2.84 2.57 2.39 2.26 2.17 - -     Table 5: Topic retrieval evaluation with LongChat (Li et al., 2023). We compare our model to other open-source long-context LLMs. This task involves retrieving target topics from a very long conversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the evaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B. It achieves comparable performance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3ADD5mu5IBIm3AXrs4AW5t\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2432594\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"Model Win-rate Wins Ties LongChat-7B (Li et al., 2023) 33.68 36 56 LongChat-v1.5-7B (Li et al., 2023) 33.59 38 53 Vicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54 Ours-7B 39.06 45 60     B.4 EVALUATION ON LONG-CONTEXT BENCHMARKS.     We evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and LEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the supervised fine-tuning method and data introduced in Section B.6. We compare our model with GPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and LongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better performance than these Llama2-based long-context models, while ours only takes about 4 hours, about 0.3 billion tokens, on a single 8\\u00d7 A100 machine.     B.5 EFFICIENCY ANALYSIS.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 14.0,\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AMz5mu5IBIm3AXrs4AW5t\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2225592\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"This indicates the effectiveness of our efficient fine-tuning method. In Table 3, for the same training and evaluation context length cases, the perplexity decreases as the context size increases. By increasing the context window size from 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72 to 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.     In Table 4, we further examine the maximum context length that we can fine-tune on a single 8\\u00d7 A100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length respectively. LongLoRA achieves promising results on these extremely large settings. In addition, we find some perplexity degradation on small context sizes for the extended models. This is a known limitation of Position Interpolation (Chen et al., 2023).     Retrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we compare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al., 2023).\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/longlora.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"longlora.pdf\",\n",
      "        \"title\": \"LONG LORA: EFFICIENT FINE-TUNING OF LONG-CONTEXT LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3ADz5mu5IBIm3AXrs4AW5t\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2102797\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"The key evaluation metrics used in the LongLoRA paper are:\\n\\n- Perplexity on language modeling tasks\\n- Topic retrieval performance on the LongChat benchmark\\n- Performance on the LEval open-ended benchmark, including single-document QA, multi-document QA, summarization, few-shot learning, and code generation tasks.\\n\\nNext, let's look at the LoftQ paper:\"}, {'toolUse': {'toolUseId': 'tooluse_Py82a7CNQcm3su6YB_wgjw', 'name': 'process_query', 'input': {'filename': 'loftq.pdf', 'query': 'evaluation metrics and results used in the paper'}}}]\n",
      "\n",
      "Tool Used: process_query\n",
      "Tool Input:\n",
      "{\n",
      "  \"filename\": \"loftq.pdf\",\n",
      "  \"query\": \"evaluation metrics and results used in the paper\"\n",
      "}\n",
      "Here is the prepared metadata filters:\n",
      "{'equals': {'key': 'filename', 'value': 'loftq.pdf'}}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"25fefc69-6df2-4013-b658-a0edd8d0f1ad\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:28:31 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"5658\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"25fefc69-6df2-4013-b658-a0edd8d0f1ad\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"2016), RTE (Dagan et al., 2007; Bar- Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al., 2012).     13Published as a conference paper at ICLR 2024     Corpus Task #Train #Dev #Test #Label Metrics Single-Sentence Classification (GLUE)     CoLA Acceptability 8.5k 1k 1k 2 Matthews corr SST Sentiment 67k 872 1.8k 2 Accuracy     Pairwise Text Classification (GLUE) MNLI NLI 393k 20k 20k 3 Accuracy RTE NLI 2.5k 276 3k 2 Accuracy QQP Paraphrase 364k 40k 391k 2 Accuracy/F1 MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1 QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy     Text Similarity (GLUE) STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr     Table 10: Summary of the GLUE benchmark.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 13.0,\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A4z5mu5IBIm3AXrs4DG5a\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2819684\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"The best results on each dataset are shown in bold.     Rank Method MNLI QNLI RTE SST MRPC CoLA QQP STSB SQuAD ANLI m / mm Acc Acc Acc Acc Matt Acc P/S Corr EM/F1 Acc     - Full FT 90.5/90.6 94.0 82.0 95.3 89.5/93.3 69.2 92.4/89.8 91.6/91.1 88.5/92.8 59.8     16 LoRA 90.4/90.5 94.6 85.1 95.1 89.9/93.6 69.9 92.0/89.4 91.7/91.1 87.3/93.1 60.2     16 QLoRA 75.4/75.6 82.4 55.9 86.5 73.8/82.8 N.A. 86.8/82.3 83.0/82.8 61.5 / 71.2 N.A.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 6.0,\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Awj5mu5IBIm3AXrs4DG5a\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2769694\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"We report the median over five random seeds.     Method Bit LLAMA-2-7b LLAMA-2-13b WikiText-2\\u2193 GSM8K\\u2191 WikiText-2\\u2193 GSM8K\\u2191     LoRA 16 5.08 38.5 5.12 48.8     QLoRA 4 5.70 38.2 5.22 48.8 LoftQ 4 5.24 38.0 5.16 49.1     QLoRA 3 5.73 32.1 5.22 40.7 LoftQ 3 5.63 36.2 5.13 45.4     QLoRA 2.5 N.A. N.A. 19.39 N.A. LoftQ 2.5 5.78 31.1 5.22 41.1     QLoRA 2.25 N.A. N.A. N.A. N.A. LoftQ 2.25 6.13 27.5 5.45 38.1     QLoRA 2 N.A N.A. N.A. N.A. LoftQ 2 7.85 26.5 7.69 33.4     4.4 ANALYSIS     Effectiveness of Alternating Optimization.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 8.0,\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AzT5mu5IBIm3AXrs4DG5a\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2580861\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"N.A. indicates the model does not converge. Full FT: full fine-tuning. We report the median over five seeds.     Quantization Rank Method XSum CNN/DailyMail     Full Precision - Lead-3 16.30/1.60/11.95 40.42/17.62/36.67     Full FT 45.14/22.27/37.25 44.16/21.28/40.90     8 LoRA 43.40/20.20/35.20 44.72/21.58/41.84 16 LoRA 43.95/20.72/35.68 45.03/21.84/42.15     NF4 8 QLoRA 42.91/19.72/34.82 43.10/20.22/40.06     LoftQ 44.08/20.72/35.89 43.81/20.95/40.84     16 QLoRA 43.29/20.05/35.15 43.42/20.62/40.44 LoftQ 44.51/21.14/36.18 43.96/21.06/40.96     Uniform 8 QLoRA 41.84/18.71/33.74 N.A.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AyD5mu5IBIm3AXrs4DG5a\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.256018\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"The fifth pascal recognizing textual entailment challenge. In TAC, 2009.     Daniel Cer, Mona Diab, Eneko Agirre, In\\u0303igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1\\u201314, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001.     Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.     Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2007.     Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/loftq.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 10.0,\n",
      "        \"filename\": \"loftq.pdf\",\n",
      "        \"title\": \"LOFTQ: LORA-FINE-TUNING-AWARE QUANTIZATION FOR LARGE LANGUAGE MODELS\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A1T5mu5IBIm3AXrs4DG5a\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2113006\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: tool_use\n",
      "Content: [{'text': \"The key evaluation metrics used in the LoftQ paper are:\\n\\n- Accuracy on the GLUE benchmark tasks, including MNLI, QNLI, RTE, SST, MRPC, CoLA, QQP, and STS-B.\\n- Exact Match (EM) and F1 score on the SQuAD question answering task.\\n- Accuracy on the ANLI natural language inference task.\\n- Perplexity on the WikiText-2 language modeling task.\\n- Perplexity on the GSM8K math word problem task.\\n\\nFinally, let's look at the metagpt paper:\"}, {'toolUse': {'toolUseId': 'tooluse_kbYBqquSRSeE7KDkPuBwgQ', 'name': 'process_query', 'input': {'filename': 'metagpt.pdf', 'query': 'evaluation metrics and results used in the paper'}}}]\n",
      "\n",
      "Tool Used: process_query\n",
      "Tool Input:\n",
      "{\n",
      "  \"filename\": \"metagpt.pdf\",\n",
      "  \"query\": \"evaluation metrics and results used in the paper\"\n",
      "}\n",
      "Here is the prepared metadata filters:\n",
      "{'equals': {'key': 'filename', 'value': 'metagpt.pdf'}}\n",
      "\n",
      "Tool Result:\n",
      "{\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"15c31191-2184-4be3-9db4-325e1df18036\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Wed, 23 Oct 2024 22:28:35 GMT\",\n",
      "      \"content-type\": \"application/json\",\n",
      "      \"content-length\": \"7703\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"15c31191-2184-4be3-9db4-325e1df18036\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  },\n",
      "  \"retrievalResults\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"In the comparisons, we randomly select seven representative tasks for evaluation.     Evaluation Metrics For HuamnEval and MBPP, we follow the unbiased version of Pass@k as presented by (Chen et al., 2021a; Dong et al., 2023), to evaluate the functional accuracy of the top-k     generated codes: Pass@k = EProblems     [ 1\\u2212 (n\\u2212c     k ) (nk)     ] .     For SoftwareDev, we prioritize practical use and evaluate performance through human evaluations (A, E) or statistical analysis (B, C, D): (A) Executability: this metric rates code from 1 (failure/non- functional) to 4 (flawless). \\u20181\\u2019 is for non-functional, \\u20182\\u2019 for runnable but imperfect, \\u20183\\u2019 for nearly perfect, and \\u20184\\u2019 for flawless code. (B) Cost: the cost evaluations here include the (1) running time, (2) token usage, and (3) expenses. (C) Code Statistics: this includes (1) code files, (2) lines of code per file, and (3) total code lines. (D) Productivity: basically, it is defined as the number of token usage divided by the number of lines of code, which refers to the consumption of tokens per code line.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aoc9ku5IBE8iR1J-h1vNc\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.4961321\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"Table 8 displays the names and detailed prompts of 11 tasks within the dataset. Note that the first seven tasks listed are used in the main experiments of this paper.     C.2 ADDITIONAL RESULTS     Quantitative results of MetaGPT As shown in Table 4, MetaGPT achieves an average score of 3.9, surpassing ChatDev\\u2019s score of 2.1 Zhao et al. (2023), which is based on the Chat chain. Compare the scores of general intelligent algorithms, including AutoGPT Torantulino et al. (2023), which all score 1.0, failing to generate executable code. We observe that the generated code is often short, lacks comprehensive logic, and tends to fail to handle cross-file dependencies correctly.     While models such as AutoGPT (Torantulino et al., 2023), Langchain (Chase, 2022), and Agent- Verse (Chen et al., 2023) display robust general problem-solving capabilities, they lack an essential element for developing complex systems: systematically deconstructing requirements. Conversely, MetaGPT simplifies the process of transforming abstract requirements into detailed class and func- tion designs through a specialized division of labor and SOPs workflow. When compared to Chat- Dev (Zhao et al., 2023), MetaGPT\\u2019s structured messaging and feedback mechanisms not only reduce loss of communication information but also improve the execution of code.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 23.0,\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A0M9ku5IBE8iR1J-h1vNc\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2707026\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"Std. A gpt-4-0613 0.732 0.707 0.732 0.713 0.738 0.724 0.013 A gpt-3.5-turbo-0613 0.360 0.366 0.360 0.348 0.354 0.357 0.007 B gpt-4-0613 0.787 0.811 0.817 0.829 0.817 0.812 0.016 B gpt-3.5-turbo-0613 0.348 0.354 0.348 0.335 0.348 0.346 0.007 C gpt-4-0613 0.805 0.805 0.817 0.793 0.780 0.800 0.014 C gpt-3.5-turbo-0613 0.585 0.567 0.573 0.579 0.579 0.577 0.007     Qualitative results Figure 11 and Figure 12 illustrate the outcomes of the Architect agent\\u2019s ef- forts to design a complex recommender system. These figures showcase the comprehensive system interface design and program call flow. The latter is essential for creating a sophisticated automated system.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 25.0,\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A189ku5IBE8iR1J-h1vNc\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2665528\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"(D) Productivity: basically, it is defined as the number of token usage divided by the number of lines of code, which refers to the consumption of tokens per code line. (E) Human Revision Cost: refers to times of manual code corrections, which tackle problems like package import errors, incorrect class names, or incomplete reference paths. Typically, each correction involves up to 3 lines of code.     Baselines We compare our method with recent domain-specific LLMs in the code generation field, including AlphaCode (Li et al., 2022), Incoder (Fried et al., 2022), CodeGeeX (Zheng et al., 2023), CodeGen (Nijkamp et al., 2023), CodeX (Chen et al., 2021a), and CodeT (Chen et al., 2022) and general domain LLMs such as PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023). Several results of baselines (such as Incoder, CodeGeeX) are provided by Dong et al. (2023). In HumanEval and MBPP, we slightly modified the prompts to align with response format requirements. These modifications aim to address format-specific issues (i.e., Python problems). With the SoftwareDev benchmark, we provide a comprehensive comparison between MetaGPT, AutoGPT (Torantulino et\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 7.0,\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3Aos9ku5IBE8iR1J-h1vNc\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2225889\n",
      "    },\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"text\": \"The performance of GPT variants in HumanEval benchmark We use the GPT-4\\u2019s 67% Hu- manEval score (OpenAI, 2023) as our baseline, acknowledging its acceptance in the HumanEval benchmark. We further extend to experiments(five times) with GPT-4 (gpt-4-0613) and GPT-3.5- Turbo (gpt-3.5-turbo-0613) under various conditions to assess performance. (A) We directly called the OpenAI API with the prompt in HumanEval. (B) We called the OpenAI API and parsed the code with regex in the response. (C) We added an additional system prompt, then called the OpenAI API. The prompt is \\u201dYou are an AI that only responds with Python code, NOT ENGLISH. You will     24Preprint     be given a function signature and its docstring by the user. Write your full implementation (restate the function signature).\\u201d As shown in Table 7, GPT-4 is more sensitive to prompt, code parser, and post-processing results on the HumanEval data set. It is difficult for GPT-3.5-Turbo to return the correct completion code without prompt words.     Table 7: Performance of GPT models on HumanEval.\"\n",
      "      },\n",
      "      \"location\": {\n",
      "        \"s3Location\": {\n",
      "          \"uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\"\n",
      "        },\n",
      "        \"type\": \"S3\"\n",
      "      },\n",
      "      \"metadata\": {\n",
      "        \"x-amz-bedrock-kb-source-uri\": \"s3://kbs948efd22-bucket/PDFs/metagpt.pdf\",\n",
      "        \"x-amz-bedrock-kb-document-page-number\": 24.0,\n",
      "        \"filename\": \"metagpt.pdf\",\n",
      "        \"title\": \"MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework\",\n",
      "        \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3A1c9ku5IBE8iR1J-h1vNc\",\n",
      "        \"x-amz-bedrock-kb-data-source-id\": \"VO37NBGNVE\"\n",
      "      },\n",
      "      \"score\": 1.2168375\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Response:\n",
      "Stop Reason: end_turn\n",
      "Content: [{'text': 'The key evaluation metrics used in the metagpt paper are:\\n\\n- Pass@k metric for evaluating the functional accuracy of the top-k generated code on the HumanEval and MBPP benchmarks.\\n- Executability rating (1-4 scale) for evaluating the quality of generated code on the SoftwareDev benchmark.\\n- Cost metrics, including running time, token usage, and expenses, for the SoftwareDev benchmark.\\n- Code statistics, including number of code files, lines of code per file, and total lines of code, for the SoftwareDev benchmark.\\n- Productivity metric, defined as the number of tokens used per line of code, for the SoftwareDev benchmark.\\n- Human revision cost, measuring the number of manual code corrections required, for the SoftwareDev benchmark.\\n\\nIn summary, the LongLoRA paper focused on perplexity, topic retrieval, and open-ended benchmark performance; the LoftQ paper evaluated on the GLUE benchmark, SQuAD, and ANLI; and the metagpt paper used a combination of functional accuracy, code quality, cost, and productivity metrics, as well as human evaluation, to assess its performance.'}]\n",
      "\n",
      "Final Response: The key evaluation metrics used in the metagpt paper are:\n",
      "\n",
      "- Pass@k metric for evaluating the functional accuracy of the top-k generated code on the HumanEval and MBPP benchmarks.\n",
      "- Executability rating (1-4 scale) for evaluating the quality of generated code on the SoftwareDev benchmark.\n",
      "- Cost metrics, including running time, token usage, and expenses, for the SoftwareDev benchmark.\n",
      "- Code statistics, including number of code files, lines of code per file, and total lines of code, for the SoftwareDev benchmark.\n",
      "- Productivity metric, defined as the number of tokens used per line of code, for the SoftwareDev benchmark.\n",
      "- Human revision cost, measuring the number of manual code corrections required, for the SoftwareDev benchmark.\n",
      "\n",
      "In summary, the LongLoRA paper focused on perplexity, topic retrieval, and open-ended benchmark performance; the LoftQ paper evaluated on the GLUE benchmark, SQuAD, and ANLI; and the metagpt paper used a combination of functional accuracy, code quality, cost, and productivity metrics, as well as human evaluation, to assess its performance.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"What are the evaluation metrics used in the three papers?\"\n",
    "final_response, chat_history = chatbot_interaction(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a chatbot widget\n",
    "\n",
    "Let's create an interactive widget for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1588863c65f4462983188ad886d433be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', continuous_update=False, description='User:', placeholder='Type your message her…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbecc2e25304872ad3845c593fdae8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def handle_user_input(user_message):\n",
    "    global chat_history\n",
    "\n",
    "    response, messages = chatbot_interaction(user_message, chat_history)\n",
    "\n",
    "    chat_history = messages\n",
    "\n",
    "    with chat_output:\n",
    "        clear_output()\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "    user_input.value = ''\n",
    "\n",
    "def handle_button_click(sender):\n",
    "    handle_user_input(user_input.value)\n",
    "\n",
    "user_input = widgets.Text(\n",
    "    placeholder='Type your message here...',\n",
    "    description='User:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    on_submit=handle_user_input\n",
    ")\n",
    "\n",
    "chat_output = widgets.Output()\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "send_button = widgets.Button(description='Send')\n",
    "send_button.on_click(handle_button_click)\n",
    "\n",
    "display(widgets.HBox([user_input, send_button]))\n",
    "print(\"\\n\")\n",
    "display(chat_output)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now interact with the chatbot using the widget above. Try asking questions about the LoRA papers or the MetaGPT paper, such as:\n",
    "\n",
    "- \"Compare and contrast the LoRA papers (LongLoRA, LoftQ) and the one from metagpt. \"\n",
    "- \"What are the evaluation metrics used in each study?\"\n",
    "- \"Which are the Retrieval-based Evaluation results for LongLora?\"\n",
    "\n",
    "Remember to run the above cell if you want to start a new conversation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to delete the created resources and avoid unnecesary costs. This should take about 2-3 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All objects in kbs948efd22-bucket have been deleted.\n",
      "Deleting stack: KB-E2E-KB-kbs948efd22\n",
      "Stack KB-E2E-KB-kbs948efd22 deleted successfully.\n",
      "Deleting stack: KB-E2E-Base-kbs948efd22\n",
      "Stack KB-E2E-Base-kbs948efd22 deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# First, set up the session with the correct profile\n",
    "session = boto3.Session()\n",
    "\n",
    "# Now, create all clients using this session\n",
    "s3_client = session.client('s3')\n",
    "cloudformation = session.client('cloudformation')\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_client.delete_object(Bucket=s3_bucket, Key=obj['Key'])\n",
    "        print(f\"All objects in {s3_bucket} have been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting objects from {s3_bucket}: {e}\")\n",
    "\n",
    "time.sleep(60) # Wait until the objects have been deleted\n",
    "\n",
    "# Define the stack names to delete\n",
    "stack_names = [\"KB-E2E-KB-{}\".format(solution_id),\"KB-E2E-Base-{}\".format(solution_id)]\n",
    "\n",
    "# Iterate over the stack names and delete each stack\n",
    "for stack_name in stack_names:\n",
    "    try:\n",
    "        # Retrieve the stack information\n",
    "        stack_info = cloudformation.describe_stacks(StackName=stack_name)\n",
    "        stack_status = stack_info['Stacks'][0]['StackStatus']\n",
    "\n",
    "        # Check if the stack exists and is in a deletable state\n",
    "        if stack_status != 'DELETE_COMPLETE':\n",
    "            # Delete the stack\n",
    "            cloudformation.delete_stack(StackName=stack_name)\n",
    "            print(f'Deleting stack: {stack_name}')\n",
    "\n",
    "            # Wait for the stack deletion to complete\n",
    "            waiter = cloudformation.get_waiter('stack_delete_complete')\n",
    "            waiter.wait(StackName=stack_name)\n",
    "            print(f'Stack {stack_name} deleted successfully.')\n",
    "        else:\n",
    "            print(f'Stack {stack_name} does not exist or has already been deleted.')\n",
    "\n",
    "    except cloudformation.exceptions.ClientError as e:\n",
    "        print(f'Error deleting stack {stack_name}: {e.response[\"Error\"][\"Message\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
