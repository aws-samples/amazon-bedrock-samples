{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Q&A Dataset Generator for RAG Evaluation\n",
    "\n",
    "This script automates the generation of synthetic question-answer pairs from PDF documents for evaluating Retrieval-Augmented Generation (RAG) systems. It uses LangChain with Amazon Bedrock's LLama2 model to:\n",
    "- Extract meaningful chunks from PDF documents\n",
    "- Generate relevant questions based on the content\n",
    "- Create corresponding answers and identify source contexts\n",
    "- Output the data in two formats: prompt-only and prompt-with-ground-truth\n",
    "- Perform quality checks to ensure valid content\n",
    "\n",
    "### Prerequisites\n",
    "- Amazon Bedrock access with required model access\n",
    "- PDF documents in a specified directory\n",
    "- Required packages: langchain, boto3, pandas, tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and connect to Bedrock.\n",
    "\n",
    "Please ignore any pip dependency error (if you see any while installing libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install -r ../requirements.txt --no-deps --quiet\n",
    "%pip install -r ../requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "import json\n",
    "import boto3\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is part of the setup and used to :\n",
    "- Add the parent directory to the python system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = Path().resolve()\n",
    "current_path = current_path.parent\n",
    "\n",
    "if str(current_path) not in sys.path:\n",
    "    sys.path.append(str(current_path))\n",
    "\n",
    "# Print sys.path to verify\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Amazon Bedrock\n",
    "\n",
    "Sets up the connection to Amazon Bedrock and configures the LLama2 model with appropriate parameters for consistent output generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client('bedrock-runtime',region_name='us-east-1')\n",
    "llama_3_70B = \"meta.llama3-70b-instruct-v1:0\"\n",
    "inference_modifier_llama = {\n",
    "    \"max_gen_len\": 4096,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id = llama_3_70B,\n",
    "    client = boto3_bedrock, \n",
    "    model_kwargs = inference_modifier_llama \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - S3 Bucket Configuration & Load document(s)\n",
    "\n",
    "Before we proceed, lets add the S3 bucket name where you have enabled `CORS` and have permission to use. This dummy dataset will be uploaded in the S3 bucket and it will also be used by Evaluation job.\n",
    "\n",
    "Check `CORS` requirements on our [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"<YOUR_EVAL_BUCKET_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Process PDF Documents\n",
    "\n",
    "This section loads PDF documents and splits them into manageable chunks for processing. The RecursiveCharacterTextSplitter ensures context-aware splitting with overlap to maintain coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(f\"{current_path}/synthetic_dataset/\") \n",
    "loader.glob = f\"**/octank_financial_10K.pdf\"\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2500,  \n",
    "    chunk_overlap  = 100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define Prompt Templates\n",
    "\n",
    "These templates guide the LLM in generating questions, answers, and identifying relevant context. Each template is carefully structured to ensure:\n",
    "- Questions are meaningful and answerable\n",
    "- Answers are precise and based on context\n",
    "- Source contexts are accurately extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_question_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"\n",
    "    [INST]\n",
    "    <Instructions>\n",
    "    Here is some context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Your task is to generate 1 question that can be answered using the provided context, following these rules:\n",
    "\n",
    "    <rules>\n",
    "    1. The question should make sense to humans even when read without the given context.\n",
    "    2. The question should be fully answered from the given context.\n",
    "    3. The question should be framed from a part of context that contains important information. It can also be from tables, code, etc.\n",
    "    4. The answer to the question should not contain any links.\n",
    "    5. The question should be of moderate difficulty.\n",
    "    6. The question must be reasonable and must be understood and responded by humans.\n",
    "    7. Do not use phrases like 'provided context', etc. in the question.\n",
    "    8. Avoid framing questions using the word \"and\" that can be decomposed into more than one question.\n",
    "    9. The question should not contain more than 10 words, make use of abbreviations wherever possible.\n",
    "    </rules>\n",
    "\n",
    "    Output only the generated question with a \"?\" at the end, no other text or characters.\n",
    "    </Instructions>\n",
    "    [/INST]\n",
    "    \"\"\")\n",
    "\n",
    "answer_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    [INST]\n",
    "    <Instructions>\n",
    "    <Task>\n",
    "    <role>You are an experienced QA Engineer for building large language model applications.</role>\n",
    "    <task>It is your task to generate an answer to the following question <question>{question}</question> only based on the <context>{context}</context></task>\n",
    "    The output should be only the answer generated from the context.\n",
    "\n",
    "    <rules>\n",
    "    1. Only use the given context as a source for generating the answer.\n",
    "    2. Be as precise as possible with answering the question.\n",
    "    3. Be concise in answering the question and only answer the question at hand rather than adding extra information.\n",
    "    </rules>\n",
    "\n",
    "    Only output the generated answer as a sentence. No extra characters.\n",
    "    </Task>\n",
    "    </Instructions>\n",
    "    [/INST]\n",
    "    Assistant:\n",
    "    \"\"\")\n",
    "\n",
    "source_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Human:\n",
    "    [INST]\n",
    "    <Instructions>\n",
    "    Here is the context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Your task is to extract the relevant sentences from the given context that can potentially help answer the following question. You are not allowed to make any changes to the sentences from the context.\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Output only the relevant sentences you found, one sentence per line, without any extra characters or explanations.\n",
    "    </Instructions>\n",
    "    [/INST]\n",
    "    Assistant:\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Define Helper Functions\n",
    "\n",
    "These core functions handle the interaction with the LLM to generate questions, answers, and extract relevant source contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_question(doc, llm):\n",
    "    initial_question_prompt = initial_question_prompt_template.format(context=doc)\n",
    "    initial_question = llm.invoke(initial_question_prompt)\n",
    "    return initial_question\n",
    "\n",
    "def generate_answer(question: str, doc, llm):\n",
    "    answer_prompt = answer_prompt_template.format(question = question, context=doc)\n",
    "    answer = llm.invoke(answer_prompt)\n",
    "    return answer\n",
    "\n",
    "def generate_source(question: str, doc, llm):\n",
    "    source_prompt = source_prompt_template.format(question = question, context=doc)\n",
    "    source = llm.invoke(source_prompt)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Dataset Generation Functions\n",
    "\n",
    "These functions orchestrate the QA pair generation process, managing the creation and storage of questions, answers, and contexts in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_qa_dataset_doc(doc, llm, dataset, doc_number):\n",
    "    question = generate_question(doc, llm)\n",
    "    dataset.at[doc_number, \"question\"] = question.content\n",
    "    \n",
    "    answer = generate_answer(question, doc, llm)\n",
    "    dataset.at[doc_number, \"reference_answer\"] = answer.content\n",
    "        \n",
    "    source_sentence = generate_source(question, doc, llm)\n",
    "    dataset.at[doc_number, \"source_sentence\"] = source_sentence.content\n",
    "    \n",
    "    dataset.at[doc_number, \"source_raw\"] = doc.page_content\n",
    "    dataset.at[doc_number, \"source_document\"] = doc.metadata[\"source\"]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def generate_dataset(documents, llm, dataset):\n",
    "    for doc in tqdm(range(len(documents))):\n",
    "        dataset = generate_qa_dataset_doc(doc = documents[doc], llm = llm, dataset = dataset, doc_number = doc)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Schema Conversion Functions\n",
    "\n",
    "These functions handle data validation and conversion into two specific JSON schemas:\n",
    "- prompt_only: Contains just the question for evaluation\n",
    "- prompt_with_gt: Contains question, reference answer, and contexts\n",
    "The functions include quality checks to ensure no empty or invalid content makes it to the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_valid_content(text):\n",
    "    return bool(text and text.strip())\n",
    "\n",
    "def convert_schema(example, schema_type=\"prompt_only\"):\n",
    "    if not is_valid_content(example[\"query\"]):\n",
    "        return None\n",
    "    \n",
    "    query = example[\"query\"].strip()\n",
    "\n",
    "    if schema_type == \"prompt_only\":\n",
    "        new_schema = {\n",
    "            \"conversationTurns\": [\n",
    "                {\n",
    "                    \"prompt\": {\n",
    "                        \"content\": [{\"text\": query}]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    elif schema_type == \"prompt_with_gt\":\n",
    "        reference_answer = example[\"reference_answer\"].strip()\n",
    "        if not (is_valid_content(reference_answer) and example[\"reference_contexts\"]):\n",
    "            return None\n",
    "            \n",
    "        valid_contexts = [\n",
    "            context.strip() for context in example[\"reference_contexts\"] \n",
    "            if is_valid_content(context)\n",
    "        ]\n",
    "        \n",
    "        if not valid_contexts:\n",
    "            return None\n",
    "\n",
    "        new_schema = {\n",
    "            \"conversationTurns\": [\n",
    "                {\n",
    "                    \"prompt\": {\n",
    "                        \"content\": [{\"text\": query}]\n",
    "                    },\n",
    "                    \"referenceResponses\": [\n",
    "                        {\"content\": [{\"text\": reference_answer}]}\n",
    "                    ],\n",
    "                    \"referenceContexts\": [\n",
    "                        {\"content\": [{\"text\": context}]} for context in valid_contexts\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid schema_type: {schema_type}. Must be either 'prompt_only' or 'prompt_with_gt'\")\n",
    "    return new_schema\n",
    "\n",
    "def save_to_jsonl(df, output_file_prefix, schema_type):\n",
    "    valid_records = 0\n",
    "    skipped_records = 0\n",
    "    \n",
    "    with open(f'{output_file_prefix}_{schema_type}.jsonl', 'w') as file:\n",
    "        for _, row in df.iterrows():\n",
    "            example = {\n",
    "                \"query\": row[\"query\"],\n",
    "                \"query_by\": {\"model_name\": row[\"model_name\"], \"type\": row[\"type\"]},\n",
    "                \"reference_contexts\": row[\"reference_contexts\"].split(\", \"),\n",
    "                \"reference_answer\": row[\"reference_answer\"],\n",
    "                \"reference_answer_by\": {\"model_name\": row[\"model_name\"], \"type\": row[\"type\"]}\n",
    "            }\n",
    "            \n",
    "            schema = convert_schema(example, schema_type)\n",
    "            if schema:\n",
    "                json.dump(schema, file)\n",
    "                file.write('\\n')\n",
    "                valid_records += 1\n",
    "            else:\n",
    "                skipped_records += 1\n",
    "    \n",
    "    print(f\"Schema type: {schema_type}\")\n",
    "    print(f\"Valid records written: {valid_records}\")\n",
    "    print(f\"Skipped records: {skipped_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Generate Dataset\n",
    "\n",
    "Initializes the dataset generation process with a subset of documents for testing or full processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_subset = docs[:20]\n",
    "dataset = pd.DataFrame(columns=[\"question\", \"reference_answer\", \"source_sentence\",\"source_raw\",\"source_document\"])\n",
    "dataset_df = generate_dataset(docs_subset, llm, dataset)\n",
    "dataset_df['reference_answer'] = dataset_df['reference_answer'].str.replace(r'\\[\\/INST\\]', '', regex=True)\n",
    "dataset_df['source_raw'] = dataset_df['source_raw'].str.replace(r'\\[\\/INST\\]', '', regex=True)\n",
    "\n",
    "filtered_df = dataset_df.drop([\"source_sentence\", \"source_document\"], axis=1)\n",
    "filtered_df = filtered_df.rename(columns={\n",
    "    'question': 'query',\n",
    "    'reference_answer': 'reference_answer',\n",
    "    'source_raw': 'reference_contexts'\n",
    "})\n",
    "\n",
    "filtered_df[\"model_name\"] = \"llama_3_70B\"\n",
    "filtered_df[\"type\"] = \"ai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Dataset Files\n",
    "\n",
    "Creates the final JSONL files in both formats and organizes them in an evaluation_data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_to_jsonl(filtered_df, 'rag_dataset', 'prompt_only')\n",
    "save_to_jsonl(filtered_df, 'rag_dataset', 'prompt_with_gt')\n",
    "\n",
    "if not os.path.exists(\"evaluation_data\"):\n",
    "    os.mkdir(\"evaluation_data\")\n",
    "\n",
    "for file in ['rag_dataset_prompt_only.jsonl', 'rag_dataset_prompt_with_gt.jsonl']:\n",
    "    shutil.move(file, 'evaluation_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Upload to S3 (Optional)\n",
    "\n",
    "Optional functionality to upload the generated datasets to Amazon S3 for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "\n",
    "for file in ['rag_dataset_prompt_only.jsonl', 'rag_dataset_prompt_with_gt.jsonl']:\n",
    "    s3_client.upload_file(f'evaluation_data/{file}', bucket_name, f'evaluation_data/{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script generates two types of evaluation datasets in JSONL format, stored in the 'evaluation_data' directory:\n",
    "1. prompt_only.jsonl: Contains only questions for basic evaluation\n",
    "2. prompt_with_gt.jsonl: Contains questions, reference answers, and contexts for comprehensive evaluation\n",
    "\n",
    "These datasets can be used to evaluate RAG systems by comparing their responses against the generated reference answers and contexts."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "bedrock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
