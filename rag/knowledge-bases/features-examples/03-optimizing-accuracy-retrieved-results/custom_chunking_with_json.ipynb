{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSON Processing with Transformation Functions in Amazon Bedrock Knowledge Bases\n",
    "\n",
    "In modern RAG applications, the ability to effectively process and transform data before it reaches your Foundation Models is crucial for optimal performance. While standard JSON processing works for many use cases, complex enterprise applications often require more nuanced control over how their data is structured and presented. \n",
    "\n",
    "Just as query reformulation helps break down complex queries for better retrieval, transformation functions allow you to reshape and refine your JSON data to better serve your specific use case. This capability is particularly valuable when working with varied data sources or when you need to standardize information across different formats. By customizing how your JSON data is processed, you can enhance the quality of responses from your RAG applications while maintaining efficiency and scalability.\n",
    "\n",
    "This example will explore how to leverage transformation functions in Amazon Bedrock Knowledge Bases to optimize your JSON processing pipeline and achieve more precise and relevant results from your GenAI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the needed libraries\n",
    "\n",
    "First step is to install the pre-requisites packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install -r ../requirements.txt --no-deps --quiet\n",
    "%pip install -r ../requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "\n",
    "\n",
    "sys.path.insert(0, \".\")\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "\n",
    "from utils.knowledge_base import BedrockKnowledgeBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are clients and variables that will be used across this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clients\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "region =  session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime') \n",
    "\n",
    "\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\"\n",
    "\n",
    "knowledge_base_name_custom = 'custom-chunking-kb'\n",
    "knowledge_base_description = \"Knowledge Base containing complex Json\"\n",
    "bucket_name = f'{knowledge_base_name_custom}-{suffix}'\n",
    "intermediate_bucket_name = f'{knowledge_base_name_custom}-intermediate-{suffix}'\n",
    "lambda_function_name = f'{knowledge_base_name_custom}-lambda-{suffix}'\n",
    "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# Define data sources\n",
    "data_source=[{\"type\": \"S3\", \"bucket_name\": bucket_name}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Create Lambda Function\n",
    "\n",
    "Following customized Lambda function will work as a transformation function to process JSON elements from input datasets and split it before ingest on Vector Database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_function.py\n",
    "import json\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def read_s3_file(s3_client, bucket, key):\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "def write_to_s3(s3_client, bucket, key, content):\n",
    "    s3_client.put_object(Bucket=bucket, Key=key, Body=json.dumps(content))\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    logger.info('input={}'.format(json.dumps(event)))\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Extract relevant information from the input event\n",
    "    input_files = event.get('inputFiles')\n",
    "    input_bucket = event.get('bucketName')\n",
    "\n",
    "    if not all([input_files, input_bucket]):\n",
    "        raise ValueError(\"Missing required input parameters\")\n",
    "\n",
    "    output_files = []\n",
    "\n",
    "    for input_file in input_files:\n",
    "        logger.info('input file ={}'.format(input_file))\n",
    "        content_batches = input_file.get('contentBatches', [])\n",
    "        original_file_location = input_file.get('originalFileLocation', {})\n",
    "\n",
    "        processed_batches = []\n",
    "\n",
    "        for batch in content_batches:\n",
    "            input_key = batch.get('key')\n",
    "\n",
    "            if not input_key:\n",
    "                    raise ValueError(\"Missing key in content batch\")\n",
    "\n",
    "            file_content = read_s3_file(s3, input_bucket, input_key)\n",
    "\n",
    "            # Process content\n",
    "            file_key = \"\"\n",
    "            if 'cities' in file_content['fileContents'][0]['contentBody']:\n",
    "                file_key = 'cities'\n",
    "            elif 'ratings' in file_content['fileContents'][0]['contentBody']:\n",
    "                file_key = 'ratings'\n",
    "            else:\n",
    "                raise Exception(\"Key Not Found on File\")\n",
    "\n",
    "            for i in json.loads(file_content['fileContents'][0]['contentBody'])[file_key]:\n",
    "                output_key = \"output/{}_{}.json\".format(file_key, i['id'])\n",
    "\n",
    "                processed_content = {'fileContents': []}\n",
    "                processed_content['fileContents'].append({\n",
    "                        'contentType': 'json', \n",
    "                        'contentBody': json.dumps(i)\n",
    "                })\n",
    "                \n",
    "                # Write processed content back to S3\n",
    "                write_to_s3(s3, input_bucket, output_key, processed_content)\n",
    "\n",
    "                # Add processed batch information\n",
    "                processed_batches.append({\n",
    "                    'key': output_key\n",
    "                })\n",
    "        \n",
    "        output_file = {\n",
    "            'originalFileLocation': original_file_location,\n",
    "            'contentBatches': processed_batches\n",
    "        }\n",
    "\n",
    "        output_files.append(output_file)\n",
    "\n",
    "    result = {'outputFiles': output_files}\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Create Knowledge Base with custom chunking strategy\n",
    "\n",
    "Let's start by creating a Amazon Bedrock Knowledge Base to store two datasets (on `synthetic_dataset` folder):\n",
    "\n",
    "- `destinations.json`: data from travel destinations, with country and city names and a quick summary of places to visit.\n",
    "- `ratings.json`: people ratings and experience from previous experiences on previous cities.\n",
    "\n",
    "**Note: Both datasets are synthetic, they were generated using Bedrock**\n",
    "\n",
    "Knowledge Bases allow you to integrate with different vector databases including Amazon OpenSearch Serverless, Amazon Aurora, Pinecone, Redis Enterprise and MongoDB Atlas. For this example, we will integrate the knowledge base with Amazon OpenSearch Serverless. To do so, we will use the helper class BedrockKnowledgeBase which will create the knowledge base and all of its pre-requisites:\n",
    "\n",
    "1. IAM roles and policies\n",
    "1. S3 bucket\n",
    "1. Amazon OpenSearch Serverless encryption, network and data access policies\n",
    "1. Amazon OpenSearch Serverless collection\n",
    "1. Amazon OpenSearch Serverless vector index\n",
    "1. Knowledge base\n",
    "1. Knowledge base data source\n",
    "1. Create a knowledge base using CUSTOM chunking strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_custom = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_custom}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    lambda_function_name=lambda_function_name,\n",
    "    intermediate_bucket_name=intermediate_bucket_name, \n",
    "    chunking_strategy = \"CUSTOM\", \n",
    "    suffix = f'{suffix}-c'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Upload datasets to S3 and start ingestion Job\n",
    "\n",
    "After Knowledge Base creation, let's upload both datasets into a S3 Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'destinations.json'\n",
    "s3_client.upload_file(f'synthetic_dataset/{file_name}', bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ratings.json'\n",
    "s3_client.upload_file(f'synthetic_dataset/{file_name}', bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the ingestion job to process those files.\n",
    "\n",
    "If you want to check processing logs, you can find lambda function attached to your Knowledge Base and go to monitoring tab, to find Cloud Watch Logs link and see the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_custom.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Test Knowledge Base\n",
    "\n",
    "Now the Knowlegde Base is available we can test it out using the [retrieve](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [retrieve_and_generate](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions.\n",
    "\n",
    "First, let's retrieve Knowledge Base ID and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_custom = knowledge_base_custom.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Now, let's start with a simple question, asking about a place called Elephanta Caves and languages they speak over there.\n",
    "\n",
    "The answer is in the `\"id\":1037` on the `destinations.json` file, which means Mumbai is the expected answer with Marathi, Hindi, and English being the languages spoken there.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is there a place called Elephanta Caves? If so, what languages do they speak over there?\" \n",
    "# Expected: Mumbai - India, Marathi, Hindi, English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={        \n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_custom,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask another question, about places where they speak Japanese and also visualize both APIs, to see data returned from knowledge base and model thinking with those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"Can you suggest me a good place to be they speak Japanese?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_custom,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the retrieve and generate API, we get the final response directly. Now let's observe the citations for the RetrieveAndGenerate API.\n",
    "\n",
    "Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_rag_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret,1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom))\n",
    "citations_rag_print(response_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Testing Knowledge Base with Retrieve API\n",
    "\n",
    "If you need an extra layer of control, you can retrieve the chunks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_custom, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom_ret['retrievalResults']))\n",
    "response_print(response_custom_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, with CUSTOM chunking, we get 5 retrieved results as requested in the API using semantic similarity, which is the default for the Retrieve API.\n",
    "\n",
    "Those references are stored separately in the Vector Database, following the JSON structure, but all of them are part of the same file. This makes our model return better responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Testing Knowledge Base with Both Files\n",
    "\n",
    "Now, let's ask something to force Knowledge base to look into content that are in both files, like ratings from an specific place.\n",
    "\n",
    "In the following example, we will expect an answer considering Fez from Morocco, and an 8.6 rating with this `id:261` in the `ratings.json` file.\n",
    "\n",
    "This is complete Json structure:\n",
    "```\n",
    "ratings.json: {\"id\":261,\"rating\":8.6,\"review\":\"Desert capital with stunning mosque. Traditional markets offer authentic shopping experience.\",\"visit_date\":1676937600000,\"traveler_type\":\"Couple\",\"length_of_stay\":7,\"photos_shared\":26,\"helpful_votes\":22,\"destination_id\":1064}\n",
    "\n",
    "destinations.json: {\"id\":1064,\"city\":\"Fez\",\"country\":\"Morocco\",\"latitude\":34.0181,\"longitude\":-5.0078,\"main_attractions\":\"Fez El Bali, Bou Inania Madrasa, Chouara Tannery, Al-Qarawiyyin Mosque\",\"best_season\":\"Spring\",\"local_transport\":\"Petit taxi, Bus, Walking, Donkey\",\"languages\":\"Arabic, Berber, French\",\"currency\":\"MAD\",\"timezone\":\"Africa/Casablanca\"},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What People say about Morocco? Give me highest rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_custom,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Clean Up\n",
    "\n",
    "To clean up resources, execute following method from helper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_custom.delete_kb(delete_s3_bucket=True, delete_lambda_function=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
