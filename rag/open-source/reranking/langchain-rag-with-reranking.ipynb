{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c28851c-774f-4c77-b0d5-417dc36abeaf",
   "metadata": {},
   "source": [
    "# Improving accuracy for RAG based applications using Langchain, OpenSearch Serverless, Amazon Bedrock and a Re-ranking model \n",
    "\n",
    "In this notebook, we are going to demonstrate how to build a RAG solution that utilises Langchain, OpenSearch Serverless, Amazon Bedrock and a Re-ranking model. Additionally, we'll also evaluate the performance of each approach (i.e. standard RAG approach vs RAG + reranking model) and perform analysis and share the results.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "When it comes to building a chatbot using GenAI LLMs, RAG is a popular architectural choice. It combines the strengths of knowledge base retrieval and generative models for text generation. Using RAG approach for building a chatbot has many advantages. For example, retrieving responses from its database before generating a response could provide more relevant and coherent responses. This helps improve the conversational flow. RAG also scales better with more data compared to pure generative models, and it doesn’t require fine-tuning of the model when new data is added to the knowledge base. Additionally, the retrieval component enables the model to incorporate external knowledge by retrieving relevant background information from its database. This approach helps provide factual, in-depth and knowledgeable responses.\n",
    "\n",
    "## RAG Challenges\n",
    "Despite clear advantages of using RAG for building Chatbots, there are some challenges when it comes to applying it for practical use. \n",
    "In order to find an answer, RAG takes an approach that uses vector search across the documents. The advantage of using vector search is the speed and scalability. Rather than scanning every single document to find the answer, using RAG approach, we would turn the texts (knowledge base) into embeddings and store these embeddings in the database. The embeddings are compressed version of the documents, represented by array of numerical values. After the embeddings are stored,  vector search queries the vector database to find the similarity based on the vectors associated with the documents. Typically, vector search will return the top k most relevant documents based on the user question, and return the k results. However, since the similarity algorithm in vector database works on vectors and not documents, vector search does not always return the most relevant information in the top k results. This directly impacts the accuracy of the response if the most relevant contexts are not available to the LLM. \n",
    "\n",
    "A proposed solution to address the challenge of RAG approach is called Reranking. Reranking is a technique that can further improve the responses by selecting the best option out of several candidate responses. Here is how reranking could work, described in the sequential order:\n",
    "\n",
    "1. The chatbot generates its top k response candidates using RAG.\n",
    "2. These candidates are fed into a reranking model. This model scores each response based on how relevant, natural and informative they are.\n",
    "3. The response with the highest reranking score is selected as the context to feed the LLM in generating a response .\n",
    "\n",
    "In summary, reranking allows the chatbot to filter out poor responses and pick the best one to send back. This further improves the quality and consistency of the conversations.\n",
    "\n",
    "Reference links for research papers and Bedrock documentation:\n",
    "- https://arxiv.org/pdf/2404.07221\n",
    "- https://arxiv.org/pdf/2409.07691\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/rerank.html\n",
    "\n",
    "## Improve the relevance of query responses with a reranker model in Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock provides access to reranker models that you can use when querying to improve the relevance of the retrieved results. A reranker model calculates the relevance of chunks to a query and reorders the results based on the scores that it calculates. By using a reranker model, you can return responses that are better suited to answering the query. Or, you can include the results in a prompt when running model inference to generate more pertinent and accurate responses. With a reranker model, you can retrieve fewer, but more relevant, results. By feeding these results to the foundation model that you use to generate a response, you can also decrease cost and latency.\n",
    "\n",
    "Reranker models are trained to identify relevance signals based on a query and then use those signals to rank documents. Because of this, the models can provide more relevant, more accurate results.\n",
    "\n",
    "## Prerequisites\n",
    "A user requires the following permissions to use reranking:\n",
    "\n",
    "Access to the reranking models that they plan to use. For more information, see https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html.\n",
    "\n",
    "Permissions for their role: bedrock:Rerank and bedrock:InvokeModel"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Install Required Dependencies\n",
    "This cell installs all the necessary Python packages listed in the `requirements.txt` file. The `--quiet` flag reduces the output verbosity.\n",
    "\n",
    "Note: Make sure the `requirements.txt` file is present in your working directory before running this cell.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b532186b88724954"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:58:27.611128Z",
     "start_time": "2024-12-26T21:58:26.537869Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install -r requirements.txt --quiet",
   "id": "7e33c014-05fd-4e70-b3e3-a71b01b9a590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Configuration Constants\n",
    "This cell defines essential configuration variables for Amazon OpenSearch Serverless (AOSS) setup.\n",
    "\n",
    "These constants will be used throughout the notebook for consistent resource naming and configuration.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "da6a949f-ddd0-4c4c-a2bb-39debceb45f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:09:45.686752Z",
     "start_time": "2024-12-26T22:09:45.682327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#const\n",
    "region_name = 'us-west-2'\n",
    "encryption_policy_name = f\"rag-ep-oss\"\n",
    "network_policy_name = f\"rag-np-oss\"\n",
    "access_policy_name = f\"rag-ap-oss\"\n",
    "vector_store_name = f'vector-store-oss'\n",
    "index_name = f\"index-name-oss\"\n",
    "service = \"aoss\""
   ],
   "id": "969e5cbbc9e3fefb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c687c110-8cfd-4190-ab17-ca47d9fb3677",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "To demonstrate RAG semantic search, we'll need to first ingest documents into a vector database. For this example, we'll ingest 4 Amazon Shareholder Letters\n",
    "#### PDF Document Loading and Processing\n",
    "We will Import required libraries, set up document processing, process PDF files\n",
    "Note: Ensure PDF files are placed in the \"./data/\" directory before running this cell."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:09:50.462956Z",
     "start_time": "2024-12-26T22:09:48.878464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "data_root = \"./data/\"\n",
    "folder_path = data_root\n",
    "documents = []\n",
    "\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "     file_path = os.path.join(folder_path, filename)\n",
    "     loader = PyPDFLoader(file_path)\n",
    "     # Load the PDF data\n",
    "     data = loader.load()\n",
    "     # Add the loaded data to the documents list\n",
    "     documents.extend(data)\n",
    "\n",
    "# Print the text of the first page of the first document\n",
    "if documents:\n",
    "    print(documents[0].page_content)\n",
    "else:\n",
    "    print(\"No PDF files found in the folder.\")"
   ],
   "id": "46d527bcf7b076ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To our shareowners:\n",
      "In Amazon’s 1997 letter to shareholders, our first, I talked about our hope to create an “enduring franchise,”\n",
      "one that would reinvent what it means to serve customers by unlocking the internet’s power. I noted that\n",
      "Amazon had grown from having 158 employees to 614, and that we had surpassed 1.5 million customer\n",
      "accounts. We had just gone public at a split-adjusted stock price of $1.50 per share. I wrote that it was Day 1.\n",
      "We’ve come a long way since then, and we are working harder than ever to serve and delight customers.\n",
      "Last year, we hired 500,000 employees and now directly employ 1.3 million people around the world. We have\n",
      "more than 200 million Prime members worldwide. More than 1.9 million small and medium-sized businesses\n",
      "sell in our store, and they make up close to 60% of our retail sales. Customers have connected more than\n",
      "100 million smart home devices to Alexa. Amazon Web Services serves millions of customers and ended 2020\n",
      "with a $50 billion annualized run rate. In 1997, we hadn’t invented Prime, Marketplace, Alexa, or AWS.\n",
      "They weren’t even ideas then, and none was preordained. We took great risk with each one and put sweat\n",
      "and ingenuity into each one.\n",
      "Along the way, we’ve created $1.6 trillion of wealth for shareowners. Who are they? Y our Chair is one, and\n",
      "my Amazon shares have made me wealthy. But more than 7/8ths of the shares, representing $1.4 trillion of\n",
      "wealth creation, are owned by others. Who are they? They’re pension funds, universities, and 401(k)s, and\n",
      "they’re Mary and Larry, who sent me this note out of the blue just as I was sitting down to write this\n",
      "shareholder letter:\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e507d99b-4013-4b2a-8efc-5fea6992dd46",
   "metadata": {},
   "source": [
    "This cell sets up the necessary AWS SDK (boto3) components and utilities:\n",
    "1. Creates a boto3 session with the specified region\n",
    "2. Initializes an Amazon OpenSearch Serverless client"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:09:53.566146Z",
     "start_time": "2024-12-26T22:09:53.383506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "boto3_session = boto3.session.Session(region_name=region_name)\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n"
   ],
   "id": "680560e601cebfd3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell creates the required security policies for an OpenSearch Serverless collection:\n",
    "\n",
    "1. Gets the AWS caller identity ARN using STS client\n",
    "\n",
    "2. Defines a function `create_policies_in_oss()` that creates 3 types of policies:\n",
    "\n",
    "   - **Encryption Policy**: Configures encryption settings using AWS owned KMS keys\n",
    "   - **Network Policy**: Controls network access (allows public access in this case) \n",
    "   - **Access Policy**: Sets up data access permissions for:\n",
    "     - Collection operations (create, delete, update, describe)\n",
    "     - Index operations (create, delete, update, describe)\n",
    "     - Document operations (read, write)\n",
    "\n",
    "The policies are scoped to the specified vector store collection and the caller's IAM identity.\n",
    "\n",
    "Parameters:\n",
    "- vector_store_name: Name of the OpenSearch collection\n",
    "- aoss_client: OpenSearch Serverless client instance\n",
    "\n",
    "Returns tuple of created policies (encryption_policy, network_policy, access_policy)"
   ],
   "id": "c182726be2273e29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:11:11.785448Z",
     "start_time": "2024-12-26T22:11:10.935643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "identity = sts_client.get_caller_identity()[\"Arn\"]\n",
    "\n",
    "\n",
    "\n",
    "def create_policies_in_oss(\n",
    "    vector_store_name, aoss_client\n",
    "):\n",
    "    encryption_policy = aoss_client.create_security_policy(\n",
    "        name=encryption_policy_name,\n",
    "        policy=json.dumps(\n",
    "            {\n",
    "                \"Rules\": [\n",
    "                    {\n",
    "                        \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                        \"ResourceType\": \"collection\",\n",
    "                    }\n",
    "                ],\n",
    "                \"AWSOwnedKey\": True,\n",
    "            }\n",
    "        ),\n",
    "        type=\"encryption\",\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name=network_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    \"Rules\": [\n",
    "                        {\n",
    "                            \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                            \"ResourceType\": \"collection\",\n",
    "                        }\n",
    "                    ],\n",
    "                    \"AllowFromPublic\": True,\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "        type=\"network\",\n",
    "    )\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name=access_policy_name,\n",
    "        policy=json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    \"Rules\": [\n",
    "                        {\n",
    "                            \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                            \"Permission\": [\n",
    "                                \"aoss:CreateCollectionItems\",\n",
    "                                \"aoss:DeleteCollectionItems\",\n",
    "                                \"aoss:UpdateCollectionItems\",\n",
    "                                \"aoss:DescribeCollectionItems\",\n",
    "                            ],\n",
    "                            \"ResourceType\": \"collection\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"Resource\": [\"index/\" + vector_store_name + \"/*\"],\n",
    "                            \"Permission\": [\n",
    "                                \"aoss:CreateIndex\",\n",
    "                                \"aoss:DeleteIndex\",\n",
    "                                \"aoss:UpdateIndex\",\n",
    "                                \"aoss:DescribeIndex\",\n",
    "                                \"aoss:ReadDocument\",\n",
    "                                \"aoss:WriteDocument\",\n",
    "                            ],\n",
    "                            \"ResourceType\": \"index\",\n",
    "                        },\n",
    "                    ],\n",
    "                    \"Principal\": [identity],\n",
    "                    \"Description\": \"Easy data policy\",\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "        type=\"data\",\n",
    "    )\n",
    "    return encryption_policy, network_policy, access_policy"
   ],
   "id": "188059dca604c79b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell performs two key operations:\n",
    "\n",
    "1. **Create Security Policies**:\n",
    "   - Creates three essential security policies for the OpenSearch vector store:\n",
    "     - Encryption policy\n",
    "     - Network policy\n",
    "     - Access policy\n",
    "   - The `create_policies_in_oss()` function handles the creation of these policies\n",
    "   - Policies are associated with the specified vector store name\n",
    "\n",
    "2. **Create Vector Search Collection**:\n",
    "   - Initializes a new collection in OpenSearch using the AWS OpenSearch client\n",
    "   - Collection is configured for vector search operations\n",
    "   - Named according to the `vector_store_name` parameter\n",
    "   - Type is set to \"VECTORSEARCH\" for vector similarity search capabilities\n",
    "\n",
    "These operations are fundamental setup steps for implementing vector search functionality in OpenSearch."
   ],
   "id": "58444ab428b62e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:11:35.718691Z",
     "start_time": "2024-12-26T22:11:34.682876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(\n",
    "    vector_store_name=vector_store_name,\n",
    "    aoss_client=aoss_client,\n",
    ")\n",
    "collection = aoss_client.create_collection(name=vector_store_name, type=\"VECTORSEARCH\")"
   ],
   "id": "5d8c228f1736994",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "This cell sets up the necessary connection parameters for Amazon OpenSearch:\n",
    "\n",
    "1. Extracts the collection ID from the previously created collection\n",
    "2. Constructs the OpenSearch host URL using the collection ID and AWS region\n",
    "3. Creates the full HTTPS endpoint URL with port 443\n",
    "\n",
    "The host URL follows the format: `<collection_id>.<region>.aoss.amazonaws.com`\n",
    "\n",
    "The final OpenSearch URL will be: `https://<host>:443`\n",
    "\n",
    "This URL will be used to connect to the OpenSearch service in subsequent steps."
   ],
   "id": "baba0e549faea93c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:11:39.531455Z",
     "start_time": "2024-12-26T22:11:39.526649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collection_id = collection[\"createCollectionDetail\"][\"id\"]\n",
    "host = collection_id + \".\" + region_name + \".aoss.amazonaws.com\"\n",
    "print(host)\n",
    "opensearch_url=\"https://\" + host +\":443\",\n",
    "print(opensearch_url)"
   ],
   "id": "c7a2a391a578f7be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680rk9clnq24bkyx4sx9.us-west-2.aoss.amazonaws.com\n",
      "('https://680rk9clnq24bkyx4sx9.us-west-2.aoss.amazonaws.com:443',)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell performs two essential tasks for document processing:\n",
    "\n",
    "1. **Text Splitting**\n",
    "   - Uses `RecursiveCharacterTextSplitter` to break documents into manageable chunks\n",
    "   - Sets `chunk_size=1000` (characters per chunk)\n",
    "   - Uses `chunk_overlap=200` to maintain context between chunks\n",
    "   - Splits the input documents into smaller segments\n",
    "\n",
    "2. **Embedding Model Configuration**\n",
    "   - Initializes Amazon Bedrock client for the specified region\n",
    "   - Sets up the Bedrock embeddings model using `BedrockEmbeddings`\n",
    "   - Uses Amazon's Titan text embedding model (version 2.0)\n",
    "\n",
    "This preprocessing step is crucial for:\n",
    "- Making large documents more manageable\n",
    "- Preparing text for vector embeddings\n",
    "- Enabling efficient similarity searches"
   ],
   "id": "ef3676cb72700d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:11:44.162103Z",
     "start_time": "2024-12-26T22:11:43.949271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region_name)\n",
    "\n",
    "embeddings_model = BedrockEmbeddings(\n",
    "    client=bedrock_client, model_id=\"amazon.titan-embed-text-v2:0\"\n",
    ")"
   ],
   "id": "8c7fca2be2e2cd3e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9f11cf2fd2beb7a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell configures and initializes an OpenSearch vector store using AWS authentication:\n",
    "\n",
    "1. Imports required dependencies for AWS authentication and OpenSearch vector storage\n",
    "2. Creates AWS authentication credentials using AWS4Auth\n",
    "3. Initializes OpenSearchVectorSearch with:\n",
    "   - Document embeddings\n",
    "   - OpenSearch endpoint URL\n",
    "   - AWS authentication\n",
    "   - SSL/TLS security settings\n",
    "   - Connection and timeout configurations\n",
    "   - Index name and vector engine (FAISS)\n",
    "\n",
    "The vector store will be used to index and search document embeddings for semantic similarity.\n",
    "\n",
    "!NB it can take a few minutes for collection to be created and ready for indexing. Security policy updates in OpenSearch Serverless may need time to take effect. If you get AuthenticationException, please retry in a couple of minutes."
   ],
   "id": "ce36f56eec14522e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:13:38.970819Z",
     "start_time": "2024-12-26T22:13:28.808446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from opensearchpy import RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "\n",
    "\n",
    "credentials = boto3_session.get_credentials()\n",
    "#service = 'aoss'\n",
    "awsauth = AWS4Auth(region=region_name, service=service, refreshable_credentials=credentials)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    documents,\n",
    "    embeddings_model,\n",
    "    opensearch_url=opensearch_url,#\"https://813704av6f47ieeyku2k.us-west-2.aoss.amazonaws.com:443\",#host,\n",
    "    http_auth=awsauth,#AWSV4SignerAuth(boto3_session.get_credentials(), region_name),#awsauth,\n",
    "    timeout=300,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    bulk_size=100,\n",
    "\n",
    ")"
   ],
   "id": "a6c0cb09f23e2a4f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### !NB - Wait for indexing to complete - it can take about 2-5 minutes",
   "id": "1de96670a28ee422"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Test vectore store\n",
    "This cell performs a similarity search in the document collection to find relevant text passages about Amazon's investment philosophy. \n",
    "The `similarity_search()` method will return documents that most closely match the query \"The key elements of Amazon's investment philosophy\".\n",
    "\n",
    "If print return 0, it means indexing is not completed. Wait a  bit longer and try again."
   ],
   "id": "11446163ff4dbc28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:38.085746Z",
     "start_time": "2024-12-26T22:16:36.797260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_query = \"The key elements of Amazon's investment philosophy\"\n",
    "docs = docsearch.similarity_search(example_query)\n",
    "print (len(docs))"
   ],
   "id": "a6929a7a1b5ce268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell counts and displays metadata for each document chunk:\n",
    "\n",
    "1. First, we count the total number of chunks in our `docs` collection\n",
    "2. Then, we iterate through each chunk and print its associated metadata\n",
    "\n",
    "This helps us verify:\n",
    "- The total number of document chunks created\n",
    "- The metadata properties of each chunk (e.g., source, page numbers)\n",
    "- Proper document segmentation"
   ],
   "id": "eccedee140000225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:40.669808Z",
     "start_time": "2024-12-26T22:16:40.666069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "number_of_chunks = len(docs)\n",
    "print(number_of_chunks)\n",
    "for i in range(number_of_chunks):\n",
    "    print(docs[i].metadata)\n"
   ],
   "id": "aac959cb7f72a30c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'source': './data/AMZN-1997-Shareholder-Letter.pdf', 'page': 0}\n",
      "{'source': './data/AMZN-1997-Shareholder-Letter.pdf', 'page': 2}\n",
      "{'source': './data/AMZN-2022-Shareholder-Letter.pdf', 'page': 0}\n",
      "{'source': './data/AMZN-2022-Shareholder-Letter.pdf', 'page': 6}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see we got 4 chunks in the order: \n",
    "\n",
    "0-letter from 1997, page 1\n",
    "\n",
    "1-letter from 1997, page 3\n",
    "\n",
    "2-letter from 2022, page 1\n",
    "\n",
    "3-letter from 2022, page 7\n",
    "\n",
    "(if you use different chunking strategy, different embedding model, or with changes over time the chunks and their order  can be different)\n",
    "\n",
    "## Reranking\n",
    "\n",
    "#### Now let's have a look for order of chunks if we use reranking model.\n",
    "\n",
    "This code cell:\n",
    "This cell initializes a client for Amazon Bedrock Agent Runtime service\n",
    "It creates a connection to AWS Bedrock Agent Runtime using boto3, AWS's SDK for Python\n",
    "The client is configured for the specified AWS region (stored in region_name variable)\n",
    "This client will be used to make API calls to the Bedrock Agent Runtime service for reranking operations\n"
   ],
   "id": "e5be9041bd819614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:45.190800Z",
     "start_time": "2024-12-26T22:16:45.174510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Bedrock Agent Runtime client\n",
    "rerank_client = boto3.client('bedrock-agent-runtime', region_name=region_name)"
   ],
   "id": "8de7c8e02a1f8362",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The cell below:\n",
    "\n",
    "- Define the Bedrock model ID and construct the model package ARN\n",
    "\n",
    "- Specify the model ID for Amazon's reranking model (we will use Amazon Rerank model, alternatively can use Cohere's reranking model)\n",
    "\n",
    "- Construct the complete ARN (Amazon Resource Name) for the Bedrock foundation model.\n",
    "This combines the AWS region, model ID, and required ARN format for Bedrock models"
   ],
   "id": "d99cd112016559b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:48.990213Z",
     "start_time": "2024-12-26T22:16:48.987193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "modelId = \"amazon.rerank-v1:0\"#\"cohere.rerank-v3-5:0\"\n",
    "model_package_arn = f\"arn:aws:bedrock:{region_name}::foundation-model/{modelId}\""
   ],
   "id": "7855f43d1633d874",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This function, rerank_text, is designed to rerank a list of text sources based on their relevance to a given query using Amazon Bedrock's reranking service. Here's what the function does:\n",
    "\n",
    "1. It takes four parameters:\n",
    "\n",
    "- text_query: The search query text\n",
    "- text_sources: A list of text documents to be reranked\n",
    "- num_results: The number of results to return\n",
    "- model_package_arn: The Amazon Resource Name (ARN) of the Bedrock reranking model to use\n",
    "2. It calls the rerank client's rerank method with:\n",
    "\n",
    "  - A query configuration specifying the text query\n",
    "  - The source documents to be reranked\n",
    "  - Reranking configuration including:\n",
    "    + The type of reranking model (Bedrock)\n",
    "    + Number of results to return\n",
    "    + The specific model to use (via ARN)\n",
    "3. Finally, it returns the 'results' portion of the API response, which contains the reranked documents in order of relevance.\n",
    "\n",
    "This function is useful for improving search results by reordering documents based on their semantic relevance to the query, rather than just keyword matching."
   ],
   "id": "804fe32c2bf97a13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:51.514379Z",
     "start_time": "2024-12-26T22:16:51.509121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rerank_text(text_query, text_sources, num_results, model_package_arn):\n",
    "    api_response = rerank_client.rerank(\n",
    "        queries=[\n",
    "            {\n",
    "                \"type\": \"TEXT\",\n",
    "                \"textQuery\": {\n",
    "                    \"text\": text_query\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        sources=text_sources,\n",
    "        rerankingConfiguration={\n",
    "            \"type\": \"BEDROCK_RERANKING_MODEL\",\n",
    "            \"bedrockRerankingConfiguration\": {\n",
    "                \"numberOfResults\": num_results,\n",
    "                \"modelConfiguration\": {\n",
    "                    \"modelArn\": model_package_arn,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return api_response['results']"
   ],
   "id": "108a99fa535d59ea",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb59ec1d22c60f71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code snippet creates a list called text_sources by iterating through a collection of documents (docs). For each text document, it creates a dictionary with a specific structure that appears to be formatting document sources in a particular way. The structure includes:\n",
    "\n",
    "1. A \"type\" field set to \"INLINE\"\n",
    "2. An \"inlineDocumentSource\" object containing:\n",
    "- A \"type\" field set to \"TEXT\"\n",
    "- A \"textDocument\" object with the actual text content from the document\n",
    "\n",
    "This formatting might be used for preparing documents for processing in a document management system, API request, or text analysis tool."
   ],
   "id": "beed01e6dd3c6f74"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "Example of resulting structure:\n",
    "[\n",
    "    {\n",
    "        \"type\": \"INLINE\",\n",
    "        \"inlineDocumentSource\": {\n",
    "            \"type\": \"TEXT\",\n",
    "            \"textDocument\": {\n",
    "                \"text\": \"actual content here\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    # ... more similar entries\n",
    "]"
   ],
   "id": "e73d9bbf63917550"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:16:56.836570Z",
     "start_time": "2024-12-26T22:16:56.831627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_sources = []\n",
    "for text in docs:\n",
    "    text_sources.append({\n",
    "        \"type\": \"INLINE\",\n",
    "        \"inlineDocumentSource\": {\n",
    "            \"type\": \"TEXT\",\n",
    "            \"textDocument\": {\n",
    "                \"text\": text.page_content,\n",
    "            }\n",
    "        }\n",
    "    })"
   ],
   "id": "185b419505aee1e4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b28db5795b624a07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell is executing a text reranking operation using a rerank_text function. Here's what it does:\n",
    "\n",
    "1. Takes an example query (a search query or question)\n",
    "2. Uses text_sources (a collection of text documents or passages)\n",
    "3. Specifies '3' as a parameter (the number of results to return)\n",
    "4. Uses a specific model_package_arn (to specify which reranking model to use)\n",
    "5. Prints the reranked results (higher relevance score means the text is more relevant to the query)\n",
    "\n",
    "The function reorders the text_sources based on their relevance to the example_query, using a reranking model specified by the model_package_arn, and returns the top 3 most relevant results.\n"
   ],
   "id": "29656738dbc9ed17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T22:17:01.206373Z",
     "start_time": "2024-12-26T22:17:00.139208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = rerank_text(example_query, text_sources, 3, model_package_arn)\n",
    "print(response)"
   ],
   "id": "f3cb7e704fcdda80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 1, 'relevanceScore': 0.01833445206284523}, {'index': 0, 'relevanceScore': 0.0028449096716940403}, {'index': 2, 'relevanceScore': 0.0020190139766782522}]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see order of documents changed:\n",
    "\n",
    "1-letter from 1997, page 3\n",
    "\n",
    "0-letter from 1997, page 1\n",
    "\n",
    "2-letter from 2022, page 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can use reranking model to narrow down amount of chunks to reduce context side to LLM and use most relevant chunks.\n"
   ],
   "id": "b40fa59c0a14e02f"
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
