{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49b07c8-f4df-42ab-a3e2-d3cb993cb9e7",
   "metadata": {},
   "source": [
    "<h2>Building Q&A Application with LlamaIndex and Amazon Bedrock Knowledge Base</h2>\n",
    "\n",
    "\n",
    "<h2>Overview</h2>\n",
    "\n",
    "In this notebook we will leverage Amazon Bedrock Knowledge Base that we created in [0_how_to_create_index_and_ingest_documents_in_knowledge_base.ipynb](./0_how_to_create_index_and_ingest_documents_in_knowledge_base.ipynb) and use it with LlamaIndex to create a Q&A Application.\n",
    "\n",
    "\n",
    "<h2>Context</h2>\n",
    "\n",
    "Implementing RAG requires organizations to perform several cumbersome steps to convert data into embeddings (vectors), store the embeddings in a specialized vector database, and build custom integrations into the database to search and retrieve text relevant to the user’s query. This can be time-consuming and inefficient.\n",
    "\n",
    "With Knowledge Bases for Amazon Bedrock, simply point to the location of your data in Amazon S3, and Knowledge Bases for Amazon Bedrock takes care of the entire ingestion workflow into your vector database. If you do not have an existing vector database, Amazon Bedrock creates an Amazon OpenSearch Serverless vector store for you. For retrievals, use the LLamaIndex - Amazon Bedrock integration via the Retrieve API to retrieve relevant results for a user query from knowledge bases.\n",
    "\n",
    "In this notebook, we will dive deep into building Q&A application. We will query the knowledge base to get the desired number of document chunks based on similarity search, integrate it with LlamaIndex retriever and use Anthropic Claude 3 Haiku model from Amazon Bedrock for answering questions.\n",
    "\n",
    "Following is the Architecture Diagram of the orchestration done by LlamaIndex by leveraging Large Language Model and Knowledge Base from Amazon Bedrock\n",
    "\n",
    "<img src=\"./assets/images/retrieveAPI.png\" alt=\"Custom RAG Workflow\" style=\"margin:auto\">\n",
    "\n",
    "<h2>Prerequisites</h2>\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and ingested in vector database as shown on [0_how_to_create_index_and_ingest_documents_in_knowledge_base.ipynb](./0_how_to_create_index_and_ingest_documents_in_knowledge_base.ipynb). We will making use of the Knowledge Base ID that we stored in this notebook.\n",
    "\n",
    "In case you are wanting to create the Knowledge Base from Console then you can follow the [official documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html).\n",
    "\n",
    "<h3>Dataset</h3>\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the Knowledge Bases for Amazon Bedrock. You will need the `knowledge_base_id` to run this example. In your specific use case, you can sync different files for different domain topics and query this notebook in the same manner to evaluate model responses using the retrieve API from knowledge bases.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Note:</b> This notebook has been tested in <strong>Mumbai (ap-south-1)</strong> in <strong>Python 3.10.14</strong>\n",
    "\n",
    "</div>\n",
    "\n",
    "<h2>Setup</h2>\n",
    "\n",
    "To run this notebook you would need to install following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704bf34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3==1.34.162 botocore==1.34.162\n",
    "!pip install --upgrade llama-index==0.10.30 llama-index-retrievers-bedrock==0.1.1 llama-index-llms-bedrock==0.1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13369e",
   "metadata": {},
   "source": [
    "<strong>Restart the kernel with the updated packages that are installed through the dependencies above</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7984495",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>\n",
    "\n",
    "<b>Follow the steps below to initilize the required python modules</b>\n",
    "\n",
    "<ol>\n",
    "<li>Import necessary libraries and initialize bedrock client required by the Langchain module to communicate with Foundation Models (FM) or Large Language Models (LLM) available in Amazon Bedrock.</li>\n",
    "<li>Import and Initialize Knoweledge Base Retriver available in LlamaIndex to communicate with Knowledge Base from Amazon Bedrock</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae08094-e5c1-4eac-9d80-9fd2b42d1442",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note :</b> If the following cell execution gives you error then please manually restart the kernel, the error will go away.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63b63b-278e-4c65-b2ab-752ee24922e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "import json\n",
    "\n",
    "import llama_index\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.llms.bedrock.base import Bedrock\n",
    "from llama_index.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805af769",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name   # use can you the region of your choice.\n",
    "botocore_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={'max_attempts': 0}\n",
    ")\n",
    "# bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n",
    "\n",
    "llm = Bedrock(\n",
    "    region_name=region,\n",
    "    botocore_config= botocore_config,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\", # Model ID of the LLM of our choice from Amazon Bedrock\n",
    "    temperature=0, \n",
    "    max_tokens=3000\n",
    ")\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id, # we are using the id of the knowledge base that we created in earlier notebook\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": 3,\n",
    "            \"overrideSearchType\": \"HYBRID\",\n",
    "            # \"filter\": {\"equals\": {\"key\": \"tag\", \"value\": \"space\"}}, # Optional Field for for metadata filtering.\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccddbab",
   "metadata": {},
   "source": [
    "Above we initialized the following two objects from Langchain:\n",
    "<ol>\n",
    "<li><strong>ChatBedrock</strong> - This object will orchestrates the communication with  the LLM from Amazon Bedrock. It will take care of structuring the prompt/messages, model arguments, etc for us whenever it invokes the LLM.</li>\n",
    "<li><strong>AmazonKnowledgeBasesRetriever</strong> - This objects will calls APIs of Knowledge Bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Usage</h3>\n",
    "\n",
    "Below is the method to directly fetch the relevant documents usign the `AmazonKnowledgeBasesRetriever` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87c9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"By what percentage did AWS revenue grow year-over-year in 2021?\"\n",
    "\n",
    "response = retriever.retrieve(query)\n",
    "\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cd10f",
   "metadata": {},
   "source": [
    "<h2>Code</h2>\n",
    "\n",
    "<h3>Using Knowledge Base within a LlamaIndex Based RAG architecture</h3>\n",
    "\n",
    "<h4>Prompt specific to the model to personalize responses</h4>\n",
    "\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the Retrieve API responses from above as a part of the {context} in the prompt for the model to refer to, along with the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c82a32-de40-4850-ba42-cb590a5c4365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"By what percentage did AWS revenue grow year-over-year in 2021?\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "qa_template = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, \n",
    "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"}\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\", llm=llm\n",
    ")\n",
    "\n",
    "response_synthesizer.update_prompts(\n",
    "    {\"text_qa_template\": qa_template}\n",
    ")\n",
    "retrieved_results = retriever.retrieve(query)\n",
    "\n",
    "response = response_synthesizer.synthesize(query, retrieved_results)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cd85a",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "We saw how easy it is to use Amazon Bedrock with LlamaIndex. Specifically we saw how LLM models form Amazon Bedrock and Knowledge base from Amazon Bedrock can be used by LlamaIndex to orchestrate the Q&A capability. \n",
    "\n",
    "<h2>Next Steps</h2>\n",
    "\n",
    "Cleaning up the resources that we created.\n",
    "\n",
    "\n",
    "<h2>Clean Up</h2>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In case you are done with your labs and the sample codes then remember to Clean Up the resources at the end of your session by following [3_clean_up.ipynb](./3_clean_up.ipynb)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e7ee7-7cfa-43fb-8e14-868bcb7427ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
