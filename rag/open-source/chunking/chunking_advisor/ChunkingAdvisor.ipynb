{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c9d089-3132-48cf-88f9-99f7b20227a7",
   "metadata": {},
   "source": [
    "## Important note :  Pre-requisite\n",
    "\n",
    "\n",
    "#### This Notebook requires an existing Knowldge base on bedrock .To create a knowldge base you can execute the code provided in : https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a64fff-2c1e-4f34-9c7f-0c8180bb039a",
   "metadata": {},
   "source": [
    "# Automated Document Processing Pipeline: Using Foundation Models for Smart Document Chunking and Knowledge Base Integration\n",
    "\n",
    "This notebook demonstrates an end-to-end automated solution for intelligent document processing using Foundation Models (FM). The pipeline performs three key functions:\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "The pipeline streamlines the entire process from document analysis to knowledge base population, making it efficient to prepare documents for advanced query and retrieval operations.\n",
    "\n",
    "##### 1.Document Structure Analysis: FM Automatically analyzes document structure and content to determine the optimal chunking strategy\n",
    "##### 2. Configuration Generation: Creates customized chunking parameters based on the analysis\n",
    "\n",
    "##### 3.Knowledge Base Integration: Processes and loads the chunked documents into a Bedrock Knowledge Base powered by OpenSearch Serverless\n",
    "\n",
    "![data_ingestion](./img/chunkingAdvs.jpg)\n",
    "\n",
    "#### Steps: \n",
    "\n",
    "1. Setup Access and Permissions:\n",
    "        Create Amazon Bedrock Knowledge Base execution role\n",
    "        Configure necessary IAM policies for:\n",
    "            S3 data access\n",
    "            OpenSearch Serverless writing permissions\n",
    "        Reference template available in \"0_create_ingest_documents_test_kb\" notebook\n",
    "\n",
    "2. Document Analysis Using Claude Sonnet:\n",
    "        Process files within target folder.For each document, Claude analyzes and recommends:\n",
    "            Optimal chunking strategy (FIXED_SIZE/NONE/HIERARCHICAL/SEMANTIC)\n",
    "            Specific configuration parameters\n",
    "            Custom processing requirements\n",
    "\n",
    "3. Data Preparation and Storage:\n",
    "        Upload analyzed files to designated S3 buckets\n",
    "        Configure buckets as data source for Bedrock KB DataStore\n",
    "\n",
    "4. Ingestion Process:\n",
    "        Initiate ingestion job via Knowledge Base APIs\n",
    "5. Validation:\n",
    "        Test ingestion completion\n",
    "        Verify data accessibility and accuracy\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "This notebook requires permissions to:\n",
    "\n",
    "1. Create and delete Amazon IAM roles\n",
    "2. Create, update and delete Amazon S3 buckets\n",
    "3. Access Amazon Bedrock\n",
    "4. Bedrock roles to access s3 buckets (3 bucket)\n",
    "5. Access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "\n",
    "- IAMFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "- AmazonS3FullAccess-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066b04e-bc6f-42e6-8836-817d2e0854b1",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f36a15a-6cc3-464a-a2f1-fcb2d45ea7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.1.1 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "aiobotocore 2.13.3 requires botocore<1.34.163,>=1.34.70, but you have botocore 1.35.90 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.1 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-core 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-features 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-multimodal 1.1.1 requires torch<2.4,>=2.2, but you have torch 2.4.1.post100 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scikit-learn<1.4.1,>=1.3.0, but you have scikit-learn 1.5.2 which is incompatible.\n",
      "autogluon-tabular 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires gluonts==0.15.1, but you have gluonts 0.14.3 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires scipy<1.13,>=1.5.4, but you have scipy 1.14.1 which is incompatible.\n",
      "autogluon-timeseries 1.1.1 requires torch<2.4,>=2.2, but you have torch 2.4.1.post100 which is incompatible.\n",
      "jupyter-scheduler 2.9.0 requires fsspec==2023.6.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "jupyter-scheduler 2.9.0 requires pytz==2023.3, but you have pytz 2024.2 which is incompatible.\n",
      "mlflow 2.17.0 requires pyarrow<18,>=4.0.0, but you have pyarrow 18.1.0 which is incompatible.\n",
      "sagemaker 2.227.0 requires attrs<24,>=23.1.0, but you have attrs 24.3.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --force-reinstall -q -r ./requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f694d1b-c537-4bf2-a5b1-d4f5c7025620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25239d0e-972d-4fff-b200-f20c39714a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'c17d25dc-bf21-4d92-a638-1bbfb146074f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 31 Dec 2024 07:51:04 GMT', 'content-type': 'application/json', 'content-length': '928', 'connection': 'keep-alive', 'x-amzn-requestid': 'c17d25dc-bf21-4d92-a638-1bbfb146074f', 'x-amz-apigw-id': 'DpZQWHw3PHcEaig=', 'x-amzn-trace-id': 'Root=1-6773a268-4cd5639654aa10be09575d02'}, 'RetryAttempts': 0}, 'knowledgeBase': {'createdAt': datetime.datetime(2024, 12, 21, 19, 45, 35, 398774, tzinfo=tzlocal()), 'description': 'Amazon shareholder letter knowledge base.', 'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:305810904819:knowledge-base/QTAWVUUY9G', 'knowledgeBaseConfiguration': {'type': 'VECTOR', 'vectorKnowledgeBaseConfiguration': {'embeddingModelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1'}}, 'knowledgeBaseId': 'QTAWVUUY9G', 'name': 'bedrock-sample-knowledge-base-410', 'roleArn': 'arn:aws:iam::305810904819:role/AmazonBedrockExecutionRoleForKnowledgeBase_504', 'status': 'ACTIVE', 'storageConfiguration': {'opensearchServerlessConfiguration': {'collectionArn': 'arn:aws:aoss:us-west-2:305810904819:collection/bzubj0cin1xvue3szt9c', 'fieldMapping': {'metadataField': 'text-metadata', 'textField': 'text', 'vectorField': 'vector'}, 'vectorIndexName': 'bedrock-sample-index-410'}, 'type': 'OPENSEARCH_SERVERLESS'}, 'updatedAt': datetime.datetime(2024, 12, 21, 19, 45, 35, 398774, tzinfo=tzlocal())}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "import json\n",
    "\n",
    "# create a boto3 session to dynamically get and set the region name\n",
    "session = boto3.Session() \n",
    "\n",
    "AWS_REGION = session.region_name\n",
    "bedrock = boto3.client('bedrock-runtime',region_name=AWS_REGION)\n",
    "bedrock_agent_client = session.client('bedrock-agent', region_name=AWS_REGION)\n",
    "\n",
    "MODEL_NAME = \"anthropic.claude-3-5-sonnet-20241022-v2:0\" \n",
    "datasources =[]\n",
    "#create a folder data if not yet done and \n",
    "path=\"data\"\n",
    "# To get knowledgeBaseId look int Amazon Bedrock > knowledgeBaseId > knowledgeBaseId\n",
    "# This ID should bave being created from first notebook\n",
    "# https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb\n",
    "\n",
    "kb_id= \"XXXXX\" # Retrieve KB First \n",
    "kb =  bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb_id)\n",
    "print(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e1241-fa8d-4dc2-a298-959a61ae2665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::305810904819:role/AmazonBedrockExecutionRoleForKnowledgeBase_504\n",
      "AmazonBedrockExecutionRoleForKnowledgeBase_504\n"
     ]
    }
   ],
   "source": [
    "#bedrock_kb_execution_role = kb['roleArn']\n",
    "bedrock_kb_execution_role_arn = kb ['knowledgeBase']['roleArn']\n",
    "bedrock_kb_execution_role=  bedrock_kb_execution_role_arn.split('/')[-1]\n",
    "print  (bedrock_kb_execution_role_arn)\n",
    "print  (bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9291ec4-e2dc-47c1-950b-9fa7e737bee3",
   "metadata": {},
   "source": [
    "### Supporting functions\n",
    "##### Createbucket: Checks if an S3 bucket exists and creates it if it doesn't. \n",
    "##### Upload_file: Upload_files to bucket: Upload a file to an S3 bucket\n",
    "##### Listfile: List all files in a specified directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7578f6f2-1cd2-4683-b150-1b6900ff77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "def createbucket(bucketname):\n",
    "    \"\"\"\n",
    "    Checks if an S3 bucket exists and creates it if it doesn't. \n",
    "    Args:\n",
    "        bucket_name (str): Name of the S3 bucket to create (must be globally unique)\n",
    "    Raises:\n",
    "        ClientError: If there's an error accessing or creating the bucket\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_client.head_bucket(Bucket=bucketname)\n",
    "        print(f'Bucket {bucketname} Exists')\n",
    "    except ClientError as e:\n",
    "        print(f'Creating bucket {bucketname}')\n",
    "        if AWS_REGION  == \"us-east-1\":\n",
    "            s3bucket = s3_client.create_bucket(\n",
    "                Bucket=bucketname)\n",
    "        else:\n",
    "            s3bucket = s3_client.create_bucket(\n",
    "            Bucket=bucketname,\n",
    "            CreateBucketConfiguration={ 'LocationConstraint': AWS_REGION }\n",
    "        )\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def listfile (folder):\n",
    "    \"\"\"\n",
    "    List all files in a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): Path to the directory to list files from\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of filenames in the specified directory\n",
    "        \"\"\"\n",
    "    dir_list = os.listdir(folder)\n",
    "    return dir_list\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "\n",
    "def delete_bucket_and_objects(bucket_name):\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    # Create an S3 resource\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    bucket.objects.all().delete()\n",
    "    # Delete the bucket itself\n",
    "    bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549427c-9d3d-485c-a542-93ef49b540fe",
   "metadata": {},
   "source": [
    "#### Download and prepare dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217702d0-42bb-4da0-b14b-37b4d0c0b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not yet created, create folder already\n",
    "#!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://www.apple.com/newsroom/pdfs/Q3FY18ConsolidatedFinancialStatements.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'Q3FY18ConsolidatedFinancialStatements.pdf'\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48551cb4-02a8-4426-88aa-089516a4c1d9",
   "metadata": {},
   "source": [
    "#### Create 3 S3 buckets for 3 data sources \n",
    "##### Check if bucket exists, and if not create S3 bucket by knowledge base data source,  each bucket will be used to load files with corrspending strategy :semantic, fixed_size , HIERARCHICAl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a84784-fbfe-4ba5-88a5-d2e2a382bfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket kb-dataset-bucket-semantic-422\n",
      "Creating bucket kb-dataset-bucket-fixed-422\n",
      "Creating bucket kb-dataset-bucket-hierarchical-422\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "suffix = random.randrange(200, 900)\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name_semantic = 'kb-dataset-bucket-semantic-' + str(suffix)                                                        #### Provide your bucket name which is already created\n",
    "bucket_name_fixed = 'kb-dataset-bucket-fixed-'+str(suffix)  \n",
    "bucket_name_hierachical='kb-dataset-bucket-hierarchical-'+str(suffix) \n",
    "s3_policy_name = 'AmazonBedrockS3PolicyForKnowledgeBase_'+ str(suffix)\n",
    "\n",
    "createbucket(bucket_name_semantic)\n",
    "createbucket(bucket_name_fixed)\n",
    "createbucket(bucket_name_hierachical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54bc11-8de7-4a67-b8ed-0ba2944fb9e4",
   "metadata": {},
   "source": [
    "##### Create read and list S3 policy on the 3 buckets and attach it to existing Bedrock role \"bedrock_kb_execution_role\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6230aaf0-2aa6-429e-9824-0ef2f9b12579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '72fa0faf-7f08-4488-a0dd-a51a6ec3a1b9',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 31 Dec 2024 07:51:23 GMT',\n",
       "   'x-amzn-requestid': '72fa0faf-7f08-4488-a0dd-a51a6ec3a1b9',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "account_number = boto3.client('sts').get_caller_identity().get('Account')\n",
    "iam_client = session.client('iam')\n",
    "s3_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket_name_semantic}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name_semantic}/*\", \n",
    "                    f\"arn:aws:s3:::{bucket_name_fixed}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name_fixed}/*\", \n",
    "                    f\"arn:aws:s3:::{bucket_name_hierachical}\",\n",
    "                    f\"arn:aws:s3:::{bucket_name_hierachical}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:ResourceAccount\": f\"{account_number}\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "s3_policy = iam_client.create_policy(\n",
    "            PolicyName=s3_policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy_document),\n",
    "            Description='Policy for reading documents from s3')\n",
    "\n",
    "        # fetch arn of this policy \n",
    "s3_policy_arn = s3_policy[\"Policy\"][\"Arn\"]\n",
    "iam_client = session.client('iam')\n",
    "fm_policy_arn = f\"arn:aws:iam::{account_number}:policy/{s3_policy_name}\"\n",
    "iam_client.attach_role_policy(\n",
    "        RoleName=bedrock_kb_execution_role,\n",
    "        PolicyArn=fm_policy_arn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55725547-428a-4568-8b4c-c7a8abab6e3d",
   "metadata": {},
   "source": [
    "##### Sends a prompt to Claude Sonnet  via Amazon Bedrock and returns the generated response.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee11cad-ea50-4e87-8bd2-b8f159bf4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Claude Sonnet via Amazon Bedrock and returns the generated response.\n",
    "    Args:\n",
    "        prompt (str): The input text prompt to send to the AI model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text response \"\"\"\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": '',\n",
    "            \"max_tokens\": 2000,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1,\n",
    "            \"system\": ''\n",
    "        }\n",
    "    )\n",
    "    response = bedrock.invoke_model(body=body, modelId=MODEL_NAME)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    return response_body.get('content')[0].get('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eb2ae-e825-435f-b77b-0524144f081c",
   "metadata": {},
   "source": [
    " #### Chunkingadvise function\n",
    " ##### Analyzes a PDF document and recommends optimal LLM chunking strategy with parameters. This function loads a PDF file, analyzes its content using LLM, and provides  recommendations for chunking strategy (FIXED_SIZE, NONE, HIERARCHICAL, or SEMANTIC) along with specific configuration parameters.\n",
    "   - Args:\n",
    "        - file (str): Name of the PDF file located in the 'data' directory\n",
    "        -  Returns:\n",
    "            - dict: JSON containing recommended chunking strategy and parameters:\n",
    "              For HIERARCHICAL:\n",
    "                - Recommend only one Strategy\n",
    "                - Maximum Parent chunk token size\n",
    "                - Maximum child chunk token size\n",
    "                - Overlap Tokens\n",
    "                - Rational\n",
    "            - For SEMANTIC:\n",
    "                - Recommend only one Strategy\n",
    "                - Maximum tokens\n",
    "                - Buffer size\n",
    "                - Breakpoint percentile threshold\n",
    "                - Rational:\n",
    "            - For FIXED-SIZE\n",
    "              -  overlapPercentage'\n",
    "              -  parsed_data['maxTokens'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99efd3-d16a-411a-9ad7-70522b9e1641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5801051-5411-4659-a303-c06aed74af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chunkingadvise (file):\n",
    "    \"\"\"\n",
    "    Analyzes a PDF document and recommends optimal LLM chunking strategy with parameters.\n",
    "\n",
    "    This function loads a PDF file, analyzes its content using LLM, and provides \n",
    "    recommendations for chunking strategy (FIXED_SIZE, NONE, HIERARCHICAL, or SEMANTIC)\n",
    "    along with specific configuration parameters.\n",
    "    Args:\n",
    "        file (str): Name of the PDF file located in the 'data' directory\n",
    "    Returns:\n",
    "        dict: JSON containing recommended chunking strategy and parameters:\n",
    "            For HIERARCHICAL:\n",
    "                - Recommend only one Strategy\n",
    "                - Maximum Parent chunk token size\n",
    "                - Maximum child chunk token size\n",
    "                - Overlap Tokens\n",
    "                - Rational\n",
    "            For SEMANTIC:\n",
    "                - Recommend only one Strategy\n",
    "                - Maximum tokens\n",
    "                - Buffer size\n",
    "                - Breakpoint percentile threshold\n",
    "                - Rational\n",
    "  \"\"\"\n",
    "    my_docs = []\n",
    "    my_strategies =[]\n",
    "    strategy=\"\"\n",
    "    strategytext=\"\"\n",
    "    path=\"data\"\n",
    "    strategylist =[]\n",
    "    metadata = [\n",
    "        dict(year=2023, source=file)]\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    file = path +\"/\"+ file\n",
    "    loader = PyPDFLoader(file)\n",
    "    document = loader.load()\n",
    "    loader = PyPDFLoader(file)\n",
    "   # print (\"path + file :: \", file)\n",
    "    document = loader.load()\n",
    "   # print (\"path + file :: \", document)\n",
    "   \n",
    "    prompt = f\"\"\"SYSTEM you are an advisor expert in LLM chunking strategies,\n",
    "    USER can you analyze the type,content, format, structure and size of {document}. \n",
    "    Can you advise on best LLM chunking Strategy based on this analysis. Recommend only one Strategy, however show recommended strategy prefernece ratio \n",
    "    Available strategies to recommend from are : FIXED_SIZE or NONE or HIERARCHICAL or SEMANTIC\n",
    "    Decide on recommendatin first and then , what is the recommendation?   \"\"\"\n",
    "    res = get_completion(prompt)\n",
    "    print(res)\n",
    "    prompt = f\"\"\" USER based on recommnedation provide in {res} \n",
    "    if you recommend HIERARCHICAL chunking then provide recommendation for: \n",
    "    Parent: Maximum parent chunk token size. \n",
    "    Child: Maximum child chunk token size and Overlap Tokens: Number of overlap tokens between each parent chunk and between each parent and its children.\n",
    "    If recommendation is HIERARCHICAL then provide response using JSON format\n",
    "    with the keys as \\\"Recommend only one Strategy\\\", \\\"Maximum Parent chunk token size\\\", \\\"Maximum child chunk token size\\\",\\\"Overlap Tokens\\\", \n",
    "    \\\"Rational:please explain rational for decision and explain why each other choice is not prefered, keep reational to 100 words maximum. \\\" . provide crisp and clear answer, \n",
    "    if you recommend  SEMANTIC then provide response using  JSON format with\n",
    "    the keys as \\\"Recommend only one Strategy\\\",\\\" Maximum tokens\\\", \\\"Buffer size\\\",\\\"Breakpoint percentile threshold\\\", \n",
    "    Buffer size should be less or equal than 1 , Breakpoint percentile threshold should >= 50\n",
    "    \\\"Rational:please explain rational for decision and explain why each other choice is not prefered, keep reational to 100 words maximum. \\\" . provide crisp and clear answer, \n",
    "    do not provide recommendation if not enough data inputs and say sorry I need more data,\n",
    "    if you recommend  FIXED_SIZE then provide response using  JSON format with\n",
    "    the keys as \\\"Recommend only one Strategy\\\",\\\" maxTokens\\\", \\\"overlapPercentage \\\",\n",
    "    \\\"Rational:please explain rational for decision and explain why each other choice is not prefered, keep reational to 100 words maximum. \\\" . provide crisp and clear answer, \n",
    "    do not provide recommendation if not enough data inputs and say sorry I need more data\"\"\"\n",
    "    res = get_completion(prompt)\n",
    "    print(res)\n",
    "    parsed_data = json.loads(res )\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579229ec-23b4-4e9f-99db-cb78c7453e4e",
   "metadata": {},
   "source": [
    "#### ingestbystrategy: function that configure chunking strategy parameters for Bedrock Knowledge Base ingestion based on recommended strategy.\n",
    "   - Args:\n",
    "       - parsed_data (dict): Dictionary containing chunking strategy recommendation and parameters\n",
    "    - Returns:\n",
    "        - tuple: Contains:\n",
    "            - chunking_strategy_config (dict): Configuration for the chosen chunking strategy\n",
    "            - bucket_name (str): S3 bucket name for storage\n",
    "            - name (str): Knowledge base name\n",
    "            - description (str): Knowledge base description \n",
    "            - s3_configuration (dict): S3 configuration with bucket ARN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a016a3c8-6759-40fe-a997-91357a3f48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingestbystrategy(parsed_data):\n",
    "    \"\"\"\n",
    "    Configures chunking strategy parameters for Bedrock Knowledge Base ingestion based on recommended strategy.\n",
    "\n",
    "    Args:\n",
    "        parsed_data (dict): Dictionary containing chunking strategy recommendation and parameters\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains:\n",
    "            - chunking_strategy_config (dict): Configuration for the chosen chunking strategy\n",
    "            - bucket_name (str): S3 bucket name for storage\n",
    "            - name (str): Knowledge base name\n",
    "            - description (str): Knowledge base description \n",
    "            - s3_configuration (dict): S3 configuration with bucket ARN\n",
    "\n",
    "    Example:\n",
    "        >>> strategy_config, bucket, kb_name, desc, s3_config = ingest_by_strategy(strategy_data)\n",
    "    \"\"\"\n",
    "    chunkingStrategyConfiguration ={}\n",
    "    # print(\"Strategy::\", parsed_data)\n",
    "    strategy= parsed_data['Recommend only one Strategy']\n",
    "\n",
    "    if strategy =='HIERARCHICAL':\n",
    "       p1 = parsed_data['Maximum Parent chunk token size']\n",
    "       p2= parsed_data['Maximum child chunk token size']\n",
    "       p3= parsed_data['Overlap Tokens'] \n",
    "       bucket_name=bucket_name_hierachical \n",
    "       name = f\"bedrock-sample-knowledge-base-HIERARCHICAL\"\n",
    "       description = \"Bedrock Knowledge Bases for and S3 HIERARCHICAL\"\n",
    "    # HIERARCHICAL Chunking\n",
    "       chunkingStrategyConfiguration = {\n",
    "                                            \"chunkingStrategy\": \"HIERARCHICAL\",      \n",
    "                                            \"hierarchicalChunkingConfiguration\": {  \n",
    "                                                                                    'levelConfigurations': [\n",
    "                                                                                        {\n",
    "                                                                                            'maxTokens': p1\n",
    "                                                                                        },\n",
    "                                                                                        {\n",
    "                                                                                            'maxTokens': p2\n",
    "                                                                                        }\n",
    "                                                                                    ],\n",
    "                                                                                    'overlapTokens': p3\n",
    "                                                                                }\n",
    "                                        }\n",
    "    \n",
    "    # # SEMANTIC Chunking \n",
    "    if strategy =='SEMANTIC':\n",
    "        p3 = parsed_data['Maximum tokens']\n",
    "        p2= int(parsed_data['Buffer size'])\n",
    "        p1= parsed_data['Breakpoint percentile threshold']\n",
    "        bucket_name= bucket_name_semantic\n",
    "        name = f\"bedrock-sample-knowledge-base-SEMANTIC\"\n",
    "        description = \"Bedrock Knowledge Bases for and S3 SEMANTIC\"\n",
    "        chunkingStrategyConfiguration = { \"chunkingStrategy\": \"SEMANTIC\",\n",
    "                                         \"semanticChunkingConfiguration\": {          \n",
    "                                                                              'breakpointPercentileThreshold': p1,\n",
    "                                                                              'bufferSize': p2,\n",
    "                                                                              'maxTokens': p3\n",
    "                                                                        }\n",
    "                                    }\n",
    "\n",
    "\n",
    "    if strategy =='FIXED_SIZE':\n",
    "        p2= int(parsed_data['overlapPercentage'])\n",
    "        p1= int (parsed_data['maxTokens'])\n",
    "        bucket_name=bucket_name_fixed\n",
    "        name = f\"bedrock-sample-knowledge-base-FIXED\"\n",
    "        description = \"Bedrock Knowledge Bases for and S3 FIXED\"\n",
    "      \n",
    "        chunkingStrategyConfiguration = { \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "                                         \"semanticChunkingConfiguration\": {          \n",
    "                                                                              \"maxTokens\": p1,\n",
    "                                                                              \"overlapPercentage\":p2\n",
    "                                                                     \n",
    "                                                                        }\n",
    "                                    }\n",
    "    \n",
    "    s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    } \n",
    "    return chunkingStrategyConfiguration ,bucket_name , name , description ,s3Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc202b3-12b9-45e7-a610-e76c28b142ad",
   "metadata": {},
   "source": [
    "#### Function to Creates or retrieves a data source in an Amazon Bedrock Knowledge Base\n",
    " \n",
    "#### First checks if a data source with the given name exists. If found, returns the existing  data source. Otherwise creates a new one with specified configurations.\n",
    "- Args:\n",
    "    - name (str): Name of the data source\n",
    "    - description (str): Description of the data source\n",
    "    - knowledge_base_id (str): ID of the knowledge base to create data source in\n",
    "    - s3_configuration (dict): S3 bucket configuration for the data source\n",
    "    - chunking_strategy_configuration (dict): Configuration for text chunking strategy\n",
    "- Returns:\n",
    "    - dict: Response containing the data source details from Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90628e2d-913d-4290-8d40-78dacd3d0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDS (name, description,knowledgeBaseId , s3Configuration , chunkingStrategyConfiguration ):\n",
    "    \"\"\"\n",
    "    Creates or retrieves a data source in an Amazon Bedrock Knowledge Base.\n",
    "    \n",
    "    First checks if a data source with the given name exists. If found, returns the existing \n",
    "    data source. Otherwise creates a new one with specified configurations.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the data source\n",
    "        description (str): Description of the data source\n",
    "        knowledge_base_id (str): ID of the knowledge base to create data source in\n",
    "        s3_configuration (dict): S3 bucket configuration for the data source\n",
    "        chunking_strategy_configuration (dict): Configuration for text chunking strategy\n",
    "\n",
    "    Returns:\n",
    "        dict: Response containing the data source details from Bedrock\n",
    "\n",
    "    Raises:\n",
    "        ClientError: If there's an error accessing or creating the data source\n",
    "    \"\"\"\n",
    "    response = bedrock_agent_client.list_data_sources(\n",
    "            knowledgeBaseId=kb_id,\n",
    "            maxResults=12\n",
    "        )\n",
    "    for  i in range (len(response[\"dataSourceSummaries\"])):\n",
    "            print (response[\"dataSourceSummaries\"][i] [\"name\"] ,\"::\", name)\n",
    "            print (response[\"dataSourceSummaries\"][i][\"dataSourceId\"])\n",
    "            if response[\"dataSourceSummaries\"][i] [\"name\"] == name:\n",
    "                ds =  bedrock_agent_client.get_data_source(knowledgeBaseId = knowledgeBaseId, dataSourceId = response[\"dataSourceSummaries\"][i-1][\"dataSourceId\"]  )\n",
    "                return ds\n",
    "           \n",
    "    ds  = bedrock_agent_client.create_data_source(\n",
    "                                                                        name = name,\n",
    "                                                                        description = description,\n",
    "                                                                        knowledgeBaseId = knowledgeBaseId,\n",
    "                                                                        dataDeletionPolicy = 'DELETE',\n",
    "                                                                        dataSourceConfiguration = {\n",
    "                                                                            # # For S3 \n",
    "                                                                            \"type\": \"S3\",\n",
    "                                                                            \"s3Configuration\" : s3Configuration\n",
    "                                                                            # # For Web URL \n",
    "                                                                            # \"type\": \"WEB\",\n",
    "                                                                            # \"webConfiguration\":webConfiguration                                                                    \n",
    "                                                                        },\n",
    "                                                                        vectorIngestionConfiguration = {\n",
    "                                                                            \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "                                                                        })\n",
    "        \n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e60ea6a-a1dc-47f8-9922-b7c70be750d1",
   "metadata": {},
   "source": [
    "### Process PDF files by analyzing content, creating data sources, and uploading to S3.\n",
    "\n",
    "#### Workflow:\n",
    "1. Lists all files in specified directory\n",
    "2. For each PDF:\n",
    "        - Analyzes for optimal chunking strategy\n",
    "        - Creates data source with recommended configuration\n",
    "        - Uploads file to appropriate S3 bucket \n",
    "<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cee5b445-2945-448d-8bb9-250d47f63672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'AMZN-2022-Shareholder-Letter.pdf', 'AMZN-2021-Shareholder-Letter.pdf', 'Q3FY18ConsolidatedFinancialStatements.pdf']\n",
      " print(f) .ipynb_checkpoints\n",
      " print(f) AMZN-2022-Shareholder-Letter.pdf\n",
      "Based on analyzing these documents, let me recommend a chunking strategy:\n",
      "\n",
      "RECOMMENDED STRATEGY: SEMANTIC\n",
      "\n",
      "Preference Ratio:\n",
      "- SEMANTIC: 70%\n",
      "- HIERARCHICAL: 20%\n",
      "- FIXED_SIZE: 8%\n",
      "- NONE: 2%\n",
      "\n",
      "Reasoning for SEMANTIC recommendation:\n",
      "\n",
      "1. Content Type Analysis:\n",
      "- The document is a shareholder letter with clear thematic sections\n",
      "- Contains distinct topic transitions and subject matter boundaries\n",
      "- Has natural semantic groupings around business units, strategies, and updates\n",
      "\n",
      "2. Structure Analysis:\n",
      "- Document has clear paragraph-level organization\n",
      "- Contains meaningful section breaks\n",
      "- Topics are self-contained but interrelated\n",
      "\n",
      "3. Format Analysis:\n",
      "- Consistent formatting with clear paragraph breaks\n",
      "- Mixed content including narrative text and bullet points\n",
      "- Natural semantic boundaries between different business discussions\n",
      "\n",
      "4. Size/Length Considerations:\n",
      "- Document sections vary in length but maintain topical coherence\n",
      "- Natural semantic breaks would create more meaningful chunks than arbitrary fixed sizes\n",
      "- Content flows in logical thematic units\n",
      "\n",
      "SEMANTIC chunking would be most effective here because it would:\n",
      "- Preserve the natural topic boundaries\n",
      "- Keep related content together\n",
      "- Maintain context within chunks\n",
      "- Allow for more meaningful QA responses\n",
      "- Better handle the varying lengths of different topics while keeping related information together\n",
      "\n",
      "This would result in more contextually relevant chunks that align with how the information is naturally organized in the shareholder letter.\n",
      "{\n",
      "    \"Recommend only one Strategy\": \"SEMANTIC\",\n",
      "    \"Maximum tokens\": 512,\n",
      "    \"Buffer size\": 0.8,\n",
      "    \"Breakpoint percentile threshold\": 75,\n",
      "    \"Rational\": \"SEMANTIC chunking is optimal for shareholder letters due to their natural thematic organization and distinct topic transitions. HIERARCHICAL would add unnecessary complexity since the content isn't deeply nested. FIXED_SIZE would break coherent topics arbitrarily, reducing context quality. SEMANTIC preserves natural topic boundaries, maintains contextual relationships, and ensures meaningful QA responses. The chosen parameters (512 tokens, 0.8 buffer, 75th percentile threshold) balance chunk size with topic coherence while allowing flexible boundary detection at natural break points.\"\n",
      "}\n",
      "name bedrock-sample-knowledge-base-SEMANTIC\n",
      "{'ResponseMetadata': {'RequestId': '724a1a3e-6028-4383-a6c7-707c987b69e9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 31 Dec 2024 08:04:03 GMT', 'content-type': 'application/json', 'content-length': '631', 'connection': 'keep-alive', 'x-amzn-requestid': '724a1a3e-6028-4383-a6c7-707c987b69e9', 'x-amz-apigw-id': 'DpbKHFK2vHcEFwQ=', 'x-amzn-trace-id': 'Root=1-6773a573-01d6eca3587ba86652915182'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 12, 31, 8, 4, 3, 568522, tzinfo=tzlocal()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::kb-dataset-bucket-semantic-422'}, 'type': 'S3'}, 'dataSourceId': 'WXBJWYU11D', 'description': 'Bedrock Knowledge Bases for and S3 SEMANTIC', 'knowledgeBaseId': 'QTAWVUUY9G', 'name': 'bedrock-sample-knowledge-base-SEMANTIC', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 12, 31, 8, 4, 3, 568522, tzinfo=tzlocal()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'SEMANTIC', 'semanticChunkingConfiguration': {'breakpointPercentileThreshold': 75, 'bufferSize': 0, 'maxTokens': 512}}}}}\n",
      "kb-dataset-bucket-semantic-422\n",
      "<_io.BufferedReader name='data/AMZN-2022-Shareholder-Letter.pdf'>\n",
      " print(f) AMZN-2021-Shareholder-Letter.pdf\n",
      "Let me analyze the document characteristics:\n",
      "\n",
      "Type: Corporate shareholder letter\n",
      "Content: Long-form narrative text with:\n",
      "- Multiple distinct sections\n",
      "- Natural topic transitions\n",
      "- Hierarchical structure (main letter + 1997 reprint)\n",
      "- Clear semantic boundaries between topics\n",
      "Format: PDF with page breaks\n",
      "Structure: \n",
      "- Sequential but topically organized\n",
      "- Contains lists, examples, and detailed explanations\n",
      "Size: Multiple pages with varying section lengths\n",
      "\n",
      "Based on this analysis, I recommend:\n",
      "\n",
      "SEMANTIC chunking strategy\n",
      "\n",
      "Preference ratio:\n",
      "SEMANTIC: 70%\n",
      "HIERARCHICAL: 20%\n",
      "FIXED_SIZE: 8%\n",
      "NONE: 2%\n",
      "\n",
      "Recommendation rationale:\n",
      "1. The content has clear semantic boundaries between different topics (AWS, Prime Video, Fulfillment, etc.)\n",
      "2. Each topic section contains complete contextual information that should be kept together\n",
      "3. The narrative flow within topics is important for maintaining context\n",
      "4. The document has natural semantic transitions that would be better preserved than with fixed-size chunks\n",
      "5. The content is explanatory in nature where context and complete thoughts are crucial\n",
      "6. Topics vary in length but have logical boundaries\n",
      "7. The semantic relationships between ideas are more important than strict hierarchical structure or fixed sizes\n",
      "\n",
      "This strategy would help maintain the contextual integrity of each topic while allowing for more meaningful and coherent chunks that preserve the document's natural semantic structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Recommend only one Strategy\": \"SEMANTIC\",\n",
      "    \"Maximum tokens\": 1000,\n",
      "    \"Buffer size\": 0.8,\n",
      "    \"Breakpoint percentile threshold\": 75,\n",
      "    \"Rational\": \"SEMANTIC chunking is optimal for this corporate letter due to its natural topic transitions and clear semantic boundaries between sections. The content's narrative flow and contextual relationships need to be preserved. HIERARCHICAL would be suboptimal as the document's structure isn't deeply nested. FIXED_SIZE would arbitrarily break coherent topics. NONE would make the content too large to process effectively. The chosen parameters allow for flexible chunk sizes while maintaining topic integrity and context.\"\n",
      "}\n",
      "name bedrock-sample-knowledge-base-SEMANTIC\n",
      "bedrock-sample-knowledge-base-SEMANTIC :: bedrock-sample-knowledge-base-SEMANTIC\n",
      "WXBJWYU11D\n",
      "{'ResponseMetadata': {'RequestId': '27b11ba7-4ee5-40fe-b328-8fcc11d0b9e5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 31 Dec 2024 08:04:22 GMT', 'content-type': 'application/json', 'content-length': '631', 'connection': 'keep-alive', 'x-amzn-requestid': '27b11ba7-4ee5-40fe-b328-8fcc11d0b9e5', 'x-amz-apigw-id': 'DpbNCEqyPHcEJzg=', 'x-amzn-trace-id': 'Root=1-6773a586-582e50fc364a483711ce4f00'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 12, 31, 8, 4, 3, 568522, tzinfo=tzlocal()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::kb-dataset-bucket-semantic-422'}, 'type': 'S3'}, 'dataSourceId': 'WXBJWYU11D', 'description': 'Bedrock Knowledge Bases for and S3 SEMANTIC', 'knowledgeBaseId': 'QTAWVUUY9G', 'name': 'bedrock-sample-knowledge-base-SEMANTIC', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 12, 31, 8, 4, 3, 568522, tzinfo=tzlocal()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'SEMANTIC', 'semanticChunkingConfiguration': {'breakpointPercentileThreshold': 75, 'bufferSize': 0, 'maxTokens': 512}}}}}\n",
      "kb-dataset-bucket-semantic-422\n",
      "<_io.BufferedReader name='data/AMZN-2021-Shareholder-Letter.pdf'>\n",
      " print(f) Q3FY18ConsolidatedFinancialStatements.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze the documents and recommend a chunking strategy:\n",
      "\n",
      "Analysis:\n",
      "1. Type: Financial statements (structured tabular data)\n",
      "2. Content: Numerical data, financial metrics, organized in standard accounting format\n",
      "3. Format: Consistent tabular layout with clear sections\n",
      "4. Structure: \n",
      "   - Hierarchical organization (main statements → sections → line items)\n",
      "   - Clear semantic relationships between numbers\n",
      "   - Natural groupings (assets/liabilities, income/expenses, etc.)\n",
      "5. Size: Moderate (~3 pages of dense financial data)\n",
      "\n",
      "Strategy Preference Ratio:\n",
      "HIERARCHICAL: 70%\n",
      "SEMANTIC: 20%\n",
      "FIXED_SIZE: 8%\n",
      "NONE: 2%\n",
      "\n",
      "Recommendation: HIERARCHICAL\n",
      "\n",
      "Reasoning:\n",
      "1. Financial statements have a natural hierarchical structure (statements → sections → subsections → line items)\n",
      "2. Each statement (Income, Balance Sheet, Cash Flow) forms a logical hierarchy\n",
      "3. Preserves the relationship between parent totals and child components\n",
      "4. Maintains the context of financial metrics within their respective statements\n",
      "5. Allows for more intelligent querying of financial relationships\n",
      "6. Better handles the nested nature of financial data compared to other chunking methods\n",
      "\n",
      "The hierarchical chunking strategy would allow the LLM to:\n",
      "- Maintain the structural integrity of financial statements\n",
      "- Preserve mathematical relationships\n",
      "- Keep related financial metrics together\n",
      "- Enable more accurate financial analysis and querying\n",
      "{\n",
      "    \"Recommend only one Strategy\": \"HIERARCHICAL\",\n",
      "    \"Maximum Parent chunk token size\": 1500,\n",
      "    \"Maximum child chunk token size\": 500,\n",
      "    \"Overlap Tokens\": 100,\n",
      "    \"Rational\": \"Financial statements have inherent hierarchical structure where parent-child relationships are crucial (e.g., total assets and its components). Parent size of 1500 tokens allows complete statement overview, while child size of 500 tokens accommodates detailed line items. 100 token overlap maintains continuity and context. Semantic chunking would break financial relationships, while fixed-size would arbitrarily split related items. No chunking would make the data too unwieldy for effective processing. Hierarchical preserves both structure and mathematical relationships essential for financial analysis.\"\n",
      "}\n",
      "name bedrock-sample-knowledge-base-HIERARCHICAL\n",
      "bedrock-sample-knowledge-base-SEMANTIC :: bedrock-sample-knowledge-base-HIERARCHICAL\n",
      "WXBJWYU11D\n",
      "{'ResponseMetadata': {'RequestId': '7f6c179d-6c7b-4d78-9c03-f8c1232d7d95', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 31 Dec 2024 08:04:40 GMT', 'content-type': 'application/json', 'content-length': '666', 'connection': 'keep-alive', 'x-amzn-requestid': '7f6c179d-6c7b-4d78-9c03-f8c1232d7d95', 'x-amz-apigw-id': 'DpbPzH37vHcEQFw=', 'x-amzn-trace-id': 'Root=1-6773a597-4f32d98147df38230eb44f3b'}, 'RetryAttempts': 0}, 'dataSource': {'createdAt': datetime.datetime(2024, 12, 31, 8, 4, 40, 8763, tzinfo=tzlocal()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::kb-dataset-bucket-hierarchical-422'}, 'type': 'S3'}, 'dataSourceId': 'IVAXJWS33Q', 'description': 'Bedrock Knowledge Bases for and S3 HIERARCHICAL', 'knowledgeBaseId': 'QTAWVUUY9G', 'name': 'bedrock-sample-knowledge-base-HIERARCHICAL', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 12, 31, 8, 4, 40, 8763, tzinfo=tzlocal()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'HIERARCHICAL', 'hierarchicalChunkingConfiguration': {'levelConfigurations': [{'maxTokens': 1500}, {'maxTokens': 500}], 'overlapTokens': 100}}}}}\n",
      "kb-dataset-bucket-hierarchical-422\n",
      "<_io.BufferedReader name='data/Q3FY18ConsolidatedFinancialStatements.pdf'>\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "dir_list1= listfile (\"data\")\n",
    "print(dir_list1)\n",
    "strategylist= []\n",
    "for file in dir_list1:\n",
    "    print (\" print(f)\" , file)\n",
    "    if \".pdf\" in file:\n",
    "        chunkingStrategyConfiguration=[]\n",
    "        strategy = Chunkingadvise (file)\n",
    "        chunkingStrategyConfiguration ,bucket_name , name , description ,s3Configuration  = ingestbystrategy(strategy)\n",
    "        print (\"name\", name)\n",
    "        datasources = createDS (name, description,kb_id, s3Configuration , chunkingStrategyConfiguration )\n",
    "        print (datasources)\n",
    "        #ds_id = datasources[0][\"dataSource\"][\"dataSourceId\"]\n",
    "        with open( path +\"/\"+ file, \"rb\") as f:\n",
    "            print(bucket_name)\n",
    "            print(f)\n",
    "            s3_client.upload_fileobj(f, bucket_name, file)\n",
    "       #print (strategylist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa41da-ecab-4592-8d78-59815e0dfb62",
   "metadata": {},
   "source": [
    "#### Starts Ingestin  and monitors ingestion jobs for all data sources in a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d0d10e-40e3-4769-a5e1-d115fce38041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds [dataSourceId] WXBJWYU11D\n",
      "{'dataSourceId': 'WXBJWYU11D', 'ingestionJobId': 'XFZYESD1KE', 'knowledgeBaseId': 'QTAWVUUY9G', 'startedAt': datetime.datetime(2024, 12, 31, 8, 5, 39, 476885, tzinfo=tzlocal()), 'statistics': {'numberOfDocumentsDeleted': 0, 'numberOfDocumentsFailed': 0, 'numberOfDocumentsScanned': 0, 'numberOfMetadataDocumentsModified': 0, 'numberOfMetadataDocumentsScanned': 0, 'numberOfModifiedDocumentsIndexed': 0, 'numberOfNewDocumentsIndexed': 0}, 'status': 'STARTING', 'updatedAt': datetime.datetime(2024, 12, 31, 8, 5, 39, 476885, tzinfo=tzlocal())}\n",
      "ds [dataSourceId] IVAXJWS33Q\n",
      "{'dataSourceId': 'IVAXJWS33Q', 'ingestionJobId': 'CGMPTICGX6', 'knowledgeBaseId': 'QTAWVUUY9G', 'startedAt': datetime.datetime(2024, 12, 31, 8, 6, 7, 38280, tzinfo=tzlocal()), 'statistics': {'numberOfDocumentsDeleted': 0, 'numberOfDocumentsFailed': 0, 'numberOfDocumentsScanned': 0, 'numberOfMetadataDocumentsModified': 0, 'numberOfMetadataDocumentsScanned': 0, 'numberOfModifiedDocumentsIndexed': 0, 'numberOfNewDocumentsIndexed': 0}, 'status': 'STARTING', 'updatedAt': datetime.datetime(2024, 12, 31, 8, 6, 7, 38280, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime    \n",
    "import time\n",
    "\"\"\"\n",
    "    Starts and monitors ingestion jobs for all data sources in a knowledge base.\n",
    "\"\"\"\n",
    "sources  = bedrock_agent_client.list_data_sources(knowledgeBaseId = kb_id  )\n",
    "for i in  range(len(sources[\"dataSourceSummaries\"])):\n",
    "    print (\"ds [dataSourceId]\", sources[\"dataSourceSummaries\"] [i-1] [\"dataSourceId\"])\n",
    "    start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb_id, dataSourceId = sources[\"dataSourceSummaries\"] [i-1] [\"dataSourceId\"])\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    print (job)\n",
    "       # Get job \n",
    "    while(job['status']!='COMPLETE' ):\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId = kb_id, dataSourceId = sources[\"dataSourceSummaries\"][i-1] [\"dataSourceId\"], ingestionJobId = job[\"ingestionJobId\"]\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5e219-97bd-4219-8f97-cd7be339cc5e",
   "metadata": {},
   "source": [
    "#### Try out KB using RetrieveAndGenerate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba77eec6-d83e-4263-ab09-1eefaefa8b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot determine the specific iPhone sales numbers for 2018 from the provided search results. While there is financial data from Apple Inc. for 2018 showing overall net sales of $53,265 million for the three months ended June 30, 2018, and $202,695 million for the nine months ended June 30, 2018, the results do not break down sales specifically for iPhones.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"anthropic.claude-3-5-sonnet-20241022-v2:0\"                                            # <Change it to any model of your choice which is supported by KB>\n",
    "model_arn = f'arn:aws:bedrock:us-west-2::foundation-model/{model_id}'\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=AWS_REGION)\n",
    "# ucomment to test \n",
    "#query = \"what is AWS annualized revenue run rate\"\n",
    "query = \"what is iphone sales in 2018\"\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "generated_text = response['output']['text']\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec94bfe-c99e-4e1c-9e97-bad5b3d0c09e",
   "metadata": {},
   "source": [
    "##### Clean buckets \n",
    "#####  NOTE : please delete also Bedrock  KB if not required by other works and data sources \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e67d50d0-8963-40d6-90be-be4c654c015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delete_bucket_and_objects(bucket_name_semantic)\n",
    "delete_bucket_and_objects(bucket_name_fixed)\n",
    "delete_bucket_and_objects(bucket_name_hierachical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157092f-7534-41f5-b664-e9f1ca67d6bc",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "\n",
    "This notebook demonstrates an experimental approach using Foundation Models to determine optimal chunking strategies for different document types. While showing promising initial results, the current methodology is exploratory and requires further refinement.\n",
    "\n",
    "#### Disclaimer:\n",
    "\n",
    "Recommendations are based on base Foundation Models without fine-tuning\n",
    "Results, validation against ground truth data would be required to validate results accurracy \n",
    "\n",
    "#### Proposed Next Steps:\n",
    "\n",
    "1.Model Enhancement\n",
    "        Implement fine-tuning on domain-specific data\n",
    "        Experiment with different prompt engineering mechanisms\n",
    "\n",
    "2.Validation Framework\n",
    "        Establish ground truth dataset for testing\n",
    "        Develop evaluation metrics for chunking quality\n",
    "        Create a systematic testing methodology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443821d-ad4c-4361-883f-002682160108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
