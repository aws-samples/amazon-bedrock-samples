{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Prompt Decomposition Example\n",
    "This notebook contains an example of prompt decomposition, or taking one long prompt and breaking it down into smaller parts.  These smaller parts can then be run independantly, often in parallel, and often on smaller models.  This can lead to significant performance enhancements and cost savings, may increase quality, and will make your prompts much easier to maintain as the workload grows.  This is because each step can be maintained and tested independantly, without any tweaks on one step impacting all the others (which may occur if they're all one huge prompt).\n",
    "\n",
    "Here are three common times when Prompt Decomposition can be helpful:\n",
    "  1) Preprocessing of RAG or context data.  For example, if context data is large, such a a long support document, consider a prompt that summarizes that content once, then future queries retrieve the summary rather than the long document. \n",
    "  2)  Breaking multi-step prompts into a maintainable DAG.  This can be helpful when a prompt reads like code, with a large number of steps or if/then instructions.  Instead, consider breaking these out and generating a flow diagram, resulting in maintainable individual pieces.\n",
    "  3)  Streamlining long linear prompts.  Even where prompts have only a single logical flow, if they are long, it can help to break the flow into small, sequential steps.  These steps combined may execute faster than the long original prompt, and they can also be maintained and tested independently. \n",
    "\n",
    "The notebook follows this structure:\n",
    "  1) Set up the envionment\n",
    "  2) Examples of decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e1be-d464-4f6c-8019-31e15a5450a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Set up the envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de410f15-afe9-4600-845d-343fbf68b7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "735e1180-1b93-49e2-b9c5-445938069561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use Anthropics library only to count tokens locally\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "def count_tokens(text):\n",
    "    return client.count_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v3 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 1 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(messages,system=\"\", DEBUG=False, model=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    #print (\"Calling %s on Bedrock.  Prompt length (tokens):%s\"%(model,count_tokens(raw_prompt_text)))\n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 10000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if DEBUG: print(\"sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",\"\\n\".join(messages))\n",
    "    \n",
    "    if model== \"opus\":\n",
    "        modelId = 'error'\n",
    "    elif model== \"sonnet\":\n",
    "        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    elif model== \"haiku\":\n",
    "        modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'error'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say the number four.\n",
      "Four.\n",
      "CPU times: user 5.24 ms, sys: 0 ns, total: 5.24 ms\n",
      "Wall time: 422 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check that it's working:\n",
    "session_cache = {} \n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    #query = [{\"role\": \"user\", \"content\": \"Please say the number two.\"},{\"role\": \"assistant\", \"content\": \"Two.\"},{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "    result = ask_claude(query)\n",
    "    print(query)\n",
    "    print(result[1])\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1],model=work[2])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,model=\"haiku\",DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i],model))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707/3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Please say the number one.', 'One.'], ['Please say the number two.', 'two'], ['Please say the number three.', 'three'], ['Please say the number four.', 'Four.'], ['Please say the number five.', 'five']]\n",
      "CPU times: user 29.9 ms, sys: 4.31 ms, total: 34.2 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "session_cache = {} \n",
    "#q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "#q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "#q3 = [{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "#print(ask_claude_threaded([q1,q2,q3]))\n",
    "print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\",\"Please say the number three.\",\"Please say the number four.\",\"Please say the number five.\"],model='sonnet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cb22-d6b7-44e9-b451-5dd11cea93fd",
   "metadata": {},
   "source": [
    "## 2) Examples of decomposition\n",
    "\n",
    "Here we'll consider an example use case of a user who would like to undersand how many unique characters are in a novel, and a bit about the three most common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f434a-070a-4f3c-8619-acbf4fc1e478",
   "metadata": {},
   "source": [
    "### Start by downloading the novel.  Here we use Frankenstein by Mary Shelley, as it is in public domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8ce4203-a672-43ec-9d45-d9f3c84c3548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3573c8e5-9d04-437e-a0b4-f5c530591913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#grab the text from the Gutenberg project, a collection of public domain works.\n",
    "#We use Beautiful Soup to parse the HTML of the webpage.\n",
    "url = \"https://www.gutenberg.org/files/84/84-h/84-h.htm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "raw_full_text_webpage = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a425cf36-5bc9-4715-9639-2cf0aafd32cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate word count: 76553\n",
      "Approximate page count: 153\n",
      "Approximate token count: 92970\n"
     ]
    }
   ],
   "source": [
    "#Cut the top and bottom of the webpage so that we only have the text of the book.\n",
    "raw_full_text = raw_full_text_webpage[raw_full_text_webpage.index(\"Letter 1\\n\\nTo Mrs. Saville, England.\"):raw_full_text_webpage.index(\"*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\")].replace(\"\\r\\n\",\" \").replace(\"\\n\", \" \")\n",
    "#encode some misc unicode charaters.\n",
    "full_text = raw_full_text.encode('raw_unicode_escape').decode()\n",
    "#show that we found the expected length\n",
    "words_count = len(full_text.split(\" \"))\n",
    "pages_count = int(words_count/500)#quick estimate, real page count is dependant on page and font size.\n",
    "token_count = count_tokens(full_text)\n",
    "print (\"Approximate word count:\",words_count)\n",
    "print (\"Approximate page count:\",pages_count)\n",
    "print (\"Approximate token count:\",token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982aa16-8f91-49f2-a5b4-14a8182d10c3",
   "metadata": {},
   "source": [
    "### Now that we have our novel, let's try to find all the unique characters with a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8518fc8-561b-4141-a990-528f18f34b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 93036\n"
     ]
    }
   ],
   "source": [
    "long_prompt_template = \"\"\"Consider the following novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "How many unique characters are there with at least one spoken line of dialog?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = long_prompt_template.replace(\"{{NOVEL}}\",full_text)\n",
    "print (\"Approximate prompt token count:\",count_tokens(long_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf7d679c-9a1a-46b9-b8eb-9fd7bf3a3d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text, there are 10 unique characters that have at least one spoken line of dialogue:\n",
      "\n",
      "1. Victor Frankenstein\n",
      "Victor Frankenstein is the protagonist and narrator for most of the novel. He is a scientist who creates a hideous but sentient creature through an unorthodox scientific experiment. His obsession with his work and the consequences of his creation drive the plot.\n",
      "\n",
      "2. The Creature/Monster\n",
      "The Creature, often referred to as the Monster, is Frankenstein's creation. He is intelligent and articulate, but his grotesque appearance causes him to be shunned by society, leading him to seek revenge against his creator.\n",
      "\n",
      "3. Robert Walton\n",
      "Robert Walton is the explorer who rescues Victor Frankenstein near the end of the novel. He serves as the narrator for the opening and closing sections of the book, providing a frame for Frankenstein's story.\n",
      "CPU times: user 11.7 ms, sys: 0 ns, total: 11.7 ms\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "long_responce = ask_claude(long_prompt, model=\"sonnet\")[1]\n",
    "print(long_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe75e7-432c-4764-bef0-46ed14dd979c",
   "metadata": {},
   "source": [
    "## Not bad!  93K tokens processed in about 40 seconds.  Let's see if we can make that faster and cheaper using prompt decomposition.\n",
    "### We'll divide the novel into thirds, run each third in parallel, then write a fourth prompt to combine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f29a01a5-0f19-4fbe-a5db-a86607d3e329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 30740\n"
     ]
    }
   ],
   "source": [
    "short_prompt_template = \"\"\"Consider the following portion of a novel:\n",
    "<novel>\n",
    "{{NOVEL}}\n",
    "</novel>\n",
    "\n",
    "Please provide a list of unique characters, each in a character tag.  Inside the character tag should be a name tag with their name,\n",
    "a count tag with an exact count of times they appear, and a description tag with a brief description of that character.\n",
    "Only count charaters that have at least one spoken line of dialog.\n",
    "\"\"\"\n",
    "\n",
    "#let's cut the novel into thirds.\n",
    "third = int(len(full_text)/3)\n",
    "short_prompt_1 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[:third])\n",
    "short_prompt_2 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third:third+third])\n",
    "short_prompt_3 = short_prompt_template.replace(\"{{NOVEL}}\",full_text[third+third:])\n",
    "print (\"Approximate prompt token count:\",count_tokens(short_prompt_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8494205-5ae6-4fd6-9f87-345aa5d36b5b",
   "metadata": {},
   "source": [
    "### Now let's run these three prompts in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2ca0d24-c013-4af4-a647-7ac0e426886b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707/3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of unique characters with their names, counts, and descriptions, for characters that have at least one spoken line of dialog:\n",
      "\n",
      "<character>\n",
      "  <name>Robert Walton</name>\n",
      "  <count>3</count>\n",
      "  <description>The narrator who is leading an expedition to the North Pole and encounters Victor Frankenstein.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Victor Frankenstein</name>\n",
      "  <count>27</count>\n",
      "  <description>The protagonist who creates a monster and relates his tragic story to Robert Walton.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Elizabeth Lavenza</name>\n",
      "  <count>4</count>\n",
      "  <description>Victor Frankenstein's adopted sister and love interest, who defends Justine Moritz at her trial.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Alphonse Frankenstein</name>\n",
      "  <count>2</count>\n",
      "  <description>Victor Frankenstein's father, who writes him a letter about the murder of William.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Justine Moritz</name>\n",
      "  <count>2</count>\n",
      "  <description>A kind servant in the Frankenstein household who is falsely accused and tried for the murder of William.</description>\n",
      "</character>\n",
      "\n",
      "<character>\n",
      "  <name>Henry Clerval</name>\n",
      "  <count>4</count>\n",
      "  <description>Victor Frankenstein's close friend who tries to help him recover from his illness.</description>\n",
      "</character>\n",
      "CPU times: user 47.4 ms, sys: 0 ns, total: 47.4 ms\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "short_responces = ask_claude_threaded([short_prompt_1,short_prompt_2,short_prompt_3],model='sonnet')\n",
    "#show the reply from one of the three prompts\n",
    "print(short_responces[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a8345-7e65-42e4-a030-37e079c9869b",
   "metadata": {},
   "source": [
    "## So far it's looking good!  We've processed the whole novel in around 17 seconds, down from 42.  Let's make a final call to get a final result that matches our original long prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f49b632f-8909-4eeb-b247-9e2dcbb2669f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate prompt token count: 1306\n"
     ]
    }
   ],
   "source": [
    "final_prompt_template = \"\"\"Consider the following list of charaters from a novel.  Each entry contains the character's name,\n",
    "a count of the number of times they appeared, and a brief description of that charater:\n",
    "<characters>\n",
    "{{CHARACTERS}}\n",
    "</characters>\n",
    "Some charaters may be listed more than once.  Use the name and description to determine that two entries are the same, \n",
    "and if they are, sum their count to support your responce.\n",
    "\n",
    "How many unique characters are there?  Please also provide a brief description of the top three most common characters in separate paragraphs. \n",
    "\"\"\"\n",
    "\n",
    "characters = short_responces[0][1]+short_responces[1][1]+short_responces[2][1]\n",
    "\n",
    "final_prompt = final_prompt_template.replace(\"{{CHARACTERS}}\",characters)\n",
    "print (\"Approximate prompt token count:\",count_tokens(final_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6e003ef-8553-4c1e-b2f9-39e72a5a85bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided list of characters, there are 11 unique characters:\n",
      "\n",
      "1. Victor Frankenstein\n",
      "2. The Creature/Monster/Daemon\n",
      "3. Robert Walton\n",
      "4. Elizabeth Lavenza\n",
      "5. Henry Clerval\n",
      "6. Justine Moritz\n",
      "7. Alphonse Frankenstein\n",
      "8. De Lacey\n",
      "9. Agatha\n",
      "10. Felix\n",
      "11. Safie\n",
      "\n",
      "The top three most common characters are:\n",
      "\n",
      "Victor Frankenstein (count: 175) - The protagonist, a young scientist who creates a hideous sapient creature in an unorthodox scientific experiment. He is the central figure of the novel, and his story is narrated to Robert Walton.\n",
      "\n",
      "The Creature/Monster/Daemon (count: 68) - Frankenstein's creation, who is initially benevolent but becomes murderous after being rejected by his creator and society. The creature's quest for acceptance and companionship drives much of the novel's conflict.\n",
      "\n",
      "Robert Walton (count: 20) - The explorer who rescues Frankenstein and records his story. He serves as the frame narrator, introducing and concluding Frankenstein's narrative.\n",
      "CPU times: user 6.83 ms, sys: 0 ns, total: 6.83 ms\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "final_responce = ask_claude(final_prompt, model=\"sonnet\")[1]\n",
    "print(final_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824c0df-796e-4daa-8200-27ca955773c1",
   "metadata": {},
   "source": [
    "### This final prompt took about 5 seconds to run.  The original long prompt took 42 seconds to run, and our decomposed version took 17s + 5, or 22 seconds total.  Almost twice as fast to do the same amount work!\n",
    "Note that the decomposed version actually found 11 characters, not 10.  This is somewhat common, that the quality will slightly improve with smaller, more focused prompts, because the LLM can focus more when the prompt is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c32177-3b65-480d-8e45-a94e1982dff1",
   "metadata": {},
   "source": [
    "## For fun, let's repeate the exact same tests as above, but with the smaller, faster Haiku model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5ce8f17-ade2-489a-a9d5-2dc757c343e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 unique characters with at least one spoken line of dialog in the novel:\n",
      "\n",
      "1. Victor Frankenstein\n",
      "2. The Monster\n",
      "3. Walton (the narrator)\n",
      "4. Elizabeth Lavenza\n",
      "5. Alphonse Frankenstein (Victor's father)\n",
      "6. Ernest Frankenstein (Victor's brother)\n",
      "7. Justine Moritz\n",
      "8. Henry Clerval\n",
      "9. The magistrate\n",
      "10. The old woman (the nurse)\n",
      "\n",
      "The three most common characters are:\n",
      "\n",
      "Victor Frankenstein\n",
      "Victor Frankenstein is the central character of the novel. He is the creator of the monster and the one who narrates the majority of the story. Frankenstein is driven by his ambition and desire for knowledge, which leads him to create the monster. However, he is horrified by his creation and abandons it, setting off a chain of tragic events. Frankenstein is tormented by guilt and remorse over the destruction his creation has caused.\n",
      "\n",
      "The Monster\n",
      "The monster, also known as the creature, is the other central character. He is the being that Frankenstein creates and brings to life. The monster is initially kind and innocent, but he is shunned and rejected by society due to his hideous appearance. This rejection and isolation leads the monster to become vengeful and murderous, as he seeks to destroy Frankenstein and all that he loves.\n",
      "\n",
      "Walton\n",
      "Walton is the narrator of the frame story. He is the captain of a ship exploring the Arctic, and he encounters Frankenstein and the monster on his journey. Walton serves as a sympathetic listener to Frankenstein's tragic tale, and he is deeply affected by Frankenstein's suffering and the monster's anguish. Walton's own ambitions and desires are also explored through his interactions with Frankenstein.\n",
      "CPU times: user 25.1 ms, sys: 0 ns, total: 25.1 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "long_responce_haiku = ask_claude(long_prompt, model=\"haiku\")[1]\n",
    "print(long_responce_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecc5f6-7892-4634-87dc-06f633c2e6c9",
   "metadata": {},
   "source": [
    "### Now the full prompt takes only 11s, down from about 40 with the larger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c78ec4-a818-4a97-8e5c-8e861cb836b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_707/3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided list of characters, there are 11 unique characters.\n",
      "\n",
      "The top three most common characters are:\n",
      "\n",
      "1. Victor Frankenstein:\n",
      "Victor Frankenstein is the protagonist of the story, who creates a monster and is then tormented by his own creation. He is the central figure in the narrative, recounting his story to the ship captain, Walton. Frankenstein is driven by his ambition to create life, but the consequences of his actions haunt him throughout the novel.\n",
      "\n",
      "2. The Monster:\n",
      "The Monster, also known as the Creature, is the being that Frankenstein creates. Abandoned by his creator, the Monster becomes a tormented and vengeful figure, seeking to destroy Frankenstein and his loved ones. The Monster's story is a tragic one, as he longs for companionship and understanding but is rejected by society due to his hideous appearance.\n",
      "\n",
      "3. Walton:\n",
      "Walton is the captain of the ship who rescues Frankenstein and hears his story. He serves as a framing device for the narrative, as Frankenstein recounts his tale to Walton, who then records it in his letters to his sister. Walton's role is to provide an outside perspective on Frankenstein's story and to offer a potential alternative path for the protagonist.\n",
      "CPU times: user 60.3 ms, sys: 4.11 ms, total: 64.5 ms\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "short_responces_haiku = ask_claude_threaded([short_prompt_1,short_prompt_2,short_prompt_3],model='haiku')\n",
    "characters_haiku = short_responces_haiku[0][1]+short_responces_haiku[1][1]+short_responces_haiku[2][1]\n",
    "final_prompt_haiku = final_prompt_template.replace(\"{{CHARACTERS}}\",characters_haiku)\n",
    "final_responce_haiku = ask_claude(final_prompt_haiku, model=\"haiku\")[1]\n",
    "print(final_responce_haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2a9e2-614d-4020-8039-143b6da09426",
   "metadata": {},
   "source": [
    "### Here we see that with such a fast model and small prompt, we don't get a perfomance improvment from decomposition.  Part of this is that with the shorter times involved, system operations and data transfer takes a larger percent of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4a75b-5abd-4a47-9409-cfcd999fc3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
