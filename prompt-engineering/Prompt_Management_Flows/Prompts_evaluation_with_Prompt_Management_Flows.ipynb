{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts Evaluation with Prompt Management & Prompt Flows\n",
    "\n",
    "In this example, we'll explore how to evaluate prompts using a combination of Prompt Management and Prompt Flows in Amazon Bedrock.\n",
    "\n",
    "Evaluating prompts is an essential step in the prompt lifecycle. Using LLM-as-a-judge for validating your prompts according to your own criteria allows you to efficiently quantify the prompts quality for optimization at scale.\n",
    "\n",
    "\n",
    "<img src=\"./images/prompt_eval_diagram.png\" width=\"30%\">\n",
    "\n",
    "\n",
    "[Amazon Bedrock Prompt Management](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-management.html) streamlines the creation, evaluation, deployment, and sharing of prompts in the Amazon Bedrock console and via APIs in the SDK. This feature helps developers and business users obtain the best responses from foundation models for their specific use cases.\n",
    "\n",
    "[Amazon Bedrock Prompt Flows](https://docs.aws.amazon.com/bedrock/latest/userguide/flows.html) allows you to easily link multiple foundation models (FMs), prompts, and other AWS services, reducing development time and effort. It introduces a visual builder in the Amazon Bedrock console and a new set of APIs in the SDK, that simplifies the creation of complex generative AI workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making sure we have the lastest version of the Amazon Bedrock SDK, importing the libraries, and setting-up the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time...\n",
    "!pip3 install boto3 botocore matplotlib -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the Prompt Management and Flows features are part of the Bedrock Agent SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust with your preferred region accordingly:\n",
    "region = \"us-east-1\"\n",
    "\n",
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = region)\n",
    "\n",
    "### Adjust with your preferred model IDs for invocations and evaluation - Note some models are only available in certain regions:\n",
    "modelInvokeId = \"amazon.titan-text-premier-v1:0\"\n",
    "modelEvalId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Prompt\n",
    "\n",
    "Let's create our sample evaluation prompt by leveraging on Prompt Management for Amazon Bedrock. Here, you can adjust the sample prompt template and evaluation criteria provided according to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt(\n",
    "    name = f\"prompt-evaluator\",\n",
    "    description = \"Prompt template for evaluating prompt responses with LLM-as-a-judge\",\n",
    "    variants = [\n",
    "        {\n",
    "            \"inferenceConfiguration\": {\n",
    "            \"text\": {\n",
    "                \"maxTokens\": 2000,\n",
    "                \"temperature\": 0,\n",
    "            }\n",
    "            },\n",
    "            \"modelId\": modelEvalId,\n",
    "            \"name\": \"variantOne\",\n",
    "            \"templateConfiguration\": {\n",
    "                \"text\": {\n",
    "                    \"inputVariables\": [\n",
    "                        {\n",
    "                            \"name\": \"input\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"output\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"text\": \"\"\"\n",
    "You're an evaluator for the prompts and answers provided by a generative AI model. Consider the input prompt in \\\n",
    "the <input> tags, the output answer in the <output> tags, the prompt evaluation criteria in the <prompt_criteria> tags, \\\n",
    "and the answer evaluation criteria in the <answer_criteria> tags.\n",
    "\n",
    "<input>\n",
    "{{input}}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "{{output}}\n",
    "</output>\n",
    "\n",
    "<prompt_criteria>\n",
    "- The prompt should be clear, direct, and detailed.\n",
    "- The question, task, or goal should be well explained and be grammatically correct.\n",
    "- The prompt is better if containing examples.\n",
    "- The prompt is better if specifies a role or sets a context.\n",
    "- The prompt is better if provides details about the format and tone of the expected answer.\n",
    "</prompt_criteria>\n",
    "\n",
    "<answer_criteria>\n",
    "- The answers should be correct, well structured, and technically complete.\n",
    "- The answers should not have any hallucinations, made up content, or toxic content.\n",
    "- The answer should be grammatically correct.\n",
    "- The answer should be fully aligned with the question or instruction in the prompt.\n",
    "</answer_criteria>\n",
    "\n",
    "Evaluate the answer the generative AI model provided in the <output> with a score from 0 to 100 according to the <answer_criteria> provided; \\\n",
    "any hallucinations, even if small, should dramatically impact the evaluation score.\n",
    "Also evaluate the prompt passed to that generative AI model provided in the <input> with a score from 0 to 100 according to the <prompt_criteria> provided.\n",
    "Respond only with a JSON having:\n",
    "- An 'answer-score' key with the score number you evaluated the answer with.\n",
    "- A 'prompt-score' key with the score number you evaluated the prompt with.\n",
    "- A 'justification' key with a justification for the two evaluations you provided to the answer and the prompt; make sure to explicitely include any errors or hallucinations in this part.\n",
    "- An 'input' key with the content of the <input> tags.\n",
    "- An 'output' key with the content of the <output> tags.\n",
    "- A 'prompt-recommendations' key with recommendations for improving the prompt based on the evaluations performed.\n",
    "Skip any preamble or any other text apart from the JSON in your answer.\n",
    "                    \"\"\"\n",
    "                }\n",
    "            },\n",
    "            \"templateType\": \"TEXT\"\n",
    "        }\n",
    "    ],\n",
    "    defaultVariant = \"variantOne\"\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "promptEvalId = response[\"id\"]\n",
    "promptEvalArn = response[\"arn\"]\n",
    "promptEvalName = response[\"name\"]\n",
    "print(f\"Prompt ID: {promptEvalId}\\nPrompt ARN: {promptEvalArn}\\nPrompt Name: {promptEvalName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a draft prompt, we can create a version from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_prompt_version(\n",
    "    promptIdentifier = promptEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Flow\n",
    "\n",
    "Now that we have our evaluation prompt, we can work on a workflow for running the evaluations with it. For this we'll rely on Prompt Flows for Amazon Bedrock.\n",
    "\n",
    "Let's create a simple flow that will invoke a given model with our input prompts for obtaining the outputs or responses, and then load the evaluation prompt from our catalog for obtaining the evaluation score.\n",
    "\n",
    "<img src=\"./images/prompt_eval_flow.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need an AWS IAM role for creating the Prompt Flow in Amazon Bedrock. If you already have a role with your permissions you can directly replace the ```flowRole``` variable with your role's ARN.\n",
    "\n",
    "For simplicity in this example we'll create a new role and attach the ```AmazonBedrockFullAccess``` policy to it. In general, it's recommended that you further limit the policies with conditions.\n",
    "\n",
    "You can check further details in the [How Prompt Flows for Amazon Bedrock works](https://docs.aws.amazon.com/bedrock/latest/userguide/flows-how-it-works.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new AWS IAM role for this example (comment this section if using your own role)\n",
    "iam = boto3.client('iam')\n",
    "response = iam.create_role(\n",
    "    RoleName = 'MyBedrockFlowsRole',\n",
    "    AssumeRolePolicyDocument = json.dumps({\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    ")\n",
    "flowRole = response['Role']['Arn']\n",
    "response = iam.attach_role_policy(\n",
    "    RoleName = 'MyBedrockFlowsRole',\n",
    "    PolicyArn = 'arn:aws:iam::aws:policy/AmazonBedrockFullAccess'\n",
    ")\n",
    "\n",
    "### Or specify your own AWS IAM role (uncomment and replace the ARN if using your own role)\n",
    "#flowRole = 'YOUR-IAM-ROLE-ARN'\n",
    "\n",
    "print(f'Using flowRole: {flowRole}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow(\n",
    "    name = f\"prompt-eval-flow\",\n",
    "    description = \"Prompt Flow for evaluating prompts with LLM-as-a-judge.\",\n",
    "    executionRoleArn = flowRole,\n",
    "    definition = {\n",
    "        \"nodes\": [\n",
    "          {\n",
    "            \"name\": \"Start\",\n",
    "            \"type\": \"Input\",\n",
    "            \"configuration\": {\n",
    "              \"input\": {}\n",
    "            },\n",
    "            \"outputs\": [\n",
    "              {\n",
    "                \"name\": \"document\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"End\",\n",
    "            \"type\": \"Output\",\n",
    "            \"configuration\": {\n",
    "              \"output\": {}\n",
    "            },\n",
    "            \"inputs\": [\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"document\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"Invoke\",\n",
    "            \"type\": \"Prompt\",\n",
    "            \"configuration\": {\n",
    "              \"prompt\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                  \"inline\": {\n",
    "                    \"inferenceConfiguration\": {\n",
    "                      \"text\": {\n",
    "                        \"maxTokens\": 2000,\n",
    "                        \"temperature\": 0,\n",
    "                      }\n",
    "                    },\n",
    "                    \"modelId\": modelInvokeId,\n",
    "                    \"templateConfiguration\": {\n",
    "                      \"text\": {\n",
    "                        \"inputVariables\": [\n",
    "                          {\n",
    "                            \"name\": \"input\"\n",
    "                          }\n",
    "                        ],\n",
    "                        \"text\": \"{{input}}\"\n",
    "                      }\n",
    "                    },\n",
    "                    \"templateType\": \"TEXT\"\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"input\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "              {\n",
    "                \"name\": \"modelCompletion\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"Evaluate\",\n",
    "            \"type\": \"Prompt\",\n",
    "            \"configuration\": {\n",
    "              \"prompt\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                  \"resource\": {\n",
    "                    \"promptArn\": promptEvalArn\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"inputs\": [\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"input\",\n",
    "                \"type\": \"String\"\n",
    "              },\n",
    "              {\n",
    "                \"expression\": \"$.data\",\n",
    "                \"name\": \"output\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "            \"outputs\": [\n",
    "              {\n",
    "                \"name\": \"modelCompletion\",\n",
    "                \"type\": \"String\"\n",
    "              }\n",
    "            ],\n",
    "          }\n",
    "        ],\n",
    "        \"connections\": [\n",
    "          {\n",
    "            \"name\": \"StartToInvoke\",\n",
    "            \"source\": \"Start\",\n",
    "            \"target\": \"Invoke\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "              \"data\": {\n",
    "                \"sourceOutput\": \"document\",\n",
    "                \"targetInput\": \"input\"\n",
    "              }\n",
    "            },\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"InvokeToEvaluate\",\n",
    "            \"source\": \"Invoke\",\n",
    "            \"target\": \"Evaluate\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "              \"data\": {\n",
    "                \"sourceOutput\": \"modelCompletion\",\n",
    "                \"targetInput\": \"output\"\n",
    "              }\n",
    "            },\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"StartToEvaluate\",\n",
    "            \"source\": \"Start\",\n",
    "            \"target\": \"Evaluate\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "              \"data\": {\n",
    "                \"sourceOutput\": \"document\",\n",
    "                \"targetInput\": \"input\"\n",
    "              }\n",
    "            },\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"EvaluateToEnd\",\n",
    "            \"source\": \"Evaluate\",\n",
    "            \"target\": \"End\",\n",
    "            \"type\": \"Data\",\n",
    "            \"configuration\": {\n",
    "              \"data\": {\n",
    "                \"sourceOutput\": \"modelCompletion\",\n",
    "                \"targetInput\": \"document\"\n",
    "              }\n",
    "            },\n",
    "          }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "flowEvalId = response[\"id\"]\n",
    "flowEvalArn = response[\"arn\"]\n",
    "flowEvalName = response[\"name\"]\n",
    "print(f\"Flow ID: {flowEvalId}\\nFlow ARN: {flowEvalArn}\\nFlow Name: {flowEvalName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our first flow, we can prepare it. This basically builds and validates our flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.prepare_flow(\n",
    "    flowIdentifier = flowEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the flow to double-check the \"status\" is \"prepared\", otherwise go back to the previous steps for solving any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.get_flow(\n",
    "    flowIdentifier = flowEvalId\n",
    ")\n",
    "print(\"Status:\", response[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a version from our draft flow. Note flow versions are read-only, meaning these cannot be modified once created as they're intended for using in production. If you need to make changes to a flow you can update your draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_version(\n",
    "    flowIdentifier = flowEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create flow alises, so that we can point our application front-ends and any other integrations to these. This allows creating new versions without impacting our service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.create_flow_alias(\n",
    "    flowIdentifier = flowEvalId,\n",
    "    name = flowEvalName,\n",
    "    description = \"Alias for my prompt evaluation flow\",\n",
    "    routingConfiguration = [\n",
    "        {\n",
    "            \"flowVersion\": \"1\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "flowEvalAliasId = response['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Evaluation Flow\n",
    "\n",
    "Now that we have our prompt evaluation flow, we can test it with a few invocations. For this we'll rely on the Bedrock Agent Runtime SDK.\n",
    "\n",
    "You can invoke flows from any application front-end or your own systems as required. It effectively exposes all the logic of your flow through an Agent Endpoint API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_runtime = boto3.client(service_name = 'bedrock-agent-runtime', region_name = region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a handy function for running the evaluation against a given prompt..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePrompt(prompt):\n",
    "    response = bedrock_agent_runtime.invoke_flow(\n",
    "        flowIdentifier = flowEvalId,\n",
    "        flowAliasIdentifier = flowEvalAliasId,\n",
    "        inputs = [\n",
    "            { \n",
    "                \"content\": { \n",
    "                    \"document\": prompt\n",
    "                },\n",
    "                \"nodeName\": \"Start\",\n",
    "                \"nodeOutputName\": \"document\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    event_stream = response[\"responseStream\"]\n",
    "    for event in event_stream:\n",
    "        #print(json.dumps(event, indent=2, ensure_ascii=False))\n",
    "        if \"flowOutputEvent\" in event:\n",
    "            evalResponse = json.loads(event[\"flowOutputEvent\"][\"content\"][\"document\"])\n",
    "    if evalResponse:\n",
    "        evalResponse[\"modelInvoke\"] = modelInvokeId\n",
    "        evalResponse[\"modelEval\"] = modelEvalId\n",
    "        #print(json.dumps(evalResponse, indent=2, default=str))\n",
    "        return evalResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test with a sample prompt, and visualize the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatePrompt(\"What is cloud computing in a single paragraph?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "### Prompt Evaluation at Scale\n",
    "\n",
    "Now that we have a flow that is able to evaluate our prompt, we can programmatically extend this for running against a full dataset of prompts at scale. For this, you can either leverage on a dataset stored in an Amazon S3 bucket, or load a file locally like we do in this example notebook.\n",
    "\n",
    "In the same way, you can write the results in another file in Amazon S3, or visualize it locally like we do in this example notebook.\n",
    "\n",
    "For our example, let's load a sample dataset with 4 simple prompts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read prompts dataset file locally\n",
    "\n",
    "import json\n",
    "promptsDataset = []\n",
    "with open('prompts_dataset.jsonl') as f:\n",
    "    for line in f:\n",
    "        promptsDataset.append(json.loads(line))\n",
    "\n",
    "if promptsDataset:\n",
    "    for i, j in enumerate(promptsDataset):\n",
    "        print(f'{i+1}. {j[\"input\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate each prompt in our dataset with the function we created before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "if promptsDataset:\n",
    "    results = []\n",
    "    for i, j in enumerate(promptsDataset):\n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} - Evaluating prompt {i+1} of {len(promptsDataset)}...\")\n",
    "        try:\n",
    "            results.append(evaluatePrompt(j[\"input\"]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating prompt {i+1}: {e}\")\n",
    "            results.append({\"error\": str(e)})\n",
    "    print(\"All prompts evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the results of the evaluations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results:\n",
    "    print(json.dumps(i, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a graph with the scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the scores from the data\n",
    "scores = [result['prompt-score'] for result in results]\n",
    "\n",
    "# Create a list of labels for the x-axis\n",
    "labels = [f\"Prompt {i+1}\" for i in range(len(scores))]\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Create the bar plot\n",
    "ax.bar(labels, scores)\n",
    "\n",
    "# Set the title and axis labels\n",
    "ax.set_title(\"Evaluation Scores\", fontsize=12)\n",
    "ax.set_xlabel(\"Prompts\", fontsize=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=10)\n",
    "\n",
    "# Rotate the x-axis labels for better visibility\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(axis='y', linestyle='--')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "# Add a horizontal line at score 80\n",
    "ax.axhline(y=80, color='r', linestyle='--', label='Passing threshold')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the example above, we have iterated manually through our dataset using a Python loop writing to a list.\n",
    "\n",
    "Alternatively, you might want to orchestrate this iteration using your own CI/CD pipeline, or even relying on the ```Iterator```, ```Collector```, ```S3 Retrieval``` and ```S3 Storage``` nodes  in Prompt Flows for Amazon Bedrock as per the following sample flow.\n",
    "\n",
    "<img src=\"./images/prompt_eval_flow_scale.png\" width=\"80%\">\n",
    "\n",
    "These alternative methods are out of the scope of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Cleaning-up Resources (optional)\n",
    "\n",
    "Before leaving, here's how to delete the resources that we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_flow_alias(\n",
    "    flowIdentifier = flowEvalId,\n",
    "    aliasIdentifier = flowEvalAliasId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_flow_version(\n",
    "    flowIdentifier = flowEvalId,\n",
    "    flowVersion = '1'\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_flow(\n",
    "    flowIdentifier = flowEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_prompt(\n",
    "    promptIdentifier = promptEvalId\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = iam.detach_role_policy(\n",
    "    RoleName='MyBedrockFlowsRole',\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonBedrockFullAccess'\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))\n",
    "\n",
    "response = iam.delete_role(\n",
    "    RoleName = 'MyBedrockFlowsRole'\n",
    ")\n",
    "print(json.dumps(response, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
