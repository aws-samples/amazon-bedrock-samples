
## Build a contextual text and image search engine for product recommendations using Amazon Bedrock and Amazon OpenSearch Serverless 

[WIP - 02/07/2024]

This repository aims at building a Large Language Model (LLM) powered search engine prototype to retrieve and recommend products based on text or image queries. This is a step-by-step guide on how to create [Amazon Bedrock Titan](https://aws.amazon.com/bedrock/titan) models to encode images and text into embeddings, ingest embeddings into [Amazon OpenSearch Service Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) index, and query the index using OpenSearch Service [k-nearest neighbors (KNN) functionality](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html).


### <ins> Introduction </ins>

The rise of contextual and semantic search has made ecommerce and retail businesses search easier for its consumers. Search engines and recommendation systems powered by Generative AI can improve product search experience exponentially by understanding natural language queries and returning more accurate results. This enhances the overall user experience helping customers to find what exactly they are looking for.

### <ins> Solution overview </ins>

This solution includes the following components:
- [Amazon Titan Multimodal Embeddings model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html): This FM is used to generate embeddings of the products images used in this blog. Using Titan Multimodal Embeddings, you can generate embeddings for your content and store them in a vector database. When an end user submits any combination of text and image as a search query, the model generates embeddings for the search query and matches them to the stored embeddings to provide relevant search and recommendations results to end users. You can further customize the model to enhance its understanding of your unique content and provide more meaningful results using image-text pairs for fine-tuning. By default, the model generates vectors (embeddings) of 1024 dimensions, and is accessed via the Amazon Bedrock service. You can also generate smaller dimensions to optimize for speed and performance

 - [Amazon OpenSearch Service Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html): OpenSearch Service Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. We use OpenSearch Service Serverless as a vector database for storing embeddings generated by the Titan Multimodal Embeddings model. An index created in the OpenSearch Service Serverless collection serves as the vector store for our RAG solution.

 - [Amazon SageMaker Studio](https://aws.amazon.com/sagemaker/studio): SageMaker Studio is an integrated development environment (IDE) for machine learning (ML). ML practitioners can perform all ML development steps—from preparing their data to building, training, and deploying ML models

### <ins> Solution Design </ins>

The solution design consists of two parts – Data Indexing and Contextual Search. During Data Indexing, we process the product images to generate embeddings for these images and then populate the vector data store. These steps are completed prior to the user interaction steps.
In the Contextual Search phase, a search query (text or image) from the user is converted into embeddings and a similarity search is run on the vector database to find the similar product images based on similarity search. We then display the top similar results.

ARCHITECTURE: 
![alt text](images/contextual_search_arch.png)

Following are the solution workflow steps:
1.	Download the product description text & images from public S3 bucket
2.	Review and Prepare the dataset.
3.	Generate embeddings for the product images using Titan Multimodal Embeddings (amazon. titan-embed-image-v1) multi-modal. If you have huge number of images and descriptions, then you can optionally use Amazon Bedrock Batch API.
4.	Store embeddings into vector engine for Amazon OpenSearch Service Serverless as the search engine.
5.	Finally, fetch the user query in natural language, convert it into embedding using Titan Multimodal Embeddings (amazon. titan-embed-image-v1) multi-modal, and perform a k-NN search. To get the relevant search results.


### <ins> Dataset </ins>

The [Amazon Berkeley Objects Dataset](https://registry.opendata.aws/amazon-berkeley-objects/) is used in the implementation. The dataset is a collection of 147,702 product listings with multilingual metadata and 398,212 unique catalogue images. We will only make use of the item images and item names in US English. For demo purposes we are going to use ~1,600 products.

## Running Costs

This section outlines cost considerations for running this demo. Completing the POC will deploy a OpenSearch Cluster, a SageMaker Studio and will use Amazon Bedrock, which will cost approx. $X per hour. Noted: the price listed below is calculated using us-east-1 region. The cost varies from region to region. And the cost may change over time as well (the price here is recorded 2024-02-04). 

Further cost breakdowns are below.

- **Amazon Bedrock** – Please visit the [Amazon Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/) to learn more about the pricing options.
  - On-demand pricing for text embeddings, The Titan Multimodal Embeddings costs $0.0008 per 1,000 input tokens
  - On-demand pricing fo image embeddings, The Titan Multimodal Embeddings costs $0.00006 per 1,000 input tokens

- **OpenSearch Service** – Prices vary based on instance type usage and Storage cost. For more information, see [Amazon OpenSearch Service pricing](https://aws.amazon.com/opensearch-service/pricing/).  
  - The `t3.small.search` instance runs for approx 1 hour at \$0.036 per hour.

- **SageMaker** – Prices vary based on EC2 instance usage for the Studio Apps, Batch Transform jobs and Serverless Inference endpoints. For more information, see [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/).

  - The `ml.t3.medium` instance for *Studio Notebooks* runs for approx 1 hour at \$0.05 per hour.
  - The `ml.c5.xlarge` instance for *Batch Transform* runs for approx 6 minutes at \$0.204 per hour.

  
- **Amazon S3** – Low cost, prices will vary depending on the size of the models/artifacts stored. The first 50 TB each month will cost only $0.023 per GB stored. For more information, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/).
## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

