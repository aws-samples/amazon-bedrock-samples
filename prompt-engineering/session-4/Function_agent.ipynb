{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Retrieving Data Automatically from APIs\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Another important type of retrieval which customers can take advantage of with LLM is **structured data retrieval** from APIs. Structured data retrieval is extremely useful for augmenting LLM applications with up to date information which can be retrieved in a repeatable manner, but the outputs are always changing. An example of a question you might ask an LLM which uses this type of retrieval might be \"How long will it take for my Amazon.com order containing socks to arrive?\". In this notebook, we will show how to integrate an LLM with a backend API service which has the ability to answer a user's question through RAG.\n",
    "\n",
    "Specifically, we will be building a tool which is able to tell you the weather based on natural language. This is a fairly trivial example, but it does a good job of showing how multiple API tools can be used by an LLM to retrieve dynamic data to augment a prompt. Here is a visual of the architecture we will be building today.\n",
    "\n",
    "![api](./images/api.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cced6",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85063bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display, Pretty\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3e58",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the API Tools\n",
    "\n",
    "The first thing we need to do for our LLM is define the tools it has access to. In this case we will be defining local Python functions, but it important to not that these could be any type of application service. Examples of what these tools might be on AWS include...\n",
    "\n",
    "* An AWS Lambda function\n",
    "* An Amazon RDS database connection\n",
    "* An Amazon DynamnoDB table\n",
    "  \n",
    "More generic examples include...\n",
    "\n",
    "* REST APIs\n",
    "* Data warehouses, data lakes, and databases\n",
    "* Computation engines\n",
    "\n",
    "In this case, we define two tools which reach external APIs below with two python functions\n",
    "1. the ability to retrieve the latitude and longitude of a place given a natural language input\n",
    "2. the ability to retrieve the weather given an input latitude and longitude\n",
    "\n",
    "#### ^^^ The API for getting the latitude is an open source and so we might encounter a throtlling by then ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "def get_weather(latitude: str, longitude: str):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    return response.json()\n",
    "\n",
    "def get_lat_long(place: str):\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    \n",
    "    user_agent_list = [ \n",
    "\t    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36', \n",
    "\t    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36', \n",
    "\t    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15', \n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    headers = {'User-Agent': random.choice(user_agent_list)}\n",
    "\n",
    "    response = requests.get(url, params=params,headers=headers)\n",
    "    #print(f\"get_lat_long::{response}\")\n",
    "    response = response.json()\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def call_function(tool_name, parameters):\n",
    "    func = globals()[tool_name]\n",
    "    output = func(**parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2d1c0",
   "metadata": {},
   "source": [
    "We also define a function called `call_function` which is used to abstract the tool name. You can see an example of determining the weather in Las Vegas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92319d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 'Las Vegas'\n",
    "lat_long_response = call_function('get_lat_long', {'place' : place})\n",
    "weather_response = call_function('get_weather', lat_long_response)\n",
    "print(f'Weather in {place} is...')\n",
    "weather_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203223",
   "metadata": {},
   "source": [
    "As you might expect, we have to describe our tools to our LLM, so it knows how to use them. The strings below describe the python functions for lat/long and weather to Claude in an XML friendly format which we have seen previously in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb27ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_description = \"\"\"\\\n",
    "<tool_description>\n",
    "<tool_name>get_weather</tool_name>\n",
    "<parameters>\n",
    "<name>latitude</name>\n",
    "<name>longitude</name>\n",
    "</parameters>\n",
    "</tool_description>\n",
    "\"\"\"\n",
    "\n",
    "get_lat_long_description = \"\"\"\n",
    "<tool_description>\n",
    "<tool_name>get_lat_long</tool_name>\n",
    "<parameters>\n",
    "<name>place</name>  \n",
    "</parameters>\n",
    "</tool_description>\"\"\"\n",
    "\n",
    "list_of_tools_specs = [get_weather_description, get_lat_long_description]\n",
    "tools_string = ''.join(list_of_tools_specs)\n",
    "print(tools_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fac8ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Prompts to Orchestrate our LLM Using Tools\n",
    "\n",
    "Now that the tools are defined both programmatically and as a string, we can start orchestrating the flow which will answer user questions. The first step to this is creating a prompt which defines the rules of operation for Claude. In the prompt below, we provide explicit direction on how Claude should use tools to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "TOOL_TEMPLATE = \"\"\"\\\n",
    "Your job is to formulate a solution to a given <user-request> based on the instructions and tools below.\n",
    "\n",
    "Use these Instructions: \n",
    "1. In this environment you have access to a set of tools and functions you can use to answer the question.\n",
    "2. You can call the functions by using the <function_calls> format below.\n",
    "3. Only invoke one function at a time and wait for the results before invoking another function.\n",
    "4. The Results of the function will be in xml tag <function_results>. Never make these up. The values will be provided for you.\n",
    "5. Only use the information in the <function_results> to answer the question.\n",
    "6. Once you truly know the answer to the question, place the answer in <answer></answer> tags. Make sure to answer in a full sentence which is friendly.\n",
    "7. Never ask any questions\n",
    "\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Here are the tools available:\n",
    "<tools>\n",
    "{tools_string}\n",
    "</tools>\n",
    "\n",
    "<user-request>\n",
    "{user_input}\n",
    "</user-request>\n",
    "\n",
    "Human: What is the first step in order to solve this problem?\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "TOOL_PROMPT = PromptTemplate.from_template(TOOL_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdbf85",
   "metadata": {},
   "source": [
    "---\n",
    "## Executing the RAG Workflow\n",
    "\n",
    "Armed with our prompt and structured tools, we can now write an orchestration function which will iteratively step through the logical tasks to answer a user question. In the cell below we use the `invoke_model` function to generate a response with Claude and the `single_retriever_step` function to iteratively call tools when the LLM tells us we need to. The general flow works like this...\n",
    "\n",
    "1. The user enters an input to the application\n",
    "2. The user input is merged with the original prompt and sent to the LLM to determine the next step\n",
    "3. If the LLM knows the answer, it will answer and we are done. If not, go to next step 4.\n",
    "4. The LLM will determine which tool to use to answer the question.\n",
    "5. We will use the tool as directed by the LLM and retrieve the results.\n",
    "6. We provide the results back into the original prompt as more context.\n",
    "7. We ask the LLM the next step or if knows the answer.\n",
    "8. Return to step 3.\n",
    "\n",
    "If this is a bit confusing do not panic, we will walk through this flow in an example shortly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "\n",
    "def invoke_model(prompt):\n",
    "    client = boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),)\n",
    "    body = json.dumps({\"prompt\": prompt, \"max_tokens_to_sample\": 500, \"temperature\": 0,})\n",
    "    modelId = \"anthropic.claude-instant-v1\"\n",
    "    response = client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=\"application/json\", contentType=\"application/json\"\n",
    "    )\n",
    "    return json.loads(response.get(\"body\").read()).get(\"completion\")\n",
    "\n",
    "def single_retriever_step(prompt, output):\n",
    "\n",
    "    # first check if the model has answered the question\n",
    "    done = False\n",
    "    if '<answer>' in output:\n",
    "        answer = output.split('<answer>')[1]\n",
    "        answer = answer.split('</answer>')[0]\n",
    "        done = True\n",
    "        return done, answer\n",
    "    \n",
    "    # if the model has not answered the question, go execute a function\n",
    "    else:\n",
    "\n",
    "        # parse the output for any \n",
    "        function_xml = output.split('<function_calls>')[1]\n",
    "        function_xml = function_xml.split('</function_calls>')[0]\n",
    "        function_dict = xmltodict.parse(function_xml)\n",
    "        func_name = function_dict['invoke']['tool_name']\n",
    "        parameters = function_dict['invoke']['parameters']\n",
    "\n",
    "        # call the function which was parsed\n",
    "        func_response = call_function(func_name, parameters)\n",
    "\n",
    "        # create the next human input\n",
    "        func_response_str = '\\n\\nHuman: Here is the result from your function call\\n\\n'\n",
    "        func_response_str = func_response_str + f'<function_results>\\n{func_response}\\n</function_results>'\n",
    "        func_response_str = func_response_str + '\\n\\nIf you know the answer, say it. If not, what is the next step?\\n\\nAssistant:'\n",
    "\n",
    "        # augment the prompt\n",
    "        prompt = prompt + output + func_response_str\n",
    "    return done, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d569ad",
   "metadata": {},
   "source": [
    "Let's start our first example `What is the weather in Las Vegas?`. The code below asks the LLM what the first step is and you will notice that the LLM is able to ascertain it needs to use the `get_lat_long` tool first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Las Vegas?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f27a09",
   "metadata": {},
   "source": [
    "Great, Claude has figured out that we should first call the lat and long tool. The next step is then orchestrated just like the first. This time, Claude uses the lat/long from the first request to now ask for the weather of that specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f62b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb8dcd",
   "metadata": {},
   "source": [
    "Finally the LLM is able to answer the question based on the input function above. Very cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56361993",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = invoke_model(next_step).strip()\n",
    "done, next_step = single_retriever_step(next_step, output)\n",
    "if not done:\n",
    "    display(Pretty(f'{output}'))\n",
    "else:\n",
    "    display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d64a45a",
   "metadata": {},
   "source": [
    "Let's try another example to show how a different place (Singapore) can be used in this example. Notice how we set the for loop to 5 iterations even though the model only uses 3 of these. This iteration capping is common in agent workflows and should be tuned according to your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Singapore?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "for i in range(5):\n",
    "    output = invoke_model(next_step).strip()\n",
    "    done, next_step = single_retriever_step(next_step, output)\n",
    "    if not done:\n",
    "        display(Pretty(f'{output}'))\n",
    "    else:\n",
    "        display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532481a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'What is the weather in Chandigarh India?'\n",
    "next_step = TOOL_PROMPT.format(tools_string=tools_string, user_input=user_input)\n",
    "\n",
    "for i in range(5):\n",
    "    output = invoke_model(next_step).strip()\n",
    "    done, next_step = single_retriever_step(next_step, output)\n",
    "    if not done:\n",
    "        display(Pretty(f'{output}'))\n",
    "    else:\n",
    "        display(Pretty('Final answer from LLM:\\n'+f'{next_step}'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5d5e9",
   "metadata": {},
   "source": [
    "---\n",
    "## Leveraging Open Source Agents with Bedrock\n",
    "\n",
    "As with the previous notebook, there are many ways to orchestrate agent workflows with LLMs. One of the popular open source frameworks for this is [LangChain's agent module](https://python.langchain.com/docs/modules/agents/). LangChain provides a variety of agent types as well as the same [integration to Amazon Bedrock](https://python.langchain.com/docs/integrations/llms/bedrock) being available for these agent based workflows. Let's work on implementing the same workflow as above into the LangChain ecosystem.\n",
    "\n",
    "First, we can define our LLM with the `Bedrock` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "agent_llm = Bedrock(\n",
    "    client=boto3.client(service_name='bedrock-runtime', region_name=os.environ.get(\"AWS_REGION\"),),\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406909ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import ast\n",
    "\n",
    "@tool (\"get_lat_long\")\n",
    "def get_lat_long(place: str) -> dict:\n",
    "    \"\"\"Returns the latitude and longitude for a given place name as a dict object of python.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    user_agent_list = [ \n",
    "\t    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36', \n",
    "\t    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36', \n",
    "\t    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15', \n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'\n",
    "    ]\n",
    "    headers = {'User-Agent': random.choice(user_agent_list)}\n",
    "\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    response = requests.get(url, params=params, headers=headers).json()\n",
    "\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@tool (\"get_weather_dict\")\n",
    "def get_weather_dict(co_ord_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns weather data for a given a dictionary having latitude and longitude.\n",
    "    \"\"\"\n",
    "    if '{' in co_ord_str:\n",
    "        co_ord_str = ast.literal_eval(co_ord_str)\n",
    "        latitude = co_ord_str['latitude']\n",
    "        longitude = co_ord_str['longitude']\n",
    "    else:\n",
    "        latitude = float(co_ord_str.split(\",\")[0].strip())\n",
    "        longitude = float(co_ord_str.split(\",\")[1].strip())\n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e139d",
   "metadata": {},
   "source": [
    "As well as being able to define our own tools, LangChain has some pre-built tools which are general purpose. One well documented issue with LLMs is that they are not good at arithmetic only from natural language. We can use the `LLMMathChain` from LangChain to help solve this problem. In this chain, we are able to ask an LLM simple math questions and the question will be sent to a calculator in order to return a perfect mathematical result. An example is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10784e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "\n",
    "math_tool_template = \"\"\"\\\n",
    "Given a question from the Human with a math problem, provide only a single line mathematical expression that solves the problem in the following format.\n",
    "Never solve the expression only create a parsable expression.\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "\n",
    "Here is an example response with a single line mathematical expression for solving a math problem:\n",
    "```text\n",
    "37593**(1/5)\n",
    "```\n",
    "\n",
    "Human: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=agent_llm, verbose=True)\n",
    "llm_math_chain.llm_chain.prompt.template = math_tool_template\n",
    "result = llm_math_chain('What is nine times 465?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8379ea",
   "metadata": {},
   "source": [
    "Now that all the tools are defined, lets create a [ReACT agent](https://python.langchain.com/docs/modules/agents/agent_types/react) (Reason + Act) which will leverage the LLM to generate the next set of actions to solve a given question. This next cell creates the agent from a list of tools we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools_list = [get_lat_long, get_weather_dict]\n",
    "tools_list.append(\n",
    "    Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"Useful for when you need to answer questions about math.\",\n",
    "    )\n",
    ")\n",
    "react_agent = initialize_agent(\n",
    "    tools_list, \n",
    "    agent_llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True,\n",
    "    max_iteration=4,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82281d22",
   "metadata": {},
   "source": [
    "The ReAct framework also expects a specific format to be followed by our LLM. This custom prompt adds place holders for the 3 components which need to be followed.\n",
    "\n",
    "1. **Action** which is the tool to use\n",
    "2. **Action Input** is the parameters coming from the LLM which will go into the tool or function to be called\n",
    "3. **Observation** is the final result of the call from the tool\n",
    "\n",
    "These three components to ReAct are recursively executed until there is an answer to the question or we reach the limit of execution steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5755c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather_dict\", \"Calculator\"]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "react_agent.agent.llm_chain.prompt.template=prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466544d1",
   "metadata": {},
   "source": [
    "Now we can send in the first question just like the one we used in the DIY system. Notice how the logging allows you to see how the LLM is reasoning through the task within the ReAct framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = react_agent(\"Can you check the weather in Las Vegas for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4051a8",
   "metadata": {},
   "source": [
    "Another example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = react_agent(\"can you check the weather in Seattle WA for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34784a47",
   "metadata": {},
   "source": [
    "In this last example, we will give a different type of problem to the LLM where it will be able to take advantage of the math tool as well as the lat/long tool instead of only talking about weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = react_agent(\"what is the latitude of Washington DC multiplied by 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b217d",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that you have used a few different retrieval systems, lets move on to the next notebook where you can apply the skills you've learned so far!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
