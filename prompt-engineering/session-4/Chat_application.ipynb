{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "## Chat with LLMs Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users.\n",
    "\n",
    "The key technical detail which we need to include in our system to enable a chat feature is conversational memory. This way, customers can ask follow up questions and the LLM will understand what the customer has already said in the past. The image below shows how this is orchestrated at a high level.\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "## Extending Chat with RAG\n",
    "\n",
    "However, in our workshop's situation, we want to be able to enable a customer to ask follow up questions regarding documentation we provide through RAG. This means we need to build a system which has conversational memory AND contextual retrieval built into the text generation.\n",
    "\n",
    "![4](./images/context-aware-chatbot.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5603",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518733a",
   "metadata": {},
   "source": [
    "#### Libraries needed for the installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a9b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    \"faiss-cpu>=1.7.4,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0 \\\n",
    "    llama-index==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba54e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433f8ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6380876",
   "metadata": {},
   "source": [
    "### Use the following COT prompt to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563c1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Human: You are a supply chain inspector. Your job is to assess the risk of a supplier. You measure the supplier risk by evaluating the supplier across three dimensions: country, size, and reputation.\\nCountry - north America and west Europe countries are considered low risk. The rest of the world is considered medium risk.\\nSize - supplier with over 1000 employees is low risk. Supplier with 50 to 999 employees is medium risk, and a supplier with under 50 employees is high risk.\\nReputation - reputation scores are between 1 to 10 where a score of 1 to 3 is low risk, a score of 4 to 7 is medium risk, and a reputation score of 8 to 10 is high risk.\\nThe risk formula is to take the maximum risk across the three dimensions.\\n##\\nExample:\\nSupplier: A\\nCountry: Chad\\nSize: 30\\nReputation: 8\\nLet's think step by step:\\nChad is not in North America or West Europe therefore country risk is medium.\\nA size of 30 is below 50 and therefore considered high risk.\\nA reputation score of 8 is between 8 to 10 and therefore considered high risk.\\nFinal Answer taking the maximum risk among all: Supplier A is at High risk.\\n##\\nSupplier: B\\nCountry: USA\\nSize: 40\\nReputation: 2\\nLet's think step by step: \\nAssistant: \"\n",
    "\n",
    "\n",
    "model_output = \" Okay, let's evaluate Supplier B:\\n\\n- Country: USA is in North America, so the country risk is low.\\n\\n- Size: 40 employees is below 50, so the size risk is high. \\n\\n- Reputation: A score of 2 is between 1-3, so the reputation risk is low.\\n\\nTo determine the overall risk, we take the maximum risk across the three dimensions. \\n\\nThe maximum risk for Supplier B is high due to its small size.\\n\\nTherefore, my assessment is that Supplier B is at high risk overall.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c1f4a-d295-478a-b2df-487b67c0472d",
   "metadata": {},
   "source": [
    "---\n",
    "## Using LangChain for Conversation Memory\n",
    "\n",
    "We will use LangChain's `ConversationBufferMemory` class provides an easy way to capture conversational memory for LLM chat applications. Let's check out an example of Claude being able to retrieve context through conversational memory below.\n",
    "\n",
    "Similar to the last workshop, we will use both a prompt template and a LangChain LLM for this example. Note that this time our prompt template includes a `{history}` variable where our chat history will be included to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48e507d-89be-4e1f-87b7-0fa1f7ffb187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = '''You are a helpful conversational assistant.\n",
    "{history}\n",
    "\n",
    "Human: {human_input}\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "PROMPT = PromptTemplate.from_template(CHAT_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961b106c-baa4-474c-97f9-eca7d868e52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24656b5",
   "metadata": {},
   "source": [
    "The `ConversationBufferMemory` class is instantiated here and you will notice that we use Claude specific human and assistant prefixes. When we initialize the memory, the history is blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e84d14-9b71-4235-83e3-045b87e6cbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "history = memory.load_memory_variables({})['history']\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217109c8",
   "metadata": {},
   "source": [
    "We now ask Claude a simple question \"How can I check for imbalances in my model?\". The LLM responds to the question and we can use the `add_user_message` and `add_ai_message` functions to save the input and output into memory. We can then retrieve the entire conversation history and print the response. Currently the model will still return answer using the data it was trained upon. Further will examine how to get a curated answer using our own FAq's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38515d05-6437-43fa-865e-1fa80b6c3fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Human: How can I check for imbalances in my model?\n",
       "\n",
       "Assistant: Here are a few things you can do to check for imbalances in your model:\n",
       "\n",
       "- Evaluate model performance on different demographic groups. Look for significant differences in accuracy, recall, precision, etc. across groups. Large gaps may indicate the model has learned biases.\n",
       "\n",
       "- Check the data distribution of sensitive attributes like gender, race, age in the training data compared to the overall population. Imbalances here could influence the model. \n",
       "\n",
       "- Look at the model's predictions on individual examples and check if they align with your sense of fairness. Manual inspection can catch subtle biases automated metrics may miss.\n",
       "\n",
       "- Run the model on synthetic edge case data you construct to test specific fairness hypotheses. For example, all male profiles with an otherwise identical profile. \n",
       "\n",
       "- Use bias quantification metrics like disparate impact, statistical parity difference, equal opportunity difference to measure fairness numerically across groups. \n",
       "\n",
       "- Profile which features/variables the model relies most heavily on for predictions. If these correlate too much with a sensitive attribute, it could indicate unfair biases.\n",
       "\n",
       "- Perform causal analysis to better understand how sensitive attributes may directly or indirectly influence predictions.\n",
       "\n",
       "Regularly checking models for potential biases through multiple techniques helps catch issues early and build more equitable systems. Oversight helps ensure models act with fairness and for the benefit of all groups."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'How can I check for imbalances in my model?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history)\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "history = memory.load_memory_variables({})['history']\n",
    "display(Markdown(f'{history}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4df10c",
   "metadata": {},
   "source": [
    "Now we will ask a follow up question about the kind of imbalances does it detect and save the input and outputs again. Notice how the model is able to understand that when the human says \"it\", because it has access to the context of the chat history, the model is able to accurately understand what the user is asking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b6b6f1-7101-4603-87a8-9b6b0032226d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " When checking for imbalances or biases in a model, some common types of potential issues that can be detected include:\n",
       "\n",
       "- Demographic biases - Where the model performs significantly differently based on attributes like gender, race, age, location, etc. This includes things like lower accuracy for minority groups.\n",
       "\n",
       "- Historical biases in data - If the training data reflects past real-world biases or lack of representation, it can perpetuate those through the model predictions. \n",
       "\n",
       "- Redlining/discrimination - When a model systematically disadvantages or denies opportunities/resources to certain segments of the population, like specific neighborhoods.\n",
       "\n",
       "- Stereotypical biases - Relying too heavily on attributes that correlate with protected groups, like associating career traits with gender roles.  \n",
       "\n",
       "- Proxy variables - Variables in the data that correlate with and act as a proxy for sensitive attributes, allowing discrimination through proxies.\n",
       "\n",
       "- Disparate impact - When a model negatively impacts or has worse outcomes for protected groups, even if not explicitly using sensitive attributes.\n",
       "\n",
       "- Lack of counterfactual fairness - The model's predictions are not robust and may change drastically under small perturbations to inputs that have little causal influence.\n",
       "\n",
       "- Unfair treatment of outliers/edge cases - Certain individuals or profiles may be systemically disadvantaged due to their unique attributes not being well represented.\n",
       "\n",
       "Regular bias detection aims to uncover these types of issues to ensure all groups are treated fairly by a model and have equal access to opportunities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'What kind does it detect?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history)\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "#display(Markdown(f'{history}'))\n",
    "display(Markdown(f'{ai_output}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b107b4f-358b-4f29-887a-15ef5341abaf",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a class to help facilitate conversation\n",
    "\n",
    "To help create some structure around these conversations, we create a custom `Conversation` class below. This class will hold a stateful conversational memory and be the base for conversational RAG later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396cac77-00a4-4ae2-913f-56ae1dbc60ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, client, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = Bedrock(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.9,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, human_input=user_input)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a24eed",
   "metadata": {},
   "source": [
    "Let's see the class in action with two contextual questions. Again, notice the model is able to correctly interpret the context because it has memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154f8e9b-6088-4a65-859b-e6087b1949c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Conversation(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d35169-0dee-430e-9646-5d79dc713109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few things you can do to check for and address imbalances in your machine learning model:\n",
       "\n",
       "- Check the distribution of your target variable(s) across different groups. Look for significant skews that could indicate imbalance. For example, if one class is 90% of your data.\n",
       "\n",
       "- Look at evaluation metrics like precision, recall, F1 score separately for each group. Imbalances may show up as much lower scores for minority groups. \n",
       "\n",
       "- Stratify your training, validation and test sets to ensure each group is properly represented in each set. Random splits can exacerbate imbalances.\n",
       "\n",
       "- Use resampling techniques like oversampling minority classes or undersampling majority classes to balance your training data. \n",
       "\n",
       "- Weight your loss function to place more emphasis on correct predictions for minority classes. This helps address skews during model training.\n",
       "\n",
       "- Evaluate calibration and ensure predicted probabilities are well-calibrated across groups. Imbalanced data can lead to poorly calibrated predictions.\n",
       "\n",
       "- For classification, check the confusion matrix to see if some groups are being disproportionately misclassified compared to others.\n",
       "\n",
       "- Analyze feature importances to check if the model is relying too heavily on features strongly correlated with the majority class.\n",
       "\n",
       "Regular monitoring and metrics by group can help spot model issues stemming from data or label imbalances early. Resampling and loss weighting are also commonly used to directly address imbalances."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43fb63f8-d8fc-4a4e-9bc3-6ccb1c30d002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main types of imbalances that can be detected when checking a machine learning model:\n",
       "\n",
       "- Class imbalance - When the distribution of target classes is uneven, with some classes under-represented. This is common with rare event prediction.\n",
       "\n",
       "- Sample size imbalance - Unequal number of samples from different subgroups defined by variables like gender, location, etc. \n",
       "\n",
       "- Feature distribution imbalance - Differences in how features are distributed between subgroups. For example, an important predictor only present for one group.\n",
       "\n",
       "- Missing data imbalance - Certain subgroups more likely to have missing or incomplete data for specific features.\n",
       "\n",
       "- Label noise imbalance - Uneven label accuracy between subgroups, such as different error rates during manual labeling. \n",
       "\n",
       "- Evaluation metric imbalance - Subgroups show very different performance on common metrics like accuracy, F1 score, AUC. \n",
       "\n",
       "- Prediction calibration imbalance - Predicted probabilities are not well-calibrated across all subgroups.\n",
       "\n",
       "- Feature importance imbalance - Model overly relies on features strongly correlated with majority subgroups.\n",
       "\n",
       "- Outcome imbalance - Subgroups see disproportionate outcomes even with balanced predictions, revealing hidden biases.\n",
       "\n",
       "The key things checked are imbalances in: class/target distributions, sample sizes, feature distributions, missingness, label quality, model performance metrics, prediction reliability, feature effects, and actual outcome distributions between defined subgroups. Identifying these skews helps diagnose potential fairness, representation or bias issues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192a46c-5a1f-4683-8c66-c47f05477468",
   "metadata": {},
   "source": [
    "---\n",
    "## Combining RAG with Conversation\n",
    "\n",
    "Now that we have a conversational system built, lets incorporate the RAG system we built in notebook 02 into the chat paradigm. \n",
    "\n",
    "First, we will create the same vector store with LangChain and FAISS from the last notebook.\n",
    "\n",
    "Our goal is to create a curated response from the model and only use the FAQ's we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23b07c75-896c-4a99-b214-ad42c12651e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.load_local('./faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb169d",
   "metadata": {},
   "source": [
    "### Visualize Semantic Search \n",
    "\n",
    "⚠️ ⚠️ ⚠️ This section is for Advanced Practioners. Please feel free to run through these cells and come back later to re-examine the concepts ⚠️ ⚠️ ⚠️ \n",
    "\n",
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store\n",
    "\n",
    "\n",
    "##### Citation\n",
    "We will also be able to get the `citation` or the underlying documents which our Vector Store matched to our query. This is useful for debugging and also measuring the quality of the vector stores. let us look at how the underlying Vector store calculates the matches\n",
    "\n",
    "##### Vector DB Indexes\n",
    "One of the key components of the Vector DB is to be able to retrieve documents matching the query with accuracy and speed. There are multiple algorithims for the same and some examples can be [read here](https://thedataquarry.com/posts/vector-db-3/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#- helpful function to display in tabular format\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td>%s</td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0dd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14746094, 0.77734375, 0.26953125, -0.55859375, 0.047851562, -0.43554688, -0.057617188, -0.00030326843, -0.5703125, -0.33789062]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Let us look at the documents which had the relevant information pertaining to our query"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "------------------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "How do I build an ML model to generate accurate predictions in SageMaker Canvas?,\" Once you have connected sources, selected a dataset, and prepared your data, you can select the target column that you want to predict to initiate a model creation job. SageMaker Canvas will automatically identify the problem type, generate new relevant features, test a comprehensive set of prediction models using ML techniques such as linear regression, logistic regression, deep learning, time-series forecasting, and gradient boosting, and build the model that makes accurate predictions based on your dataset.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "------------------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "v = embedding_model.embed_query(\"How can I check for imbalances in my model?\")\n",
    "print(v[0:10])\n",
    "results = vs.similarity_search_by_vector(v, k=2)\n",
    "display(Markdown('Let us look at the documents which had the relevant information pertaining to our query'))\n",
    "for r in results:\n",
    "    display(Markdown(f'{r.page_content}'))\n",
    "    display(Markdown(f'------------------------------------'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "##### Distance scoring in Vector Data bases\n",
    "[Distance scores](https://weaviate.io/blog/distance-metrics-in-vector-search) are the key in vector searches. Here are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance ( Squared Euclidean) . Therefore, a lower score is better. Further in FAISS we have similarity_search_with_score (ranked by distance: low to high) and similarity_search_with_relevance_scores ( ranked by relevance: high to low) with both using the distance strategy. The similarity_search_with_relevance_scores calculates the relevance score as 1 - score. For more details of the various distance scores [read here](https://milvus.io/docs/metric.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Let us look at the documents based on EUCLIDEAN_DISTANCE which will be used to answer our question 'What kind of bias does Clarify detect ?'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "------------------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Documents</td><td>Meta-data</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"</td><td>{}</td></tr><tr><td>How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.</td><td>{}</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"##### Let us look at the documents based on {vs.distance_strategy.name} which will be used to answer our question 'What kind of bias does Clarify detect ?'\"))\n",
    "\n",
    "context = vs.similarity_search('What kind of bias does Clarify detect ?', k=2)\n",
    "#-  langchain.schema.document.Document\n",
    "display(Markdown(f'------------------------------------'))\n",
    "list_context = [[doc.page_content, doc.metadata] for doc in context]\n",
    "list_context.insert(0, ['Documents', 'Meta-data'])\n",
    "display_table(list_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the Page context and the meta data associated with the documents. Now let us look at the L2 scores based on the distance scoring as explained above. Lower score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Similarity Search Table with relevancy score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "------------------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Documents</td><td>Relevancy Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model\\'s prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"'</td><td>130.9253</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70457</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#- relevancy of the documents\n",
    "results = vs.similarity_search_with_score(\"What kind of bias does Clarify detect ?\", k=2, fetch_k=3)\n",
    "display(Markdown(f'##### Similarity Search Table with relevancy score.'))\n",
    "display(Markdown(f'------------------------------------'))   \n",
    "results.insert(0,['Documents', 'Relevancy Score'])\n",
    "display_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal Relevancy score\n",
    "\n",
    "Maximal Marginal Relevance  has been introduced in the paper [The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf). Maximal Marginal Relevance tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. In the below results since we have a very limited data set it might not make a difference but for larger data sets the query will theoritically run faster while still preserving the over all relevancy of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Let us look at MRR scores"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>MRR Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model\\'s prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"'</td><td>130.9253</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70457</td></tr><tr><td>page_content='What is the underlying tuning algorithm for Automatic Model Tuning?,\" Currently, the algorithm for tuning hyperparameters is a customized implementation of Bayesian Optimization. It aims to optimize a customer-specified objective metric throughout the tuning process. Specifically, it checks the object metric of completed training jobs, and uses the knowledge to infer the hyperparameter combination for the next training job.\"\\nDoes Automatic Model Tuning recommend specific hyperparameters for tuning?,\" No. How certain hyperparameters impact the model performance depends on various factors, and it is hard to definitively say one hyperparameter is more important than the others and thus needs to be tuned. For built-in algorithms within SageMaker, we do call out whether or not a hyperparameter is tunable.\"'</td><td>281.98907</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#- normalizing the relevancy\n",
    "display(Markdown('##### Let us look at MRR scores'))\n",
    "results = vs.max_marginal_relevance_search_with_score_by_vector(embedding_model.embed_query(\"What kind of bias does Clarify detect ?\"), k=3)\n",
    "results.insert(0, [\"Document\", \"MRR Score\"])\n",
    "display_table(results)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update embeddings of the Vector Databases\n",
    "\n",
    "Update of documents happens all the time and we have multiple versions of the documents. Which means we need to also factor how do we update the embeddings in our Vector Data bases. Fortunately we have and can leverage the meta data to update embeddings\n",
    "\n",
    "The key steps are:\n",
    "1. Load the new embeddings and add the meta data stating the version as 2\n",
    "2. Merge to the exisiting Vector database\n",
    "3. Run the query using the filter to only search in the new index and get the latest documents for the same query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split docs=6\n"
     ]
    }
   ],
   "source": [
    "# create vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./data/sagemaker/sm_faq_v2.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\"Question\", \"Answer\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "#docs_split = loader.load()\n",
    "docs_split = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\",\").split_documents(loader.load())\n",
    "list_of_documents = [Document(page_content=doc.page_content, metadata=dict(page='v2')) for idx, doc in enumerate(docs_split)]\n",
    "print(f\"Number of split docs={len(docs_split)}\")\n",
    "db = FAISS.from_documents(list_of_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a query against version 2 of the documents\n",
    "Let us run the query agsint our exisiting vector data base and we will see the the exisiting or the version 1 of the documents coming back. If we run with the filter since those do not exist in our vector Database we will see no results returned or an empty list back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "\n",
      "Running the query with V2 of the document we get []:\n"
     ]
    }
   ],
   "source": [
    "# Run the query with requesting data from version 2 which does not exist\n",
    "vs = FAISS.load_local('./faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)\n",
    "search_query = \"How can I check for imbalances in my model?\"\n",
    "#print(f\"Running with v1 of the documents we get response of {vs.similarity_search_with_score(query=search_query, k=1, fetch_k=4)}\")\n",
    "print(\"------\\n\")\n",
    "print(f\"Running the query with V2 of the document we get {vs.similarity_search_with_score(query=search_query, filter=dict(page='v2'), k=1)}:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new version of the document\n",
    "We will create the version 2 of the documents and use meta data to add to our original index. Once done we will then apply a filter in our query which will return to us the documents newly added. Run the query now after adding version of the documents\n",
    "\n",
    "We will also examine a way to speed up our searches and queries and look at another way to narrow the search using the  fetch_k parameter when calling similarity_search with filters. Usually you would want the fetch_k to be more than the k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - now let us add version 2 of the data set and run query from that\n",
    "\n",
    "vs.merge_from(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query complete merged data base with no filters\n",
    "Run the query against the fully merged DB without any filters for the meta data and we see that it returns the top results of the new V2 data and also the top results of the v1 data. Essentially it will match and return data closest to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model?\n",
       "Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83801</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"</td><td>{}</td><td>229.07718</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query with Filter\n",
    "Now we will ask to search only against the version 2 of the data and use filter criteria against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model?\n",
       "Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83801</td></tr><tr><td>Question: What kind of bias does SageMaker Clarify detect?\n",
       "Answer: Measuring bias in ML models is a first step to mitigating bias.</td><td>{'page': 'v2'}</td><td>268.96283</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, filter=dict(page='v2'), k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for new data\n",
    "Now let us ask a question which exists only on the version 2 of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: Can i use Quantum computing?\n",
       "Answer: Yes SageMaker version sometime in future will let you run quantum computing</td><td>{'page': 'v2'}</td><td>97.10314</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - now let us ask a question which ONLY exits in the version 2 of the document\n",
    "search_query_v2 = \"Can i use Quantum computing?\"\n",
    "results_with_scores = vs.similarity_search_with_score(query=search_query_v2, filter=dict(page='v2'), k=1, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us continue to build our chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61cab4",
   "metadata": {},
   "source": [
    "The prompt template is now altered to include both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create vector store and continue\n",
    "vs = FAISS.load_local('./faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b25ac933-0cfb-4982-997b-f43365b7bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"You are a helpful conversational assistant.\n",
    "\n",
    "If you are unsure about the answer OR the answer does not exist in the context, respond with\n",
    "\"Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊\n",
    "NEVER make up the answer.\n",
    "\n",
    "If the human greets you, simply introduce yourself.\n",
    "\n",
    "The context will be placed in <context></context> XML tags. \n",
    "\n",
    "<context>{context}</context>\n",
    "\n",
    "Do not include any xml tags in your response.\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f5867",
   "metadata": {},
   "source": [
    "The new `ConversationWithRetrieval` class now includes a `get_context` function which searches our vector database based on the human input and combines it into the base prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c905e61-fc05-400d-9efb-5b42b151b5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationWithRetrieval:\n",
    "    def __init__(self, client, vector_store: FAISS=None, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            vector_store (FAISS, optional): pre-populated vector store for searching context. Defaults to None.\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # store vector store\n",
    "        self.vector_store = vector_store\n",
    "        \n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"Human\", ai_prefix=\"Assistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = Bedrock(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "            search_results (list): context used in the completion\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        context_string, search_results = self.get_context(user_input)\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, input=user_input, context=context_string)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output, search_results\n",
    "\n",
    "    def get_context(self, user_input, k=5):\n",
    "        \"\"\"returns context used in the completion\n",
    "\n",
    "        Args:\n",
    "            user_input (str): user input as a string\n",
    "            k (int, optional): number of results to return. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            context_string (str): context used in the completion as a string\n",
    "            search_results (list): context used in the completion as a list of Document objects\n",
    "        \"\"\"\n",
    "        search_results = self.vector_store.similarity_search(\n",
    "            user_input, k=k\n",
    "        )\n",
    "        context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content for ind, i in enumerate(search_results)])\n",
    "        return context_string, search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0bbee",
   "metadata": {},
   "source": [
    "Now the model can answer some specific domain questions based on our document database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c282d088-aa3c-493e-b1f4-0d47c27ffed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ConversationWithRetrieval(boto3_bedrock, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "faaaad87-48fe-495f-98d9-4c7cd95ad5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SageMaker Clarify allows you to detect various types of biases in ML models, including disparities in performance across demographic groups. Some common metrics it supports measuring include differences in error rates, precision and recall across groups. Checking for these types of imbalances in your model is recommended before and after training to help identify and mitigate any unfair treatment of certain populations in your predictions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output, context = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e995518-35e8-4106-b454-fc443c68f1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "** Ai Assistant Answer: ** \n",
       "SageMaker Clarify can detect various types of biases in ML models, including:\n",
       "\n",
       "- Representation bias, which refers to imbalances in the representation of different groups in the training data. This could involve checking if one group is underrepresented. \n",
       "\n",
       "- Statistical bias, which involves detecting differences in the label distributions across groups in the training data. \n",
       "\n",
       "- Prediction bias, which measures whether the performance of a trained model differs across groups. This includes comparing error rates, precision, recall and other metrics between groups to identify disparities in how accurately the model predicts outcomes for different populations.\n",
       "\n",
       "- Breaking these metrics down further can provide more detailed insight, such as comparing precision and recall separately between groups.\n",
       "\n",
       "So in summary, SageMaker Clarify detects biases related to data representation and label distributions prior to training, as well as performance differences between groups for the trained model, to help identify and mitigate unfair treatment of certain populations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "** Relevant Documentation: ** \n",
       "[Document(page_content='What is Amazon SageMaker Autopilot?,\" SageMaker Autopilot is the industry’s first automated machine learning capability that gives you complete control and visibility into your ML models. SageMaker Autopilot automatically inspects raw data, applies feature processors, picks the best set of algorithms, trains and tunes multiple models, tracks their performance, and then ranks the models based on performance, all with just a few clicks. The result is the best-performing model that you can deploy at a fraction of the time normally required to train the model. You get full visibility into how the model was created and what’s in it, and SageMaker Autopilot integrates with SageMaker Studio. You can explore up to 50 different models generated by SageMaker Autopilot inside SageMaker Studio so it’s easy to pick the best model for your use case. SageMaker Autopilot can be used by people without ML experience to easily produce a model, or it can be used by experienced developers to quickly develop a baseline model on which teams can further iterate.\"'), Document(page_content='What is Amazon SageMaker Studio Lab?,\" SageMaker Studio Lab is a free ML development environment that provides the compute, storage (up to 15 GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email ID; you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker Studio Lab automatically saves your work so you don’t need to restart between sessions. It’s as easy as closing your laptop and coming back later.\"'), Document(page_content='What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model\\'s prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\"'), Document(page_content='How can I reproduce a feature from a given moment in time?, SageMaker Feature Store maintains time stamps for all features at every instance of time. This helps you retrieve features at any period of time for business or compliance requirements. You can easily explain model features and their values from when they were first created to the present time by reproducing the model from a given moment in time.\\nWhat are offline features?,\" Offline features are used for training because you need access to very large volumes over a long period of time. These features are served from a high-throughput, high-bandwidth repository.\"\\nWhat are online features?, Online features are used in applications required to make real-time predictions. Online features are served from a high-throughput repository with single-digit millisecond latency for fast predictions.'), Document(page_content='Why should I use SageMaker for shadow testing?,\" SageMaker simplifies the process of setting up and monitoring shadow variants so you can evaluate the performance of the new ML model on live production traffic. SageMaker eliminates the need for you to orchestrate infrastructure for shadow testing. It lets you control testing parameters such as the percentage of traffic mirrored to the shadow variant and the duration of the test. As a result, you can start small and increase the inference requests to the new model after you gain confidence in model performance. SageMaker creates a live dashboard displaying performance differences across key metrics, so you can easily compare model performance to evaluate how the new model differs from the production model.\"')]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output, context = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(f'** Ai Assistant Answer: ** \\n{output}'))\n",
    "display(Markdown(f'\\n\\n** Relevant Documentation: ** \\n{context}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c22f06",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LangChain for Orchestration of RAG\n",
    "\n",
    "Beyond the primitive classes for prompt handling and conversational memory management, LangChain also provides a framework for [orchestrating RAG flows](https://python.langchain.com/docs/expression_language/cookbook/retrieval) with what purpose built \"chains\". In this section, we will see how to be a retrieval chain with LangChain which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process...\n",
    "\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve relevant documents.\n",
    "3. Relevant documents and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. Return to step 1.\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and other instructions. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "3. Reformatted user input is queried against the vector database to retrieve relevant documents.\n",
    "4. The reformatted user input and relevant documents are inserted into a new prompt in order to answer the user question.\n",
    "5. Return to step 1.\n",
    "\n",
    "Let's now build out this second workflow using LangChain below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611a13a",
   "metadata": {},
   "source": [
    "First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e314e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the follow up message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d7099",
   "metadata": {},
   "source": [
    "The next prompt we need is the prompt which will answer the user's question based on the retrieved information. In this case, we provide specific instructions about how to answer the question as well as provide the context retrieved from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55d854a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Human: Given the context above, answer the question inside the <q></q> XML tags.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\". Do not use any XML tags in the answer.\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f9fbf",
   "metadata": {},
   "source": [
    "Now that we have our prompts set up, let's set up the conversational memory buffer just like we did earlier in the notebook. Notice how we inject an example human and assistant message in order to help guide our AI assistant on what its job is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "920aefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\"max_tokens_to_sample\": 500, \"temperature\": 0.9}\n",
    ")\n",
    "memory_chain = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"Assistant\"\n",
    ")\n",
    "memory_chain.chat_memory.add_user_message(\n",
    "    'Hello, what are you able to do?'\n",
    ")\n",
    "memory_chain.chat_memory.add_ai_message(\n",
    "    'Hi! I am a help chat assistant which can answer questions about Amazon SageMaker.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223df37",
   "metadata": {},
   "source": [
    "Lastly, we will used the `ConversationalRetrievalChain` from LangChain to orchestrate this whole system. If you would like to see some more logs about what is happening in the orchestration and not just the final output, make sure to change the `verbose` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5da087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, # this is our claude model\n",
    "    retriever=vs.as_retriever(), # this is our FAISS vector database\n",
    "    memory=memory_chain, # this is the conversational memory storage class\n",
    "    condense_question_prompt=condense_prompt, # this is the prompt for condensing user inputs\n",
    "    verbose=False, # change this to True in order to see the logs working in the background\n",
    ")\n",
    "qa.combine_docs_chain.llm_chain.prompt = respond_prompt # this is the prompt in order to respond to condensed questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46fba3",
   "metadata": {},
   "source": [
    "Let's go ahead and generate some responses from our RAG solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d10f20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " You can check for imbalances in your model when using Amazon SageMaker in several ways:\n",
       "\n",
       "Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time, and also includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'How can I check for imbalances in my model?'})}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7af21687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " SageMaker Clarify checks for imbalances in the following types:\n",
       "\n",
       "Representativeness - Whether one group is underrepresented in the training data. \n",
       "Label distribution - Whether there are differences in the label distribution across groups.\n",
       "Performance - Whether the performance of the model differs across groups, such as differences in error rates, precision, or recall across groups."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'What kind does it detect?' })}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61caec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Detecting imbalances in representativeness, label distribution, and performance across groups with Amazon SageMaker Clarify can help explain predictions from a model by highlighting potential unfairness or biases in the model's behavior. Identifying differences in how the model performs on different groups is an important part of understanding a model's limitations and ensuring it does not discriminate."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'How does this improve model explainability?' })}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c11819",
   "metadata": {},
   "source": [
    "## Let us use LLM to validate if the response was factual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68954164",
   "metadata": {},
   "source": [
    "#### We first create a sanity prompt, which will use the vector DB results and ask the LLM to validate if the respinse given was acurate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20bf776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user asking Questions. In the following interactions, Falcon will converse in natural language, and Falcon will answer the questions based only on the provided Context. Falcon will provide accurate, short and direct answers to the questions.\n",
      "Context: {context}\n",
      "Statement: {statement}\n",
      "Question: Is the above statement based directly on the provided context? Answer with yes or no.\n",
      "Answer:\n",
      "Content: What kind of bias does SageMaker Clarify detect?,\" Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example).\", Metadata: {}, Score: 229.07717895507812\n",
      "Content: How do I build an ML model to generate accurate predictions in SageMaker Canvas?,\" Once you have connected sources, selected a dataset, and prepared your data, you can select the target column that you want to predict to initiate a model creation job. SageMaker Canvas will automatically identify the problem type, generate new relevant features, test a comprehensive set of prediction models using ML techniques such as linear regression, logistic regression, deep learning, time-series forecasting, and gradient boosting, and build the model that makes accurate predictions based on your dataset.\", Metadata: {}, Score: 273.8197021484375\n",
      "Content: What does SageMaker Model Dashboard do?,\" SageMaker Model Dashboard gives you a comprehensive overview of deployed models and endpoints, letting you track resources and model behavior violations through one pane. It allows you to monitor model behavior in four dimensions, including data and model quality, and bias and feature attribution drift through its integration with SageMaker Model Monitor and SageMaker Clarify. SageMaker Model Dashboard also provides an integrated experience to set up and receive alerts for missing and inactive model monitoring jobs, and deviations in model behavior for model quality, data quality, bias drift, and feature attribution drift. You can further inspect individual models and analyze factors impacting model performance over time. Then, you can follow up with ML practitioners to take corrective measures.\", Metadata: {}, Score: 278.7319641113281\n",
      "Content: What is the underlying tuning algorithm for Automatic Model Tuning?,\" Currently, the algorithm for tuning hyperparameters is a customized implementation of Bayesian Optimization. It aims to optimize a customer-specified objective metric throughout the tuning process. Specifically, it checks the object metric of completed training jobs, and uses the knowledge to infer the hyperparameter combination for the next training job.\"\n",
      "Does Automatic Model Tuning recommend specific hyperparameters for tuning?,\" No. How certain hyperparameters impact the model performance depends on various factors, and it is hard to definitively say one hyperparameter is more important than the others and thus needs to be tuned. For built-in algorithms within SageMaker, we do call out whether or not a hyperparameter is tunable.\", Metadata: {}, Score: 288.2359619140625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sanity check prompt\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def create_sanity_prompt(instruction_start: str = None,instruction_end: str = None,) -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Create a prompt template for LLM sanity check\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instruction_start : str, optional\n",
    "        Instrcution in the beginning of the prompt, by default None\n",
    "    instruction_end : str, optional\n",
    "        Instrcution in the end of the prompt, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PromptTemplate\n",
    "        Prompt template in the LangChain format\n",
    "    \"\"\"\n",
    "\n",
    "    # first instruction\n",
    "    prompt_template_build = instruction_start + \"\\n\"\n",
    "\n",
    "    # add context\n",
    "    prompt_template_build += \"Context: {context}\" + \"\\n\"\n",
    "\n",
    "    # add statement\n",
    "    prompt_template_build += \"Statement: {statement}\" + \"\\n\"\n",
    "\n",
    "    # addinstruction\n",
    "    prompt_template_build += \"Question: \" + instruction_end + \"\\n\"\n",
    "\n",
    "    # add answer placeholder\n",
    "    prompt_template_build += \"Answer:\"\n",
    "\n",
    "    print(prompt_template_build)\n",
    "    # build the template\n",
    "    llm_prompt = PromptTemplate(\n",
    "        template=str(prompt_template_build),\n",
    "        input_variables=[\"context\", \"statement\"],\n",
    "    )\n",
    "    return llm_prompt\n",
    "\n",
    "sanity_prompt = create_sanity_prompt(\n",
    "    instruction_start=\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user asking Questions. In the following interactions, Falcon will converse in natural language, and Falcon will answer the questions based only on the provided Context. Falcon will provide accurate, short and direct answers to the questions.\"\"\",\n",
    "    instruction_end=\"Is the above statement based directly on the provided context? Answer with yes or no.\",\n",
    ")\n",
    "\n",
    "docs = vs.similarity_search_with_score('How can I check for imbalances in my model?')\n",
    "contexts = []\n",
    "source = []\n",
    "for doc, score in docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n",
    "    if score <= 0.9:\n",
    "        contexts.append(doc)\n",
    "        source.append(doc.metadata['source'])\n",
    "        print(f\"\\n INPUT CONTEXT:{contexts}\")\n",
    "\n",
    "sanity_chain = load_qa_chain(llm=llm, prompt=sanity_prompt)\n",
    "sanity_check = sanity_chain({\"input_documents\": contexts, \"statement\": output},return_only_outputs=True)['output_text']\n",
    "\n",
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0b01f",
   "metadata": {},
   "source": [
    "#### We can see the Vector database responded accurately to our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc38c881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502732a",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LlamaIndex for Orchestration of RAG\n",
    "\n",
    "Another popular open source framework for orchestrating RAG is [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html). Let's take a look below at how to use our SageMaker FAQ vector index to have a conversational RAG application with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c059f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69279fe5",
   "metadata": {},
   "source": [
    "First we need to set up the system setting to define the embedding model and LLM. Again, we will be using titan and claude respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2677f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = BedrockEmbeddings(client=boto3_bedrock, model_id=\"amazon.titan-embed-text-v1\")\n",
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embed_model, chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d5083",
   "metadata": {},
   "source": [
    "The next step would be to create a FAISS index from our document base. In this lab, this is already done for you and stored in the [faiss-index/llama-index/](../faiss-index/llama-index/) folder.\n",
    "\n",
    "If you are interested in how this was accomplished, follow [this tutorial](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/FaissIndexDemo.html) from LlamIndex. The code below is the basics of how this was accomplished as well.\n",
    "\n",
    "```python\n",
    "# faiss_index = faiss.IndexFlatL2(1536)\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# documents = SimpleDirectoryReader(\"./../data/sagemaker\").load_data()\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, service_context=service_context\n",
    "# )\n",
    "# index.storage_context.persist('../faiss-index/llama-index/')\n",
    "```\n",
    "\n",
    "Once the index is created, we can load the persistent files to a `FaissVectorStore` object and create a `query_engine` from the vector index. To learn more about indicies in LlamaIndex, read more [here](https://gpt-index.readthedocs.io/en/latest/understanding/indexing/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf66c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./faiss-index/llama-index\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./faiss-index/llama-index\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92161d17",
   "metadata": {},
   "source": [
    "Now let's set up a retrieval based chat application similar to LangChain. We will use the same condensing question strategy as before and can reuse the same prompt to condense the question for vector searching. Notice how we include some custom chat history to inject context into the prompt for the model to understand what we are asking questions about. The resulting `chat_engine` object is now fully ready to chat about our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "942c2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts  import PromptTemplate\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content='Hello assistant, I have some questions about using Amazon SageMaker today.'\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content='Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    service_context=service_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59d3ba",
   "metadata": {},
   "source": [
    "Let's go ahead and ask our first question. Notice that the verbose `chat_engine` will print out the condensed question as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c61a330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with:  When using Amazon SageMaker, how can I check for imbalances in the model I want to train during our previous discussion?\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"How can I check for imbalances in my model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e737e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Based on the context provided, here is how you can check for imbalances in the model you want to train when using Amazon SageMaker:\n",
       "\n",
       "Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. With SageMaker Clarify, you can check for imbalances during data preparation, after training your model, and ongoing over time. \n",
       "\n",
       "Specifically, SageMaker Clarify provides the following capabilities to detect and help mitigate bias:\n",
       "\n",
       "- It checks whether the training data is representative and whether there are differences in the label distribution across groups, before training the model. This helps identify potential data imbalances.\n",
       "\n",
       "- After training your model, it measures whether and by how much the model's performance (e.g. error rates, precision, recall) differs across groups. This identifies if the trained model exhibits statistical biases. \n",
       "\n",
       "- During deployment, it continues monitoring for biases or differences in model performance across groups over time.\n",
       "\n",
       "SageMaker Clarify integrates with SageMaker Experiments to provide detailed feature importance graphs and enable explanations of individual predictions through an API. This improves the explainability of any biases detected.\n",
       "\n",
       "So in summary, to check for imbalances in your SageMaker model, you can leverage the bias detection and explainability capabilities of Amazon SageMaker Clarify."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd137da",
   "metadata": {},
   "source": [
    "Now follow up questions can be asked with conversational context in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30d2d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with:  How does Amazon SageMaker Clarify's capabilities for detecting biases in models through feature importance graphs and explanations of individual predictions through an API improve the explainability of any biases detected in a SageMaker model as discussed?\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"How does this improve model explainability?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "456f4f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Based on the context provided, SageMaker Clarify improves model explainability by helping detect potential biases in models and providing explanations for those biases:\n",
       "\n",
       "- SageMaker Clarify integrates with SageMaker Experiments to provide feature importance graphs after model training. These graphs detail the importance of each input feature for the model's overall predictions. This allows developers to determine if any features have more influence than they should, which could indicate potential biases. \n",
       "\n",
       "- For example, the graphs may show that a feature like gender has a high influence on the model's predictions, even when it should not. This helps detect potential biases.\n",
       "\n",
       "- SageMaker Clarify also provides explanations for individual predictions through an API. These explanations shed light on why the model made a particular prediction for a given example. \n",
       "\n",
       "- By examining these individual explanations, developers can further analyze any potential biases detected in the feature importance graphs. The explanations help validate and provide context around those initial bias findings.\n",
       "\n",
       "- Together, the feature importance graphs and individual prediction explanations improve model explainability by allowing developers to both detect potential biases and then understand the reasons behind them, through examining feature influences and prediction rationales. This helps address and remedy any issues to ensure the model is fair and unbiased."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccda9787",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34014f07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
