{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Prompt Evaluation Automation\n",
    "This notebook contains an example of how to build a testing framework for prompt evaluation.  The basic idea is that for most prompts, they consist of system instructions, role assignment, few shot examples, etc, which we call \"instructions\" and then they have the user query, which we will call the \"question\".  This notebook allows users to test changes to the instructions, and then see how those changes will impact the responses to a series of questions.  This requires first generating a list of questions and correct responses, preferably manually checked for correctness by a human.\n",
    "\n",
    "The notebook follows this structure:\n",
    "  1) Set up the envionment\n",
    "  2) Create the testing functionality\n",
    "  3) Examples of using the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e1be-d464-4f6c-8019-31e15a5450a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Set up the envionment\n",
    "First start by importing and setting up the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v3 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print this out if you want to see all the models you have access to.\n",
    "    #print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01062755-97eb-4b8a-9a5a-8fa2f4a04b0f",
   "metadata": {},
   "source": [
    "### Next, create helper functions to make it easy to send a query to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 3 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(messages,system=\"\", DEBUG=False, model_version=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    if DEBUG: print(\"sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",\"\\n\".join(messages))\n",
    "    \n",
    "    if model_version== \"opus\":#comming soon to Bedrock!\n",
    "        modelId = 'error'\n",
    "    elif model_version== \"sonnet\":\n",
    "        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    elif model_version== \"haiku\":\n",
    "        modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model version, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'error'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say the number four.\n",
      "Four.\n",
      "CPU times: user 9.6 ms, sys: 4.15 ms, total: 13.8 ms\n",
      "Wall time: 830 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check that it's working:\n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    #query = [{\"role\": \"user\", \"content\": \"Please say the number two.\"},{\"role\": \"assistant\", \"content\": \"Two.\"},{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "    result = ask_claude(query)\n",
    "    print(query)\n",
    "    print(result[1])\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83eabe-9c7b-441a-bd23-7ec12159b118",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finally, create a threaded function for calling Claude multiple times at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i]))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171/179716452.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "[[\"[{'role': 'user', 'content': 'Please say the number one.'}]\", '1'], [\"[{'role': 'user', 'content': 'Please say the number two.'}]\", 'Two.'], [\"[{'role': 'user', 'content': 'Please say the number three.'}]\", 'Three.']]\n",
      "CPU times: user 44.5 ms, sys: 14.5 ms, total: 59 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "q3 = [{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "\n",
    "#print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\",\"Please say the number three.\",\"Please say the number four.\",\"Please say the number five.\"]))\n",
    "print(ask_claude_threaded([q1,q2,q3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cb22-d6b7-44e9-b451-5dd11cea93fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create the testing functionality\n",
    "Here, we'll set up the functions that use an LLM to run our unit tests.  Start by defining a \"judge prompt\" which we will use to have the LLM compare the output we want to test to the gold standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc389973-0550-448c-8ceb-91cb88a935c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scoring_prompt_template = \"\"\"You are a teacher.  Consider the following question along with its correct answer and a student submitted answer.\n",
    "Here is the question:\n",
    "<question>{{QUESTION}}</question>\n",
    "Here is the correct answer:\n",
    "<correct_answer>{{ANSWER}}</correct_answer>\n",
    "Here is the student's answer:\n",
    "<student_answer>{{TEST_ANSWER}}</student_answer>\n",
    "Please provide a score from 0 to 100 on how well the student answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some facts are missing or incorrect, or if extra unnecessary facts have been included.\n",
    "The score should be 0 for entirely wrong answers.  Put the score in <SCORE> tags. and your reasoning in <REASON> tags.\n",
    "Do not consider your own answer to the question, but instead score based on the correct_answer above.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbab346-abb0-4532-a564-e6dff6e2f34a",
   "metadata": {},
   "source": [
    "### Next, we create a helper funtion which will take the prompt we want to test, and use it generate answers to every question in our gold standard set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a165f065-c88e-41a1-94db-28dac30b91c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(prompt_template, questions):\n",
    "    '''\n",
    "    get answers for each of our sample questions using the prompt template we are testing.\n",
    "    question_answers is a dict type.\n",
    "    '''\n",
    "    prompts = []\n",
    "    for question in questions:\n",
    "        prompts.append(prompt_template.replace(\"{{QUESTION}}\",question))\n",
    "    return ask_claude_threaded(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d839b8c9-5fe5-4276-a24c-864ac9f7874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answers(prompt_template, question_answers):\n",
    "    '''\n",
    "    ask our LLM to score each of the generated answers.\n",
    "    '''\n",
    "    print (\"Generating answers to score...\")\n",
    "    answers_to_test = get_answers(prompt_template, question_answers)\n",
    "    print (\"Done.  Scoring answers...\")\n",
    "    \n",
    "    \n",
    "    #pack answers with questions in templated form.\n",
    "    question_answers_with_template = {}\n",
    "    for question in question_answers:\n",
    "        question_answers_with_template[prompt_template.replace(\"{{QUESTION}}\",question)] = question_answers[question]\n",
    "    #pack questions to templated form\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in question_answers:\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    prompts = []\n",
    "    for question,test_answer in answers_to_test:\n",
    "        original_question = question_with_template_to_questions[question]\n",
    "        correct_answer = question_answers_with_template[question]\n",
    "        prompts.append(scoring_prompt_template.replace(\"{{QUESTION}}\",original_question).replace(\"{{ANSWER}}\",correct_answer).replace(\"{{TEST_ANSWER}}\",test_answer))\n",
    "\n",
    "    return ask_claude_threaded(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32d22a1-df20-4ceb-9c12-7d4cef349ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f95044-a1c1-4de7-8df6-e64a846dc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(prompt_template, question_answers, threshhold):\n",
    "    \"\"\"\n",
    "    Call score answers and format the results once all threads have returned.\n",
    "    \"\"\"\n",
    "    scored_answers = score_answers(prompt_template, question_answers)\n",
    "    print (\"Done.\")\n",
    "    #pack questions to templated form\n",
    "    question_with_template_to_questions = {}\n",
    "    for question in question_answers:\n",
    "        question_with_template_to_questions[prompt_template.replace(\"{{QUESTION}}\",question)]=question\n",
    "    \n",
    "    scores = []\n",
    "    for prompt,response in scored_answers:\n",
    "        soup = BS(prompt)\n",
    "        question = soup.find('question').text\n",
    "        correct_answer = soup.find('correct_answer').text\n",
    "        prompt_answer = soup.find('student_answer').text\n",
    "        soup = BS(response)\n",
    "        score = soup.find('score').text\n",
    "        reason = soup.find('reason').text\n",
    "        passed = True\n",
    "        if int(score)<threshhold:\n",
    "            passed = False\n",
    "        scores.append([question,correct_answer,prompt_answer,score,reason,passed])\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989bad4-d380-4a45-8f82-cff6c5273782",
   "metadata": {},
   "source": [
    "## Examples of using the tests\n",
    "### Start by defining the gold standard question/answers, and two prompts we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89051433-43e3-4662-b408-67ed179b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our gold standard list of question answer pairs.  Don't use generic ones here, write them for your use case!\n",
    "#a good test has a couple hundred questions.\n",
    "question_answers = {\n",
    " \"What is heavier, 1kg of feathers or 1kg of iron?\":\"They are the same.\",\n",
    " \"What is my current bank account balance?\":\"I don't have access to that information.\",\n",
    " \"Who was the president in the year 2000?\":\"Bill Clinton\",   \n",
    " \"A boy runs down the stairs in the morning and sees a tree in his living room, and some boxes under the tree. What's going on?\":\"It is Christmas.\",\n",
    " \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts?\":\"5 hours.\"\n",
    "}\n",
    "questions = list(question_answers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e052ae-0f95-4e39-b5ba-9cc96cbaa533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#here, we use {{QUESTION}} as the placeholder where each of the questions will be interested for testing.\n",
    "#the automated test will replace {{QUESTION}} with each question in our gold standard list one at a time.\n",
    "#A pretty good one\n",
    "test_prompt = \"You are a helpful assistant that loves to give full, complete, accurate answers.  Please answer this question:{{QUESTION}}\"\n",
    "\n",
    "# A bad one, for comparason.\n",
    "test_prompt_2 = \"You are a boat fanatic and always talk like a pirate.  You do answer questions, but you also always include a fun fact about boats.  Please answer this question:{{QUESTION}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f1af2-ce4d-4935-b4be-e1d979a87a8c",
   "metadata": {},
   "source": [
    "### Now, let's score both of our two prompts that we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6682f0b-4751-48c4-92a1-f21bf8e56f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers to score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1171/179716452.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Done.  Scoring answers...\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Done.\n",
      "Generating answers to score...\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Done.  Scoring answers...\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_prompt(test_prompt, question_answers,threshhold=90)\n",
    "scores_2 = evaluate_prompt(test_prompt_2, question_answers,threshhold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126326d0-66ff-4304-b96c-337433af57cb",
   "metadata": {},
   "source": [
    "### We can take a more detailed look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1136f185-f6aa-4f3d-8341-0691f0439af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Answer                         | Prompt Answer                       | Reason                              | Score\n",
      "_________________________________________________________________________________\n",
      "They are the same.                  | Okay, let's think this through s... | The student's answer correctly e... | 100\n",
      "I don't have access to that info... | I'm afraid I don't actually have... | The student's answer matches the... | 100\n",
      "Bill Clinton                        | The president of the United Stat... | The student's answer accurately ... | 100\n",
      "It is Christmas.                    | Based on the information provide... | The student's answer correctly i... | 90\n",
      "5 hours.                            | To solve this problem, we can us... | The student's answer is partiall... | 50\n",
      "\n",
      "Total average score:  88.0\n",
      "Total number passed:  4\n"
     ]
    }
   ],
   "source": [
    "all_scores = 0\n",
    "number_passed = 0\n",
    "padding = 35 #column width in output\n",
    "print (\"Gold Answer\".ljust(padding),\"|\",\"Prompt Answer\".ljust(padding),\"|\",\"Reason\".ljust(padding),\"|\",\"Score\")\n",
    "print (\"_________________________________________________________________________________\")\n",
    "for question,correct_answer,prompt_answer,score,reason,passed in scores:\n",
    "    if len(correct_answer)>padding-3:\n",
    "        correct_answer = correct_answer[:padding-3]+\"...\"\n",
    "    if len(prompt_answer)>padding-3:\n",
    "        prompt_answer = prompt_answer[:padding-3]+\"...\"\n",
    "    if len(reason)>padding-3:\n",
    "        reason = reason[:padding-3]+\"...\"\n",
    "    if passed:\n",
    "        number_passed+=1\n",
    "    print (correct_answer.ljust(padding),\"|\",prompt_answer.ljust(padding),\"|\",reason,\"|\",score)\n",
    "    all_scores+=float(score)\n",
    "print (\"\")\n",
    "average_score = all_scores/len(scores)\n",
    "print (\"Total average score: \",average_score)\n",
    "print (\"Total number passed: \",number_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efde8bfc-3b81-4f46-b3ab-0fb94d375021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Answer                         | Prompt 2 Answer                     | Reason                              | Score\n",
      "_________________________________________________________________________________\n",
      "They are the same.                  | Ahoy, me hearty! As a true boat ... | The student's answer correctly s... | 90\n",
      "I don't have access to that info... | *clears throat and speaks in a g... | The student's answer matches the... | 90\n",
      "Bill Clinton                        | Ahoy, me hearty! Ye be askin' ab... | The student's answer does not ma... | 0\n",
      "It is Christmas.                    | *clears throat and speaks in a g... | The student's answer correctly i... | 90\n",
      "5 hours.                            | Ahoy, me hearty! As a boat fanat... | The student's answer is quite of... | 20\n",
      "\n",
      "Total average score:  58.0\n",
      "Total number passed:  3\n"
     ]
    }
   ],
   "source": [
    "all_scores_2 = 0\n",
    "number_passed_2 = 0\n",
    "padding = 35 #column width in output\n",
    "print (\"Gold Answer\".ljust(padding),\"|\",\"Prompt 2 Answer\".ljust(padding),\"|\",\"Reason\".ljust(padding),\"|\",\"Score\")\n",
    "print (\"_________________________________________________________________________________\")\n",
    "for question,correct_answer,prompt_answer,score,reason,passed in scores_2:\n",
    "    if len(correct_answer)>padding-3:\n",
    "        correct_answer = correct_answer[:padding-3]+\"...\"\n",
    "    if len(prompt_answer)>padding-3:\n",
    "        prompt_answer = prompt_answer[:padding-3]+\"...\"\n",
    "    if len(reason)>padding-3:\n",
    "        reason = reason[:padding-3]+\"...\"\n",
    "    if passed:\n",
    "        number_passed_2+=1\n",
    "    print (correct_answer.ljust(padding),\"|\",prompt_answer.ljust(padding),\"|\",reason,\"|\",score)\n",
    "    all_scores_2+=float(score)\n",
    "print (\"\")\n",
    "average_score_2 = all_scores_2/len(scores_2)\n",
    "print (\"Total average score: \",average_score_2)\n",
    "print (\"Total number passed: \",number_passed_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6999097-afec-4370-a0b7-22589b1e239a",
   "metadata": {},
   "source": [
    "## Let's also grab a quick summary of the reasons, to get a general feel for how each prompt is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef7c0f95-0b53-4eba-9167-5131088371a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasons = []\n",
    "reasons_2 = []\n",
    "\n",
    "for question,correct_answer,prompt_answer,score,reason,passed in scores:\n",
    "    reasons.append(reason)\n",
    "for question,correct_answer,prompt_answer,score,reason,passed in scores_2:\n",
    "    reasons_2.append(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1f29c46-65be-413d-b89a-962abc5958a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_summary_prompt = \"\"\"\n",
    "Consider the following comments.  Each one was made by a teacher grading the same student's work.\n",
    "<comments>\n",
    "{{COMMENTS}}\n",
    "</comments>\n",
    "Please provide a breif summary of common trends you see in this student's work, both positive and negative, if any.\n",
    "In your answer, it is important to protect privacy by refering to the student as the \"prompt\".  Never say \"the student\" instead say \"the prompt\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ac95b0-181b-44a7-b90f-500599beb252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_summary_prompt_1 = reasoning_summary_prompt.replace(\"{{COMMENTS}}\",\"<comment>\\n\"+\"</comment>\\n<comment>\\n\".join(reasons)+\"\\n</comment>\")\n",
    "reasoning_summary_prompt_2 = reasoning_summary_prompt.replace(\"{{COMMENTS}}\",\"<comment>\\n\"+\"</comment>\\n<comment>\\n\".join(reasons_2)+\"\\n</comment>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04bf8d6d-3fce-4148-90a2-0fb467b021b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking Claude to generate a summary of the reasoning on each prompt.\n",
      "Error with calling Bedrock: An error occurred (ModelErrorException) when calling the InvokeModel operation: The system encountered an unexpected error during processing. Try your request again.\n",
      "Prompt 1:\n",
      "Based on the provided comments, the following trends can be observed in the prompt's work:\n",
      "\n",
      "Positive Trends:\n",
      "1. The prompt generally provides accurate and comprehensive answers that fully capture the essence of the correct answers. The responses demonstrate a strong understanding of the subject matter.\n",
      "2. The prompt's answers often include additional relevant details and context, which further reinforce the correctness of the responses.\n",
      "3. The prompt's step-by-step reasoning is well-aligned with the correct answers, indicating a thorough understanding of the concepts.\n",
      "\n",
      "Negative Trends:\n",
      "1. In one instance, the prompt's answer includes unnecessary steps and arrives at an incorrect final answer, despite setting up the problem correctly. This suggests a need for the prompt to focus on streamlining their problem-solving approach and double-checking their final answers.\n",
      "\n",
      "Overall, the comments suggest that the prompt's work is generally of high quality, with accurate and well-reasoned responses. The prompt demonstrates a strong grasp of the subject matter and the ability to provide comprehensive answers. However, the one instance of an incorrect final answer indicates a need for the prompt to be more careful in their problem-solving approach and to ensure the accuracy of their final results.\n",
      "\n",
      "Prompt 2:\n",
      "Based on the provided comments, a few common trends can be observed in the prompt's work:\n",
      "\n",
      "Positive Trends:\n",
      "1. The prompt often provides relevant and accurate core responses that match the correct answers, demonstrating a good understanding of the subject matter.\n",
      "2. The prompt occasionally includes additional context or details that, while not strictly necessary, add an engaging or creative element to the responses.\n",
      "3. The prompt's answers tend to effectively convey the same message as the correct answers, with only minor extraneous information added.\n",
      "\n",
      "Negative Trends:\n",
      "1. The prompt sometimes includes irrelevant or tangential information that is not directly related to the question, which can detract from the core response.\n",
      "2. In some instances, the prompt's answers are completely off-base and do not address the actual question, resulting in incorrect responses.\n",
      "3. The prompt's logic or reasoning can be flawed, leading to answers that do not align with the correct solution.\n",
      "\n",
      "Overall, the prompt's work exhibits a mix of strengths and weaknesses. While the prompt often provides accurate core responses, the inclusion of unnecessary or irrelevant information, as well as occasional lapses in logic or understanding, can result in suboptimal answers. Consistent focus on addressing the core of the question directly would likely improve the prompt's performance.\n"
     ]
    }
   ],
   "source": [
    "print (\"Asking Claude to generate a summary of the reasoning on each prompt.\")\n",
    "prompt_1_summary = ask_claude(reasoning_summary_prompt_1)[1]\n",
    "prompt_2_summary = ask_claude(reasoning_summary_prompt_2)[1]\n",
    "print (\"Prompt 1:\")\n",
    "print (prompt_1_summary)\n",
    "print (\"\\nPrompt 2:\")\n",
    "print (prompt_2_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a616d2-8d48-446a-9097-27f2b6cc3590",
   "metadata": {},
   "source": [
    "### Testing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4856fd82-2a4c-4b32-a850-ce9a5808a37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template 1\n",
      "Prompt: You are a helpful assistant that loves to give full, complete, accurate answers.  Please answer this question:{{QUESTION}}\n",
      "Average Score: 88.0\n",
      "Number passed: 4/5\n",
      "\n",
      "Prompt Template 2\n",
      "Prompt: You are a boat fanatic and always talk like a pirate.  You do answer questions, but you also always include a fun fact about boats.  Please answer this question:{{QUESTION}}\n",
      "Average Score: 58.0\n",
      "Number passed: 3/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Prompt Template 1\")\n",
    "print (\"Prompt:\",test_prompt)\n",
    "print(\"Average Score:\",average_score)\n",
    "print(\"Number passed: %s/%s\"%(number_passed,len(question_answers)))\n",
    "print (\"\")\n",
    "print (\"Prompt Template 2\")\n",
    "print (\"Prompt:\",test_prompt_2)\n",
    "print(\"Average Score:\",average_score_2)\n",
    "print(\"Number passed: %s/%s\"%(number_passed_2,len(question_answers)))\n",
    "print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf3afb-e9a5-4544-a1ed-218e3dbffb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
