{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e388aa2",
   "metadata": {},
   "source": [
    "# TwelveLabs Marengo on Amazon Bedrock Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4d77",
   "metadata": {},
   "source": [
    "TwelveLabs is a leading provider of multimodal AI models specializing in video understanding and analysis. TwelveLabs' advanced models enable sophisticated video search, analysis, and content generation capabilities through state-of-the-art computer vision and natural language processing technologies. [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html) now offers two TwelveLabs models: [TwelveLabs Pegasus 1.2](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-pegasus.html), which provides comprehensive video understanding and analysis, and [TwelveLabs Marengo Embed 2.7](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-marengo.html), which generates high-quality embeddings for video, text, audio, and image content. These models empower developers to build applications that can intelligently process, analyze, and derive insights from video data at scale.\n",
    "\n",
    "In this notebook, we'll be using TwelveLabs Marengo model for generating embeddings for content in texts, images and videos to enable multimodal search and analysis capabilities across different media types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6b3e",
   "metadata": {},
   "source": [
    "### TwelveLabs Video Understanding Models\n",
    "TwelveLabsâ€™ video understanding models consist of a family of deep neural networks built on our multimodal foundation model for video understanding that you can use for the following downstream tasks:\n",
    "- Search using natural language queries\n",
    "- Analyze videos to generate text\n",
    "\n",
    "Videos contain multiple types of information, including visuals, sounds, spoken words, and texts. The human brain combines all types of information and their relations with each other to comprehend the overall meaning of a scene. For example, youâ€™re watching a video of a person jumping and clapping, both visual cues, but the sound is muted. You might realize theyâ€™re happy, but you canâ€™t understand why theyâ€™re happy without the sound. However, if the sound is unmuted, you could realize theyâ€™re cheering for a soccer team that scored a goal.\n",
    "\n",
    "Thus, an application that analyzes a single type of information canâ€™t provide a comprehensive understanding of a video. TwelveLabsâ€™ video understanding models, however, analyze and combine information from all the modalities to accurately interpret the meaning of a video holistically, similar to how humans watch, listen, and read simultaneously to understand videos.\n",
    "\n",
    "Our video understanding models have the ability to identify, analyze, and interpret a variety of elements, including but not limited to the following:\n",
    "| Element | Modality | Example |\n",
    "|---------|----------|---------|\n",
    "| People, including famous individuals | Visual | Michael Jordan, Steve Jobs |\n",
    "| Actions | Visual | Running, dancing, kickboxing |\n",
    "| Objects | Visual | Cars, computers, stadiums |\n",
    "| Animals or pets | Visual | Monkeys, cats, horses |\n",
    "| Nature | Visual | Mountains, lakes, forests |\n",
    "| Text displayed on the screen (OCR) | Visual | License plates, handwritten words, number on a player's jersey |\n",
    "| Brand logos | Visual | Nike, Starbucks, Mercedes |\n",
    "| Shot techniques and effects | Visual | Aerial shots, slow motion, time-lapse |\n",
    "| Counting objects | Visual | Number of people in a crowd, items on a shelf, vehicles in traffic |\n",
    "| Sounds | Audio | Chirping (birds), applause, fireworks popping or exploding |\n",
    "| Human speech | Audio | \"Good morning. How may I help you?\" |\n",
    "| Music | Audio | Ominous music, whistling, lyrics |\n",
    "\n",
    "### Modalities\n",
    "Modalities represent the types of information that the models process and analyze in a video. These modalities are central to both indexing and searching video content.\n",
    "\n",
    "The models support the following modalities: \n",
    "\n",
    "- **Visual**: Analyzes visual content in a video, including actions, objects, events, text (through Optical Character Recognition, or OCR), and brand logos.\n",
    "- **Audio**: Analyzes audio content in a video, including ambient sounds, music, and human speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce269fb",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you download the latest botocore and boto3 libraries.\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_uv_installed():\n",
    "    if shutil.which(\"uv\") is None:\n",
    "        print(\"ðŸ”§ 'uv' not found. Installing with pip...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"uv\"])\n",
    "    else:\n",
    "        print(\"âœ… 'uv' is already installed.\")\n",
    "\n",
    "def uv_install(*packages):\n",
    "    ensure_uv_installed()\n",
    "    uv_path = shutil.which(\"uv\")\n",
    "    print(f\"ðŸ“¦ Installing {', '.join(packages)} using uv...\")\n",
    "    subprocess.check_call([uv_path, \"pip\", \"install\", *packages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff9e6c",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a876ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0652dbc",
   "metadata": {},
   "source": [
    "## Part 1: Multimodal Embeddings with Marengo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde65ef",
   "metadata": {},
   "source": [
    "### Part 1a: What is an embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f27a98",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Use TwelveLabs Marengo to create multimodal embeddings for videos, texts, images, and audio files. These embeddings are contextual vector representations (a series of numbers) that capture interactions between modalities, such as visual expressions, body language, spoken words, and video context. You can apply these embeddings to downstream tasks like training custom multimodal models for anomaly detection, diversity sorting, sentiment analysis, recommendations, or building Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Key features:\n",
    "- **Native multimodal support**: Process all modalities natively without separate models or frame conversion.\n",
    "- **State-of-the-art performance**: Captures motion and temporal information for accurate video interpretation.\n",
    "- **Unified vector space**: Combines embeddings from different modalities for holistic understanding.\n",
    "- **Fast and reliable**: Reduces processing time for large video sets.\n",
    "- **Flexible segmentation**: Generate embeddings for video segments or the entire video.\n",
    "\n",
    "Use cases:\n",
    "- **Anomaly detection**: Identify unusual patterns, such as corrupt videos with black backgrounds, to improve data set quality.\n",
    "- **Diversity sorting**: Organize data for broad representation, reducing bias and improving AI model training.\n",
    "- **Sentiment analysis**: Combine vocal tone, facial expressions, and spoken language for accurate insights, which particularly useful for customer service.\n",
    "- **Recommendations**: Use embeddings in similarity-based retrieval and ranking systems for recommendations.\n",
    "\n",
    "To learn more about embeddings, check out [The Multimodal Evolution of Vector Embeddings](https://www.twelvelabs.io/blog/multimodal-embeddings) on the TwelveLabs Blog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76298876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embeddings\n",
    "sample_embedding_1 = np.random.rand(1, 1024)\n",
    "sample_embedding_2 = np.random.rand(1, 1024)\n",
    "\n",
    "df_embedding_1 = pd.DataFrame(sample_embedding_1)\n",
    "df_embedding_2 = pd.DataFrame(sample_embedding_2)\n",
    "\n",
    "df_embedding_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video embedding\n",
    "sample_video_embedding = np.random.rand(5, 1024)\n",
    "df_video_embedding = pd.DataFrame(sample_video_embedding)\n",
    "df_video_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d44fe",
   "metadata": {},
   "source": [
    "### Part 1b: Calculating cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e7a32",
   "metadata": {},
   "source": [
    "Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them in high-dimensional space. Unlike distance metrics that consider magnitude, cosine similarity focuses purely on the orientation or direction of vectors, making it particularly useful for comparing text embeddings, documents, and other high-dimensional data.\n",
    "\n",
    "The multimodal vector embeddings from TwelveLabs Marengo can be used to calculate the similarity across text, image, audio, and video.\n",
    "\n",
    "***Formula***\n",
    "\n",
    "The cosine similarity between two vectors **A** and **B** is calculated as:\n",
    "\n",
    "```\n",
    "cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **A Â· B** is the dot product of vectors A and B\n",
    "- **||A||** and **||B||** are the magnitudes (norms) of vectors A and B respectively\n",
    "- **Î¸** is the angle between the two vectors\n",
    "\n",
    "***Key Characteristics***\n",
    "- **Range**: Values range from -1 to 1\n",
    "  - **1**: Identical direction (perfect similarity)\n",
    "  - **0**: Orthogonal vectors (no similarity)\n",
    "  - **-1**: Opposite directions (perfect dissimilarity)\n",
    "- **Magnitude Independence**: Only considers vector direction, not size\n",
    "- **Symmetric**: cos(A,B) = cos(B,A)\n",
    "\n",
    "***Benefits***\n",
    "- **Scale Invariant**: Ideal for comparing vectors of different magnitudes\n",
    "- **Computationally Efficient**: Fast calculation, especially with sparse vectors\n",
    "- **Robust for Text Analysis**: Perfect for document similarity and text embeddings\n",
    "- **Handles High Dimensions**: Works well in high-dimensional spaces without curse of dimensionality issues\n",
    "- **Intuitive Results**: Easy to interpret similarity scores between 0 and 1 for most applications\n",
    "\n",
    "***Drawbacks***\n",
    "- **Ignores Magnitude**: Completely disregards vector size, which may contain important information\n",
    "- **Limited with Negative Values**: Can be less meaningful when dealing with vectors containing negative components\n",
    "- **Not Always Intuitive**: May not align with human perception of similarity in certain domains\n",
    "- **Loses Information**: Discarding magnitude means losing potentially valuable signal strength data\n",
    "- **Poor for Sparse Positive Data**: May not distinguish well between vectors with very few non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15411bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between two single segment embeddings\n",
    "similarity = cosine_similarity(df_embedding_1, df_embedding_2)\n",
    "pd.DataFrame(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70473e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity with a multi-segment embedding\n",
    "similarities = cosine_similarity(df_video_embedding, df_embedding_1)\n",
    "pd.DataFrame(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max similarity and the index of the max similarity\n",
    "max_similarity = np.max(similarities)\n",
    "max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "print(f\"Max similarity: {max_similarity}\")\n",
    "print(f\"Index of max similarity: {max_similarity_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e9437",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building Multimodal Video Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BedrockTwelvelabsHelper, play_video, delete_s3_bucket_objects\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "aws_account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "model_id = \"twelvelabs.marengo-embed-2-7-v1:0\"\n",
    "cris_model_id = \"us.twelvelabs.marengo-embed-2-7-v1:0\"\n",
    "s3_bucket_name = '<an S3 bucket for storing the outputs>'\n",
    "\n",
    "bedrock_twelvelabs_helper = BedrockTwelvelabsHelper(bedrock_client=bedrock_client, \n",
    "                s3_client=s3_client, \n",
    "                aws_account_id=aws_account_id, \n",
    "                model_id=model_id, \n",
    "                cris_model_id=cris_model_id, \n",
    "                s3_bucket_name=s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3cee1",
   "metadata": {},
   "source": [
    "### Part 2a: Storing videos in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c756de",
   "metadata": {},
   "source": [
    "#### Netflix Open Content\n",
    "\n",
    "The [Netflix Open Content](https://opencontent.netflix.com/) is an open source content available under the [Creative Commons Attribution 4.0 International Public License](https://www.google.com/url?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2Flegalcode&sa=D&sntz=1&usg=AOvVaw3DDX6ldzWtAO5wOs5KkByf).\n",
    "\n",
    "The assets are available for download at: http://download.opencontent.netflix.com/\n",
    "\n",
    "We will be utilizing a subset of the videos for demonstrating how to utilize the TwelveLabs models on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374882c",
   "metadata": {},
   "source": [
    "## Download a Sample Video and Upload to S3 as Input\n",
    "We'll use the TwelveLabs Marengo model to generate embeddings from this video and perform content-based search.\n",
    "\n",
    "![Meridian](./assets/images/sample-video-meridian.png)\n",
    "We will use an open-source sample video, [Meridian](https://en.wikipedia.org/wiki/Meridian_(film)), as input to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample video to local disk\n",
    "sample_name = 'NetflixMeridian.mp4'\n",
    "source_url = f'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/NetflixMeridian.mp4'\n",
    "!curl {source_url} --output {sample_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3_video_output_path = bedrock_twelvelabs_helper.upload_video(sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d059e",
   "metadata": {},
   "source": [
    "### Part 2b: Creating vector embeddings with Marengo on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeee87d",
   "metadata": {},
   "source": [
    "#### TwelveLabs Marengo\n",
    "\n",
    "Marengo is an embedding model for comprehensive video understanding. Marengo analyzes multiple modalities in video content, including visuals, audio, and text, to provide a holistic understanding similar to human comprehension.\n",
    "\n",
    "***Key features***\n",
    "- **Multimodal processing:** Combines visual, audio, and text elements for comprehensive understanding\n",
    "- **Fine-grained search:** Detects brand logos, text, and small objects (as small as 10% of the video frame)\n",
    "- **Motion search:** Identifies and analyzes movement within videos\n",
    "- **Counting capabilities:** Accurately counts objects in video frames\n",
    "- **Audio comprehension:** Analyzes music, lyrics, sound, and silence\n",
    "\n",
    "***Use cases***\n",
    "- **Search:** Use natural language queries to find specific content within videos\n",
    "- **Embeddings:** Create video embeddings for various downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9914e4",
   "metadata": {},
   "source": [
    "#### Marengo Embed 2.7 on Bedrock\n",
    "\n",
    "A multimodal embedding model that generates high-quality vector representations of video, text, audio, and image content for similarity search, clustering, and other machine learning tasks. The model supports multiple input modalities and provides specialized embeddings optimized for different use cases.\n",
    "\n",
    "The model supports asynchronous inference through the [StartAsyncInvoke API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_StartAsyncInvoke.html).\n",
    "- Provider â€” TwelveLabs\n",
    "- Categories â€” Embeddings, multimodal\n",
    "- Model ID â€” `twelvelabs.marengo-embed-2-7-v1:0`\n",
    "- Input modality â€” Video, Text, Audio, Image\n",
    "- Output modality â€” Embeddings\n",
    "- Max video size â€” 2 hours long video (< 2GB file size)\n",
    "\n",
    "**Resources:**\n",
    "- [AWS Docs: TwelveLabs Marengo Embed 2.7](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-marengo.html)\n",
    "- [TwelveLabs Docs: Marengo](https://docs.twelvelabs.io/v1.3/docs/concepts/models/marengo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67765c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create text embedding\n",
    "text_query = \"two people having a conversation in a car\"\n",
    "text_embedding_data = bedrock_twelvelabs_helper.create_text_embedding(text_query)\n",
    "print(f\"âœ… Text embedding created successfully with {len(text_embedding_data)} segment and {len(text_embedding_data[0]['embedding'])} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad120cc",
   "metadata": {},
   "source": [
    "Creating an image embedding with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image\n",
    "image_path = \"images/meridian-image-search.jpg\"\n",
    "image_embedding_data = bedrock_twelvelabs_helper.create_image_embedding(image_path)\n",
    "print(f\"âœ… Image embedding created successfully with {len(image_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefc6b8",
   "metadata": {},
   "source": [
    "Creating video embeddings with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39863a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_embedding_data, video_id = bedrock_twelvelabs_helper.create_video_embedding(s3_video_output_path)\n",
    "print(f\"âœ… Video embedding created successfully with {len(video_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7864241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in video_embedding_data if x[\"embeddingOption\"] == \"visual-image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be032a",
   "metadata": {},
   "source": [
    "# Part 2c: Creating a vector index in OpenSearch Serverless\n",
    "[OpenSearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html) makes it easy to create vector indexes for storing and searching embeddings from text, images, audio and videos. \n",
    "\n",
    "As a vector database, OpenSearch serverless allows you to quickly find similar content using semantic search without worrying about managing servers or infrastructure. In the following section, we'll use the Python API to set up an OpenSearch Serverless collection and index, load embeddings from the TwelveLabs Marengo model into our vector store, and execute semantic queries to discover the most relevant content within our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654b2f0",
   "metadata": {},
   "source": [
    "Create an OpenSearch Serverless collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name_prefix = \"twelvelabs-marengo-blog\"\n",
    "bedrock_twelvelabs_helper.create_opensearch_serverless_collection(collection_name_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc888adb",
   "metadata": {},
   "source": [
    "Setup an index for the OpenSearch Serverless collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_prefix=\"twelvelabs-marengo-blog-index\"\n",
    "index_name = bedrock_twelvelabs_helper.create_opensearch_index(index_name_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b534b",
   "metadata": {},
   "source": [
    "Ingesting video embeddings created from Marengo model into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_twelvelabs_helper.index_video_embeddings(video_embedding_data, video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3e542",
   "metadata": {},
   "source": [
    "Perform text semantic search on the video embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320f9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"a person smoking in a room\"\n",
    "text_search_results = bedrock_twelvelabs_helper.search_videos_by_text(text_query, top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173d45",
   "metadata": {},
   "source": [
    "Visualize the top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7898376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top result\n",
    "top_text_result = text_search_results[0]\n",
    "video_url = bedrock_twelvelabs_helper.find_video_from_embedding(top_text_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a41006",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = top_text_result[\"start_time\"]\n",
    "play_video(video_url, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eef988",
   "metadata": {},
   "source": [
    "Perform text semantic search on the video embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_query = \"images/meridian-image-search.jpg\"\n",
    "display(Image(filename=image_query, width=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_search_results = bedrock_twelvelabs_helper.search_videos_by_image(image_path=image_query, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27af57",
   "metadata": {},
   "source": [
    "Visualize the top results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_image_result = image_search_results[0]\n",
    "video_url = bedrock_twelvelabs_helper.find_video_from_embedding(top_image_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_image_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the video\n",
    "play_video(video_url, top_image_result[\"start_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1aa595",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74a23c",
   "metadata": {},
   "source": [
    "#### Delete OpenSearchServerless Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete OpenSearch index\n",
    "try:\n",
    "    index_name = bedrock_twelvelabs_helper.index_name\n",
    "    response = bedrock_twelvelabs_helper.opensearch_client.indices.delete(\n",
    "        index=index_name\n",
    "    )\n",
    "    print(f\"Index '{index_name}' deletion response: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting index '{index_name}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fb0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_client = boto3.client('opensearchserverless')\n",
    "collection_id = bedrock_twelvelabs_helper.opensearch_serverless_collection_name[\"createCollectionDetail\"][\"id\"]\n",
    "response = aoss_client.delete_collection(id=collection_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41eee6",
   "metadata": {},
   "source": [
    "#### Empty S3 bucket content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = bedrock_twelvelabs_helper.s3_bucket_name\n",
    "delete_s3_bucket_objects(s3_client, s3_bucket_name, \"images\")\n",
    "delete_s3_bucket_objects(s3_client, s3_bucket_name, \"videos\")\n",
    "delete_s3_bucket_objects(s3_client, s3_bucket_name, \"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137dd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
