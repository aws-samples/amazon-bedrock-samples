{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Interface - Medical Clinic\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps. In this notebook, we will build a chatbot using two popular Foundation Models (FMs) in Amazon Bedrock, Claude V3 Sonnet and Llama 3 8b.\n",
    "\n",
    "\n",
    "## Chatbot using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up: Introduction to ChatBedrock and prompt templates\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the  set up libraries if you do not have the versions installed ⚠️ ⚠️ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community>=0.2.12, langchain-core>=0.2.34\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "#     \"langchain>=0.2.14\" \\\n",
    "#     \"faiss-cpu>=1.7,<2\" \\\n",
    "#     \"pypdf>=3.8,<4\" \\\n",
    "#     \"ipywidgets>=7,<8\" \\\n",
    "#     matplotlib>=3.9.0 \\\n",
    "#     \"langchain-aws>=0.1.17\"\n",
    "#%pip install -U --no-cache-dir boto3\n",
    "#%pip install grandalf==3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up classes\n",
    "\n",
    "- helper methods to set up the boto 3 connection client which wil be used in any class used to connect to Bedrock\n",
    "- this method accepts parameters like `region` and `service` and if you want to `assume any role` for the invocations\n",
    "- if you set the  AWS credentials then it will use those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "def get_boto_client_tmp_cred(\n",
    "    retry_config = None,\n",
    "    target_region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        aws_session_token=os.getenv('AWS_SESSION_TOKEN',\"\"),\n",
    "\n",
    "    )\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client    \n",
    "\n",
    "def get_boto_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\", None)\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        signature_version = 'v4',\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "    else: # use temp credentials -- add to the client kwargs\n",
    "        print(f\"  Using temp credentials\")\n",
    "\n",
    "        return get_boto_client_tmp_cred(retry_config=retry_config,target_region=target_region, runtime=runtime, service_name=service_name)\n",
    "\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the connectivity to Bedrock service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)\n",
    "\n",
    "#os.environ[\"AWS_PROFILE\"] = '<replace with your profile if you have that set up>'\n",
    "region_aws = 'us-east-1' #- replace with your region\n",
    "boto3_client =  get_boto_client(region=region_aws, runtime=False, service_name='bedrock') # switch to the region of your choice\n",
    "boto3_client.list_foundation_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boto3 client\n",
    "- Create the run time client which we will use to run through the various classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "  Using temp credentials\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"AWS_PROFILE\"] = '<replace with your profile if you have that set up>'\n",
    "region_aws = 'us-east-1' #- replace with your region\n",
    "boto3_bedrock = get_boto_client(region=region_aws, runtime=True, service_name='bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Expression Language (LCEL):\n",
    "According to LangChain: *\"LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production).\"*\n",
    "\n",
    "On this tutorial we will be using **LangChain Expression Language** to define and invoke our Chatbots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Architectures\n",
    "Chatbots can come in many shape and sizes, all depending on its use-case. Some models are meant to return general information. Others might be catered to a particular audience thus its inferences might be curtained to a particular tone. And others might need relevant context to give out an informed response to the user. Most robust ones will draw from all architectures and build on it. Below are a few popular types of Chatbots.\n",
    "\n",
    "1. **Chatbot (Naive)** - Zero-Shot chatbot with using FM model trained knowledge.\n",
    "2. **Chatbot using prompt** - Template driven - Chatbot with some context provided in the prompt template.\n",
    "3. **Chatbot with persona** - Chatbot with defined roles. i.e. Career Coach and Human interactions.\n",
    "4. **Contextual-aware chatbot** - Passing in context through an external file by generating embeddings.\n",
    "\n",
    "On this demo we will build a chatbot that will leverage various prompting architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chatbot: Naive approach (without context)\n",
    "\n",
    "The simplest form of chatbot is one that responds to the user simply by answering the question or completing the task. At the core these chatbots use the knowledge they were trained on to mimic human text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:10.398455Z",
     "start_time": "2024-09-15T18:07:07.101318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In classical physics, energy is thought of as a continuous spectrum, meaning that it can take on any value within a certain range. However, in quantum mechanics, energy is quantized, meaning that it comes in discrete packets or \"quanta\".\n",
      "\n",
      "Think of it like a piano keyboard. In classical music, you can play any note on the piano, and the note can have any pitch or volume. But in quantum mechanics, the piano is more like a xylophone, where each key can\n",
      "--- Latency: 1425ms - Input tokens:58 - Output tokens:100 ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nQuantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, such as atoms and subatomic particles. It provides a new and different framework for understanding physical phenomena, and it has been incredibly successful in explaining many experimental results and predicting new phenomena.\\n\\nThe core idea of quantum mechanics is that, at the atomic and subatomic level, particles such as electrons and photons do not have definite positions and properties until they are measured. Instead, they exist in a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "modelId = 'meta.llama3-8b-instruct-v1:0'\n",
    "\n",
    "messages_list=[\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"What is quantum mechanics? \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'assistant', \n",
    "        \"content\":[{\n",
    "            'text': \"It is a branch of physics that describes how matter and energy interact with discrete energy values \"\n",
    "        }]\n",
    "    },\n",
    "    { \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': \"Can you explain a bit more about discrete energies?\"\n",
    "        }]\n",
    "    }\n",
    "]\n",
    "\n",
    "    \n",
    "response = boto3_bedrock.converse(\n",
    "    messages=messages_list, \n",
    "    modelId='meta.llama3-8b-instruct-v1:0',\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0.5,\n",
    "        \"maxTokens\": 100,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    ")\n",
    "response_body = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "\n",
    "print(response_body)\n",
    "\n",
    "\n",
    "def invoke_meta_converse(prompt_str,boto3_bedrock ):\n",
    "    modelId = \"meta.llama3-8b-instruct-v1:0\"\n",
    "    messages_list=[{ \n",
    "        \"role\":'user', \n",
    "        \"content\":[{\n",
    "            'text': prompt_str\n",
    "        }]\n",
    "    }]\n",
    "  \n",
    "    response = boto3_bedrock.converse(\n",
    "        messages=messages_list, \n",
    "        modelId=modelId,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.5,\n",
    "            \"maxTokens\": 100,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "    response_body = response['output']['message']['content'][0]['text']\n",
    "    return response_body\n",
    "\n",
    "\n",
    "invoke_meta_converse(\"what is quantum mechanics\", boto3_bedrock)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ChatBedrock` and `HumanMessage` objects to wrap up our message and invoke the LLM\n",
    "\n",
    "- `ChatBedrock` abstracts out the complexity of the invocations. The above boiler plat code can be reduced to a few lines like below\n",
    "- Due to to the converse API the messages gets formulated correctly.\n",
    "- we use the message api format for all of our conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:18.951273Z",
     "start_time": "2024-09-15T18:07:12.454423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSeattle, Washington is known for its mild and wet climate, with significant rainfall throughout the year. Here's a breakdown of the typical weather patterns in Seattle:\\n\\n1. Rainfall: Seattle is famous for its rain, with an average annual rainfall of around 37 inches (94 cm). The rainiest months are November to March, with an average of 15-20 rainy days per month.\\n2. Temperature: Seattle's average temperature ranges from 35°F (2°C) in January (the coldest month) to 77°F (25°C) in July (the warmest month). The average temperature is around 50°F (10°C) throughout the year.\\n3. Sunshine: Seattle gets an average of 154 sunny days per year, with the sunniest months being July and August. However, the sun can be obscured by clouds and fog, reducing the amount of direct sunlight.\\n4. Fog: Seattle is known for its fog, especially during the winter months. The city can experience fog for several days at a time, especially in the mornings.\\n5. Wind: Seattle is known for its strong winds, especially during the winter months. The city can experience gusts of up to 40 mph (64 km/h) during storms.\\n6. Snow: Seattle rarely sees significant snowfall, with an average annual snowfall of around 6 inches (15 cm). The snowiest month is usually January, with an average of 1-2 inches (2.5-5 cm) of snow.\\n7. Summer: Seattle's summer months (June to August) are mild and pleasant, with average highs in the mid-70s to mid-80s (23-30°C). However, the city can experience occasional heatwaves, with temperatures reaching up to 90°F (32°C) or more.\\n8. Winter: Seattle's winter months (December to February) are cool and wet, with average lows in the mid-30s to mid-40s (2-7°C). The city can experience occasional cold snaps, with temperatures dropping below 20°F (-7°C) for short periods.\\n\\nOverall, Seattle's weather is characterized by mild temperatures, significant rainfall, and overcast skies. It's essential to pack layers and waterproof clothing when visiting the city, especially during the winter months.\", response_metadata={'ResponseMetadata': {'RequestId': '15249142-c037-4a60-8e4e-2e683d211181', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 07 Oct 2024 04:22:46 GMT', 'content-type': 'application/json', 'content-length': '2211', 'connection': 'keep-alive', 'x-amzn-requestid': '15249142-c037-4a60-8e4e-2e683d211181'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 6071}}, id='run-9bdfa258-85fc-4cd9-9a31-f69692abc9b4-0', usage_metadata={'input_tokens': 22, 'output_tokens': 472, 'total_tokens': 494})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "bedrock_llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with a role: Adding prompt templates, Zero Shot\n",
    "Getting a factual response is important, yet another aspect to keep in mind when building a Chatbot is the tone used in giving a user the response. Prompts are a powerful way of dictating the model what the tone and approach to answer question should be.\n",
    "\n",
    "1. You can define prompts as a list of messages, all modes expect SystemMessage, and then alternate with HumanMessage and AIMessage\n",
    "2. The Variables defined in the chat template need to be sent into the chain as dict with the keys being the variable names\n",
    "3. You can define the template as a tuple with (\"system\", \"message\") or can be using the class SystemMessage \n",
    "4. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:24.678541Z",
     "start_time": "2024-09-15T18:07:22.054213Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( \n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pirate_chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    "    | bedrock_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print_ww(pirate_chain.invoke({\"input\":\"What is the weather in Texas?\"})) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding prompt templates \n",
    "\n",
    "1. You can define prompts as a list of messages, all models expect `SystemMessage`, and then alternate with `HumanMessage` and `AIMessage`\n",
    "2. This would imply `Context` needs to be part of the System message \n",
    "3. Further the `CHAT HISTORY` needs to be right after the system message as a MessagePlaceholder which is a list of alternating [Human/AI]\n",
    "4. The Variables defined in the chat template need to be send into the chain as dict with the keys being the variable names\n",
    "5. You can define the template as a tuple with (\"system\", \"message\") or can be using the in-built classes from LangChain like class SystemMessage \n",
    "6. Invoke creates a final resulting object of type <class 'langchain_core.prompt_values.ChatPromptValue'> with the variables substituted with their values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you\n",
      "can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"\\n    You are an assistant for question-answering tasks. ONLY Use\n",
      "the following pieces of retrieved context to answer the question.\\n    If the answer is not in the\n",
      "context below , just say you do not have enough context. \\n    If you don't know the answer, just\n",
      "say that you don't know. \\n    Use three sentences maximum and keep the answer concise.\\n\n",
      "Context: this is a test context \\n    \"), HumanMessage(content='test_input')]\n",
      "\n",
      "\n",
      "messages=[SystemMessage(content=\"You are an assistant for question-answering tasks. Use the\n",
      "following pieces of retrieved context to answer the question. If you don't know the answer, say that\n",
      "you don't know. Use three sentences maximum and keep the answer concise.\\n\\nthis is a test\n",
      "context\"), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy\n",
      "matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in\n",
      "Seattle.\"), HumanMessage(content='Explain this  test_input.')]\n",
      "\n",
      "\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat_history_messages = [\n",
    "        HumanMessage(\"What is the weather like in Seattle WA?\"), # - normal string converts it to a Human message always but we need ai/human pairs\n",
    "        AIMessage(\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages( # can create either as System Message Object or as TUPLE -- system, message\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"), # this assumes the messages are in list of messages format and this becomes MessagePlaceholder object\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- variable chat_history should be a list of base messages, got test_chat_history of type <class 'str'>\n",
    "#- this gets converted as a LIST of messages -- with each of the TUPLE or Object being executed with the variables when invoked\n",
    "print_ww(prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages}))\n",
    "\n",
    "# -- condense question prompt with CONTEXT\n",
    "condense_question_system_template = (\n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. ONLY Use the following pieces of retrieved context to answer the question.\n",
    "    If the answer is not in the context below , just say you do not have enough context. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context} \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_question_system_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "#- missing variables {'context'}. chat history will get ignored - variables are passed in as keys in the dict\n",
    "print(\"\\n\")\n",
    "print_ww(condense_question_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "# - Chat prompt template with Place holders\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"placeholder\", \"{contex}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"Explain this  {input}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print_ww(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"}))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(type(qa_prompt.invoke({\"input\":\"test_input\", \"chat_history\": chat_history_messages, \"context\": \"this is a test context\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of `Runnable`\n",
    "\n",
    "In LangChain all classes are `Runnable` which means they have the API's for invoke, ainvoke,..... ando so on. We demostarte this with a simple example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a pirate. Answer the following questions as best you can.'), HumanMessage(content='What is the weather like in Seattle WA?'), AIMessage(content=\"Ahoy matey! As a pirate, I don't spend much time on land, but I've heard tales of the weather in Seattle.\"), HumanMessage(content='test_input')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a pirate. Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ").invoke({'input': 'test_input', 'chat_history' : chat_history_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agents prompt template\n",
    "\n",
    "1. Use the below as an example -- we can create the template in any form, you can see the final result is the same\n",
    "2. Using from_messages will automatically create the variables required for the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crafted::prompt:template :EXPLICIT SYSTEM:HUMAN:input_variables=['agent_scratchpad', 'input']\n",
      "messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad',\n",
      "'input'], template='\\n\\nUse the following format:\\nQuestion: the input question you must\n",
      "answer\\nThought: you should always think about what to do, Also try to follow steps mentioned\n",
      "above\\nAction: the action to take, should be one of [ \"get_lat_long\", \"get_weather\"]\\nAction Input:\n",
      "the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action\n",
      "Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final\n",
      "answer to the original input question\\n\\nQuestion:\n",
      "{input}\\n\\nAssistant:\\n{agent_scratchpad}\\'\\n\\n')),\n",
      "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]\n",
      "\n",
      "Crafted::prompt:template :USING CONTSTRUCTOR:input_variables=['agent_scratchpad', 'input',\n",
      "'input_human']\n",
      "messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad',\n",
      "'input'], template='\\n\\nUse the following format:\\nQuestion: the input question you must\n",
      "answer\\nThought: you should always think about what to do, Also try to follow steps mentioned\n",
      "above\\nAction: the action to take, should be one of [ \"get_lat_long\", \"get_weather\"]\\nAction Input:\n",
      "the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action\n",
      "Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final\n",
      "answer to the original input question\\n\\nQuestion:\n",
      "{input}\\n\\nAssistant:\\n{agent_scratchpad}\\'\\n\\n')),\n",
      "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_human'],\n",
      "template='{input_human}'))]\n",
      "\n",
      "\n",
      "Crafted::prompt:template::FROM_MESSAGESinput_variables=['agent_scratchpad', 'input', 'input_human']\n",
      "messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad',\n",
      "'input'], template='\\n\\nUse the following format:\\nQuestion: the input question you must\n",
      "answer\\nThought: you should always think about what to do, Also try to follow steps mentioned\n",
      "above\\nAction: the action to take, should be one of [ \"get_lat_long\", \"get_weather\"]\\nAction Input:\n",
      "the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action\n",
      "Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final\n",
      "answer to the original input question\\n\\nQuestion:\n",
      "{input}\\n\\nAssistant:\\n{agent_scratchpad}\\'\\n\\n')),\n",
      "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_human'],\n",
      "template='{input_human}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "prompt_template_sys = \"\"\"\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above\n",
    "Action: the action to take, should be one of [ \"get_lat_long\", \"get_weather\"]\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}'\n",
    "\n",
    "\"\"\"\n",
    "messages=[\n",
    "    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template=prompt_template_sys)), \n",
    "    HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))\n",
    "]\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print_ww(f\"\\nCrafted::prompt:template :EXPLICIT SYSTEM:HUMAN:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input'], \n",
    "    messages = [\n",
    "        (\"system\", prompt_template_sys),\n",
    "        (\"human\", \"{input_human}\"),\n",
    "    ]\n",
    ")\n",
    "print_ww(f\"\\nCrafted::prompt:template :USING CONTSTRUCTOR:{chat_prompt_template}\")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    messages = [\n",
    "        (\"system\", prompt_template_sys),\n",
    "        (\"human\", \"{input_human}\"),\n",
    "    ]\n",
    ")\n",
    "print_ww(f\"\\n\\nCrafted::prompt:template::FROM_MESSAGES{chat_prompt_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot approach to prompting: Selecting the right prompt for the job\n",
    "\n",
    "Chatbots can be powerful tools for automatising workflows and relaying accurate information, however sometimes the most difficult aspect of building a chatbot is conveying the right tone at the right time and with the right context. Something that one shot prompting (one time prompt at the beginning) seems to struggle with to address this issue we can introduce few-shot prompting to our chat. The goal of few-shot prompt is to dynamically select examples based on an input, and then format the examples into a final prompt to provide for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:07:27.303840Z",
     "start_time": "2024-09-15T18:07:27.300690Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "## In this example we use an in memory store called FAISS and AWS Titan Embeddings model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:09.906261Z",
     "start_time": "2024-09-15T18:33:09.252912Z"
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"What is the weather in New York city?\", \"output\": \"Respond using professional weatherman terminology:\\n\"},\n",
    "    {\"input\": \"Tell me a joke\", \"output\": \"Respond as if you were a famous comedian that likes to joke about farm animals:\\n\"},\n",
    "    {\"input\": \"Give me book recommendations\", \"output\": \"Respond as if you are a mistery novels fan:\\n\"},\n",
    "    {\"input\": \"There is something wrong with my computer\", \"output\": \"Respond as if you are a former pirate turned IT repair expert:\\n\"},\n",
    "]\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = br_embeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(to_vectorize, embeddings, metadatas=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a vectorstore created, we can create the `example_selector` this object contains our vectorized prompt examples for later retrieval. Here we will instruct it to only fetch the top 1 example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:11.396302Z",
     "start_time": "2024-09-15T18:33:11.231605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'There is something wrong with my computer',\n",
       "  'output': 'Respond as if you are a former pirate turned IT repair expert:\\n'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"Is it hot out?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:11.949138Z",
     "start_time": "2024-09-15T18:33:11.945249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You curtain your answer based on the examples provided\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:15.146718Z",
     "start_time": "2024-09-15T18:33:13.202857Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = final_prompt | bedrock_llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"What is the weather in Seattle WA?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:34:34.717339Z",
     "start_time": "2024-09-15T18:34:34.713407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nAccording to current weather conditions, Seattle, Washington is experiencing a mix of overcast skies with a high pressure system dominating the region. The current temperature is around 55°F (13°C) with a relative humidity of 70%. Winds are moderate, blowing at approximately 10 mph (16 km/h) from the west. There is a slight chance of scattered showers throughout the day, with a high of 58°F (14°C) and a low of 48°F (9°C) expected.', response_metadata={'ResponseMetadata': {'RequestId': 'd00c64d3-330e-4685-b1ee-41595a35a6c9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 07 Oct 2024 04:38:16 GMT', 'content-type': 'application/json', 'content-length': '628', 'connection': 'keep-alive', 'x-amzn-requestid': 'd00c64d3-330e-4685-b1ee-41595a35a6c9'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': 1342}}, id='run-9a008ace-111c-4b30-a4ce-443a9d4e392c-0', usage_metadata={'input_tokens': 57, 'output_tokens': 103, 'total_tokens': 160})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:15.653489Z",
     "start_time": "2024-09-15T18:33:15.650302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nAccording to current weather conditions, Seattle, Washington is experiencing a mix of overcast skies with a high pressure system dominating the region. The current temperature is around 55°F (13°C) with a relative humidity of 70%. Winds are moderate, blowing at approximately 10 mph (16 km/h) from the west. There is a slight chance of scattered showers throughout the day, with a high of 58°F (14°C) and a low of 48°F (9°C) expected.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:33:20.599339Z",
     "start_time": "2024-09-15T18:33:17.444751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI\\'m a huge fan of mystery and thriller movies! Here are a few recommendations:\\n\\n1. \"Seven\" (1995) - A gritty and intense crime thriller that follows two detectives as they hunt for a serial killer.\\n2. \"Memento\" (2000) - A mind-bending neo-noir that tells the story of a man with short-term memory loss trying to avenge his wife\\'s murder.\\n3. \"Prisoners\" (2013) - A tense and emotional thriller about two families whose daughters go missing and the desperate measures they take to find them.\\n4. \"Gone Girl\" (2014) - A twisty and suspenseful adaptation of Gillian Flynn\\'s bestselling novel about a couple whose seemingly perfect marriage turns out to be a facade.\\n5. \"Knives Out\" (2019) - A clever and entertaining whodunit that follows a detective as he investigates the murder of a wealthy author.\\n\\nI hope you enjoy these recommendations!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"Do you have any movie recommendations?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T18:34:08.500117Z",
     "start_time": "2024-09-15T18:34:06.579198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nArrr, shiver me bytes! Computer crashin', eh? Alright then, matey, let's set sail fer troubleshootin'!\\n\\nFirst, tell me, when did this start happenin'? Was it sudden, or did it start slowin' down like a ship in shallow waters?\\n\\nAnd what's the error message sayin'? Is it a blue screen o' death, or just a plain ol' freeze?\\n\\nAlso, have ye made any recent changes to yer computer, like installin' new software or addin' hardware?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"My computer keeps crashing, can you help me fix it?\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
