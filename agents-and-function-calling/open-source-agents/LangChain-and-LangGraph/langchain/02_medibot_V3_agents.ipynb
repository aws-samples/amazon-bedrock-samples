{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6282e31b964b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "my_config = Config(\n",
    "    region_name = 'us-west-2',\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe0738",
   "metadata": {},
   "source": [
    "## Set up: Introduction to ChatBedrock and prompt templates\n",
    "\n",
    "**Supports the following**\n",
    "1. Multiple Models from Bedrock \n",
    "2. Converse API\n",
    "3. Ability to do tool binding\n",
    "4. Ability to plug with LangGraph flows\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the  set up libraries if you do not have the versions installed ⚠️ ⚠️ ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain-community>=0.2.12, langchain-core>=0.2.34\n",
    "# %pip install -U --no-cache-dir  \\\n",
    "#     \"langchain>=0.2.14\" \\\n",
    "#     \"faiss-cpu>=1.7,<2\" \\\n",
    "#     \"pypdf>=3.8,<4\" \\\n",
    "#     \"ipywidgets>=7,<8\" \\\n",
    "#     matplotlib>=3.9.0 \\\n",
    "#     \"langchain-aws>=0.1.17\"\n",
    "#%pip install -U --no-cache-dir boto3\n",
    "#%pip install grandalf==3.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744043b0",
   "metadata": {},
   "source": [
    "### Set up classes\n",
    "\n",
    "- helper methods to set up the boto 3 connection client which wil be used in any class used to connect to Bedrock\n",
    "- this method accepts parameters like `region` and `service` and if you want to `assume any role` for the invocations\n",
    "- if you set the  AWS credentials then it will use those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "def get_boto_client_tmp_cred(\n",
    "    retry_config = None,\n",
    "    target_region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        aws_session_token=os.getenv('AWS_SESSION_TOKEN',\"\"),\n",
    "\n",
    "    )\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client    \n",
    "\n",
    "def get_boto_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "    service_name: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\", None)\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        signature_version = 'v4',\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "    else: # use temp credentials -- add to the client kwargs\n",
    "        print(f\"  Using temp credentials\")\n",
    "\n",
    "        return get_boto_client_tmp_cred(retry_config=retry_config,target_region=target_region, runtime=runtime, service_name=service_name)\n",
    "\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if not service_name:\n",
    "        if runtime:\n",
    "            service_name='bedrock-runtime'\n",
    "        else:\n",
    "            service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db9184",
   "metadata": {},
   "source": [
    "### Boto3 client\n",
    "- Create the run time client which we will use to run through the various classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5cc44b316a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"AWS_PROFILE\"] = '<replace with your profile if you have that set up>'\n",
    "region_aws = 'us-east-1' #- replace with your region\n",
    "boto3_bedrock = get_boto_client(region=region_aws, runtime=True, service_name='bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29b5745d95a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "# from langchain_community.chat_models import BedrockChaat\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "llm = ChatBedrock(client=boto3_bedrock, #credentials_profile_name='~/.aws/credentials',\n",
    "                  model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                  model_kwargs=dict(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e231aa3edf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"what is the weather like in Seattle WA\"\n",
    "    )\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d730629ec3d2528",
   "metadata": {},
   "source": [
    "## Naive inferencing: The root challenge in creating Chatbots and Virtual Assistants and the Agentic solution:\n",
    "\n",
    "As seen in previous tutorials, LLM conversational interfaces such as chatbots or virtual assistants can be used to enhance the user experience of customers. These can be improved even more by giving them context from related sources such as chat history, documents, websites, social media platforms, and / or messaging apps, this is called RAG (Retrieval Augmented Generation) and is a fundamental backbone of designing robust AI solutions. \n",
    "\n",
    "One persistent bottleneck however is the inability of LLMs to assess whether data extracted and or its response, based on said data, is accurate and fully encapsulates a user requests (hallucinating). A way to mitigate this risk brought up by naive, inferencing with RAG is through the use of Agents. Agents are defined as a workflow that uses data, tools, and its own inferences to check that the response provided is accurate and meets users goals.\n",
    "\n",
    "![Amazon Bedrock - Agents Interface](./images/agents.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66929125b85703cc",
   "metadata": {},
   "source": [
    "### Key Elements of Agents\n",
    " \n",
    "- Agents are designed for tasks that require multistep reasoning; Think questions that intuitively require multiple steps, for example how old was Henry Ford when he founded his company.\n",
    "- They are designed to plan ahead, remember past actions and check its own responses.\n",
    "- Agents can be made to deconstruct complex requests into manageable smaller sub-tasks such as data retrieval, comparison and tool usage.\n",
    "- Agents might be designed as standalone solutions or paired with other agents to enhance the agentic workflow.\n",
    "\n",
    "\n",
    " Let's build an agentic workflow from scratch to see how it works, for this use case we will use Calude 3 Sonnet to power our agentic workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed423080bc7eb4",
   "metadata": {},
   "source": [
    "### Architecture [Retriever with LangGraph]\n",
    "\n",
    "The core benefit of agentic workflows lies in its flexibility to adjust to your needs. You have full control on the design the flow by properly defining what the agents do and what tools and information is available to them. One popular framework for the use of Agents is called Langgraph, a low-level framework that offers the ability of adding cycles (using previous inferences as context to either fix or build on it), controllability of the flow and state of your application, and persistence, giving the agents the ability to involve humans in the loop and the memory to recall past agentic flows.\n",
    "\n",
    "#### For this scenario we'll define 3 agents:\n",
    "\n",
    "1. We defined a supervisor agent responsible for deciding the steps needed to fulfill the users request, this can take the shape of using tools or data retrieval. \n",
    "2. Then a task-driven agent to retrieve documents which can be invoked only when the orchestrator agent deems it necessary to fulfill the users request. \n",
    "3. Finally, a data retriever agent will query an embedding database containing Medical history if its deemed necessary to use this information to answer the users question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1dc31c9f5e39b",
   "metadata": {},
   "source": [
    "### Dependencies and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dccd49d2398f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "import warnings\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588cc7b8894f329",
   "metadata": {},
   "source": [
    "### Build the retriever chain to be used with LangGraph\n",
    "1. Create `create_retriever_pain` which is used when the solution requires data retrieval from our documents\n",
    "2. Define the system prompt to enforce the correct use of context retrieved, it also ensures that the agent does not hallucinate\n",
    "3. Define the vectorstore using FAISS, a light weight in-memory vector DB and our documents stored in _'medi_history.csv'_\n",
    "4. Define the sessions persistent memory store for the agents use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595db06c23fa9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "def create_retriever_pain():\n",
    "\n",
    "    br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "    \n",
    "    loader = CSVLoader(\"./rag_data/medi_history.csv\") # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "    documents_aws = loader.load() #\n",
    "    print(f\"Number of documents={len(documents_aws)}\")\n",
    "\n",
    "    docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "    print(f\"Number of documents after split and chunking={len(docs)}\")\n",
    "        \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws: number of elements in the index={vectorstore_faiss_aws.index.ntotal}::\")\n",
    "\n",
    "    model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "    modelId = \"meta.llama3-8b-instruct-v1:0\" #\"anthropic.claude-v2\"\n",
    "    chatbedrock_llm = ChatBedrock(\n",
    "        model_id=modelId,\n",
    "        client=boto3_bedrock,\n",
    "        model_kwargs=model_parameter, \n",
    "        beta_use_converse_api=True\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If the answer is not present in the context, just say you do not have enough context to answer. \\\n",
    "    If the input is not present in the context, just say you do not have enough context to answer. \\\n",
    "    If the question is not present in the context, just say you do not have enough context to answer. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    question_answer_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "    pain_rag_chain = create_retrieval_chain(vectorstore_faiss_aws.as_retriever(), \n",
    "                                            question_answer_chain)\n",
    "\n",
    "    pain_retriever_chain = RunnableWithMessageHistory(\n",
    "        pain_rag_chain,\n",
    "        get_session_history=get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return pain_retriever_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964d2f6a7c5f74c",
   "metadata": {},
   "source": [
    "#### Testing the rag chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6aeb779b1dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pain_rag_chain = create_retriever_pain()    \n",
    "result = pain_rag_chain.invoke(\n",
    "    {\"input\": \"What all pain medications can be used for headache?\", \n",
    "     \"chat_history\": []},\n",
    "     config={'configurable': {'session_id': 'TEST-123'}},\n",
    ")\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5958bbfaf0d282",
   "metadata": {},
   "source": [
    "### Book / Cancel Appointments: An agent with tools:\n",
    "\n",
    "In this module we will create an agent responsible for booking and canceling doctor appointments. This agent will take a booking request to create or cancel an appointment and its action will be guided by the 4 tools available to it.\n",
    "1. _book_appointment_: Used by the agent to book an appointment give the users request as long as it meets the criteria, valid date and time within office hours.\n",
    "2. _cancel_appointment_: If an exiting appointment is found, it will remove its respective 'booking id' from the list of appointments.\n",
    "3. _reject_appointment_: If an appointment cannot be booked due to inability or invalid date or time the agent will use this tool to reject the users request.\n",
    "4. _need_more_info_: Returns the earliest date and time needed for the booking an appointment back to the agent as well as informing the agent that it should request further details from the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a6f83c40d33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from datetime import datetime, timedelta \n",
    "import dateparser\n",
    "\n",
    "\n",
    "appointments = ['ID_100'] # Default appointment\n",
    "def create_book_cancel_agent():\n",
    "    today = datetime.today()\n",
    "    tomorrow = today + timedelta(days=1)\n",
    "    formatted_tomorrow = tomorrow.strftime(\"%B %d, %Y\")\n",
    "    start_time = datetime.strptime(\"9:00 am\", \"%I:%M %p\").time()\n",
    "    end_time = datetime.strptime(\"5:00 pm\", \"%I:%M %p\").time()\n",
    "            \n",
    "    def check_date_time(date: str, time: str) -> str:\n",
    "        \"\"\"Helper function is used by book appointment tool to check that the date and time passed by the user are within the date time params\"\"\"\n",
    "        _date = dateparser.parse(date)\n",
    "        _time = dateparser.parse(time)\n",
    "        if not _date or not _time:\n",
    "            return 'ERROR: Date and time parameters are not valid'\n",
    "        \n",
    "        input_date = _date.date()\n",
    "        input_time = _time.time()\n",
    "        if input_date < tomorrow.date():\n",
    "            return f'ERROR: Appointment date must be at least one day from today: {today.strftime(\"%B %d, %Y\")}'\n",
    "        elif input_date.weekday() > 4:\n",
    "            return f'ERROR: Appointments are only available on weekdays, date {input_date.strftime(\"%B %d, %Y\")} falls on a weekend.'\n",
    "        elif start_time > input_time >= end_time:\n",
    "            return f'ERROR: Appointments bust be between the hours of 9:00 am to 5:00 pm'\n",
    "        return 'True'\n",
    "        \n",
    "        \n",
    "    @tool(\"book_appointment\")\n",
    "    def book_appointment(date: str, time: str) -> dict:\n",
    "        \"\"\"Use this function to book an appointment. This function returns the booking ID\"\"\"\n",
    "\n",
    "        print(date, time)\n",
    "        is_valid = check_date_time(date, time)\n",
    "        if 'ERROR' in is_valid :\n",
    "            return {\"status\" : False, \"date\": date, \"time\": time, \"booking_id\": is_valid}\n",
    "\n",
    "        last_appointment = appointments[-1]\n",
    "        new_appointment = f\"ID_{int(last_appointment[3:]) + 1}\"\n",
    "        appointments.append(new_appointment)\n",
    "            \n",
    "        return {\"status\" : True, \"date\": date, \"time\": time, \"booking_id\": new_appointment}\n",
    "    \n",
    "    @tool(\"reject_appointment\")\n",
    "    def reject_appointment() -> dict:\n",
    "        \"\"\"Use this function to reject an appointment if the status of book_appointment is False\"\"\"\n",
    "        return {\"status\" : False, \"date\": \"\", \"time\": \"\", \"booking_id\": \"\"}\n",
    "        \n",
    "    @tool(\"cancel_appointment\")\n",
    "    def cancel_appointment(booking_id: str) -> dict:\n",
    "        \"\"\"Use this function to cancel an existing appointment and remove it from the schedule. This function needs a booking id to cancel the appointment.\"\"\"\n",
    "\n",
    "        print(booking_id)\n",
    "        status = any(app == booking_id for app in appointments)\n",
    "        if not status:\n",
    "            booking_id = \"ERROR: No ID for given booking found. Please provide valid id\"\n",
    "        appointments.remove(booking_id)\n",
    "        return {\"status\" : status, \"booking_id\": booking_id}\n",
    "\n",
    "    @tool(\"need_more_info\")\n",
    "    def need_more_info() -> dict:\n",
    "        \"\"\"Use this function to get more information from the user. This function returns the earliest date and time needed for the booking an appointment \"\"\"\n",
    "        return {\"date after\": formatted_tomorrow, \"time between\": \"09:00 AM to 05:00 PM\", \"week day within\": \"Monday through Friday\"}\n",
    "\n",
    "\n",
    "    prompt_template_sys = \"\"\"\n",
    "    You are a booking assistant.\n",
    "    Make sure you use one the the following tools [\"book_appointment\", \"cancel_appointment\", \"need_more_info\", \"reject_appointment\"]\n",
    "    \"\"\"\n",
    "\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "            messages = [\n",
    "                (\"system\", prompt_template_sys),\n",
    "                (\"placeholder\", \"{chat_history}\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "                (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\" \n",
    "    model_parameter = {\"temperature\": 0.0, \"top_p\": .1, \"max_tokens_to_sample\": 400}\n",
    "    chat_bedrock_appointment = ChatBedrock(\n",
    "        model_id=model_id,\n",
    "        client=boto3_bedrock,\n",
    "        model_kwargs=model_parameter, \n",
    "        beta_use_converse_api=True\n",
    "    )\n",
    "\n",
    "    tools_list_book = [book_appointment, cancel_appointment, need_more_info, reject_appointment]\n",
    "\n",
    "    # Construct the Tools agent\n",
    "    book_cancel_agent_t = create_tool_calling_agent(chat_bedrock_appointment, \n",
    "                                                    tools_list_book, \n",
    "                                                    chat_prompt_template)\n",
    "    \n",
    "    agent_executor_t = AgentExecutor(agent=book_cancel_agent_t, \n",
    "                                     tools=tools_list_book, \n",
    "                                     verbose=True, \n",
    "                                     max_iterations=5, \n",
    "                                     return_intermediate_steps=True)\n",
    "    return agent_executor_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085f454df936ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9b197994867c",
   "metadata": {},
   "source": [
    "### Test the Booking Agent with history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab5607e63637af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add context for the agent to use\n",
    "book_cancel_history = InMemoryChatMessageHistory()\n",
    "book_cancel_history.add_user_message(\"can you book an appointment?\")\n",
    "book_cancel_history.add_ai_message(\"What is the date and time you wish for the appointment\")\n",
    "book_cancel_history.add_user_message(\"I need for Oct 10, 2023 at 10:00 am?\")\n",
    "\n",
    "user_query = \"can you book an appointment for me for September 14, 2024, at 10:00 am?\"\n",
    "agent_executor_book_cancel = create_book_cancel_agent()\n",
    "    \n",
    "result = agent_executor_book_cancel.invoke(\n",
    "    {\"input\": user_query, \n",
    "     \"chat_history\": book_cancel_history.messages}, \n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff688418e6dd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['output'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987b6d082233350",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_cancel_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe0f4b09bc4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor_book_cancel.invoke(\n",
    "    {\"input\": \"can you book an appointment for me?\", \"chat_history\": []}, \n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43930e2a951f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor_book_cancel.invoke({\"input\": \"can you cancel my appointment with booking id of ID_100\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3da629cf9efe25",
   "metadata": {},
   "source": [
    "### An AI doctor: Medical advice agent based on conversations with the patient\n",
    "This function will be the backbone of the language agent responsible for giving medical advice given the historical interactions the user had with the Chatbot. This model will use its knowledge of the medical field along with the conversations with the patient to give well founded advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae10938d36cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "\n",
    "def extract_chat_history(chat_history):\n",
    "    user_map = {'human':'user', 'ai':'assistant'}\n",
    "    if not chat_history:\n",
    "        chat_history = []\n",
    "    messages_list=[{'role':user_map.get(msg.type), 'content':[{'text':msg.content}]} for msg in chat_history]\n",
    "    return messages_list\n",
    "\n",
    "\n",
    "def ask_doctor_advice(boto3_bedrock, chat_history):\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" \n",
    "    response = boto3_bedrock.converse(\n",
    "        messages=chat_history,\n",
    "        modelId=modelId,\n",
    "        inferenceConfig={\n",
    "            \"temperature\": 0.5,\n",
    "            \"maxTokens\": 100,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    )\n",
    "    response_body = response['output']['message']['content'][0]['text']\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac44e7a7736d97",
   "metadata": {},
   "source": [
    "### Testing the AI Doc agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9fd27140a6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=InMemoryChatMessageHistory()\n",
    "chat_history.add_user_message(\"what are the effects of Asprin\")\n",
    "ask_doctor_advice(boto3_bedrock, extract_chat_history(chat_history.messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db4c014b3e75d3",
   "metadata": {},
   "source": [
    "### The supervisor agent, the orchestrator of the LangGraph workflow\n",
    "1. This agent has the list of tools / nodes it can invoke based on the nodes\n",
    "2. Based on that the supervisor will route and invoke the correct LangGraph chain and node\n",
    "3. Output will be a predefine chain of thought leveraging the available tools and agents to complete and validate the task\n",
    "4. `ToolsAgentOutputParser` is used to parse the output of the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8f54c32cabf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents.output_parsers.tools import ToolsAgentOutputParser\n",
    "\n",
    "\n",
    "members = [\"book_cancel_agent\",\"pain_retriever_chain\",\"ask_doctor_advice\" ]\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "def create_supervisor_agent():\n",
    "\n",
    "    prompt_finish_template_simple = \"\"\"\n",
    "    Given the conversation below who should act next?\n",
    "    1. To book or cancel an appointment return 'book_cancel_agent'\n",
    "    2. To answer question about pain medications return 'pain_retriever_chain'\n",
    "    3. To answer question about any medical issue return 'ask_doctor_advice'\n",
    "    4. If you have the answer return 'FINISH'\n",
    "    Or should we FINISH? ONLY return one of these {options}. Do not explain the process.Select one of: {options}\n",
    "    \n",
    "    {history_chat}\n",
    "    \n",
    "    Question: {input}\n",
    "\n",
    "    \"\"\"\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    supervisor_llm = ChatBedrock(\n",
    "        model_id=modelId,\n",
    "        client=boto3_bedrock,\n",
    "        beta_use_converse_api=True\n",
    "    )\n",
    "\n",
    "    supervisor_chain_t = (\n",
    "        RunnablePassthrough()\n",
    "        | ChatPromptTemplate.from_template(prompt_finish_template_simple)\n",
    "        | supervisor_llm\n",
    "        | ToolsAgentOutputParser()\n",
    "    )\n",
    "    return supervisor_chain_t\n",
    "\n",
    "supervisor_wrapped_chain = create_supervisor_agent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49944e6286a15534",
   "metadata": {},
   "source": [
    "### Test the supervisor agent\n",
    "Our supervisor will litigate the user query to the respective agent or end the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf9e73e8ef813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_messages = InMemoryChatMessageHistory()\n",
    "temp_messages.add_user_message(\"What does medical doctor do?\")\n",
    "\n",
    "supervisor_wrapped_chain.invoke({\n",
    "    \"input\": \"What does medical doctor do?\", \n",
    "    \"options\": options, \n",
    "    \"history_chat\": extract_chat_history(temp_messages.messages)\n",
    "})\n",
    "\n",
    "#  Adding Memory\n",
    "temp_message_2 = InMemoryChatMessageHistory()\n",
    "temp_message_2.add_user_message(\"Can you book an appointment for me?\")\n",
    "temp_message_2.add_ai_message(\"Sure I have booked the appointment booked for Sept 24, 2024 at 10 am\")\n",
    "\n",
    "response = supervisor_wrapped_chain.invoke({\n",
    "    \"input\": \"can you book an appointment for me?\", \n",
    "    \"options\": options, \n",
    "    \"history_chat\": extract_chat_history(temp_message_2.messages)})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3d86185b0af5f",
   "metadata": {},
   "source": [
    "### Putting it all together: Defining the Graph architecture\n",
    "1. The `GraphState` class defines how we want our nodes to behave  \n",
    "2. Wrap our agents into nodes that will take a graph state as input\n",
    "3. Short term or 'buffer' memory for the graph will be provided by the `ConversationBufferMemory` object\n",
    "4. Finally `add_user_message` and `add_ai_message` apis are used to add the messages to the buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b24e4572078bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Dict, Sequence, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class GraphState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next_node' field indicates where to route to next\n",
    "    next_node: str\n",
    "    # initial user query\n",
    "    user_query: str\n",
    "    # instantiate memory\n",
    "    convo_memory: InMemoryChatMessageHistory\n",
    "    # options for the supervisor agent to decide which node to follow\n",
    "    options: list\n",
    "    # session id for the supervisor since that is another option for managing memory\n",
    "    curr_session_id: str \n",
    "\n",
    "\n",
    "def input_first(state: GraphState) -> Dict[str, str]:\n",
    "    print_ww(f\"\"\"start input_first()....::state={state}::\"\"\")\n",
    "    init_input = state.get(\"user_query\", \"\").strip()\n",
    "    # store the input\n",
    "    convo_memory =  InMemoryChatMessageHistory()\n",
    "    convo_memory.add_user_message(init_input)\n",
    "    options = ['FINISH', 'book_cancel_agent', 'pain_retriever_chain', 'ask_doctor_advice'] \n",
    "    return {\"user_query\":init_input, \"options\": options, \"convo_memory\": convo_memory}\n",
    "\n",
    "\n",
    "def agent_node(state, final_result, name):\n",
    "    state.get(\"convo_memory\").add_ai_message(final_result)\n",
    "    print(f\"\\nAgent:name={name}::AgentNode:state={state}::return:result={final_result}:::returning END now\\n\")\n",
    "    return {\"next_node\": END, \"answer\": final_result}\n",
    "\n",
    "\n",
    "def retriever_node(state: GraphState) -> Dict[str, str]:\n",
    "    global pain_rag_chain\n",
    "    print_ww(f\"use this to go the retriever way to answer the question():: state::{state}\")    \n",
    "    init_input = state.get(\"user_query\", \"\").strip()\n",
    "    chat_history = extract_chat_history(state.get(\"convo_memory\").messages)\n",
    "    if pain_rag_chain == None:\n",
    "        pain_rag_chain = create_retriever_pain()    \n",
    "    \n",
    "    # This agent is used to get the context for any questions related to medical issues such as aches, headache or body pain\n",
    "    result = pain_rag_chain.invoke(\n",
    "        {\"input\": init_input, \"chat_history\": chat_history},\n",
    "        config={'configurable': {'session_id': 'TEST-123'}}\n",
    "    )\n",
    "    return agent_node(state, result['answer'], 'pain_retriever_chain')\n",
    "\n",
    "\n",
    "def doctor_advice_node(state: GraphState) -> Dict[str, str]:\n",
    "    print_ww(f\"use this to answer about the Doctors advice from FINE TUNED Model::{state}::\")\n",
    "    chat_history = extract_chat_history(state.get(\"convo_memory\").messages)\n",
    "    # init_input = state.get(\"user_query\", \"\").strip()\n",
    "    result = ask_doctor_advice(boto3_bedrock, chat_history) \n",
    "    return agent_node(state, result, name=\"ask_doctor_advice\")\n",
    "\n",
    "\n",
    "def book_cancel_node(state: GraphState) -> Dict[str, str]:\n",
    "    global book_cancel_agent, agent_executor_book_cancel\n",
    "    print_ww(f\"use this to book or cancel an appointment::{state}::\")\n",
    "    init_input = state.get(\"user_query\", \"\").strip()\n",
    "    agent_executor_book_cancel = create_book_cancel_agent()\n",
    "    \n",
    "    result = agent_executor_book_cancel.invoke(\n",
    "        {\"input\": init_input, \"chat_history\": state.get(\"convo_memory\").messages}, \n",
    "        config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    "    ) \n",
    "    ret_val = result['output'][0]['text']\n",
    "    return agent_node(state, ret_val, name=\"book_cancel_agent\")\n",
    "\n",
    "\n",
    "def error(state: GraphState) -> Dict[str, str]:\n",
    "    print_ww(f\"\"\"start error()::state={state}::\"\"\")\n",
    "    return {\"final_result\": \"error\", \"first_word\": \"error\", \"second_word\": \"error\"}\n",
    "\n",
    "\n",
    "def supervisor_node(state: GraphState) -> Dict[str, str]:\n",
    "    global supervisor_wrapped_chain\n",
    "    print_ww(f\"\"\"supervisor_node()::state={state}::\"\"\") \n",
    "    init_input = state.get(\"user_query\", \"\").strip()\n",
    "    options = state.get(\"options\", ['FINISH', 'book_cancel_agent', 'pain_retriever_chain', 'ask_doctor_advice']  )\n",
    "\n",
    "    convo_memory = state.get(\"convo_memory\")\n",
    "    print(f\"\\nsupervisor_node():History of messages so far :::{convo_memory.messages}\\n\")\n",
    "    \n",
    "    supervisor_wrapped_chain = create_supervisor_agent()    \n",
    "    result = supervisor_wrapped_chain.invoke({\n",
    "        \"input\": init_input, \n",
    "        \"options\": options, \n",
    "        \"history_chat\": extract_chat_history(convo_memory.messages)\n",
    "    })\n",
    "\n",
    "    print_ww(f\"\\n\\nsupervisor_node():result={result}......\\n\\n\")\n",
    "    return {\"next_node\": result.return_values[\"output\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3293b1a9cfb5f",
   "metadata": {},
   "source": [
    "## Set up the workflow:\n",
    "LangGraph works by seamlessly knitting together our agents into a coherent workflow allowing us to set up the flow that is essential for agentic architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a673858c236f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"pain_retriever_chain\", retriever_node)\n",
    "workflow.add_node(\"ask_doctor_advice\", doctor_advice_node)\n",
    "workflow.add_node(\"book_cancel_agent\", book_cancel_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_node)\n",
    "workflow.add_node(\"init_input\", input_first)\n",
    "print(workflow)\n",
    "\n",
    "members = ['pain_retriever_chain', 'ask_doctor_advice', 'book_cancel_agent', 'init_input'] \n",
    "print_ww(f\"members of the nodes={members}\")\n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next_node\"], conditional_map)\n",
    "\n",
    "# add end just for all the nodes  \n",
    "for member in members[:-1]:\n",
    "    workflow.add_edge(member, END)\n",
    "\n",
    "# entry node to supervisor\n",
    "workflow.add_edge(\"init_input\", \"supervisor\")\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"init_input\") \n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dc85d35ecd52c",
   "metadata": {},
   "source": [
    "##### Finally, we visualize the entire workflow to make sure it meets our expectations. In our usecase the supervisor can 'litigate' the work to other agents or end the workflow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00c6396596d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb0cd15aac33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"what is the general function of a doctor, what do they do?\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14171b337171ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"what are the effects of Asprin?\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e089034d26597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"what is the general function of a doctor, what do they do?\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0617c2b2e02104",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"Can you book an appointment for me?\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51426ef2e21ce8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"Can you book an appointment for Sept 24, 2024 10 am?\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88cfba4e344cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b9cb9993df34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(\n",
    "    {\"user_query\": \"can you cancel my appointment with booking id of ID_100\", \"recursion_limit\": 2, \"curr_session_id\": \"session_1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f1bbbd94ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "appointments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b62013c50da85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainenv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
