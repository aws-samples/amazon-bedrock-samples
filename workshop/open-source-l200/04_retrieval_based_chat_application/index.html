<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Amazon Bedrock cookbook website"><meta name=author content=Bedrock-GTM><link href=https://github.amazon-bedrock-samples.com/workshop/open-source-l200/04_retrieval_based_chat_application/ rel=canonical><link href=../03_retrieval_based_text_application/ rel=prev><link href=../05_agent_based_text_generation/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.49"><title>Retrieval Based Chat Application - Amazon Bedrock Recipes</title><link rel=stylesheet href=../../../assets/stylesheets/main.6f8fc17f.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#retrieval-augmented-generation-with-amazon-bedrock-enhancing-chat-applications-with-rag class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="Amazon Bedrock Recipes" class="md-header__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Amazon Bedrock Recipes </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Retrieval Based Chat Application </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Amazon Bedrock Recipes" class="md-nav__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../logo.png alt=logo> </a> Amazon Bedrock Recipes </label> <div class=md-nav__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Features </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> Intro to Amazon Bedrock </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Intro to Amazon Bedrock </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> API Usage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> API Usage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/01_invoke_api/ class=md-nav__link> <span class=md-ellipsis> Invoke Model API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/04_agents_api/ class=md-nav__link> <span class=md-ellipsis> Agents API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/03_knowledgebases_api/ class=md-nav__link> <span class=md-ellipsis> Knowledge Bases API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/02_guardrails_api/ class=md-nav__link> <span class=md-ellipsis> Guardrail API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/converse_api/01_converse_api/ class=md-nav__link> <span class=md-ellipsis> Converse API Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1> <label class=md-nav__link for=__nav_2_2_1 id=__nav_2_2_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/introduction-to-agents/how_to_create_custom_agents/ class=md-nav__link> <span class=md-ellipsis> How to create an Agent </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_2> <label class=md-nav__link for=__nav_2_2_1_2 id=__nav_2_2_1_2_label tabindex=0> <span class=md-ellipsis> Bedrock Agent Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_2> <span class="md-nav__icon md-icon"></span> Bedrock Agent Features </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/01-create-agent-with-function-definition/01-create-agent-with-function-definition/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Function Definition </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/02-create-agent-with-api-schema/02-create-agent-with-api-schema/ class=md-nav__link> <span class=md-ellipsis> Create Agent with API Schema </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/03-create-agent-with-return-of-control/03-create-agent-with-return-of-control/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/04-create-agent-with-single-knowledge-base/04-create-agent-with-single-knowledge-base/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Single Knowledge Base </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/05-create-agent-with-knowledge-base-and-action-group/05-create-agent-with-knowledge-base-and-action-group/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Knowledge Base and Action Group </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/06-prompt-and-session-attributes/06-prompt-and-session-attributes/ class=md-nav__link> <span class=md-ellipsis> Prompt and Session Attributes </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/07-advanced-prompts-and-custom-parsers/07-custom-prompt-and-lambda-parsers/ class=md-nav__link> <span class=md-ellipsis> Custom Prompt and Lambda Parsers </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/08-create-agent-with-guardrails/08-create-agent-with-guardrails/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Guardrails </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/09-create-agent-with-memory/09-create-agent-with-memory/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Memory </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/10-create-agent-with-code-interpreter/10-create-agent-with-code-interpreter/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Code Interpreter </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/14-create-agent-with-custom-orchestration/custom_orchestration_example/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Custom Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents/inline-agent-api-usage/ class=md-nav__link> <span class=md-ellipsis> Create Dynamic Tooling Inline Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_3> <label class=md-nav__link for=__nav_2_2_1_3 id=__nav_2_2_1_3_label tabindex=0> <span class=md-ellipsis> Bedrock Flows </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_3> <span class="md-nav__icon md-icon"></span> Bedrock Flows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/bedrock-flows/Getting_started_with_Prompt_Management_Flows/ class=md-nav__link> <span class=md-ellipsis> Getting Started with Prompt Management Flows </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_4> <label class=md-nav__link for=__nav_2_2_1_4 id=__nav_2_2_1_4_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_4> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/text-2-sql-agent/create_and_invoke_sql_agent/ class=md-nav__link> <span class=md-ellipsis> Text to SQL Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/agentsforbedrock-retailagent/workshop/test_retailagent_agentsforbedrock/ class=md-nav__link> <span class=md-ellipsis> Retail Agent Workshop </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/product-review-agent/main/ class=md-nav__link> <span class=md-ellipsis> Product Review Agent </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_2> <label class=md-nav__link for=__nav_2_2_2 id=__nav_2_2_2_label tabindex=0> <span class=md-ellipsis> Function Calling </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_2> <span class="md-nav__icon md-icon"></span> Function Calling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/function_calling_with_converse/function_calling_with_converse/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/function_calling_with_invoke/function_calling_model_with_invoke/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Invoke </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/return_of_control/return_of_control/ class=md-nav__link> <span class=md-ellipsis> Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/tool_binding/tool_bindings/ class=md-nav__link> <span class=md-ellipsis> Tool Binding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3> <label class=md-nav__link for=__nav_2_2_3 id=__nav_2_2_3_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_1> <label class=md-nav__link for=__nav_2_2_3_1 id=__nav_2_2_3_1_label tabindex=0> <span class=md-ellipsis> CrewAI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_1> <span class="md-nav__icon md-icon"></span> CrewAI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/crew.ai/Find%20dream%20destination%20with%20CrewAI/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_2> <label class=md-nav__link for=__nav_2_2_3_2 id=__nav_2_2_3_2_label tabindex=0> <span class=md-ellipsis> LangGraph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_2> <span class="md-nav__icon md-icon"></span> LangGraph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-single-agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-agents-multimodal/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi-Modal Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/02_medibot_V3_agents/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent For Medical Chatbot </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-fact-checker-feedback-loop/ class=md-nav__link> <span class=md-ellipsis> LangGraph Fact Checker with Multi Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/03_langgraph_agents_of_agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent with tools </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/Travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Managing Memory for Multi Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_3> <label class=md-nav__link for=__nav_2_2_3_3 id=__nav_2_2_3_3_label tabindex=0> <span class=md-ellipsis> Multi Agent </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_3> <span class="md-nav__icon md-icon"></span> Multi Agent </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/introduction-to-agents/how_to_create_multi_agents_from_custom_agents/ class=md-nav__link> <span class=md-ellipsis> Multi Agent Orchestration </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> RAG </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1> <label class=md-nav__link for=__nav_2_3_1 id=__nav_2_3_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Knowledge Bases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Knowledge Bases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_1> <label class=md-nav__link for=__nav_2_3_1_1 id=__nav_2_3_1_1_label tabindex=0> <span class=md-ellipsis> Zero Setup </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_1> <span class="md-nav__icon md-icon"></span> Zero Setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/00-zero-setup-chat-with-your-document/chat_with_document_kb/ class=md-nav__link> <span class=md-ellipsis> Chat with Your Document </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_2> <label class=md-nav__link for=__nav_2_3_1_2 id=__nav_2_3_1_2_label tabindex=0> <span class=md-ellipsis> RAG Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_2> <span class="md-nav__icon md-icon"></span> RAG Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/01_create_ingest_documents_test_kb_multi_ds/ class=md-nav__link> <span class=md-ellipsis> Create and Ingest Documents with Multi-Data Sources </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/02_managed_rag_custom_prompting_and_no_of_results/ class=md-nav__link> <span class=md-ellipsis> Managed RAG with Custom Prompting </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/03_customized-rag-retreive-api-hybrid-search-claude-3-sonnet-langchain/ class=md-nav__link> <span class=md-ellipsis> Customized RAG with Claude 3 and Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/04_customized-rag-retreive-api-langchain-claude-evaluation-ragas/ class=md-nav__link> <span class=md-ellipsis> RAG Evaluation with Langchain and RAGAS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_3> <label class=md-nav__link for=__nav_2_3_1_3 id=__nav_2_3_1_3_label tabindex=0> <span class=md-ellipsis> Optimizing Retrieval Results </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_3> <span class="md-nav__icon md-icon"></span> Optimizing Retrieval Results </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/advanced_chunking_options/ class=md-nav__link> <span class=md-ellipsis> Advanced Chunking Options </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/csv_metadata_customization/ class=md-nav__link> <span class=md-ellipsis> CSV Metadata Customization </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/query_reformulation/ class=md-nav__link> <span class=md-ellipsis> Query Reformulation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4> <label class=md-nav__link for=__nav_2_3_1_4 id=__nav_2_3_1_4_label tabindex=0> <span class=md-ellipsis> Advanced Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4> <span class="md-nav__icon md-icon"></span> Advanced Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/dynamic-metadata-filtering/dynamic-metadata-filtering-KB/ class=md-nav__link> <span class=md-ellipsis> Dynamic Metadata Filtering </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4_2> <label class=md-nav__link for=__nav_2_3_1_4_2 id=__nav_2_3_1_4_2_label tabindex=0> <span class=md-ellipsis> Reranking </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4_2> <span class="md-nav__icon md-icon"></span> Reranking </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/01_deploy-reranking-model-sm/ class=md-nav__link> <span class=md-ellipsis> Deploy Reranking Model </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/02_kb-reranker/ class=md-nav__link> <span class=md-ellipsis> Knowledge Base Reranker </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/qa-generator/ class=md-nav__link> <span class=md-ellipsis> QA Generator </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_5> <label class=md-nav__link for=__nav_2_3_1_5 id=__nav_2_3_1_5_label tabindex=0> <span class=md-ellipsis> Responsible AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_5> <span class="md-nav__icon md-icon"></span> Responsible AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/05-responsible-ai/contextual-grounding/ class=md-nav__link> <span class=md-ellipsis> Contextual Grounding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6> <label class=md-nav__link for=__nav_2_3_1_6 id=__nav_2_3_1_6_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_1> <label class=md-nav__link for=__nav_2_3_1_6_1 id=__nav_2_3_1_6_1_label tabindex=0> <span class=md-ellipsis> Metadata Filter Access Control </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_1> <span class="md-nav__icon md-icon"></span> Metadata Filter Access Control </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/metadata-filter-access-control/kb-end-to-end-acl/ class=md-nav__link> <span class=md-ellipsis> End-to-End ACL with Knowledge Base </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_2> <label class=md-nav__link for=__nav_2_3_1_6_2 id=__nav_2_3_1_6_2_label tabindex=0> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_2> <span class="md-nav__icon md-icon"></span> RAG with Structured and Unstructured Data </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/0-create-dummy-structured-data/ class=md-nav__link> <span class=md-ellipsis> Create Dummy Structured Data </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/1_create_sql_dataset_optional/ class=md-nav__link> <span class=md-ellipsis> Create SQL Dataset (Optional) </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/2_rag_with_structured_unstructured_data/ class=md-nav__link> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2> <label class=md-nav__link for=__nav_2_3_2 id=__nav_2_3_2_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/open-source/chatbots/qa_chatbot_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chatbot using Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/open-source/chunking/rag_chunking_strategies_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chunking strategies for RAG applications </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2_3> <label class=md-nav__link for=__nav_2_3_2_3 id=__nav_2_3_2_3_label tabindex=0> <span class=md-ellipsis> Vector Stores </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2_3> <span class="md-nav__icon md-icon"></span> Vector Stores </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/open-source/vector_stores/rag_langchain_bedrock_opensearch/ class=md-nav__link> <span class=md-ellipsis> Langchain Chatbot with Opensearch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Model Customization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Model Customization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../custom-models/model-distillation/Historical_invocation_distillation/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with Invocation Logs </span> </a> </li> <li class=md-nav__item> <a href=../../../custom-models/model-distillation/Distillation-via-S3-input/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with S3 Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Gen AI Usecases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Gen AI Usecases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Text Generation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Text Generation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Streaming Response with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_code_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Generate Python Code with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text_translation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Text Translation with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text-summarization-titan%2Bclaude/ class=md-nav__link> <span class=md-ellipsis> Text summarization with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_batch_example_for_multi_threaded_invocation/ class=md-nav__link> <span class=md-ellipsis> Generate Bulk Emails with Batch Inference </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Workshops </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Workshops </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex=0> <span class=md-ellipsis> Open-source L400 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Open-source L400 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../open-source-l400/01_usecase_introduction/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/02_Lab_Find%20a%20Dream%20Destination_RAG%20query/ class=md-nav__link> <span class=md-ellipsis> Advanced RAG for Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/02_travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Conversational Memory in Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/03_travel_agent_with_tools/ class=md-nav__link> <span class=md-ellipsis> Multi-Modal and Types of Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/04_travel_booking_multi_agent/ class=md-nav__link> <span class=md-ellipsis> Multi-Agent Collaboration with Human-in-loop </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/05_dream_destination_with_crewai/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/06_agent_evaluation_with_ragas/ class=md-nav__link> <span class=md-ellipsis> RAGAs Agents Evaluation </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/07_dynamic_tooling_agents/ class=md-nav__link> <span class=md-ellipsis> Dynamic Tool invocation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2 checked> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> Open-source L200 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=true> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Open-source L200 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../02_contextual_text_generation/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../03_retrieval_based_text_application/ class=md-nav__link> <span class=md-ellipsis> Retrieval Based Text Generation </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Retrieval Based Chat Application </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Retrieval Based Chat Application </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#chat-with-llms-overview class=md-nav__link> <span class=md-ellipsis> Chat with LLMs Overview </span> </a> </li> <li class=md-nav__item> <a href=#extending-chat-with-rag class=md-nav__link> <span class=md-ellipsis> Extending Chat with RAG </span> </a> </li> <li class=md-nav__item> <a href=#setup-boto3-connection class=md-nav__link> <span class=md-ellipsis> Setup boto3 Connection </span> </a> </li> <li class=md-nav__item> <a href=#using-langchain-for-conversation-memory class=md-nav__link> <span class=md-ellipsis> Using LangChain for Conversation Memory </span> </a> </li> <li class=md-nav__item> <a href=#creating-a-class-to-help-facilitate-conversation class=md-nav__link> <span class=md-ellipsis> Creating a class to help facilitate conversation </span> </a> </li> <li class=md-nav__item> <a href=#combining-rag-with-conversation class=md-nav__link> <span class=md-ellipsis> Combining RAG with Conversation </span> </a> <nav class=md-nav aria-label="Combining RAG with Conversation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#visualize-semantic-search class=md-nav__link> <span class=md-ellipsis> Visualize Semantic Search </span> </a> <nav class=md-nav aria-label="Visualize Semantic Search"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#citation class=md-nav__link> <span class=md-ellipsis> Citation </span> </a> </li> <li class=md-nav__item> <a href=#vector-db-indexes class=md-nav__link> <span class=md-ellipsis> Vector DB Indexes </span> </a> </li> <li class=md-nav__item> <a href=#similarity-search class=md-nav__link> <span class=md-ellipsis> Similarity Search </span> </a> <nav class=md-nav aria-label="Similarity Search"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#distance-scoring-in-vector-data-bases class=md-nav__link> <span class=md-ellipsis> Distance scoring in Vector Data bases </span> </a> </li> <li class=md-nav__item> <a href=#let-us-look-at-the-documents-based-on-euclidean_distance-which-will-be-used-to-answer-our-question-what-kind-of-bias-does-clarify-detect class=md-nav__link> <span class=md-ellipsis> Let us look at the documents based on EUCLIDEAN_DISTANCE which will be used to answer our question 'What kind of bias does Clarify detect ?' </span> </a> </li> <li class=md-nav__item> <a href=#similarity-search-table-with-relevancy-score class=md-nav__link> <span class=md-ellipsis> Similarity Search Table with relevancy score. </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#marginal-relevancy-score class=md-nav__link> <span class=md-ellipsis> Marginal Relevancy score </span> </a> <nav class=md-nav aria-label="Marginal Relevancy score"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#let-us-look-at-mrr-scores class=md-nav__link> <span class=md-ellipsis> Let us look at MRR scores </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#update-embeddings-of-the-vector-databases class=md-nav__link> <span class=md-ellipsis> Update embeddings of the Vector Databases </span> </a> </li> <li class=md-nav__item> <a href=#run-a-query-against-version-2-of-the-documents class=md-nav__link> <span class=md-ellipsis> Run a query against version 2 of the documents </span> </a> </li> <li class=md-nav__item> <a href=#add-a-new-version-of-the-document class=md-nav__link> <span class=md-ellipsis> Add a new version of the document </span> </a> </li> <li class=md-nav__item> <a href=#query-complete-merged-data-base-with-no-filters class=md-nav__link> <span class=md-ellipsis> Query complete merged data base with no filters </span> </a> </li> <li class=md-nav__item> <a href=#query-with-filter class=md-nav__link> <span class=md-ellipsis> Query with Filter </span> </a> </li> <li class=md-nav__item> <a href=#query-for-new-data class=md-nav__link> <span class=md-ellipsis> Query for new data </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#let-us-continue-to-build-our-chatbot class=md-nav__link> <span class=md-ellipsis> Let us continue to build our chatbot </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#using-langchain-for-orchestration-of-rag class=md-nav__link> <span class=md-ellipsis> Using LangChain for Orchestration of RAG </span> </a> </li> <li class=md-nav__item> <a href=#using-llamaindex-for-orchestration-of-rag class=md-nav__link> <span class=md-ellipsis> Using LlamaIndex for Orchestration of RAG </span> </a> </li> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next steps </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../05_agent_based_text_generation/ class=md-nav__link> <span class=md-ellipsis> Agent Based Text Generation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../general/tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../general/license/ class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=retrieval-augmented-generation-with-amazon-bedrock-enhancing-chat-applications-with-rag>Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG</h1> <blockquote> <p><em>PLEASE NOTE: This notebook should work well with the </em><em><code>Data Science 3.0</code></em><em> kernel in SageMaker Studio</em></p> </blockquote> <hr> <h2 id=chat-with-llms-overview>Chat with LLMs Overview</h2> <p>Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users.</p> <p>The key technical detail which we need to include in our system to enable a chat feature is conversational memory. This way, customers can ask follow up questions and the LLM will understand what the customer has already said in the past. The image below shows how this is orchestrated at a high level.</p> <p><img alt="Amazon Bedrock - Conversational Interface" src=../images/chatbot_bedrock.png></p> <h2 id=extending-chat-with-rag>Extending Chat with RAG</h2> <p>However, in our workshop's situation, we want to be able to enable a customer to ask follow up questions regarding documentation we provide through RAG. This means we need to build a system which has conversational memory AND contextual retrieval built into the text generation.</p> <p><img alt=4 src=../images/context-aware-chatbot.png></p> <p>Let's get started!</p> <hr> <h2 id=setup-boto3-connection>Setup <code>boto3</code> Connection</h2> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>boto3</span>
<span class=kn>import</span> <span class=nn>os</span>
<span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>Markdown</span><span class=p>,</span> <span class=n>display</span>

<span class=n>region</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;AWS_REGION&quot;</span><span class=p>)</span>
<span class=n>boto3_bedrock</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=n>service_name</span><span class=o>=</span><span class=s1>&#39;bedrock-runtime&#39;</span><span class=p>,</span>
    <span class=n>region_name</span><span class=o>=</span><span class=n>region</span><span class=p>,</span>
<span class=p>)</span>
</code></pre></div> <hr> <h2 id=using-langchain-for-conversation-memory>Using LangChain for Conversation Memory</h2> <p>We will use LangChain's <code>ChatMessageHistory</code> class provides an easy way to capture conversational memory for LLM chat applications. Let's check out an example of Claude being able to retrieve context through conversational memory below.</p> <p>Similar to the last workshop, we will use both a prompt template and a LangChain LLM for this example. Note that this time our prompt template includes a <code>{history}</code> variable where our chat history will be included to the prompt.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain_core.prompts</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>

<span class=n>CHAT_PROMPT_TEMPLATE</span> <span class=o>=</span> <span class=s1>&#39;&#39;&#39;You are a helpful conversational assistant.</span>
<span class=si>{history}</span>

<span class=s1>Human: </span><span class=si>{human_input}</span>

<span class=s1>Assistant:</span>
<span class=s1>&#39;&#39;&#39;</span>
<span class=c1># Creating the prompt template</span>
<span class=n>PROMPT</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=p>(</span><span class=n>input_variables</span><span class=o>=</span><span class=p>[</span><span class=s2>&quot;history&quot;</span><span class=p>,</span> <span class=s2>&quot;human_input&quot;</span><span class=p>],</span> <span class=n>template</span><span class=o>=</span><span class=n>CHAT_PROMPT_TEMPLATE</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain_aws</span> <span class=kn>import</span> <span class=n>ChatBedrock</span>
<span class=kn>import</span> <span class=nn>boto3</span>
<span class=kn>import</span> <span class=nn>os</span>

<span class=c1># Initialize the Bedrock LLM with Claude model</span>
<span class=n>region</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;AWS_REGION&quot;</span><span class=p>)</span>
<span class=n>boto3_bedrock</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=n>service_name</span><span class=o>=</span><span class=s1>&#39;bedrock-runtime&#39;</span><span class=p>,</span>
    <span class=n>region_name</span><span class=o>=</span><span class=n>region</span><span class=p>,</span>
<span class=p>)</span>

<span class=n>llm</span> <span class=o>=</span> <span class=n>ChatBedrock</span><span class=p>(</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;anthropic.claude-3-haiku-20240307-v1:0&quot;</span><span class=p>,</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span>
        <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
    <span class=p>},</span>
<span class=p>)</span>
</code></pre></div> <p>The <code>ChatMessageHistory</code> class is instantiated here and you will notice the history is blank.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.memory</span> <span class=kn>import</span> <span class=n>ChatMessageHistory</span>

<span class=c1># Initializing memory to store conversation</span>
<span class=n>memory</span> <span class=o>=</span> <span class=n>ChatMessageHistory</span><span class=p>()</span>

<span class=nb>print</span><span class=p>(</span><span class=n>memory</span><span class=o>.</span><span class=n>messages</span><span class=p>)</span>
</code></pre></div> <pre><code>[]
</code></pre> <p>We now ask Claude a simple question "How can I check for imbalances in my model?". The LLM responds to the question and we can use the <code>add_user_message</code> and <code>add_ai_message</code> functions to save the input and output into memory. We can then retrieve the entire conversation history and print the response. Currently the model will still return answer using the data it was trained upon. Further will examine how to get a curated answer using our own FAq's</p> <div class=highlight><pre><span></span><code><span class=n>human_input</span> <span class=o>=</span> <span class=s1>&#39;How can I check for imbalances in my model?&#39;</span>

<span class=c1># Formatting the prompt with human input and history</span>
<span class=n>prompt_data</span> <span class=o>=</span> <span class=n>PROMPT</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>human_input</span><span class=o>=</span><span class=n>human_input</span><span class=p>,</span> <span class=n>history</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>messages</span><span class=p>)</span>
<span class=n>ai_output</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>prompt_data</span><span class=p>)</span>

<span class=c1># Storing the conversation in memory</span>
<span class=n>memory</span><span class=o>.</span><span class=n>add_user_message</span><span class=p>(</span><span class=n>human_input</span><span class=p>)</span>
<span class=n>memory</span><span class=o>.</span><span class=n>add_ai_message</span><span class=p>(</span><span class=n>ai_output</span><span class=p>)</span>

<span class=c1># Retrieving updated conversation history</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>ai_output</span><span class=o>.</span><span class=n>content</span><span class=p>))</span>
</code></pre></div> <p>There are a few ways you can check for potential imbalances in your model:</p> <ol> <li> <p><strong>Data Imbalance</strong>: Examine the distribution of your training data. Are there any classes or categories that are significantly underrepresented compared to others? Imbalanced data can lead to biased models that perform poorly on the underrepresented classes.</p> </li> <li> <p><strong>Model Performance Metrics</strong>: Evaluate your model's performance using metrics that are sensitive to imbalanced classes, such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC). Look for large discrepancies in these metrics across different classes.</p> </li> <li> <p><strong>Confusion Matrix</strong>: Construct a confusion matrix to visualize the model's predictions against the true labels. This can help identify classes that are being misclassified more often.</p> </li> <li> <p><strong>Feature Importance</strong>: Analyze the feature importance of your model to see if certain features are dominating the predictions. This can indicate that the model is overly reliant on a subset of the features, which could be a sign of imbalance.</p> </li> <li> <p><strong>Fairness Metrics</strong>: Depending on your use case, you may want to evaluate your model's fairness across different demographic groups or sensitive attributes. Metrics like demographic parity, equal opportunity, and equalized odds can help identify potential biases.</p> </li> <li> <p><strong>Qualitative Analysis</strong>: Manually inspect a sample of your model's predictions, especially for the underrepresented classes. This can provide valuable insights into the types of errors the model is making and help you identify the root causes.</p> </li> <li> <p><strong>Robustness Checks</strong>: Test your model's performance on out-of-distribution data or adversarial examples to see how it handles edge cases and corner cases. This can reveal weaknesses that may be related to imbalances in the training data.</p> </li> </ol> <p>By using a combination of these techniques, you can gain a better understanding of the potential imbalances in your model and take appropriate steps to address them, such as collecting more balanced data, applying data augmentation techniques, or adjusting your model architecture or training process.</p> <p>Now we will ask a follow up question about the kind of imbalances does it detect and save the input and outputs again. Notice how the model is able to understand that when the human says "it", because it has access to the context of the chat history, the model is able to accurately understand what the user is asking about.</p> <div class=highlight><pre><span></span><code><span class=n>human_input</span> <span class=o>=</span> <span class=s1>&#39;What kind does it detect?&#39;</span>

<span class=c1># Formatting the new prompt with the updated conversation history</span>
<span class=n>prompt_data</span> <span class=o>=</span> <span class=n>PROMPT</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>human_input</span><span class=o>=</span><span class=n>human_input</span><span class=p>,</span> <span class=n>history</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>messages</span><span class=p>)</span>
<span class=n>ai_output</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>prompt_data</span><span class=p>)</span>

<span class=c1># Saving the new messages into memory</span>
<span class=n>memory</span><span class=o>.</span><span class=n>add_user_message</span><span class=p>(</span><span class=n>human_input</span><span class=p>)</span>
<span class=n>memory</span><span class=o>.</span><span class=n>add_ai_message</span><span class=p>(</span><span class=n>ai_output</span><span class=p>)</span>

<span class=c1># Displaying the model&#39;s response</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>ai_output</span><span class=o>.</span><span class=n>content</span><span class=p>))</span>
</code></pre></div> <p>The methods I described can detect a few different types of imbalances in a model:</p> <ol> <li> <p>Data imbalance - This refers to having significantly more data for some classes/categories compared to others in the training data. This can lead to biased models that perform poorly on the underrepresented classes.</p> </li> <li> <p>Performance metric imbalance - Looking at metrics like precision, recall, F1-score, and AUC-ROC can reveal if the model is performing much better or worse on certain classes.</p> </li> <li> <p>Prediction imbalance - The confusion matrix can show which classes are being misclassified more frequently by the model.</p> </li> <li> <p>Feature imbalance - Analyzing feature importance can indicate if the model is overly reliant on a subset of the available features, which could be a sign of imbalance.</p> </li> <li> <p>Demographic/fairness imbalance - Fairness metrics can uncover biases in how the model performs across different demographic groups or sensitive attributes.</p> </li> </ol> <p>So in summary, these techniques can detect imbalances in the data, model performance, predictions, features used, and fairness - providing a comprehensive view of potential issues in the model. The goal is to identify and address any significant imbalances to improve the overall robustness and fairness of the model.</p> <hr> <h2 id=creating-a-class-to-help-facilitate-conversation>Creating a class to help facilitate conversation</h2> <p>To help create some structure around these conversations, we create a custom <code>Conversation</code> class below. This class will hold a stateful conversational memory and be the base for conversational RAG later.</p> <div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>Conversation</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>client</span><span class=p>,</span> <span class=n>model_id</span><span class=p>:</span> <span class=nb>str</span><span class=o>=</span><span class=s2>&quot;anthropic.claude-3-haiku-20240307-v1:0&quot;</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;instantiates a new rag based conversation</span>

<span class=sd>        Args:</span>
<span class=sd>            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to &quot;anthropic.claude-3-haiku-20240307-v1:0&quot;.</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># instantiate memory</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span> <span class=o>=</span> <span class=n>ChatMessageHistory</span><span class=p>()</span>

        <span class=c1># instantiate LLM connection</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatBedrock</span><span class=p>(</span>
            <span class=n>client</span><span class=o>=</span><span class=n>client</span><span class=p>,</span>
            <span class=n>model_id</span><span class=o>=</span><span class=n>model_id</span><span class=p>,</span>
            <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span>
                <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
            <span class=p>},</span>
        <span class=p>)</span>

    <span class=k>def</span> <span class=nf>ai_respond</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_input</span><span class=p>:</span> <span class=nb>str</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;responds to the user input in the conversation with context used</span>

<span class=sd>        Args:</span>
<span class=sd>            user_input (str, optional): user input. Defaults to None.</span>

<span class=sd>        Returns:</span>
<span class=sd>            ai_output (str): response from AI chatbot</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># format the prompt with chat history and user input</span>
        <span class=c1>#history = self.memory.load_memory_variables({})[&#39;history&#39;]</span>
        <span class=n>llm_input</span> <span class=o>=</span> <span class=n>PROMPT</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>history</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>messages</span><span class=p>,</span> <span class=n>human_input</span><span class=o>=</span><span class=n>user_input</span><span class=p>)</span>

        <span class=c1># respond to the user with the LLM</span>
        <span class=n>ai_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>llm_input</span><span class=p>)</span>

        <span class=c1># store the input and output</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=o>.</span><span class=n>add_user_message</span><span class=p>(</span><span class=n>user_input</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=o>.</span><span class=n>add_ai_message</span><span class=p>(</span><span class=n>ai_output</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>ai_output</span><span class=o>.</span><span class=n>content</span>
</code></pre></div> <p>Let's see the class in action with two contextual questions. Again, notice the model is able to correctly interpret the context because it has memory of the conversation.</p> <div class=highlight><pre><span></span><code><span class=n>chat</span> <span class=o>=</span> <span class=n>Conversation</span><span class=p>(</span><span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>chat</span><span class=o>.</span><span class=n>ai_respond</span><span class=p>(</span><span class=s1>&#39;How can I check for imbalances in my model?&#39;</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <p>There are a few key ways you can check for potential imbalances in your model:</p> <ol> <li> <p><strong>Data Imbalance</strong>: Examine the distribution of your training data. Are there any classes or categories that are significantly underrepresented compared to others? Imbalanced data can lead to biased models that perform poorly on the underrepresented classes.</p> </li> <li> <p><strong>Model Performance Metrics</strong>: Evaluate your model's performance using metrics that are sensitive to imbalanced classes, such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC). Look for large discrepancies in these metrics across different classes.</p> </li> <li> <p><strong>Confusion Matrix</strong>: Construct a confusion matrix to visualize the model's predictions against the true labels. This can help identify classes that are being misclassified more often.</p> </li> <li> <p><strong>Feature Importance</strong>: Analyze the feature importance of your model to see if certain features are dominating the predictions. This can indicate that the model is overly reliant on a subset of the features, which could be a sign of imbalance.</p> </li> <li> <p><strong>Fairness Metrics</strong>: Depending on your use case, you may want to evaluate your model's fairness across different demographic groups or sensitive attributes. Metrics like demographic parity, equal opportunity, and equalized odds can help identify potential biases.</p> </li> <li> <p><strong>Qualitative Analysis</strong>: Manually inspect a sample of your model's predictions, especially for the underrepresented classes. This can provide valuable insights into the types of errors the model is making and help you identify the root causes.</p> </li> <li> <p><strong>Robustness Checks</strong>: Test your model's performance on out-of-distribution data or adversarial examples to see how it handles edge cases and corner cases. This can reveal weaknesses that may be related to imbalances in the training data.</p> </li> </ol> <p>By using a combination of these techniques, you can gain a better understanding of the potential imbalances in your model and take appropriate steps to address them, such as collecting more balanced data, applying data augmentation techniques, or adjusting your model architecture or training process.</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>chat</span><span class=o>.</span><span class=n>ai_respond</span><span class=p>(</span><span class=s1>&#39;What kind does it detect?&#39;</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <p>The methods I described can detect a few different types of imbalances in a model:</p> <ol> <li> <p>Data imbalance - This refers to having significantly more data for some classes/categories compared to others in the training data. This can lead to biased models that perform poorly on the underrepresented classes.</p> </li> <li> <p>Performance metric imbalance - Looking at metrics like precision, recall, F1-score, and AUC-ROC can reveal if the model is performing much better or worse on certain classes.</p> </li> <li> <p>Prediction imbalance - The confusion matrix can show which classes are being misclassified more frequently by the model.</p> </li> <li> <p>Feature imbalance - Analyzing feature importance can indicate if the model is overly reliant on a subset of the available features, which could be a sign of imbalance.</p> </li> <li> <p>Demographic/fairness imbalance - Fairness metrics can uncover biases in how the model performs across different demographic groups or sensitive attributes.</p> </li> </ol> <p>So in summary, these techniques can detect imbalances in the data, model performance, predictions, features used, and fairness - providing a comprehensive view of potential issues in the model. The goal is to identify and address any significant imbalances to improve the overall robustness and fairness of the model.</p> <hr> <h2 id=combining-rag-with-conversation>Combining RAG with Conversation</h2> <p>Now that we have a conversational system built, lets incorporate the RAG system we built in notebook 02 into the chat paradigm. </p> <p>First, we will create the same vector store with LangChain and FAISS from the last notebook.</p> <p>Our goal is to create a curated response from the model and only use the FAQ's we have provided.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain_aws.embeddings</span> <span class=kn>import</span> <span class=n>BedrockEmbeddings</span>
<span class=kn>from</span> <span class=nn>langchain.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>

<span class=c1># create instantiation to embedding model</span>
<span class=n>embedding_model</span> <span class=o>=</span> <span class=n>BedrockEmbeddings</span><span class=p>(</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;amazon.titan-embed-text-v1&quot;</span>
<span class=p>)</span>

<span class=c1># create vector store</span>
<span class=n>vs</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>load_local</span><span class=p>(</span><span class=s1>&#39;../faiss-index/langchain/&#39;</span><span class=p>,</span> <span class=n>embedding_model</span><span class=p>,</span> <span class=n>allow_dangerous_deserialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <h3 id=visualize-semantic-search>Visualize Semantic Search</h3> <p>⚠️ ⚠️ ⚠️ This section is for Advanced Practioners. Please feel free to run through these cells and come back later to re-examine the concepts ⚠️ ⚠️ ⚠️ </p> <p>Let's see how the semantic search works: 1. First we calculate the embeddings vector for the query, and 2. then we use this vector to do a similarity search on the store</p> <h5 id=citation>Citation</h5> <p>We will also be able to get the <code>citation</code> or the underlying documents which our Vector Store matched to our query. This is useful for debugging and also measuring the quality of the vector stores. let us look at how the underlying Vector store calculates the matches</p> <h5 id=vector-db-indexes>Vector DB Indexes</h5> <p>One of the key components of the Vector DB is to be able to retrieve documents matching the query with accuracy and speed. There are multiple algorithims for the same and some examples can be <a href=https://thedataquarry.com/posts/vector-db-3/ >read here</a> </p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>HTML</span><span class=p>,</span> <span class=n>display</span>
<span class=kn>import</span> <span class=nn>warnings</span>
<span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
<span class=c1>#- helpful function to display in tabular format</span>

<span class=k>def</span> <span class=nf>display_table</span><span class=p>(</span><span class=n>data</span><span class=p>):</span>
    <span class=n>html</span> <span class=o>=</span> <span class=s2>&quot;&lt;table&gt;&quot;</span>
    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>data</span><span class=p>:</span>
        <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;tr&gt;&quot;</span>
        <span class=k>for</span> <span class=n>field</span> <span class=ow>in</span> <span class=n>row</span><span class=p>:</span>
            <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;td&gt;</span><span class=si>%s</span><span class=s2>&lt;/td&gt;&quot;</span><span class=o>%</span><span class=p>(</span><span class=n>field</span><span class=p>)</span>
        <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;/tr&gt;&quot;</span>
    <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;/table&gt;&quot;</span>
    <span class=n>display</span><span class=p>(</span><span class=n>HTML</span><span class=p>(</span><span class=n>html</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>v</span> <span class=o>=</span> <span class=n>embedding_model</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=s2>&quot;How can I check for imbalances in my model?&quot;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>v</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>10</span><span class=p>])</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_by_vector</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;Let us look at the documents which had the relevant information pertaining to our query&#39;</span><span class=p>))</span>
<span class=k>for</span> <span class=n>r</span> <span class=ow>in</span> <span class=n>results</span><span class=p>:</span>
    <span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>r</span><span class=o>.</span><span class=n>page_content</span><span class=p>))</span>
    <span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
</code></pre></div> <pre><code>[-0.1474609375, 0.77734375, 0.26953125, -0.55859375, 0.0478515625, -0.435546875, -0.0576171875, -0.0003032684326171875, -0.5703125, -0.337890625]
</code></pre> <p>Let us look at the documents which had the relevant information pertaining to our query</p> <p>What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."</p> <hr> <p>How do I build an ML model to generate accurate predictions in SageMaker Canvas?," Once you have connected sources, selected a dataset, and prepared your data, you can select the target column that you want to predict to initiate a model creation job. SageMaker Canvas will automatically identify the problem type, generate new relevant features, test a comprehensive set of prediction models using ML techniques such as linear regression, logistic regression, deep learning, time-series forecasting, and gradient boosting, and build the model that makes accurate predictions based on your dataset."</p> <hr> <h4 id=similarity-search>Similarity Search</h4> <h5 id=distance-scoring-in-vector-data-bases>Distance scoring in Vector Data bases</h5> <p><a href=https://weaviate.io/blog/distance-metrics-in-vector-search>Distance scores</a> are the key in vector searches. Here are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance ( Squared Euclidean) . Therefore, a lower score is better. Further in FAISS we have similarity_search_with_score (ranked by distance: low to high) and similarity_search_with_relevance_scores ( ranked by relevance: high to low) with both using the distance strategy. The similarity_search_with_relevance_scores calculates the relevance score as 1 - score. For more details of the various distance scores <a href=https://milvus.io/docs/metric.md>read here</a></p> <div class=highlight><pre><span></span><code><span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;##### Let us look at the documents based on </span><span class=si>{</span><span class=n>vs</span><span class=o>.</span><span class=n>distance_strategy</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2> which will be used to answer our question &#39;What kind of bias does Clarify detect ?&#39;&quot;</span><span class=p>))</span>

<span class=n>context</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search</span><span class=p>(</span><span class=s1>&#39;What kind of bias does Clarify detect ?&#39;</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=c1>#-  langchain.schema.document.Document</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
<span class=n>list_context</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>context</span><span class=p>]</span>
<span class=n>list_context</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Documents&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-data&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>list_context</span><span class=p>)</span>
</code></pre></div> <h5 id=let-us-look-at-the-documents-based-on-euclidean_distance-which-will-be-used-to-answer-our-question-what-kind-of-bias-does-clarify-detect>Let us look at the documents based on EUCLIDEAN_DISTANCE which will be used to answer our question 'What kind of bias does Clarify detect ?'</h5> <hr> <table><tr><td>Documents</td><td>Meta-data</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."</td><td>{}</td></tr><tr><td>How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.</td><td>{}</td></tr></table> <p>Let us first look at the Page context and the meta data associated with the documents. Now let us look at the L2 scores based on the distance scoring as explained above. Lower score is better</p> <div class=highlight><pre><span></span><code><span class=c1>#- relevancy of the documents</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=s2>&quot;What kind of bias does Clarify detect ?&quot;</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;##### Similarity Search Table with relevancy score.&#39;</span><span class=p>))</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
<span class=n>results</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Documents&#39;</span><span class=p>,</span> <span class=s1>&#39;Relevancy Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</code></pre></div> <h5 id=similarity-search-table-with-relevancy-score>Similarity Search Table with relevancy score.</h5> <hr> <table><tr><td>Documents</td><td>Relevancy Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."'</td><td>130.92531</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70465</td></tr></table> <h4 id=marginal-relevancy-score>Marginal Relevancy score</h4> <p>Maximal Marginal Relevance has been introduced in the paper <a href=https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf>The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries</a>. Maximal Marginal Relevance tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. In the below results since we have a very limited data set it might not make a difference but for larger data sets the query will theoritically run faster while still preserving the over all relevancy of the documents</p> <div class=highlight><pre><span></span><code><span class=c1>#- normalizing the relevancy</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;##### Let us look at MRR scores&#39;</span><span class=p>))</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>max_marginal_relevance_search_with_score_by_vector</span><span class=p>(</span><span class=n>embedding_model</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=s2>&quot;What kind of bias does Clarify detect ?&quot;</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s2>&quot;Document&quot;</span><span class=p>,</span> <span class=s2>&quot;MRR Score&quot;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</code></pre></div> <h5 id=let-us-look-at-mrr-scores>Let us look at MRR scores</h5> <table><tr><td>Document</td><td>MRR Score</td></tr><tr><td>page_content='What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."'</td><td>130.92531</td></tr><tr><td>page_content='How does SageMaker Clarify improve model explainability?, SageMaker Clarify is integrated with SageMaker Experiments to provide a feature importance graph detailing the importance of each input for your model’s overall decision-making process after the model has been trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available through an API.'</td><td>188.70465</td></tr><tr><td>page_content='What is the underlying tuning algorithm for Automatic Model Tuning?," Currently, the algorithm for tuning hyperparameters is a customized implementation of Bayesian Optimization. It aims to optimize a customer-specified objective metric throughout the tuning process. Specifically, it checks the object metric of completed training jobs, and uses the knowledge to infer the hyperparameter combination for the next training job." Does Automatic Model Tuning recommend specific hyperparameters for tuning?," No. How certain hyperparameters impact the model performance depends on various factors, and it is hard to definitively say one hyperparameter is more important than the others and thus needs to be tuned. For built-in algorithms within SageMaker, we do call out whether or not a hyperparameter is tunable."'</td><td>281.98914</td></tr></table> <h4 id=update-embeddings-of-the-vector-databases>Update embeddings of the Vector Databases</h4> <p>Update of documents happens all the time and we have multiple versions of the documents. Which means we need to also factor how do we update the embeddings in our Vector Data bases. Fortunately we have and can leverage the meta data to update embeddings</p> <p>The key steps are: 1. Load the new embeddings and add the meta data stating the version as 2 2. Merge to the exisiting Vector database 3. Run the query using the filter to only search in the new index and get the latest documents for the same query</p> <div class=highlight><pre><span></span><code><span class=c1># create vector store</span>
<span class=kn>from</span> <span class=nn>langchain.document_loaders</span> <span class=kn>import</span> <span class=n>CSVLoader</span>
<span class=kn>from</span> <span class=nn>langchain.text_splitter</span> <span class=kn>import</span> <span class=n>CharacterTextSplitter</span>
<span class=kn>from</span> <span class=nn>langchain.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>
<span class=kn>from</span> <span class=nn>langchain.schema</span> <span class=kn>import</span> <span class=n>Document</span>

<span class=n>loader</span> <span class=o>=</span> <span class=n>CSVLoader</span><span class=p>(</span>
    <span class=n>file_path</span><span class=o>=</span><span class=s2>&quot;../data/sagemaker/sm_faq_v2.csv&quot;</span><span class=p>,</span>
    <span class=n>csv_args</span><span class=o>=</span><span class=p>{</span>
        <span class=s2>&quot;delimiter&quot;</span><span class=p>:</span> <span class=s2>&quot;,&quot;</span><span class=p>,</span>
        <span class=s2>&quot;quotechar&quot;</span><span class=p>:</span> <span class=s1>&#39;&quot;&#39;</span><span class=p>,</span>
        <span class=s2>&quot;fieldnames&quot;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&quot;Question&quot;</span><span class=p>,</span> <span class=s2>&quot;Answer&quot;</span><span class=p>],</span>
    <span class=p>},</span>
<span class=p>)</span>

<span class=c1>#docs_split = loader.load()</span>
<span class=n>docs_split</span> <span class=o>=</span> <span class=n>CharacterTextSplitter</span><span class=p>(</span><span class=n>chunk_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>separator</span><span class=o>=</span><span class=s2>&quot;,&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>split_documents</span><span class=p>(</span><span class=n>loader</span><span class=o>.</span><span class=n>load</span><span class=p>())</span>
<span class=n>list_of_documents</span> <span class=o>=</span> <span class=p>[</span><span class=n>Document</span><span class=p>(</span><span class=n>page_content</span><span class=o>=</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>metadata</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>page</span><span class=o>=</span><span class=s1>&#39;v2&#39;</span><span class=p>))</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>docs_split</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Number of split docs=</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>docs_split</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>
<span class=n>db</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span><span class=n>list_of_documents</span><span class=p>,</span> <span class=n>embedding_model</span><span class=p>)</span>
</code></pre></div> <pre><code>Number of split docs=6
</code></pre> <h4 id=run-a-query-against-version-2-of-the-documents>Run a query against version 2 of the documents</h4> <p>Let us run the query agsint our exisiting vector data base and we will see the the exisiting or the version 1 of the documents coming back. If we run with the filter since those do not exist in our vector Database we will see no results returned or an empty list back</p> <div class=highlight><pre><span></span><code><span class=c1># Run the query with requesting data from version 2 which does not exist</span>
<span class=n>vs</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>load_local</span><span class=p>(</span><span class=s1>&#39;../faiss-index/langchain/&#39;</span><span class=p>,</span> <span class=n>embedding_model</span><span class=p>,</span> <span class=n>allow_dangerous_deserialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
<span class=n>search_query</span> <span class=o>=</span> <span class=s2>&quot;How can I check for imbalances in my model?&quot;</span>
<span class=c1>#print(f&quot;Running with v1 of the documents we get response of {vs.similarity_search_with_score(query=search_query, k=1, fetch_k=4)}&quot;)</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>&quot;-&quot;</span><span class=o>*</span><span class=mi>20</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Running the query with V2 of the document we get </span><span class=si>{</span><span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>search_query</span><span class=p>,</span><span class=w> </span><span class=nb>filter</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>page</span><span class=o>=</span><span class=s1>&#39;v2&#39;</span><span class=p>),</span><span class=w> </span><span class=n>k</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=si>}</span><span class=s2>:&quot;</span><span class=p>)</span>
</code></pre></div> <pre><code>--------------------
Running the query with V2 of the document we get []:
</code></pre> <h4 id=add-a-new-version-of-the-document>Add a new version of the document</h4> <p>We will create the version 2 of the documents and use meta data to add to our original index. Once done we will then apply a filter in our query which will return to us the documents newly added. Run the query now after adding version of the documents</p> <p>We will also examine a way to speed up our searches and queries and look at another way to narrow the search using the fetch_k parameter when calling similarity_search with filters. Usually you would want the fetch_k to be more than the k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from.</p> <div class=highlight><pre><span></span><code><span class=c1># - now let us add version 2 of the data set and run query from that</span>

<span class=n>vs</span><span class=o>.</span><span class=n>merge_from</span><span class=p>(</span><span class=n>db</span><span class=p>)</span>
</code></pre></div> <h4 id=query-complete-merged-data-base-with-no-filters>Query complete merged data base with no filters</h4> <p>Run the query against the fully merged DB without any filters for the meta data and we see that it returns the top results of the new V2 data and also the top results of the v1 data. Essentially it will match and return data closest to the query</p> <div class=highlight><pre><span></span><code><span class=c1># - run the query again</span>
<span class=n>search_query_v2</span> <span class=o>=</span> <span class=s2>&quot;How can I check for imbalances in my model?&quot;</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>search_query_v2</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>,</span> <span class=n>score</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>results_with_scores</span><span class=p>]</span>
<span class=n>results_with_scores</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Document&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-Data&#39;</span><span class=p>,</span> <span class=s1>&#39;Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results_with_scores</span><span class=p>)</span>
</code></pre></div> <table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model? Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83807</td></tr><tr><td>What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model's prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."</td><td>{}</td><td>229.07724</td></tr></table> <h4 id=query-with-filter>Query with Filter</h4> <p>Now we will ask to search only against the version 2 of the data and use filter criteria against it</p> <div class=highlight><pre><span></span><code><span class=c1># - run the query again</span>
<span class=n>search_query_v2</span> <span class=o>=</span> <span class=s2>&quot;How can I check for imbalances in my model?&quot;</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>search_query_v2</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>page</span><span class=o>=</span><span class=s1>&#39;v2&#39;</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>,</span> <span class=n>score</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>results_with_scores</span><span class=p>]</span>
<span class=n>results_with_scores</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Document&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-Data&#39;</span><span class=p>,</span> <span class=s1>&#39;Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results_with_scores</span><span class=p>)</span>
</code></pre></div> <table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: How can I check for imbalances in my model? Answer: Amazon SageMaker Clarify Version 2 will helps improve model transparency. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time</td><td>{'page': 'v2'}</td><td>154.83807</td></tr><tr><td>Question: What kind of bias does SageMaker Clarify detect? Answer: Measuring bias in ML models is a first step to mitigating bias.</td><td>{'page': 'v2'}</td><td>268.96292</td></tr></table> <h4 id=query-for-new-data>Query for new data</h4> <p>Now let us ask a question which exists only on the version 2 of the document</p> <div class=highlight><pre><span></span><code><span class=c1># - now let us ask a question which ONLY exits in the version 2 of the document</span>
<span class=n>search_query_v2</span> <span class=o>=</span> <span class=s2>&quot;Can i use Quantum computing?&quot;</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>search_query_v2</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>page</span><span class=o>=</span><span class=s1>&#39;v2&#39;</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>,</span> <span class=n>score</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>results_with_scores</span><span class=p>]</span>
<span class=n>results_with_scores</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Document&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-Data&#39;</span><span class=p>,</span> <span class=s1>&#39;Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results_with_scores</span><span class=p>)</span>
</code></pre></div> <table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr><tr><td>Question: Can i use Quantum computing? Answer: Yes SageMaker version sometime in future will let you run quantum computing</td><td>{'page': 'v2'}</td><td>97.103165</td></tr></table> <h3 id=let-us-continue-to-build-our-chatbot>Let us continue to build our chatbot</h3> <p>The prompt template is now altered to include both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual.</p> <div class=highlight><pre><span></span><code><span class=c1># re-create vector store and continue</span>
<span class=n>vs</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>load_local</span><span class=p>(</span><span class=s1>&#39;../faiss-index/langchain/&#39;</span><span class=p>,</span> <span class=n>embedding_model</span><span class=p>,</span> <span class=n>allow_dangerous_deserialization</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>RAG_TEMPLATE</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;You are a helpful conversational assistant.</span>

<span class=s2>If you are unsure about the answer OR the answer does not exist in the context, respond with</span>
<span class=s2>&quot;Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊</span>
<span class=s2>NEVER make up the answer.</span>

<span class=s2>If the human greets you, simply introduce yourself.</span>

<span class=s2>The context will be placed in &lt;context&gt;&lt;/context&gt; XML tags.</span>

<span class=s2>&lt;context&gt;</span><span class=si>{context}</span><span class=s2>&lt;/context&gt;</span>

<span class=s2>Do not include any xml tags in your response.</span>

<span class=si>{history}</span>

<span class=s2>Human: </span><span class=si>{input}</span>

<span class=s2>Assistant:</span>
<span class=s2>&quot;&quot;&quot;</span>
<span class=n>PROMPT</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=n>RAG_TEMPLATE</span><span class=p>)</span>
</code></pre></div> <p>The new <code>ConversationWithRetrieval</code> class now includes a <code>get_context</code> function which searches our vector database based on the human input and combines it into the base prompt.</p> <div class=highlight><pre><span></span><code><span class=k>class</span> <span class=nc>ConversationWithRetrieval</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>client</span><span class=p>,</span> <span class=n>vector_store</span><span class=p>:</span> <span class=n>FAISS</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>model_id</span><span class=p>:</span> <span class=nb>str</span><span class=o>=</span><span class=s2>&quot;anthropic.claude-3-haiku-20240307-v1:0&quot;</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;instantiates a new rag based conversation</span>

<span class=sd>        Args:</span>
<span class=sd>            vector_store (FAISS, optional): pre-populated vector store for searching context. Defaults to None.</span>
<span class=sd>            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to &quot;anthropic.claude-3-haiku-20240307-v1:0&quot;.</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># store vector store</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>vector_store</span> <span class=o>=</span> <span class=n>vector_store</span>

        <span class=c1># instantiate memory</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span> <span class=o>=</span> <span class=n>ChatMessageHistory</span><span class=p>()</span>

        <span class=c1># instantiate LLM connection</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatBedrock</span><span class=p>(</span>
            <span class=n>client</span><span class=o>=</span><span class=n>client</span><span class=p>,</span>
            <span class=n>model_id</span><span class=o>=</span><span class=n>model_id</span><span class=p>,</span>
            <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span>
                <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>
            <span class=p>},</span>
        <span class=p>)</span>

    <span class=k>def</span> <span class=nf>ai_respond</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_input</span><span class=p>:</span> <span class=nb>str</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;responds to the user input in the conversation with context used</span>

<span class=sd>        Args:</span>
<span class=sd>            user_input (str, optional): user input. Defaults to None.</span>

<span class=sd>        Returns:</span>
<span class=sd>            ai_output (str): response from AI chatbot</span>
<span class=sd>            search_results (list): context used in the completion</span>
<span class=sd>        &quot;&quot;&quot;</span>

        <span class=c1># format the prompt with chat history and user input</span>
        <span class=n>context_string</span><span class=p>,</span> <span class=n>search_results</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_context</span><span class=p>(</span><span class=n>user_input</span><span class=p>)</span>

        <span class=n>llm_input</span> <span class=o>=</span> <span class=n>PROMPT</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>history</span><span class=o>=</span><span class=n>memory</span><span class=o>.</span><span class=n>messages</span><span class=p>,</span> <span class=nb>input</span><span class=o>=</span><span class=n>user_input</span><span class=p>,</span> <span class=n>context</span><span class=o>=</span><span class=n>context_string</span><span class=p>)</span>

        <span class=c1># respond to the user with the LLM</span>
        <span class=n>ai_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span><span class=n>llm_input</span><span class=p>)</span>

        <span class=c1># store the input and output</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=o>.</span><span class=n>add_user_message</span><span class=p>(</span><span class=n>user_input</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>memory</span><span class=o>.</span><span class=n>add_ai_message</span><span class=p>(</span><span class=n>ai_output</span><span class=p>)</span>

        <span class=k>return</span> <span class=n>ai_output</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>search_results</span>

    <span class=k>def</span> <span class=nf>get_context</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_input</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>5</span><span class=p>):</span>
<span class=w>        </span><span class=sd>&quot;&quot;&quot;returns context used in the completion</span>

<span class=sd>        Args:</span>
<span class=sd>            user_input (str): user input as a string</span>
<span class=sd>            k (int, optional): number of results to return. Defaults to 5.</span>

<span class=sd>        Returns:</span>
<span class=sd>            context_string (str): context used in the completion as a string</span>
<span class=sd>            search_results (list): context used in the completion as a list of Document objects</span>
<span class=sd>        &quot;&quot;&quot;</span>
        <span class=n>search_results</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vector_store</span><span class=o>.</span><span class=n>similarity_search</span><span class=p>(</span>
            <span class=n>user_input</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>k</span>
        <span class=p>)</span>
        <span class=n>context_string</span> <span class=o>=</span> <span class=s1>&#39;</span><span class=se>\n\n</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=sa>f</span><span class=s1>&#39;Document </span><span class=si>{</span><span class=n>ind</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>: &#39;</span> <span class=o>+</span> <span class=n>i</span><span class=o>.</span><span class=n>page_content</span> <span class=k>for</span> <span class=n>ind</span><span class=p>,</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>search_results</span><span class=p>)])</span>
        <span class=k>return</span> <span class=n>context_string</span><span class=p>,</span> <span class=n>search_results</span>
</code></pre></div> <p>Now the model can answer some specific domain questions based on our document database!</p> <div class=highlight><pre><span></span><code><span class=n>chat</span> <span class=o>=</span> <span class=n>ConversationWithRetrieval</span><span class=p>(</span><span class=n>boto3_bedrock</span><span class=p>,</span> <span class=n>vs</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>output</span><span class=p>,</span> <span class=n>context</span> <span class=o>=</span> <span class=n>chat</span><span class=o>.</span><span class=n>ai_respond</span><span class=p>(</span><span class=s1>&#39;How can I check for imbalances in my model?&#39;</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <p>Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊</p> <div class=highlight><pre><span></span><code><span class=n>output</span><span class=p>,</span> <span class=n>context</span> <span class=o>=</span> <span class=n>chat</span><span class=o>.</span><span class=n>ai_respond</span><span class=p>(</span><span class=s1>&#39;What kind does it detect?&#39;</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;** AI Assistant Answer: ** </span><span class=se>\n</span><span class=si>{</span><span class=n>output</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>))</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;</span><span class=se>\n\n</span><span class=s1>** Relevant Documentation: ** </span><span class=se>\n</span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>))</span>
</code></pre></div> <p>** AI Assistant Answer: ** Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊</p> <p>The context provided does not mention what specific types of bias Amazon SageMaker Clarify can detect. The information given focuses more on the general concept of measuring bias in machine learning models, but does not go into the details of what SageMaker Clarify is capable of detecting. Without more specific information in the context, I cannot provide a definitive answer to your question. Please let me know if you have any other questions I can try to assist with.</p> <p>** Relevant Documentation: ** [Document(page_content='What is Amazon SageMaker Autopilot?," SageMaker Autopilot is the industry’s first automated machine learning capability that gives you complete control and visibility into your ML models. SageMaker Autopilot automatically inspects raw data, applies feature processors, picks the best set of algorithms, trains and tunes multiple models, tracks their performance, and then ranks the models based on performance, all with just a few clicks. The result is the best-performing model that you can deploy at a fraction of the time normally required to train the model. You get full visibility into how the model was created and what’s in it, and SageMaker Autopilot integrates with SageMaker Studio. You can explore up to 50 different models generated by SageMaker Autopilot inside SageMaker Studio so it’s easy to pick the best model for your use case. SageMaker Autopilot can be used by people without ML experience to easily produce a model, or it can be used by experienced developers to quickly develop a baseline model on which teams can further iterate."', metadata={}), Document(page_content='What is Amazon SageMaker Studio Lab?," SageMaker Studio Lab is a free ML development environment that provides the compute, storage (up to 15 GB), and security—all at no cost—for anyone to learn and experiment with ML. All you need to get started is a valid email ID; you don’t need to configure infrastructure or manage identity and access or even sign up for an AWS account. SageMaker Studio Lab accelerates model building through GitHub integration, and it comes preconfigured with the most popular ML tools, frameworks, and libraries to get you started immediately. SageMaker Studio Lab automatically saves your work so you don’t need to restart between sessions. It’s as easy as closing your laptop and coming back later."', metadata={}), Document(page_content='What kind of bias does SageMaker Clarify detect?," Measuring bias in ML models is a first step to mitigating bias. Bias may be measured before training and after training, as well as for inference for a deployed model. Each measure of bias corresponds to a different notion of fairness. Even considering simple notions of fairness leads to many different measures applicable in various contexts. You must choose bias notions and metrics that are valid for the application and the situation under investigation. SageMaker currently supports the computation of different bias metrics for training data (as part of SageMaker data preparation), for the trained model (as part of Amazon SageMaker Experiments), and for inference for a deployed model (as part of Amazon SageMaker Model Monitor). For example, before training, we provide metrics for checking whether the training data is representative (that is, whether one group is underrepresented) and whether there are differences in the label distribution across groups. After training or during deployment, metrics can be helpful to measure whether (and by how much) the performance of the model differs across groups. For example, start by comparing the error rates (how likely a model\'s prediction is to differ from the true label) or break further down into precision (how likely a positive prediction is to be correct) and recall (how likely the model will correctly label a positive example)."', metadata={}), Document(page_content='How can I reproduce a feature from a given moment in time?, SageMaker Feature Store maintains time stamps for all features at every instance of time. This helps you retrieve features at any period of time for business or compliance requirements. You can easily explain model features and their values from when they were first created to the present time by reproducing the model from a given moment in time.\nWhat are offline features?," Offline features are used for training because you need access to very large volumes over a long period of time. These features are served from a high-throughput, high-bandwidth repository."\nWhat are online features?, Online features are used in applications required to make real-time predictions. Online features are served from a high-throughput repository with single-digit millisecond latency for fast predictions.', metadata={}), Document(page_content='Why should I use SageMaker for shadow testing?," SageMaker simplifies the process of setting up and monitoring shadow variants so you can evaluate the performance of the new ML model on live production traffic. SageMaker eliminates the need for you to orchestrate infrastructure for shadow testing. It lets you control testing parameters such as the percentage of traffic mirrored to the shadow variant and the duration of the test. As a result, you can start small and increase the inference requests to the new model after you gain confidence in model performance. SageMaker creates a live dashboard displaying performance differences across key metrics, so you can easily compare model performance to evaluate how the new model differs from the production model."', metadata={})]</p> <hr> <h2 id=using-langchain-for-orchestration-of-rag>Using LangChain for Orchestration of RAG</h2> <p>Beyond the primitive classes for prompt handling and conversational memory management, LangChain also provides a framework for <a href=https://python.langchain.com/docs/expression_language/cookbook/retrieval>orchestrating RAG flows</a> with what purpose built "chains". In this section, we will see how to be a retrieval chain with LangChain which is more comprehensive and robust than the original retrieval system we built above.</p> <p>The workflow we used above follows the following process...</p> <ol> <li>User input is received.</li> <li>User input is queried against the vector database to retrieve relevant documents.</li> <li>Relevant documents and chat memory are inserted into a new prompt to respond to the user input.</li> <li>Return to step 1.</li> </ol> <p>However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...</p> <ol> <li>User input is received.</li> <li>An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and other instructions. This could include things like condensing, rewording, addition of chat context, or stylistic changes.</li> <li>Reformatted user input is queried against the vector database to retrieve relevant documents.</li> <li>The reformatted user input and relevant documents are inserted into a new prompt in order to answer the user question.</li> <li>Return to step 1.</li> </ol> <p>Let's now build out this second workflow using LangChain below.</p> <p>First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output.</p> <div class=highlight><pre><span></span><code><span class=n>condense_prompt</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&quot;&quot;&quot;</span>
<span class=si>{context}</span>

<span class=si>{input}</span>

<span class=s2>Human: Given the conversation above, rewrite the follow-up message to be a standalone question that captures all relevant context. Answer only with the new question and nothing else.</span>

<span class=s2>Assistant: Standalone Question:</span>
<span class=s2>&quot;&quot;&quot;</span><span class=p>)</span>
</code></pre></div> <p>The next prompt we need is the prompt which will answer the user's question based on the retrieved information. In this case, we provide specific instructions about how to answer the question as well as provide the context retrieved from the vector database.</p> <div class=highlight><pre><span></span><code><span class=n>respond_prompt</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&quot;&quot;&quot;</span>
<span class=si>{context}</span>

<span class=s2>Human: Given the context above, answer the following question:</span>

<span class=si>{input}</span>

<span class=s2>If the answer is not in the context, say &quot;Sorry, I don&#39;t know as the answer was not found in the context.&quot;</span>

<span class=s2>Assistant:</span>
<span class=s2>&quot;&quot;&quot;</span><span class=p>)</span>
</code></pre></div> <p>Now that we have our prompts set up, let's set up the conversational memory just like we did earlier in the notebook. Notice how we inject an example human and assistant message in order to help guide our AI assistant on what its job is.</p> <div class=highlight><pre><span></span><code><span class=n>llm</span> <span class=o>=</span> <span class=n>ChatBedrock</span><span class=p>(</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;anthropic.claude-instant-v1&quot;</span><span class=p>,</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span> <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>}</span>
<span class=p>)</span>
<span class=n>memory_chain</span> <span class=o>=</span> <span class=n>ChatMessageHistory</span><span class=p>()</span>
<span class=n>memory_chain</span><span class=o>.</span><span class=n>add_user_message</span><span class=p>(</span>
    <span class=s1>&#39;Hello, what are you able to do?&#39;</span>
<span class=p>)</span>
<span class=n>memory_chain</span><span class=o>.</span><span class=n>add_ai_message</span><span class=p>(</span>
    <span class=s1>&#39;Hi! I am a help chat assistant which can answer questions about Amazon SageMaker.&#39;</span>
<span class=p>)</span>
</code></pre></div> <p>Lastly, we will used the <code>create_retrieval_chain</code> and <code>create_stuff_documents_chain</code> from LangChain to orchestrate this whole system</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>create_retrieval_chain</span>
<span class=kn>from</span> <span class=nn>langchain.chains.combine_documents</span> <span class=kn>import</span> <span class=n>create_stuff_documents_chain</span>


<span class=n>qa</span> <span class=o>=</span> <span class=n>create_stuff_documents_chain</span><span class=p>(</span><span class=n>llm</span><span class=p>,</span> <span class=n>condense_prompt</span><span class=p>)</span>

<span class=n>rag_chain</span> <span class=o>=</span> <span class=n>create_retrieval_chain</span><span class=p>(</span>
    <span class=n>vs</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(),</span> <span class=c1># this is our FAISS vector database</span>
    <span class=n>qa</span>
<span class=p>)</span>
</code></pre></div> <p>Let's go ahead and generate some responses from our RAG solution!</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>rag_chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s1>&#39;input&#39;</span><span class=p>:</span> <span class=s1>&#39;How can I check for imbalances in my model?&#39;</span><span class=p>})</span> <span class=c1>#qa.run({&#39;question&#39;: &#39;How can I check for imbalances in my model?&#39;})</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=s2>&quot;answer&quot;</span><span class=p>]))</span>
</code></pre></div> <p>Here are a few types of bias that SageMaker Clarify can detect:</p> <ul> <li> <p>Demographic parity: Whether the model predicts equally for different demographic groups (e.g. gender, race, etc.). </p> </li> <li> <p>Equal opportunity: Whether the model has equal true positive rates for different groups. </p> </li> <li> <p>Calibration: Whether the predicted probabilities are well-calibrated across groups.</p> </li> <li> <p>Accuracy equality: Whether the model achieves similar overall accuracy for different groups. </p> </li> <li> <p>Fairness through unawareness: Whether the model treats sensitive attributes (e.g. gender) as irrelevant to predictions.</p> </li> </ul> <p>Does this help summarize the kinds of bias SageMaker Clarify can measure? Let me know if you need any clarification or have additional questions.</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>rag_chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s1>&#39;input&#39;</span><span class=p>:</span> <span class=s1>&#39;What kind does it detect?&#39;</span> <span class=p>})</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=s2>&quot;answer&quot;</span><span class=p>]))</span>
</code></pre></div> <p>SageMaker Clarify can detect different types of algorithmic bias like representation bias, accuracy bias and calibration bias. Which specific type of bias detection capabilities are provided by SageMaker Clarify - representation bias, accuracy bias or calibration bias?</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>rag_chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>({</span><span class=s1>&#39;input&#39;</span><span class=p>:</span> <span class=s1>&#39;How does this improve model explainability?&#39;</span> <span class=p>})</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=s2>&quot;answer&quot;</span><span class=p>]))</span>
</code></pre></div> <p>SageMaker Clarify improves model explainability in several ways:</p> <p>It provides a feature importance graph to show the influence of each input on the model's decisions. This helps determine if any inputs have undue influence. </p> <p>It also exposes explanations for individual predictions via an API. This allows understanding why the model produced a certain prediction for a given example.</p> <p>Together, these capabilities offer more transparency into how the model works. Understanding feature importance and reason codes for predictions aids in interpreting a model's behavior and identifying any potential issues.</p> <hr> <h2 id=using-llamaindex-for-orchestration-of-rag>Using LlamaIndex for Orchestration of RAG</h2> <p>Another popular open source framework for orchestrating RAG is <a href=https://gpt-index.readthedocs.io/en/latest/index.html>LlamaIndex</a>. Let's take a look below at how to use our SageMaker FAQ vector index to have a conversational RAG application with LlamaIndex.</p> <div class=highlight><pre><span></span><code><span class=o>%</span><span class=n>pip</span> <span class=n>install</span> <span class=n>llama</span><span class=o>-</span><span class=n>index</span><span class=o>-</span><span class=n>llms</span><span class=o>-</span><span class=n>langchain</span>
<span class=o>%</span><span class=n>pip</span> <span class=n>install</span> <span class=n>llama</span><span class=o>-</span><span class=n>index</span><span class=o>-</span><span class=n>embeddings</span><span class=o>-</span><span class=n>langchain</span>
<span class=o>%</span><span class=n>pip</span> <span class=n>install</span> <span class=n>llama</span><span class=o>-</span><span class=n>index</span><span class=o>-</span><span class=n>vector</span><span class=o>-</span><span class=n>stores</span><span class=o>-</span><span class=n>faiss</span>
</code></pre></div> <pre><code>Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: llama-index-llms-langchain in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.4.1)
Requirement already satisfied: langchain&gt;=0.1.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-llms-langchain) (0.3.1)
Requirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-llms-langchain) (0.11.8)
Requirement already satisfied: PyYAML&gt;=5.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (6.0.2)
Requirement already satisfied: SQLAlchemy&lt;3,&gt;=1.4 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (2.0.34)
Requirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (3.10.5)
Requirement already satisfied: async-timeout&lt;5.0.0,&gt;=4.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (4.0.3)
Requirement already satisfied: langchain-core&lt;0.4.0,&gt;=0.3.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (0.3.7)
Requirement already satisfied: langchain-text-splitters&lt;0.4.0,&gt;=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (0.3.0)
Requirement already satisfied: langsmith&lt;0.2.0,&gt;=0.1.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (0.1.129)
Requirement already satisfied: numpy&lt;2,&gt;=1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.26.4)
Requirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.4 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (2.9.1)
Requirement already satisfied: requests&lt;3,&gt;=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (2.32.3)
Requirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (8.3.0)
Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (0.6.7)
Requirement already satisfied: deprecated&gt;=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.2.14)
Requirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.0.8)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (2024.6.1)
Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (0.27.2)
Requirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.6.0)
Requirement already satisfied: networkx&gt;=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (3.2.1)
Requirement already satisfied: nltk&gt;3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (3.9.1)
Requirement already satisfied: pillow&gt;=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (9.4.0)
Requirement already satisfied: tiktoken&gt;=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (0.7.0)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (4.66.5)
Requirement already satisfied: typing-extensions&gt;=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (4.12.2)
Requirement already satisfied: typing-inspect&gt;=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (0.9.0)
Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.16.0)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (2.4.0)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.3.1)
Requirement already satisfied: attrs&gt;=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (23.2.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.4.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (6.1.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.11.1)
Requirement already satisfied: jsonpatch&lt;2.0,&gt;=1.33 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain-core&lt;0.4.0,&gt;=0.3.6-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.33)
Requirement already satisfied: packaging&lt;25,&gt;=23.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langchain-core&lt;0.4.0,&gt;=0.3.6-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (23.2)
Requirement already satisfied: orjson&lt;4.0.0,&gt;=3.9.14 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from langsmith&lt;0.2.0,&gt;=0.1.17-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (3.10.7)
Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (4.4.0)
Requirement already satisfied: certifi in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (2023.7.22)
Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.0.5)
Requirement already satisfied: idna in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (3.7)
Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.3.1)
Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (0.14.0)
Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (8.1.7)
Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.4.2)
Requirement already satisfied: regex&gt;=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (2024.7.24)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.4-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (0.7.0)
Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.4-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (2.23.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&lt;3,&gt;=2-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (3.3.2)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&lt;3,&gt;=2-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (1.26.18)
Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy&lt;3,&gt;=1.4-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (3.1.0)
Requirement already satisfied: mypy-extensions&gt;=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.0.0)
Requirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (3.22.0)
Requirement already satisfied: jsonpointer&gt;=1.9 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from jsonpatch&lt;2.0,&gt;=1.33-&gt;langchain-core&lt;0.4.0,&gt;=0.3.6-&gt;langchain&gt;=0.1.3-&gt;llama-index-llms-langchain) (3.0.0)
Requirement already satisfied: exceptiongroup&gt;=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-llms-langchain) (1.2.2)
Note: you may need to restart the kernel to use updated packages.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: llama-index-embeddings-langchain in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.2.1)
Requirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-embeddings-langchain) (0.11.8)
Requirement already satisfied: PyYAML&gt;=6.0.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (6.0.2)
Requirement already satisfied: SQLAlchemy&gt;=1.4.49 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2.0.34)
Requirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.10.5)
Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.6.7)
Requirement already satisfied: deprecated&gt;=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.2.14)
Requirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.0.8)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2024.6.1)
Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.27.2)
Requirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.6.0)
Requirement already satisfied: networkx&gt;=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.2.1)
Requirement already satisfied: nltk&gt;3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.9.1)
Requirement already satisfied: numpy&lt;2.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.26.4)
Requirement already satisfied: pillow&gt;=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (9.4.0)
Requirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2.9.1)
Requirement already satisfied: requests&gt;=2.31.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2.32.3)
Requirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (8.3.0)
Requirement already satisfied: tiktoken&gt;=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.7.0)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (4.66.5)
Requirement already satisfied: typing-extensions&gt;=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (4.12.2)
Requirement already satisfied: typing-inspect&gt;=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.9.0)
Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.16.0)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2.4.0)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.3.1)
Requirement already satisfied: attrs&gt;=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (23.2.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.4.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (6.1.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.11.1)
Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (4.0.3)
Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (8.1.7)
Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.4.2)
Requirement already satisfied: regex&gt;=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2024.7.24)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.7.0)
Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2.23.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.7)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.26.18)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (2023.7.22)
Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.1.0)
Requirement already satisfied: mypy-extensions&gt;=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.0.0)
Requirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (3.22.0)
Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (4.4.0)
Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.0.5)
Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.3.1)
Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (0.14.0)
Requirement already satisfied: packaging&gt;=17.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (23.2)
Requirement already satisfied: exceptiongroup&gt;=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-embeddings-langchain) (1.2.2)
Note: you may need to restart the kernel to use updated packages.
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: llama-index-vector-stores-faiss in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (0.2.1)
Requirement already satisfied: llama-index-core&lt;0.12.0,&gt;=0.11.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-vector-stores-faiss) (0.11.8)
Requirement already satisfied: PyYAML&gt;=6.0.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (6.0.2)
Requirement already satisfied: SQLAlchemy&gt;=1.4.49 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2.0.34)
Requirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.6 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.10.5)
Requirement already satisfied: dataclasses-json in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.6.7)
Requirement already satisfied: deprecated&gt;=1.2.9.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.2.14)
Requirement already satisfied: dirtyjson&lt;2.0.0,&gt;=1.0.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.0.8)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2024.6.1)
Requirement already satisfied: httpx in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.27.2)
Requirement already satisfied: nest-asyncio&lt;2.0.0,&gt;=1.5.8 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.6.0)
Requirement already satisfied: networkx&gt;=3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.2.1)
Requirement already satisfied: nltk&gt;3.8.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.9.1)
Requirement already satisfied: numpy&lt;2.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.26.4)
Requirement already satisfied: pillow&gt;=9.0.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (9.4.0)
Requirement already satisfied: pydantic&lt;3.0.0,&gt;=2.7.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2.9.1)
Requirement already satisfied: requests&gt;=2.31.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2.32.3)
Requirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.2.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (8.3.0)
Requirement already satisfied: tiktoken&gt;=0.3.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.7.0)
Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.66.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (4.66.5)
Requirement already satisfied: typing-extensions&gt;=4.5.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (4.12.2)
Requirement already satisfied: typing-inspect&gt;=0.8.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.9.0)
Requirement already satisfied: wrapt in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.16.0)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2.4.0)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.3.1)
Requirement already satisfied: attrs&gt;=17.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (23.2.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.4.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (6.1.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.11.1)
Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.6-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (4.0.3)
Requirement already satisfied: click in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (8.1.7)
Requirement already satisfied: joblib in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.4.2)
Requirement already satisfied: regex&gt;=2021.8.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from nltk&gt;3.8.1-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2024.7.24)
Requirement already satisfied: annotated-types&gt;=0.6.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.7.0)
Requirement already satisfied: pydantic-core==2.23.3 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from pydantic&lt;3.0.0,&gt;=2.7.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2.23.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.7)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.26.18)
Requirement already satisfied: certifi&gt;=2017.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from requests&gt;=2.31.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (2023.7.22)
Requirement already satisfied: greenlet!=0.4.17 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from SQLAlchemy&gt;=1.4.49-&gt;SQLAlchemy[asyncio]&gt;=1.4.49-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.1.0)
Requirement already satisfied: mypy-extensions&gt;=0.3.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from typing-inspect&gt;=0.8.0-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.0.0)
Requirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (3.22.0)
Requirement already satisfied: anyio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (4.4.0)
Requirement already satisfied: httpcore==1.* in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.0.5)
Requirement already satisfied: sniffio in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.3.1)
Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (0.14.0)
Requirement already satisfied: packaging&gt;=17.0 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from marshmallow&lt;4.0.0,&gt;=3.18.0-&gt;dataclasses-json-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (23.2)
Requirement already satisfied: exceptiongroup&gt;=1.0.2 in /Users/sergncp/Library/Python/3.9/lib/python/site-packages (from anyio-&gt;httpx-&gt;llama-index-core&lt;0.12.0,&gt;=0.11.0-&gt;llama-index-vector-stores-faiss) (1.2.2)
Note: you may need to restart the kernel to use updated packages.
</code></pre> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>Markdown</span><span class=p>,</span> <span class=n>display</span>
<span class=kn>from</span> <span class=nn>langchain.embeddings.bedrock</span> <span class=kn>import</span> <span class=n>BedrockEmbeddings</span>
<span class=kn>from</span> <span class=nn>langchain_aws.llms</span> <span class=kn>import</span> <span class=n>BedrockLLM</span>

<span class=kn>from</span> <span class=nn>llama_index.core</span> <span class=kn>import</span> <span class=n>Settings</span><span class=p>,</span> <span class=n>StorageContext</span>
</code></pre></div> <p>First we need to set up the system setting to define the embedding model and LLM. Again, we will be using titan and claude respectively.</p> <div class=highlight><pre><span></span><code><span class=n>embed_model</span> <span class=o>=</span> <span class=n>BedrockEmbeddings</span><span class=p>(</span><span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span> <span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;amazon.titan-embed-text-v1&quot;</span><span class=p>)</span>
<span class=n>llm</span> <span class=o>=</span> <span class=n>BedrockLLM</span><span class=p>(</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;anthropic.claude-instant-v1&quot;</span><span class=p>,</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span>
        <span class=s2>&quot;max_tokens_to_sample&quot;</span><span class=p>:</span> <span class=mi>500</span><span class=p>,</span>
        <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
    <span class=p>},</span>
<span class=p>)</span>
<span class=n>Settings</span><span class=o>.</span><span class=n>llm</span> <span class=o>=</span> <span class=n>llm</span>
<span class=n>Settings</span><span class=o>.</span><span class=n>embed_model</span> <span class=o>=</span> <span class=n>embed_model</span>
<span class=n>Settings</span><span class=o>.</span><span class=n>chunk_size</span> <span class=o>=</span> <span class=mi>512</span>

<span class=sd>&quot;&quot;&quot; service_context = StorageContext.from_defaults(</span>
<span class=sd>    llm=llm, embed_model=embed_model, chunk_size=512</span>
<span class=sd>)  &quot;&quot;&quot;</span>
</code></pre></div> <pre><code>' service_context = StorageContext.from_defaults(\n    llm=llm, embed_model=embed_model, chunk_size=512\n)  '
</code></pre> <p>The next step would be to create a FAISS index from our document base. In this lab, this is already done for you and stored in the <a href=../faiss-index/llama-index/ >faiss-index/llama-index/</a> folder.</p> <p>If you are interested in how this was accomplished, follow <a href=https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/FaissIndexDemo.html>this tutorial</a> from LlamIndex. The code below is the basics of how this was accomplished as well.</p> <div class=highlight><pre><span></span><code><span class=c1># faiss_index = faiss.IndexFlatL2(1536)</span>
<span class=c1># vector_store = FaissVectorStore(faiss_index=faiss_index)</span>
<span class=c1># documents = SimpleDirectoryReader(&quot;./../data/sagemaker&quot;).load_data()</span>
<span class=c1># vector_store = FaissVectorStore(faiss_index=faiss_index)</span>
<span class=c1># storage_context = StorageContext.from_defaults(vector_store=vector_store)</span>
<span class=c1># index = VectorStoreIndex.from_documents(</span>
<span class=c1>#     documents, storage_context=storage_context, service_context=service_context</span>
<span class=c1># )</span>
<span class=c1># index.storage_context.persist(&#39;../faiss-index/llama-index/&#39;)</span>
</code></pre></div> <p>Once the index is created, we can load the persistent files to a <code>FaissVectorStore</code> object and create a <code>query_engine</code> from the vector index. To learn more about indicies in LlamaIndex, read more <a href=https://gpt-index.readthedocs.io/en/latest/understanding/indexing/indexing.html>here</a>.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>llama_index.core</span> <span class=kn>import</span> <span class=n>load_index_from_storage</span><span class=p>,</span> <span class=n>StorageContext</span>
<span class=kn>from</span> <span class=nn>llama_index.vector_stores.faiss</span> <span class=kn>import</span> <span class=n>FaissVectorStore</span>

<span class=n>vector_store</span> <span class=o>=</span> <span class=n>FaissVectorStore</span><span class=o>.</span><span class=n>from_persist_path</span><span class=p>(</span><span class=s2>&quot;../faiss-index/llama-index/vector_store.json&quot;</span><span class=p>)</span>
<span class=n>storage_context</span> <span class=o>=</span> <span class=n>StorageContext</span><span class=o>.</span><span class=n>from_defaults</span><span class=p>(</span>
    <span class=n>vector_store</span><span class=o>=</span><span class=n>vector_store</span><span class=p>,</span> <span class=n>persist_dir</span><span class=o>=</span><span class=s2>&quot;../faiss-index/llama-index&quot;</span>
<span class=p>)</span>
<span class=n>index</span> <span class=o>=</span> <span class=n>load_index_from_storage</span><span class=p>(</span><span class=n>storage_context</span><span class=o>=</span><span class=n>storage_context</span><span class=p>)</span> <span class=c1>#, service_context = service_context)</span>
<span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>()</span>
</code></pre></div> <p>Now let's set up a retrieval based chat application similar to LangChain. We will use the same condensing question strategy as before and can reuse the same prompt to condense the question for vector searching. Notice how we include some custom chat history to inject context into the prompt for the model to understand what we are asking questions about. The resulting <code>chat_engine</code> object is now fully ready to chat about our documents.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>llama_index.core.prompts</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>llama_index.core.llms</span> <span class=kn>import</span> <span class=n>ChatMessage</span><span class=p>,</span> <span class=n>MessageRole</span>
<span class=kn>from</span> <span class=nn>llama_index.core.chat_engine.condense_question</span> <span class=kn>import</span> <span class=n>CondenseQuestionChatEngine</span>
<span class=n>custom_prompt</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=p>(</span><span class=s2>&quot;&quot;&quot;</span>
<span class=si>{chat_history}</span>
<span class=si>{question}</span>
<span class=s2>Human: Given the conversation above, rewrite the message to be a standalone question that captures all relevant context from the conversation. Answer only with the new question and nothing else.</span>
<span class=s2>Assistant: Standalone Question:</span>
<span class=s2>&quot;&quot;&quot;</span><span class=p>)</span>
<span class=n>custom_chat_history</span> <span class=o>=</span> <span class=p>[</span>
 <span class=n>ChatMessage</span><span class=p>(</span>
 <span class=n>role</span><span class=o>=</span><span class=n>MessageRole</span><span class=o>.</span><span class=n>USER</span><span class=p>,</span>
 <span class=n>content</span><span class=o>=</span><span class=s1>&#39;Hello assistant, I have some questions about using Amazon SageMaker today.&#39;</span>
 <span class=p>),</span>
 <span class=n>ChatMessage</span><span class=p>(</span>
 <span class=n>role</span><span class=o>=</span><span class=n>MessageRole</span><span class=o>.</span><span class=n>ASSISTANT</span><span class=p>,</span>
 <span class=n>content</span><span class=o>=</span><span class=s1>&#39;Okay, sounds good.&#39;</span>
 <span class=p>)</span>
<span class=p>]</span>
<span class=n>query_engine</span> <span class=o>=</span> <span class=n>index</span><span class=o>.</span><span class=n>as_query_engine</span><span class=p>()</span>
<span class=n>chat_engine</span> <span class=o>=</span> <span class=n>CondenseQuestionChatEngine</span><span class=o>.</span><span class=n>from_defaults</span><span class=p>(</span>
 <span class=n>query_engine</span><span class=o>=</span><span class=n>query_engine</span><span class=p>,</span>
 <span class=n>condense_question_prompt</span><span class=o>=</span><span class=n>custom_prompt</span><span class=p>,</span>
 <span class=n>chat_history</span><span class=o>=</span><span class=n>custom_chat_history</span><span class=p>,</span>
 <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span>
<span class=p>)</span>
</code></pre></div> <p>Let's go ahead and ask our first question. Notice that the verbose <code>chat_engine</code> will print out the condensed question as well.</p> <div class=highlight><pre><span></span><code><span class=n>response</span> <span class=o>=</span> <span class=n>chat_engine</span><span class=o>.</span><span class=n>chat</span><span class=p>(</span><span class=s2>&quot;How can I check for imbalances in my model?&quot;</span><span class=p>)</span>
<span class=n>output</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <pre><code>Querying with: 
How can I check for imbalances in my model when using Amazon SageMaker?
</code></pre> <p>Based on the context provided, here is how you can check for imbalances in your model when using Amazon SageMaker:</p> <p>Amazon SageMaker Clarify helps improve model transparency by detecting statistical bias across the entire ML workflow. SageMaker Clarify checks for imbalances during data preparation, after training, and ongoing over time. It includes tools to help explain ML models and their predictions. Findings can be shared through explainability reports. </p> <p>Specifically, SageMaker Clarify can detect various types of biases by computing different bias metrics:</p> <ul> <li> <p>Before training, it provides metrics to check if the training data is representative (not underrepresented for any group) and if there are differences in label distributions across groups.</p> </li> <li> <p>After training or during deployment, it provides metrics to measure if model performance differs across groups, such as comparing error rates or precision and recall across groups. </p> </li> </ul> <p>So in summary, to check for imbalances in your SageMaker model, you can leverage the bias detection and explainability features of SageMaker Clarify. It analyzes your data, model and predictions to surface any statistical biases across protected attributes like gender or ethnicity.</p> <p>Now follow up questions can be asked with conversational context in mind!</p> <div class=highlight><pre><span></span><code><span class=n>response</span> <span class=o>=</span> <span class=n>chat_engine</span><span class=o>.</span><span class=n>chat</span><span class=p>(</span><span class=s2>&quot;How does this improve model explainability?&quot;</span><span class=p>)</span>
<span class=n>output</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <pre><code>Querying with: 
How can I leverage Amazon SageMaker Clarify to check for and analyze potential imbalances or biases in my model's training data, performance across groups, and ongoing predictions?
</code></pre> <p>Based on the context provided, here are some ways Amazon SageMaker Clarify can help check for and analyze potential imbalances or biases in a model:</p> <ul> <li> <p>SageMaker Clarify can check the training data for representativeness and differences in label distributions across groups, to detect potential data imbalances before training. </p> </li> <li> <p>After model training or during deployment, Clarify provides metrics to measure if model performance (e.g. error rates, precision, recall) differs across groups, indicating potential biases. </p> </li> <li> <p>Clarify integrates with SageMaker Experiments to provide feature importance graphs showing influence of inputs on model behavior, helping determine if certain inputs have undue influence related to biases.</p> </li> <li> <p>Clarify's explanations for individual predictions, available through an API, can help analyze ongoing predictions for potential biases over time. </p> </li> <li> <p>The findings from Clarify's various bias checks and analyses can be shared in explainability reports to improve model transparency and help mitigate any detected biases.</p> </li> </ul> <p>So in summary, SageMaker Clarify enables comprehensive checking for and analysis of potential imbalances or biases at different stages of the ML workflow and ongoing model predictions. Its integrated tools aim to improve model transparency and help address any fairness issues.</p> <hr> <h2 id=next-steps>Next steps</h2> <p>Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs.</p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9zm4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21zm0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12zM15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3zm0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by <a href="https://github.com/aws-samples/amazon-bedrock-samples/issues/new?title=[Online Feedback]: Short-Summary-of-Issue&body=Page URL: /workshop/open-source-l200/04_retrieval_based_chat_application/" target=_blank rel=noopener>creating an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../03_retrieval_based_text_application/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Retrieval Based Text Generation"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Retrieval Based Text Generation </div> </div> </a> <a href=../05_agent_based_text_generation/ class="md-footer__link md-footer__link--next" aria-label="Next: Agent Based Text Generation"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Agent Based Text Generation </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </div> </div> <div class=md-social> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["tags", "toc.integrate", "content.code.copy", "content.code.select", "content.code.annotate", "navigation.footer", "search.highlight", "search.suggest"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "tags": {"Compatibility": "compat"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.88dd0f4e.min.js></script> <script src=../../../javascript/feedback.js></script> </body> </html>