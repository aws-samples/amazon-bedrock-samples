<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Amazon Bedrock cookbook website"><meta name=author content=Bedrock-GTM><link href=https://github.amazon-bedrock-samples.com/workshop/open-source-l200/DO_NOT_USE_04_retrieval_based_chat/ rel=canonical><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.49"><title>Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG - Amazon Bedrock Recipes</title><link rel=stylesheet href=../../../assets/stylesheets/main.6f8fc17f.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#retrieval-augmented-generation-with-amazon-bedrock-enhancing-chat-applications-with-rag class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="Amazon Bedrock Recipes" class="md-header__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Amazon Bedrock Recipes </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=purple aria-hidden=true type=radio name=__palette id=__palette_0> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Amazon Bedrock Recipes" class="md-nav__button md-logo" aria-label="Amazon Bedrock Recipes" data-md-component=logo> <img src=../../../logo.png alt=logo> </a> Amazon Bedrock Recipes </label> <div class=md-nav__source> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> Github: Amazon-Bedrock-Samples </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Features </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1> <label class=md-nav__link for=__nav_2_1 id=__nav_2_1_label tabindex=0> <span class=md-ellipsis> Intro to Amazon Bedrock </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Intro to Amazon Bedrock </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_1> <label class=md-nav__link for=__nav_2_1_1 id=__nav_2_1_1_label tabindex=0> <span class=md-ellipsis> API Usage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_1_1> <span class="md-nav__icon md-icon"></span> API Usage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/01_invoke_api/ class=md-nav__link> <span class=md-ellipsis> Invoke Model API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/04_agents_api/ class=md-nav__link> <span class=md-ellipsis> Agents API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/03_knowledgebases_api/ class=md-nav__link> <span class=md-ellipsis> Knowledge Bases API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/bedrock_apis/02_guardrails_api/ class=md-nav__link> <span class=md-ellipsis> Guardrail API Example </span> </a> </li> <li class=md-nav__item> <a href=../../../introduction-to-bedrock/converse_api/01_converse_api/ class=md-nav__link> <span class=md-ellipsis> Converse API Example </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1> <label class=md-nav__link for=__nav_2_2_1 id=__nav_2_2_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Agents </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/introduction-to-agents/how_to_create_custom_agents/ class=md-nav__link> <span class=md-ellipsis> How to create an Agent </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_2> <label class=md-nav__link for=__nav_2_2_1_2 id=__nav_2_2_1_2_label tabindex=0> <span class=md-ellipsis> Bedrock Agent Features </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_2> <span class="md-nav__icon md-icon"></span> Bedrock Agent Features </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/01-create-agent-with-function-definition/01-create-agent-with-function-definition/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Function Definition </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/02-create-agent-with-api-schema/02-create-agent-with-api-schema/ class=md-nav__link> <span class=md-ellipsis> Create Agent with API Schema </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/03-create-agent-with-return-of-control/03-create-agent-with-return-of-control/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/04-create-agent-with-single-knowledge-base/04-create-agent-with-single-knowledge-base/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Single Knowledge Base </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/05-create-agent-with-knowledge-base-and-action-group/05-create-agent-with-knowledge-base-and-action-group/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Knowledge Base and Action Group </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/06-prompt-and-session-attributes/06-prompt-and-session-attributes/ class=md-nav__link> <span class=md-ellipsis> Prompt and Session Attributes </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/07-advanced-prompts-and-custom-parsers/07-custom-prompt-and-lambda-parsers/ class=md-nav__link> <span class=md-ellipsis> Custom Prompt and Lambda Parsers </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/08-create-agent-with-guardrails/08-create-agent-with-guardrails/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Guardrails </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/09-create-agent-with-memory/09-create-agent-with-memory/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Memory </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/10-create-agent-with-code-interpreter/10-create-agent-with-code-interpreter/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Code Interpreter </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/14-create-agent-with-custom-orchestration/custom_orchestration_example/ class=md-nav__link> <span class=md-ellipsis> Create Agent with Custom Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents/inline-agent-api-usage/ class=md-nav__link> <span class=md-ellipsis> Create Dynamic Tooling Inline Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_3> <label class=md-nav__link for=__nav_2_2_1_3 id=__nav_2_2_1_3_label tabindex=0> <span class=md-ellipsis> Bedrock Flows </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_3> <span class="md-nav__icon md-icon"></span> Bedrock Flows </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/bedrock-flows/Getting_started_with_Prompt_Management_Flows/ class=md-nav__link> <span class=md-ellipsis> Getting Started with Prompt Management Flows </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_1_4> <label class=md-nav__link for=__nav_2_2_1_4 id=__nav_2_2_1_4_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_1_4> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/text-2-sql-agent/create_and_invoke_sql_agent/ class=md-nav__link> <span class=md-ellipsis> Text to SQL Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/agentsforbedrock-retailagent/workshop/test_retailagent_agentsforbedrock/ class=md-nav__link> <span class=md-ellipsis> Retail Agent Workshop </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/bedrock-agents/use-case-examples/product-review-agent/main/ class=md-nav__link> <span class=md-ellipsis> Product Review Agent </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_2> <label class=md-nav__link for=__nav_2_2_2 id=__nav_2_2_2_label tabindex=0> <span class=md-ellipsis> Function Calling </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_2> <span class="md-nav__icon md-icon"></span> Function Calling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/function_calling_with_converse/function_calling_with_converse/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/function_calling_with_invoke/function_calling_model_with_invoke/ class=md-nav__link> <span class=md-ellipsis> Function Calling with Invoke </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/return_of_control/return_of_control/ class=md-nav__link> <span class=md-ellipsis> Return of Control </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/function-calling/tool_binding/tool_bindings/ class=md-nav__link> <span class=md-ellipsis> Tool Binding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3> <label class=md-nav__link for=__nav_2_2_3 id=__nav_2_2_3_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_1> <label class=md-nav__link for=__nav_2_2_3_1 id=__nav_2_2_3_1_label tabindex=0> <span class=md-ellipsis> CrewAI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_1> <span class="md-nav__icon md-icon"></span> CrewAI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/crew.ai/Find%20dream%20destination%20with%20CrewAI/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_2> <label class=md-nav__link for=__nav_2_2_3_2 id=__nav_2_2_3_2_label tabindex=0> <span class=md-ellipsis> LangGraph </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_2> <span class="md-nav__icon md-icon"></span> LangGraph </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-single-agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-agents-multimodal/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi-Modal Agent with Function Calling </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/02_medibot_V3_agents/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent For Medical Chatbot </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-fact-checker-feedback-loop/ class=md-nav__link> <span class=md-ellipsis> LangGraph Fact Checker with Multi Agent </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/langgraph-multi-agent-sql-tools/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent Orchestration </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/03_langgraph_agents_of_agent/ class=md-nav__link> <span class=md-ellipsis> LangGraph Multi Agent with tools </span> </a> </li> <li class=md-nav__item> <a href=../../../agents-and-function-calling/open-source-agents/langgraph/Travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Managing Memory for Multi Agents </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2_3_3> <label class=md-nav__link for=__nav_2_2_3_3 id=__nav_2_2_3_3_label tabindex=0> <span class=md-ellipsis> Multi Agent </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_2_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2_3_3> <span class="md-nav__icon md-icon"></span> Multi Agent </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../agents-and-function-calling/introduction-to-agents/how_to_create_multi_agents_from_custom_agents/ class=md-nav__link> <span class=md-ellipsis> Multi Agent Orchestration </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> RAG </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> RAG </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1> <label class=md-nav__link for=__nav_2_3_1 id=__nav_2_3_1_label tabindex=0> <span class=md-ellipsis> Amazon Bedrock Knowledge Bases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1> <span class="md-nav__icon md-icon"></span> Amazon Bedrock Knowledge Bases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_1> <label class=md-nav__link for=__nav_2_3_1_1 id=__nav_2_3_1_1_label tabindex=0> <span class=md-ellipsis> Zero Setup </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_1> <span class="md-nav__icon md-icon"></span> Zero Setup </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/00-zero-setup-chat-with-your-document/chat_with_document_kb/ class=md-nav__link> <span class=md-ellipsis> Chat with Your Document </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_2> <label class=md-nav__link for=__nav_2_3_1_2 id=__nav_2_3_1_2_label tabindex=0> <span class=md-ellipsis> RAG Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_2> <span class="md-nav__icon md-icon"></span> RAG Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/01_create_ingest_documents_test_kb_multi_ds/ class=md-nav__link> <span class=md-ellipsis> Create and Ingest Documents with Multi-Data Sources </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/02_managed_rag_custom_prompting_and_no_of_results/ class=md-nav__link> <span class=md-ellipsis> Managed RAG with Custom Prompting </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/03_customized-rag-retreive-api-hybrid-search-claude-3-sonnet-langchain/ class=md-nav__link> <span class=md-ellipsis> Customized RAG with Claude 3 and Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/01-rag-concepts/04_customized-rag-retreive-api-langchain-claude-evaluation-ragas/ class=md-nav__link> <span class=md-ellipsis> RAG Evaluation with Langchain and RAGAS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_3> <label class=md-nav__link for=__nav_2_3_1_3 id=__nav_2_3_1_3_label tabindex=0> <span class=md-ellipsis> Optimizing Retrieval Results </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_3> <span class="md-nav__icon md-icon"></span> Optimizing Retrieval Results </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/advanced_chunking_options/ class=md-nav__link> <span class=md-ellipsis> Advanced Chunking Options </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/csv_metadata_customization/ class=md-nav__link> <span class=md-ellipsis> CSV Metadata Customization </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/02-optimizing-accuracy-retrieved-results/query_reformulation/ class=md-nav__link> <span class=md-ellipsis> Query Reformulation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4> <label class=md-nav__link for=__nav_2_3_1_4 id=__nav_2_3_1_4_label tabindex=0> <span class=md-ellipsis> Advanced Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4> <span class="md-nav__icon md-icon"></span> Advanced Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/dynamic-metadata-filtering/dynamic-metadata-filtering-KB/ class=md-nav__link> <span class=md-ellipsis> Dynamic Metadata Filtering </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_4_2> <label class=md-nav__link for=__nav_2_3_1_4_2 id=__nav_2_3_1_4_2_label tabindex=0> <span class=md-ellipsis> Reranking </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_4_2> <span class="md-nav__icon md-icon"></span> Reranking </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/01_deploy-reranking-model-sm/ class=md-nav__link> <span class=md-ellipsis> Deploy Reranking Model </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/02_kb-reranker/ class=md-nav__link> <span class=md-ellipsis> Knowledge Base Reranker </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/03-advanced-concepts/reranking/qa-generator/ class=md-nav__link> <span class=md-ellipsis> QA Generator </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_5> <label class=md-nav__link for=__nav_2_3_1_5 id=__nav_2_3_1_5_label tabindex=0> <span class=md-ellipsis> Responsible AI </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_5> <span class="md-nav__icon md-icon"></span> Responsible AI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/features-examples/05-responsible-ai/contextual-grounding/ class=md-nav__link> <span class=md-ellipsis> Contextual Grounding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6> <label class=md-nav__link for=__nav_2_3_1_6 id=__nav_2_3_1_6_label tabindex=0> <span class=md-ellipsis> Use Case Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_1_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6> <span class="md-nav__icon md-icon"></span> Use Case Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_1> <label class=md-nav__link for=__nav_2_3_1_6_1 id=__nav_2_3_1_6_1_label tabindex=0> <span class=md-ellipsis> Metadata Filter Access Control </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_1_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_1> <span class="md-nav__icon md-icon"></span> Metadata Filter Access Control </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/metadata-filter-access-control/kb-end-to-end-acl/ class=md-nav__link> <span class=md-ellipsis> End-to-End ACL with Knowledge Base </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_1_6_2> <label class=md-nav__link for=__nav_2_3_1_6_2 id=__nav_2_3_1_6_2_label tabindex=0> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=5 aria-labelledby=__nav_2_3_1_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_1_6_2> <span class="md-nav__icon md-icon"></span> RAG with Structured and Unstructured Data </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/0-create-dummy-structured-data/ class=md-nav__link> <span class=md-ellipsis> Create Dummy Structured Data </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/1_create_sql_dataset_optional/ class=md-nav__link> <span class=md-ellipsis> Create SQL Dataset (Optional) </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/knowledge-bases/use-case-examples/rag-using-structured-unstructured-data/2_rag_with_structured_unstructured_data/ class=md-nav__link> <span class=md-ellipsis> RAG with Structured and Unstructured Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2> <label class=md-nav__link for=__nav_2_3_2 id=__nav_2_3_2_label tabindex=0> <span class=md-ellipsis> Open Source </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2> <span class="md-nav__icon md-icon"></span> Open Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/open-source/chatbots/qa_chatbot_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chatbot using Langchain </span> </a> </li> <li class=md-nav__item> <a href=../../../rag/open-source/chunking/rag_chunking_strategies_langchain_bedrock/ class=md-nav__link> <span class=md-ellipsis> Chunking strategies for RAG applications </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3_2_3> <label class=md-nav__link for=__nav_2_3_2_3 id=__nav_2_3_2_3_label tabindex=0> <span class=md-ellipsis> Vector Stores </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_2_3_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3_2_3> <span class="md-nav__icon md-icon"></span> Vector Stores </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../rag/open-source/vector_stores/rag_langchain_bedrock_opensearch/ class=md-nav__link> <span class=md-ellipsis> Langchain Chatbot with Opensearch </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Model Customization </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Model Customization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../custom-models/model-distillation/Historical_invocation_distillation/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with Invocation Logs </span> </a> </li> <li class=md-nav__item> <a href=../../../custom-models/model-distillation/Distillation-via-S3-input/ class=md-nav__link> <span class=md-ellipsis> Model Distillation with S3 Data </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Gen AI Usecases </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Gen AI Usecases </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_1> <label class=md-nav__link for=__nav_3_1 id=__nav_3_1_label tabindex=0> <span class=md-ellipsis> Text Generation </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_1_label aria-expanded=false> <label class=md-nav__title for=__nav_3_1> <span class="md-nav__icon md-icon"></span> Text Generation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Streaming Response with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_code_generation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Generate Python Code with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text_translation_w_bedrock/ class=md-nav__link> <span class=md-ellipsis> Text Translation with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_text-summarization-titan%2Bclaude/ class=md-nav__link> <span class=md-ellipsis> Text summarization with Converse </span> </a> </li> <li class=md-nav__item> <a href=../../../genai-use-cases/text-generation/how_to_work_with_batch_example_for_multi_threaded_invocation/ class=md-nav__link> <span class=md-ellipsis> Generate Bulk Emails with Batch Inference </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> <span class=md-ellipsis> Workshops </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Workshops </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 id=__nav_4_1_label tabindex=0> <span class=md-ellipsis> Open-source L400 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Open-source L400 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../open-source-l400/01_usecase_introduction/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/02_Lab_Find%20a%20Dream%20Destination_RAG%20query/ class=md-nav__link> <span class=md-ellipsis> Advanced RAG for Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/02_travel_planner_with_langgraph/ class=md-nav__link> <span class=md-ellipsis> Conversational Memory in Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/03_travel_agent_with_tools/ class=md-nav__link> <span class=md-ellipsis> Multi-Modal and Types of Agents </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/04_travel_booking_multi_agent/ class=md-nav__link> <span class=md-ellipsis> Multi-Agent Collaboration with Human-in-loop </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/05_dream_destination_with_crewai/ class=md-nav__link> <span class=md-ellipsis> Find Dream Destination with CrewAI </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/06_agent_evaluation_with_ragas/ class=md-nav__link> <span class=md-ellipsis> RAGAs Agents Evaluation </span> </a> </li> <li class=md-nav__item> <a href=../../open-source-l400/07_dynamic_tooling_agents/ class=md-nav__link> <span class=md-ellipsis> Dynamic Tool invocation </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> Open-source L200 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Open-source L200 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../02_contextual_text_generation/ class=md-nav__link> <span class=md-ellipsis> Introduction to the Use Case </span> </a> </li> <li class=md-nav__item> <a href=../03_retrieval_based_text_application/ class=md-nav__link> <span class=md-ellipsis> Retrieval Based Text Generation </span> </a> </li> <li class=md-nav__item> <a href=../04_retrieval_based_chat_application/ class=md-nav__link> <span class=md-ellipsis> Retrieval Based Chat Application </span> </a> </li> <li class=md-nav__item> <a href=../05_agent_based_text_generation/ class=md-nav__link> <span class=md-ellipsis> Agent Based Text Generation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../general/tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../general/license/ class=md-nav__link> <span class=md-ellipsis> License </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=retrieval-augmented-generation-with-amazon-bedrock-enhancing-chat-applications-with-rag>Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG</h1> <blockquote> <p><em>PLEASE NOTE: This notebook should work well with the </em><em><code>Data Science 3.0</code></em><em> kernel in SageMaker Studio</em></p> </blockquote> <hr> <h2 id=chat-with-llms-overview>Chat with LLMs Overview</h2> <p>Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users.</p> <p>The key technical detail which we need to include in our system to enable a chat feature is conversational memory. This way, customers can ask follow up questions and the LLM will understand what the customer has already said in the past. The image below shows how this is orchestrated at a high level.</p> <p><img alt="Amazon Bedrock - Conversational Interface" src=../images/chatbot_bedrock.png></p> <h2 id=extending-chat-with-rag>Extending Chat with RAG</h2> <p>However, in our workshop's situation, we want to be able to enable a customer to ask follow up questions regarding documentation we provide through RAG. This means we need to build a system which has conversational memory AND contextual retrieval built into the text generation.</p> <p><img alt=4 src=../images/context-aware-chatbot.png></p> <p>Let's get started!</p> <hr> <h2 id=setup-boto3-connection>Setup <code>boto3</code> Connection</h2> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>boto3</span>
<span class=kn>import</span> <span class=nn>os</span>
<span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>Markdown</span><span class=p>,</span> <span class=n>display</span>

<span class=kn>import</span> <span class=nn>logging</span>
<span class=kn>import</span> <span class=nn>boto3</span>


<span class=kn>from</span> <span class=nn>botocore.exceptions</span> <span class=kn>import</span> <span class=n>ClientError</span>

<span class=n>region</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;AWS_REGION&quot;</span><span class=p>)</span>
<span class=n>boto3_bedrock</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=n>service_name</span><span class=o>=</span><span class=s1>&#39;bedrock-runtime&#39;</span><span class=p>,</span>
    <span class=n>region_name</span><span class=o>=</span><span class=n>region</span><span class=p>,</span>
<span class=p>)</span>


<span class=n>logger</span> <span class=o>=</span> <span class=n>logging</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
<span class=n>logging</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>level</span><span class=o>=</span><span class=n>logging</span><span class=o>.</span><span class=n>INFO</span><span class=p>)</span>

<span class=n>logging</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>level</span><span class=o>=</span><span class=n>logging</span><span class=o>.</span><span class=n>INFO</span><span class=p>,</span><span class=nb>format</span><span class=o>=</span><span class=s2>&quot;</span><span class=si>%(levelname)s</span><span class=s2>: </span><span class=si>%(message)s</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=n>region</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&quot;AWS_REGION&quot;</span><span class=p>)</span>
<span class=n>bedrock_runtime</span> <span class=o>=</span> <span class=n>boto3</span><span class=o>.</span><span class=n>client</span><span class=p>(</span>
    <span class=n>service_name</span><span class=o>=</span><span class=s1>&#39;bedrock-runtime&#39;</span><span class=p>,</span>
    <span class=n>region_name</span><span class=o>=</span><span class=n>region</span><span class=p>,</span>
<span class=p>)</span>
<span class=n>claude3</span> <span class=o>=</span> <span class=s1>&#39;claude3&#39;</span>
<span class=n>llama2</span> <span class=o>=</span> <span class=s1>&#39;llama2&#39;</span>
<span class=n>llama3</span><span class=o>=</span><span class=s1>&#39;llama3&#39;</span>
<span class=n>mistral</span><span class=o>=</span><span class=s1>&#39;mistral&#39;</span>
<span class=n>titan</span><span class=o>=</span><span class=s1>&#39;titan&#39;</span>
<span class=n>models_dict</span> <span class=o>=</span> <span class=p>{</span>
    <span class=n>claude3</span> <span class=p>:</span> <span class=s1>&#39;anthropic.claude-3-sonnet-20240229-v1:0&#39;</span><span class=p>,</span>
    <span class=n>llama2</span><span class=p>:</span> <span class=s1>&#39;meta.llama2-13b-chat-v1&#39;</span><span class=p>,</span>
    <span class=n>llama3</span><span class=p>:</span> <span class=s1>&#39;meta.llama3-8b-instruct-v1:0&#39;</span><span class=p>,</span>
    <span class=n>mistral</span><span class=p>:</span> <span class=s1>&#39;mistral.mistral-7b-instruct-v0:2&#39;</span><span class=p>,</span>
    <span class=n>titan</span> <span class=p>:</span> <span class=s1>&#39;amazon.titan-text-premier-v1:0&#39;</span>
<span class=p>}</span>
<span class=n>max_tokens_val</span> <span class=o>=</span> <span class=mi>200</span>
<span class=n>temperature_val</span> <span class=o>=</span> <span class=mf>0.1</span>
<span class=n>dict_add_params</span> <span class=o>=</span> <span class=p>{</span>
    <span class=n>llama3</span><span class=p>:</span> <span class=p>{},</span> <span class=c1>#&quot;max_gen_len&quot;:max_tokens_val, &quot;temperature&quot;:temperature_val} , </span>
    <span class=n>claude3</span><span class=p>:</span> <span class=p>{</span><span class=s2>&quot;top_k&quot;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span> <span class=p>},</span><span class=c1># &quot;temperature&quot;: temperature_val, &quot;max_tokens&quot;: max_tokens_val},</span>
    <span class=n>mistral</span><span class=p>:</span> <span class=p>{},</span> <span class=c1>#{&quot;max_tokens&quot;:max_tokens_val, &quot;temperature&quot;: temperature_val} , </span>
    <span class=n>titan</span><span class=p>:</span>  <span class=p>{</span><span class=s2>&quot;topK&quot;</span><span class=p>:</span> <span class=mi>200</span><span class=p>,</span> <span class=p>},</span><span class=c1># &quot;maxTokenCount&quot;: max_tokens_val}</span>
<span class=p>}</span>
<span class=n>inference_config</span><span class=o>=</span><span class=p>{</span>
    <span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=n>temperature_val</span><span class=p>,</span>
    <span class=s2>&quot;maxTokens&quot;</span><span class=p>:</span> <span class=n>max_tokens_val</span><span class=p>,</span>
    <span class=s2>&quot;topP&quot;</span><span class=p>:</span> <span class=mf>0.9</span>
<span class=p>}</span>


<span class=k>def</span> <span class=nf>generate_conversation</span><span class=p>(</span><span class=n>bedrock_client</span><span class=p>,</span><span class=n>model_id</span><span class=p>,</span><span class=n>system_text</span><span class=p>,</span><span class=n>input_text</span><span class=p>):</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    Sends a message to a model.</span>
<span class=sd>    Args:</span>
<span class=sd>        bedrock_client: The Boto3 Bedrock runtime client.</span>
<span class=sd>        model_id (str): The model ID to use.</span>
<span class=sd>        system_text (JSON) : The system prompt.</span>
<span class=sd>        input text : The input message.</span>

<span class=sd>    Returns:</span>
<span class=sd>        response (JSON): The conversation that the model generated.</span>

<span class=sd>    &quot;&quot;&quot;</span>

    <span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&quot;Generating message with model </span><span class=si>%s</span><span class=s2>&quot;</span><span class=p>,</span> <span class=n>model_id</span><span class=p>)</span>

    <span class=c1># Message to send.</span>
    <span class=n>message</span> <span class=o>=</span> <span class=p>{</span>
        <span class=s2>&quot;role&quot;</span><span class=p>:</span> <span class=s2>&quot;user&quot;</span><span class=p>,</span>
        <span class=s2>&quot;content&quot;</span><span class=p>:</span> <span class=p>[{</span><span class=s2>&quot;text&quot;</span><span class=p>:</span> <span class=n>input_text</span><span class=p>}]</span>
    <span class=p>}</span>
    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span><span class=n>message</span><span class=p>]</span>
    <span class=n>system_prompts</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&quot;text&quot;</span> <span class=p>:</span> <span class=n>system_text</span><span class=p>}]</span>

    <span class=k>if</span> <span class=n>model_id</span> <span class=ow>in</span> <span class=p>[</span><span class=n>models_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>mistral</span><span class=p>),</span> <span class=n>models_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>titan</span><span class=p>)]:</span>
        <span class=n>system_prompts</span> <span class=o>=</span> <span class=p>[]</span> <span class=c1># not supported</span>

    <span class=c1># Inference parameters to use.</span>


    <span class=c1>#Base inference parameters to use.</span>
    <span class=c1>#inference_config = {&quot;temperature&quot;: temperature}</span>


    <span class=c1># Send the message.</span>
    <span class=n>response</span> <span class=o>=</span> <span class=n>bedrock_client</span><span class=o>.</span><span class=n>converse</span><span class=p>(</span>
        <span class=n>modelId</span><span class=o>=</span><span class=n>model_id</span><span class=p>,</span>
        <span class=n>messages</span><span class=o>=</span><span class=n>messages</span><span class=p>,</span>
        <span class=n>system</span><span class=o>=</span><span class=n>system_prompts</span><span class=p>,</span>
        <span class=n>inferenceConfig</span><span class=o>=</span><span class=n>inference_config</span><span class=p>,</span>
        <span class=n>additionalModelRequestFields</span><span class=o>=</span><span class=n>get_additional_model_fields</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span>
    <span class=p>)</span>

    <span class=k>return</span> <span class=n>response</span>

<span class=k>def</span> <span class=nf>get_additional_model_fields</span><span class=p>(</span><span class=n>modelId</span><span class=p>):</span>

    <span class=k>return</span> <span class=n>dict_add_params</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>modelId</span><span class=p>)</span>
    <span class=c1>#{&quot;top_k&quot;: top_k, &quot;max_tokens&quot;: max_tokens}}</span>

<span class=k>def</span> <span class=nf>get_converse_output</span><span class=p>(</span><span class=n>response_obj</span><span class=p>):</span>
    <span class=n>ret_messages</span><span class=o>=</span><span class=p>[]</span>
    <span class=n>output_message</span> <span class=o>=</span> <span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>][</span><span class=s1>&#39;message&#39;</span><span class=p>]</span>
    <span class=n>role_out</span> <span class=o>=</span> <span class=n>output_message</span><span class=p>[</span><span class=s1>&#39;role&#39;</span><span class=p>]</span>

    <span class=k>for</span> <span class=n>content</span> <span class=ow>in</span> <span class=n>output_message</span><span class=p>[</span><span class=s1>&#39;content&#39;</span><span class=p>]:</span>
        <span class=n>ret_messages</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>content</span><span class=p>[</span><span class=s1>&#39;text&#39;</span><span class=p>])</span>

    <span class=k>return</span> <span class=n>ret_messages</span><span class=p>,</span> <span class=n>role_out</span>
</code></pre></div> <hr> <h2 id=using-langchain-for-conversation-memory>Using LangChain for Conversation Memory</h2> <p>We will use LangChain's <code>ConversationBufferMemory</code> class provides an easy way to capture conversational memory for LLM chat applications. Let's check out an example of Claude being able to retrieve context through conversational memory below.</p> <p>Similar to the last workshop, we will use both a prompt template and a LangChain LLM for this example. Note that this time our prompt template includes a <code>{history}</code> variable where our chat history will be included to the prompt.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain.memory</span> <span class=kn>import</span> <span class=n>ConversationBufferMemory</span>
<span class=kn>from</span> <span class=nn>langchain.prompts</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain.prompts.chat</span> <span class=kn>import</span> <span class=n>ChatPromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain_community.chat_models</span> <span class=kn>import</span> <span class=n>BedrockChat</span>
<span class=kn>from</span> <span class=nn>langchain_core.messages</span> <span class=kn>import</span> <span class=n>HumanMessage</span>
<span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>ConversationChain</span>
<span class=kn>from</span> <span class=nn>langchain_core.output_parsers</span> <span class=kn>import</span> <span class=n>StrOutputParser</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.memory</span> <span class=kn>import</span> <span class=n>ConversationBufferMemory</span>
<span class=kn>from</span> <span class=nn>langchain.prompts</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain.prompts.chat</span> <span class=kn>import</span> <span class=n>ChatPromptTemplate</span>


<span class=c1># turn verbose to true to see the full logs and documents</span>
<span class=n>modelId</span> <span class=o>=</span> <span class=n>models_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>claude3</span><span class=p>)</span>
<span class=n>cl_llm</span> <span class=o>=</span> <span class=n>BedrockChat</span><span class=p>(</span>
    <span class=n>model_id</span><span class=o>=</span><span class=n>modelId</span><span class=p>,</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;temperature&quot;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span> <span class=s1>&#39;max_tokens&#39;</span><span class=p>:</span> <span class=mi>100</span><span class=p>},</span>
<span class=p>)</span>
</code></pre></div> <pre><code>/Users/rsgrewal/opt/anaconda3/envs/ragtest310/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `BedrockChat` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import ChatBedrock`.
  warn_deprecated(
</code></pre> <p>The <code>ConversationBufferMemory</code> class is instantiated here and you will notice that we use Claude specific human and assistant prefixes. When we initialize the memory, the history is blank.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.memory</span> <span class=kn>import</span> <span class=n>ConversationBufferMemory</span>

<span class=n>memory</span> <span class=o>=</span> <span class=n>ConversationBufferMemory</span><span class=p>(</span><span class=n>human_prefix</span><span class=o>=</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Human&quot;</span><span class=p>,</span> <span class=n>ai_prefix</span><span class=o>=</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>Assistant&quot;</span><span class=p>)</span>
<span class=n>history</span> <span class=o>=</span> <span class=n>memory</span><span class=o>.</span><span class=n>load_memory_variables</span><span class=p>({})[</span><span class=s1>&#39;history&#39;</span><span class=p>]</span>
<span class=nb>print</span><span class=p>(</span><span class=n>history</span><span class=p>)</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>human_query</span> <span class=o>=</span>  <span class=s2>&quot;How did Amazon&#39;s Advertising business do in 2023?&quot;</span>
</code></pre></div> <hr> <h2 id=combining-rag-with-conversation>Combining RAG with Conversation</h2> <p>Now that we have a conversational system built, lets incorporate the RAG system we built in notebook 02 into the chat paradigm. </p> <p>First, we will create the same vector store with LangChain and FAISS from the last notebook.</p> <p>Our goal is to create a curated response from the model and only use the FAQ's we have provided.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.embeddings</span> <span class=kn>import</span> <span class=n>BedrockEmbeddings</span>
<span class=kn>from</span> <span class=nn>langchain.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>
<span class=kn>from</span> <span class=nn>langchain.document_loaders</span> <span class=kn>import</span> <span class=n>PyPDFLoader</span>
<span class=kn>from</span> <span class=nn>langchain.tools.retriever</span> <span class=kn>import</span> <span class=n>create_retriever_tool</span>
<span class=kn>from</span> <span class=nn>langchain_community.document_loaders</span> <span class=kn>import</span> <span class=n>TextLoader</span><span class=p>,</span> <span class=n>PyPDFLoader</span>
<span class=kn>from</span> <span class=nn>langchain_community.vectorstores</span> <span class=kn>import</span> <span class=n>FAISS</span>
<span class=kn>from</span> <span class=nn>langchain_text_splitters</span> <span class=kn>import</span> <span class=n>CharacterTextSplitter</span>
<span class=kn>from</span> <span class=nn>langchain.embeddings.bedrock</span> <span class=kn>import</span> <span class=n>BedrockEmbeddings</span>

<span class=n>br_embeddings</span> <span class=o>=</span> <span class=n>BedrockEmbeddings</span><span class=p>(</span><span class=n>model_id</span><span class=o>=</span><span class=s2>&quot;amazon.titan-embed-text-v1&quot;</span><span class=p>,</span> <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>)</span>

<span class=n>loader</span> <span class=o>=</span> <span class=n>PyPDFLoader</span><span class=p>(</span><span class=s1>&#39;../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf&#39;</span><span class=p>)</span> <span class=c1># --- &gt; 219 docs with 400 chars, each row consists in a question column and an answer column</span>
<span class=n>documents_aws</span> <span class=o>=</span> <span class=n>loader</span><span class=o>.</span><span class=n>load</span><span class=p>()</span> <span class=c1>#</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Number of documents=</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>documents_aws</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>

<span class=n>docs</span> <span class=o>=</span> <span class=n>CharacterTextSplitter</span><span class=p>(</span><span class=n>chunk_size</span><span class=o>=</span><span class=mi>2000</span><span class=p>,</span> <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>400</span><span class=p>,</span> <span class=n>separator</span><span class=o>=</span><span class=s2>&quot;</span><span class=se>\n</span><span class=s2>&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>split_documents</span><span class=p>(</span><span class=n>documents_aws</span><span class=p>)</span> <span class=c1>#-  separator=&quot;,&quot;</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;Number of documents after split and chunking=</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>docs</span><span class=p>)</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>)</span>


<span class=n>vs</span> <span class=o>=</span> <span class=n>FAISS</span><span class=o>.</span><span class=n>from_documents</span><span class=p>(</span>
    <span class=n>documents</span><span class=o>=</span><span class=n>docs</span><span class=p>,</span>
     <span class=n>embedding</span> <span class=o>=</span> <span class=n>br_embeddings</span>
<span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;vectorstore_faiss_aws: number of elements in the index=</span><span class=si>{</span><span class=n>vs</span><span class=o>.</span><span class=n>index</span><span class=o>.</span><span class=n>ntotal</span><span class=si>}</span><span class=s2>::&quot;</span><span class=p>)</span>
</code></pre></div> <pre><code>Number of documents=11
Number of documents after split and chunking=31


INFO:faiss.loader:Loading faiss.
INFO:faiss.loader:Successfully loaded faiss.


vectorstore_faiss_aws: number of elements in the index=31::
</code></pre> <h3 id=visualize-semantic-search>Visualize Semantic Search</h3> <p>   This section is for Advanced Practioners. Please feel free to run through these cells and come back later to re-examine the concepts    </p> <p>Let's see how the semantic search works: 1. First we calculate the embeddings vector for the query, and 2. then we use this vector to do a similarity search on the store</p> <h5 id=citation>Citation</h5> <p>We will also be able to get the <code>citation</code> or the underlying documents which our Vector Store matched to our query. This is useful for debugging and also measuring the quality of the vector stores. let us look at how the underlying Vector store calculates the matches</p> <h5 id=vector-db-indexes>Vector DB Indexes</h5> <p>One of the key components of the Vector DB is to be able to retrieve documents matching the query with accuracy and speed. There are multiple algorithims for the same and some examples can be <a href=https://thedataquarry.com/posts/vector-db-3/ >read here</a> </p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>IPython.display</span> <span class=kn>import</span> <span class=n>HTML</span><span class=p>,</span> <span class=n>display</span>
<span class=kn>import</span> <span class=nn>warnings</span>
<span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s1>&#39;ignore&#39;</span><span class=p>)</span>
<span class=c1>#- helpful function to display in tabular format</span>

<span class=k>def</span> <span class=nf>display_table</span><span class=p>(</span><span class=n>data</span><span class=p>):</span>
    <span class=n>html</span> <span class=o>=</span> <span class=s2>&quot;&lt;table&gt;&quot;</span>
    <span class=k>for</span> <span class=n>row</span> <span class=ow>in</span> <span class=n>data</span><span class=p>:</span>
        <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;tr&gt;&quot;</span>
        <span class=k>for</span> <span class=n>field</span> <span class=ow>in</span> <span class=n>row</span><span class=p>:</span>
            <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;td&gt;</span><span class=si>%s</span><span class=s2>&lt;/td&gt;&quot;</span><span class=o>%</span><span class=p>(</span><span class=n>field</span><span class=p>)</span>
        <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;/tr&gt;&quot;</span>
    <span class=n>html</span> <span class=o>+=</span> <span class=s2>&quot;&lt;/table&gt;&quot;</span>
    <span class=n>display</span><span class=p>(</span><span class=n>HTML</span><span class=p>(</span><span class=n>html</span><span class=p>))</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>v</span> <span class=o>=</span> <span class=n>br_embeddings</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=n>human_query</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=n>v</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>10</span><span class=p>])</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_by_vector</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;Let us look at the documents which had the relevant information pertaining to our query&#39;</span><span class=p>))</span>
<span class=k>for</span> <span class=n>r</span> <span class=ow>in</span> <span class=n>results</span><span class=p>:</span>
    <span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>r</span><span class=o>.</span><span class=n>page_content</span><span class=p>))</span>
    <span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
</code></pre></div> <pre><code>[0.8125, -0.78515625, 0.27929688, -0.45703125, 0.6640625, 0.103515625, 0.14941406, -0.00026512146, 0.10644531, 0.15820312]
</code></pre> <p>Let us look at the documents which had the relevant information pertaining to our query</p> <p>expand selection and features, and move toward profitability (in Q4 2023, Mexico became our latest international Stores locale to turn profitable). We have high conviction that these new geographies willcontinue to grow and be profitable in the long run. Alongside our Stores business, Amazons Advertising progress remains strong, growing 24% Y oY from $38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. Weve added Sponsored TV to this offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streamingTV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, weve expandedour streaming TV advertising by introducing ads into Prime Video shows and movies, where brands canreach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies andshows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football . Streaming TV advertising is growing quickly and off to a strong start. Shifting to AWS, we started 2023 seeing substantial cost optimization, with most companies trying to save money in an uncertain economy. Much of this optimization was catalyzed by AWS helping customers use the cloud more efficiently and leverage more powerful, price-performant AWS capabilities like Graviton chips(our generalized CPU chips that provide ~40% better price-performance than other leading x86 processors),S3 Intelligent Tiering (a storage class that uses AI to detect objects accessed less frequently and store themin less expensive storage layers), and Savings Plans (which give customers lower prices in exchange for longercommitments). This work diminished short-term revenue, but was best for customers, much appreciated,and should bode well for customers and AWS longer-term. By the end of 2023, we saw cost optimizationattenuating, new deals accelerating, customers renewing at larger commitments over longer time periods, andmigrations growing again.</p> <hr> <p>Being sharp on price is always important, but particularly in an uncertain economy, where customers are careful about how much theyre spending. As a result, in Q4 2023, we kicked off the holiday season with Prime Big Deal Days, an exclusive event for Prime members to provide an early start on holiday shopping. Thiswas followed by our extended Black Friday and Cyber Monday holiday shopping event, open to all customers,that became our largest revenue event ever. For all of 2023, customers saved nearly $24B across millions ofdeals and coupons, almost 70% more than the prior year. We also continue to improve delivery speeds, breaking multiple company records. In 2023, Amazon delivered at the fastest speeds ever to Prime members, with more than 7 billion items arriving same or next day, including more than 4 billion in the U.S. and more than 2 billion in Europe. In the U.S., this result is thecombination of two things. One is the benefit of regionalization, where we re-architected the network tostore items closer to customers. The other is the expansion of same-day facilities, where in 2023, we increasedthe number of items delivered same day or overnight by nearly 70% Y oY . As we get items to customers thisfast, customers choose Amazon to fulfill their shopping needs more frequently, and we can see the results invarious areas including how fast our everyday essentials business is growing (over 20% Y oY in Q4 2023). Our regionalization efforts have also trimmed transportation distances, helping lower our cost to serve. In 2023, for the first time since 2018, we reduced our cost to serve on a per unit basis globally. In the U.S. alone, cost to serve was down by more than $0.45 per unit Y oY . Decreasing cost to serve allows us both to investin speed improvements and afford adding more selection at lower Average Selling Prices (ASPs). Moreselection at lower prices puts us in consideration for more purchases.</p> <hr> <h4 id=similarity-search>Similarity Search</h4> <h5 id=distance-scoring-in-vector-data-bases>Distance scoring in Vector Data bases</h5> <p><a href=https://weaviate.io/blog/distance-metrics-in-vector-search>Distance scores</a> are the key in vector searches. Here are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance ( Squared Euclidean) . Therefore, a lower score is better. Further in FAISS we have similarity_search_with_score (ranked by distance: low to high) and similarity_search_with_relevance_scores ( ranked by relevance: high to low) with both using the distance strategy. The similarity_search_with_relevance_scores calculates the relevance score as 1 - score. For more details of the various distance scores <a href=https://milvus.io/docs/metric.md>read here</a></p> <div class=highlight><pre><span></span><code><span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;##### Let us look at the documents based on </span><span class=si>{</span><span class=n>vs</span><span class=o>.</span><span class=n>distance_strategy</span><span class=o>.</span><span class=n>name</span><span class=si>}</span><span class=s2> which will be used to answer our question </span><span class=si>{</span><span class=n>human_query</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>))</span>

<span class=n>context</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search</span><span class=p>(</span><span class=s1>&#39;What kind of bias does Clarify detect ?&#39;</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
<span class=c1>#-  langchain.schema.document.Document</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
<span class=n>list_context</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span> <span class=ow>in</span> <span class=n>context</span><span class=p>]</span>
<span class=n>list_context</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Documents&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-data&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>list_context</span><span class=p>)</span>
</code></pre></div> <h5 id=let-us-look-at-the-documents-based-on-euclidean_distance-which-will-be-used-to-answer-our-question-how-did-amazons-advertising-business-do-in-2023>Let us look at the documents based on EUCLIDEAN_DISTANCE which will be used to answer our question How did Amazon's Advertising business do in 2023?</h5> <hr> <table><tr><td>Documents</td><td>Meta-data</td></tr><tr><td>service. Amazon Bedrock invented this layer and provides customers with the easiest way to build and scale GenAI applications with the broadest selection of first- and third-party FMs, as well as leading ease-of-usecapabilities that allow GenAI builders to get higher quality model outputs more quickly. Bedrock is off to avery strong start with tens of thousands of active customers after just a few months. The team continuesto iterate rapidly on Bedrock, recently delivering Guardrails (to safeguard what questions applications will answer), Knowledge Bases (to expand models knowledge base with Retrieval Augmented Generationor RAGand real-time queries), Agents (to complete multi-step tasks), and Fine-Tuning (to keep teaching and refining models), all of which improve customers application quality. We also just added new modelsfrom Anthropic (their newly-released Claude 3 is the best performing large language model in the world),Meta (with Llama 2), Mistral, Stability AI, Cohere, and our own Amazon Titan family of FMs. Whatcustomers have learned at this early stage of GenAI is that theres meaningful iteration required to build aproduction GenAI application with the requisite enterprise quality at the cost and latency needed. Customersdont want only one model. They want access to various models and model sizes for different types of applications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens. Thetoplayer of this stack is the application layer. Were building a substantial number of GenAI applications</td><td>{'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 6}</td></tr><tr><td>2023, for the first time since 2018, we reduced our cost to serve on a per unit basis globally. In the U.S. alone, cost to serve was down by more than $0.45 per unit Y oY . Decreasing cost to serve allows us both to investin speed improvements and afford adding more selection at lower Average Selling Prices (ASPs). Moreselection at lower prices puts us in consideration for more purchases. As we look toward 2024 (and beyond), were not done lowering our cost to serve. Weve challenged every closely held belief in our fulfillment network, and reevaluated every part of it, and found several areas where we believe we can lower costs even further while also delivering faster for customers. Our inbound fulfillmentarchitecture and resulting inventory placement are areas of focus in 2024, and we have optimism theresmore upside for us. Internationally, we like the trajectory of our established countries, and see meaningful progress in our emerging geographies (e.g. India, Brazil, Australia, Mexico, Middle East, Africa, etc.) as they continue to</td><td>{'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 0}</td></tr></table> <p>Let us first look at the Page context and the meta data associated with the documents. Now let us look at the L2 scores based on the distance scoring as explained above. Lower score is better</p> <div class=highlight><pre><span></span><code><span class=c1>#- relevancy of the documents</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>human_query</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;##### Similarity Search Table with relevancy score.&#39;</span><span class=p>))</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;-&#39;</span><span class=o>*</span><span class=mi>20</span><span class=p>))</span>
<span class=n>results</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Documents&#39;</span><span class=p>,</span> <span class=s1>&#39;Relevancy Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</code></pre></div> <h5 id=similarity-search-table-with-relevancy-score>Similarity Search Table with relevancy score.</h5> <hr> <table><tr><td>Documents</td><td>Relevancy Score</td></tr><tr><td>page_content='expand selection and features, and move toward profitability (in Q4 2023, Mexico became our latest\ninternational Stores locale to turn profitable). We have high conviction that these new geographies willcontinue to grow and be profitable in the long run.\nAlongside our Stores business, Amazons Advertising progress remains strong, growing 24% Y oY from\n$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. Weve added Sponsored TV to this\noffering, a self-service solution for brands to create campaigns that can appear on up to 30+ streamingTV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, weve expandedour streaming TV advertising by introducing ads into Prime Video shows and movies, where brands canreach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies andshows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football . Streaming\nTV advertising is growing quickly and off to a strong start.\nShifting to AWS, we started 2023 seeing substantial cost optimization, with most companies trying to save\nmoney in an uncertain economy. Much of this optimization was catalyzed by AWS helping customers use the\ncloud more efficiently and leverage more powerful, price-performant AWS capabilities like Graviton chips(our generalized CPU chips that provide ~40% better price-performance than other leading x86 processors),S3 Intelligent Tiering (a storage class that uses AI to detect objects accessed less frequently and store themin less expensive storage layers), and Savings Plans (which give customers lower prices in exchange for longercommitments). This work diminished short-term revenue, but was best for customers, much appreciated,and should bode well for customers and AWS longer-term. By the end of 2023, we saw cost optimizationattenuating, new deals accelerating, customers renewing at larger commitments over longer time periods, andmigrations growing again.' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 1}</td><td>126.094025</td></tr><tr><td>page_content='Being sharp on price is always important, but particularly in an uncertain economy, where customers are\ncareful about how much theyre spending. As a result, in Q4 2023, we kicked off the holiday season with Prime\nBig Deal Days, an exclusive event for Prime members to provide an early start on holiday shopping. Thiswas followed by our extended Black Friday and Cyber Monday holiday shopping event, open to all customers,that became our largest revenue event ever. For all of 2023, customers saved nearly $24B across millions ofdeals and coupons, almost 70% more than the prior year.\nWe also continue to improve delivery speeds, breaking multiple company records. In 2023, Amazon\ndelivered at the fastest speeds ever to Prime members, with more than 7 billion items arriving same or next\nday, including more than 4 billion in the U.S. and more than 2 billion in Europe. In the U.S., this result is thecombination of two things. One is the benefit of regionalization, where we re-architected the network tostore items closer to customers. The other is the expansion of same-day facilities, where in 2023, we increasedthe number of items delivered same day or overnight by nearly 70% Y oY . As we get items to customers thisfast, customers choose Amazon to fulfill their shopping needs more frequently, and we can see the results invarious areas including how fast our everyday essentials business is growing (over 20% Y oY in Q4 2023).\nOur regionalization efforts have also trimmed transportation distances, helping lower our cost to serve. In\n2023, for the first time since 2018, we reduced our cost to serve on a per unit basis globally. In the U.S. alone,\ncost to serve was down by more than $0.45 per unit Y oY . Decreasing cost to serve allows us both to investin speed improvements and afford adding more selection at lower Average Selling Prices (ASPs). Moreselection at lower prices puts us in consideration for more purchases.' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 0}</td><td>168.52493</td></tr></table> <h4 id=marginal-relevancy-score>Marginal Relevancy score</h4> <p>Maximal Marginal Relevance has been introduced in the paper <a href=https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf>The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries</a>. Maximal Marginal Relevance tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. In the below results since we have a very limited data set it might not make a difference but for larger data sets the query will theoritically run faster while still preserving the over all relevancy of the documents</p> <div class=highlight><pre><span></span><code><span class=c1>#- normalizing the relevancy</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=s1>&#39;##### Let us look at MRR scores&#39;</span><span class=p>))</span>
<span class=n>results</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>max_marginal_relevance_search_with_score_by_vector</span><span class=p>(</span><span class=n>br_embeddings</span><span class=o>.</span><span class=n>embed_query</span><span class=p>(</span><span class=n>human_query</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s2>&quot;Document&quot;</span><span class=p>,</span> <span class=s2>&quot;MRR Score&quot;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</code></pre></div> <h5 id=let-us-look-at-mrr-scores>Let us look at MRR scores</h5> <table><tr><td>Document</td><td>MRR Score</td></tr><tr><td>page_content='expand selection and features, and move toward profitability (in Q4 2023, Mexico became our latest\ninternational Stores locale to turn profitable). We have high conviction that these new geographies willcontinue to grow and be profitable in the long run.\nAlongside our Stores business, Amazons Advertising progress remains strong, growing 24% Y oY from\n$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. Weve added Sponsored TV to this\noffering, a self-service solution for brands to create campaigns that can appear on up to 30+ streamingTV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, weve expandedour streaming TV advertising by introducing ads into Prime Video shows and movies, where brands canreach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies andshows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football . Streaming\nTV advertising is growing quickly and off to a strong start.\nShifting to AWS, we started 2023 seeing substantial cost optimization, with most companies trying to save\nmoney in an uncertain economy. Much of this optimization was catalyzed by AWS helping customers use the\ncloud more efficiently and leverage more powerful, price-performant AWS capabilities like Graviton chips(our generalized CPU chips that provide ~40% better price-performance than other leading x86 processors),S3 Intelligent Tiering (a storage class that uses AI to detect objects accessed less frequently and store themin less expensive storage layers), and Savings Plans (which give customers lower prices in exchange for longercommitments). This work diminished short-term revenue, but was best for customers, much appreciated,and should bode well for customers and AWS longer-term. By the end of 2023, we saw cost optimizationattenuating, new deals accelerating, customers renewing at larger commitments over longer time periods, andmigrations growing again.' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 1}</td><td>126.094025</td></tr><tr><td>page_content='So, how do you build the right set of primitives?Pursuing primitives is not a guarantee of success. There are many you could build, and even more ways to\ncombine them. But, a good compass is to pick real customer problems youre trying to solve .\nOur logistics primitives are an instructive example. In Amazons early years, we built core capabilities\naround warehousing items, and then picking, packing, and shipping them quickly and reliably to customers.As we added third-party sellers to our marketplace, they frequently requested being able to use these samelogistics capabilities. Because wed built this initial set of logistics primitives, we were able to introduceFulfillment by Amazon (FBA ) in 2006, allowing sellers to use Amazons Fulfillment Network to storeitems, and then have us pick, pack, and ship them to customers, with the bonus of these products beingavailable for fast, Prime delivery. This service has saved sellers substantial time and money (typically about70% less expensive than doing themselves), and remains one of our most popular services. As more merchantsbegan to operate their own direct-to-consumer (DTC) websites, many yearned to still use our fulfillmentcapabilities, while also accessing our payments and identity primitives to drive higher order conversion ontheir own websites (as Prime members have already shared this payment and identity information withAmazon). A couple years ago, we launched Buy with Prime to address this customer need. Prime memberscan check out quickly on DTC websites like they do on Amazon, and receive fast Prime shipping speeds onBuy with Prime itemsincreasing order conversion for merchants by ~25% vs. their default experience.\nAs our Stores business has grown substantially, and our supply chain become more complex, weve had to' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 3}</td><td>256.7152</td></tr><tr><td>page_content='(By the way, dont underestimate the importance of security in GenAI. Customers AI models contain some\nof their most sensitive data. AWS and its partners offer the strongest security capabilities and track recordin the world; and as a result, more and more customers want to run their GenAI on AWS.)\n===Recently, I was asked a provocative questionhow does Amazon remain resilient? While simple in its\nwording, its profound because it gets to the heart of our success to date as well as for the future. The answerlies in our discipline around deeply held principles: 1/ hiring builders who are motivated to continuallyimprove and expand whats possible; 2/ solving real customer challenges, rather than what we think may beinteresting technology; 3/ building in primitives so that we can innovate and experiment at the highest rate;4/ not wasting time trying to fight gravity (spoiler alert: you always lose)when we discover technologythat enables better customer experiences, we embrace it; 5/ accepting and learning from failed experimentsactually becoming more energized to try again, with new knowledge to employ.\nToday, we continue to operate in times of unprecedented change that come with unusual opportunities for\ngrowth across the areas in which we operate. For instance, while we have a nearly $500B consumer business,about 80% of the worldwide retail market segment still resides in physical stores. Similarly, with a cloudcomputing business at nearly a $100B revenue run rate, more than 85% of the global IT spend is still' metadata={'source': '../data/sagemaker/Amazon-com-Inc-2023-Shareholder-Letter.pdf', 'page': 6}</td><td>213.93944</td></tr></table> <h4 id=query-with-filter>Query with Filter</h4> <p>Now we will ask to search only against the version 2 of the data and use filter criteria against it</p> <div class=highlight><pre><span></span><code><span class=c1># - run the query again</span>

<span class=n>results_with_scores</span> <span class=o>=</span> <span class=n>vs</span><span class=o>.</span><span class=n>similarity_search_with_score</span><span class=p>(</span><span class=n>human_query</span><span class=p>,</span> <span class=nb>filter</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>page</span><span class=o>=</span><span class=s1>&#39;v2&#39;</span><span class=p>),</span> <span class=n>k</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>fetch_k</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
<span class=n>results_with_scores</span> <span class=o>=</span> <span class=p>[[</span><span class=n>doc</span><span class=o>.</span><span class=n>page_content</span><span class=p>,</span> <span class=n>doc</span><span class=o>.</span><span class=n>metadata</span><span class=p>,</span> <span class=n>score</span><span class=p>]</span> <span class=k>for</span> <span class=n>doc</span><span class=p>,</span> <span class=n>score</span> <span class=ow>in</span> <span class=n>results_with_scores</span><span class=p>]</span>
<span class=n>results_with_scores</span><span class=o>.</span><span class=n>insert</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=p>[</span><span class=s1>&#39;Document&#39;</span><span class=p>,</span> <span class=s1>&#39;Meta-Data&#39;</span><span class=p>,</span> <span class=s1>&#39;Score&#39;</span><span class=p>])</span>
<span class=n>display_table</span><span class=p>(</span><span class=n>results_with_scores</span><span class=p>)</span>
</code></pre></div> <table><tr><td>Document</td><td>Meta-Data</td><td>Score</td></tr></table> <h3 id=let-us-continue-to-build-our-chatbot>Let us continue to build our chatbot</h3> <p>The prompt template is now altered to include both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual.</p> <hr> <h2 id=using-langchain-for-orchestration-of-rag>Using LangChain for Orchestration of RAG</h2> <p>Beyond the primitive classes for prompt handling and conversational memory management, LangChain also provides a framework for <a href=https://python.langchain.com/docs/expression_language/cookbook/retrieval>orchestrating RAG flows</a> with what purpose built "chains". In this section, we will see how to be a retrieval chain with LangChain which is more comprehensive and robust than the original retrieval system we built above.</p> <p>The workflow we used above follows the following process...</p> <ol> <li>User input is received.</li> <li>User input is queried against the vector database to retrieve relevant documents.</li> <li>Relevant documents and chat memory are inserted into a new prompt to respond to the user input.</li> <li>Return to step 1.</li> </ol> <p>However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...</p> <ol> <li>User input is received.</li> <li>An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and other instructions. This could include things like condensing, rewording, addition of chat context, or stylistic changes.</li> <li>Reformatted user input is queried against the vector database to retrieve relevant documents.</li> <li>The reformatted user input and relevant documents are inserted into a new prompt in order to answer the user question.</li> <li>Return to step 1.</li> </ol> <p>Let's now build out this second workflow using LangChain below.</p> <p>First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output.</p> <div class=highlight><pre><span></span><code><span class=n>condense_prompt</span> <span class=o>=</span> <span class=n>PromptTemplate</span><span class=o>.</span><span class=n>from_template</span><span class=p>(</span><span class=s2>&quot;&quot;&quot;</span><span class=se>\</span>
<span class=s2>&lt;chat-history&gt;</span>
<span class=si>{chat_history}</span>
<span class=s2>&lt;/chat-history&gt;</span>

<span class=s2>&lt;follow-up-message&gt;</span>
<span class=si>{question}</span>
<span class=s2>&lt;follow-up-message&gt;</span>

<span class=s2>Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, </span><span class=se>\</span>
<span class=s2>rewrite the follow up message to be a standalone question that captures all relevant context </span><span class=se>\</span>
<span class=s2>from the conversation. Answer only with the new question and nothing else.</span>

<span class=s2>Assistant: Standalone Question:&quot;&quot;&quot;</span><span class=p>)</span>
</code></pre></div> <h4 id=however-we-are-goign-to-continue-to-use-the-in-built-template>However we are goign to continue to use the in-built template</h4> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.chains.conversational_retrieval.prompts</span> <span class=kn>import</span> <span class=n>CONDENSE_QUESTION_PROMPT</span>

<span class=nb>print</span><span class=p>(</span><span class=n>CONDENSE_QUESTION_PROMPT</span><span class=o>.</span><span class=n>template</span><span class=p>)</span>
</code></pre></div> <pre><code>Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:
</code></pre> <p>The next prompt we need is the prompt which will answer the user's question based on the retrieved information. In this case, we provide specific instructions about how to answer the question as well as provide the context retrieved from the vector database.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain.memory</span> <span class=kn>import</span> <span class=n>ConversationBufferMemory</span>
<span class=kn>from</span> <span class=nn>langchain.prompts</span> <span class=kn>import</span> <span class=n>PromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain.prompts.chat</span> <span class=kn>import</span> <span class=n>ChatPromptTemplate</span>
<span class=kn>from</span> <span class=nn>langchain_community.chat_models</span> <span class=kn>import</span> <span class=n>BedrockChat</span>
<span class=kn>from</span> <span class=nn>langchain_core.messages</span> <span class=kn>import</span> <span class=n>HumanMessage</span>
<span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>ConversationChain</span>
<span class=kn>from</span> <span class=nn>langchain_core.output_parsers</span> <span class=kn>import</span> <span class=n>StrOutputParser</span>


<span class=n>SYSTEM_MESSAGE</span> <span class=o>=</span> <span class=s2>&quot;&quot;&quot;</span>
<span class=s2>System: You are a helpful conversational assistant.If you are unsure about the answer OR the answer does not exist in the context, respond with</span>
<span class=s2>&quot;Sorry but I do not understand your request. I am still learning so I appreciate your patience! . NEVER make up the answer.</span>
<span class=s2>Here is some important context which can help inform the questions the Human asks. Make sure to not make anything up to answer the question if it is not provided in the context. </span>

<span class=s2>Context: </span><span class=si>{context}</span>

<span class=s2>History: </span><span class=si>{chat_history}</span>

<span class=s2>&quot;&quot;&quot;</span>
<span class=n>HUMAN_MESSAGE</span> <span class=o>=</span> <span class=s2>&quot;</span><span class=si>{question}</span><span class=s2>&quot;</span>

<span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
    <span class=p>(</span><span class=s2>&quot;system&quot;</span><span class=p>,</span> <span class=n>SYSTEM_MESSAGE</span><span class=p>),</span>
    <span class=p>(</span><span class=s2>&quot;human&quot;</span><span class=p>,</span> <span class=n>HUMAN_MESSAGE</span><span class=p>)</span>
<span class=p>]</span>

<span class=n>chat_prompt_template</span> <span class=o>=</span> <span class=n>ChatPromptTemplate</span><span class=o>.</span><span class=n>from_messages</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</code></pre></div> <p>Now that we have our prompts set up, let's set up the conversational memory buffer just like we did earlier in the notebook. Notice how we inject an example human and assistant message in order to help guide our AI assistant on what its job is.</p> <div class=highlight><pre><span></span><code><span class=n>llm</span> <span class=o>=</span> <span class=n>BedrockChat</span><span class=p>(</span>
    <span class=n>client</span><span class=o>=</span><span class=n>boto3_bedrock</span><span class=p>,</span>
    <span class=n>model_id</span><span class=o>=</span><span class=n>models_dict</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>claude3</span><span class=p>),</span>
    <span class=n>model_kwargs</span><span class=o>=</span><span class=n>dict_add_params</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>claude3</span><span class=p>)</span> <span class=c1>#{&quot;max_tokens_to_sample&quot;: 500, &quot;temperature&quot;: 0.9}</span>
<span class=p>)</span>
<span class=n>memory_chain</span> <span class=o>=</span> <span class=n>ConversationBufferMemory</span><span class=p>(</span>
    <span class=n>memory_key</span><span class=o>=</span><span class=s2>&quot;chat_history&quot;</span><span class=p>,</span>
    <span class=n>return_messages</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=c1># human_prefix=&quot;Human&quot;,</span>
    <span class=c1># ai_prefix=&quot;Assistant&quot;</span>
<span class=p>)</span>
</code></pre></div> <p>Lastly, we will used the <code>ConversationalRetrievalChain</code> from LangChain to orchestrate this whole system. If you would like to see some more logs about what is happening in the orchestration and not just the final output, make sure to change the <code>verbose</code> argument to <code>True</code>.</p> <div class=highlight><pre><span></span><code><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>ConversationalRetrievalChain</span>
<span class=n>qa</span> <span class=o>=</span> <span class=n>ConversationalRetrievalChain</span><span class=o>.</span><span class=n>from_llm</span><span class=p>(</span>
    <span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span> <span class=c1># this is our claude model</span>
    <span class=n>retriever</span><span class=o>=</span><span class=n>vs</span><span class=o>.</span><span class=n>as_retriever</span><span class=p>(),</span> <span class=c1># this is our FAISS vector database</span>
    <span class=n>memory</span><span class=o>=</span><span class=n>memory_chain</span><span class=p>,</span> <span class=c1># this is the conversational memory storage class</span>
    <span class=n>condense_question_prompt</span><span class=o>=</span><span class=n>CONDENSE_QUESTION_PROMPT</span><span class=p>,</span> <span class=c1># this is the prompt for condensing user inputs</span>
    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=c1># change this to True in order to see the logs working in the background</span>
<span class=p>)</span>
<span class=n>qa</span><span class=o>.</span><span class=n>combine_docs_chain</span><span class=o>.</span><span class=n>llm_chain</span><span class=o>.</span><span class=n>prompt</span> <span class=o>=</span> <span class=n>chat_prompt_template</span><span class=c1>#  respond_prompt # this is the prompt in order to respond to condensed questions</span>
</code></pre></div> <div class=highlight><pre><span></span><code><span class=n>chat_prompt_template</span>
</code></pre></div> <pre><code>ChatPromptTemplate(input_variables=['chat_history', 'context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'context'], template='\nSystem: You are a helpful conversational assistant.If you are unsure about the answer OR the answer does not exist in the context, respond with\n"Sorry but I do not understand your request. I am still learning so I appreciate your patience! . NEVER make up the answer.\nHere is some important context which can help inform the questions the Human asks. Make sure to not make anything up to answer the question if it is not provided in the context. \n\nContext: {context}\n\nHistory: {chat_history}\n\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])
</code></pre> <div class=highlight><pre><span></span><code><span class=n>qa</span><span class=o>.</span><span class=n>run</span><span class=p>({</span><span class=s1>&#39;question&#39;</span><span class=p>:</span> <span class=n>human_query</span><span class=p>})</span>
</code></pre></div> <pre><code>"According to the context provided, Amazon's Advertising business had strong progress in 2023, growing 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. This growth was primarily driven by Amazon's sponsored ads offerings. The context also mentions that Amazon expanded its advertising offerings by introducing Sponsored TV ads that can appear on streaming services like Amazon Freevee, Twitch, and Prime Video shows/movies reaching over 200 million monthly viewers."
</code></pre> <p>Let's go ahead and generate some responses from our RAG solution!</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>qa</span><span class=o>.</span><span class=n>run</span><span class=p>({</span><span class=s1>&#39;question&#39;</span><span class=p>:</span> <span class=s1>&#39;How  is it predicted for 2024?&#39;</span><span class=p>})</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <p>Sorry, the provided context does not mention any predictions or forecasts about how Amazon's Advertising business will perform in 2024. It only discusses the performance and growth of the Advertising business in 2023 compared to 2022. There are no details given about projected performance for 2024.</p> <div class=highlight><pre><span></span><code><span class=n>output</span> <span class=o>=</span> <span class=n>qa</span><span class=o>.</span><span class=n>run</span><span class=p>({</span><span class=s1>&#39;question&#39;</span><span class=p>:</span> <span class=s1>&#39;How did it do in 2022?&#39;</span> <span class=p>})</span>
<span class=n>display</span><span class=p>(</span><span class=n>Markdown</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</code></pre></div> <p>According to the context, it states that Amazon's Advertising business grew from $38 billion in 2022 to $47 billion in 2023.</p> <p>So the context directly provides that in 2022, Amazon's Advertising business had revenue of $38 billion.</p> <hr> <h2 id=next-steps>Next steps</h2> <p>Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs.</p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Was this page helpful? </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="This page was helpful" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5 9v12H1V9zm4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21zm0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03z"/></svg> </button> <button class="md-feedback__icon md-icon" type=submit title="This page could be improved" data-md-value=0> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 15V3h4v12zM15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3zm0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97z"/></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> Thanks for your feedback! </div> <div data-md-value=0 hidden> Thanks for your feedback! Help us improve this page by <a href="https://github.com/aws-samples/amazon-bedrock-samples/issues/new?title=[Online Feedback]: Short-Summary-of-Issue&body=Page URL: /workshop/open-source-l200/DO_NOT_USE_04_retrieval_based_chat/" target=_blank rel=noopener>creating an issue</a>. </div> </div> </div> </fieldset> </form> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </div> </div> <div class=md-social> <a href=https://github.com/aws-samples/amazon-bedrock-samples/tree/main target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["tags", "toc.integrate", "content.code.copy", "content.code.select", "content.code.annotate", "navigation.footer", "search.highlight", "search.suggest"], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "tags": {"Compatibility": "compat"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.88dd0f4e.min.js></script> <script src=../../../javascript/feedback.js></script> </body> </html>